---
title: 'SGC_3A: The Insight Hypothesis'
subtitle: 'Response Rescoring'
author: 'Amy Rae Fox'
always_allow_html: true  
header-includes:
   - \usepackage{amsmath}
output:
  html_document:
    theme: yeti
    code_folding: hide
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document: 
    toc: true
    toc_depth: 3
    latex_engine: xelatex
font-family: "DejaVu Sans"
mainfont: "DejaVu Sans"
---

\newpage  

```{r SETUP, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE) 

#IMPORT LIBRARIES
library(tidyverse) #ALL THE THINGS
library(kableExtra) #printing tables 

```

_The purpose of this notebook is to **rescore** the response data for the SGC_3A study.  This is required because the question type on the graph comprehension task used a 'Multiple Answer Multiple Choice' design (MCMA), also known as 'Multiple True False' (MTF)._  

```{r IMPORT-ITEMS}

#set datafiles
fall17 <- "data/fall17_sgc3a_blocks.csv"
spring18 <- "data/spring18_sgc3a_blocks.csv"
fall21 <- "data/fall21_sgc3a_blocks.csv"
winter22 <- "data/winter22_sgc3a_items.csv"

#read datafiles, set mode and term
df_items_fall17 <- read.csv(fall17) %>% mutate(mode = "lab-synch", term = "fall17")
df_items_spring18 <- read.csv(spring18) %>% mutate(mode = "lab-synch", term = "spring18")
df_items_fall21 <- read.csv(fall21) %>% mutate(mode = "online-asynch", term = "fall21")
df_items_winter22 <- read.csv(winter22) %>% mutate(mode = "online-asynch", term = "winter22")

#get relations mapping encoded in winter22 data files
map_relations <- df_items_winter22 %>% group_by(q) %>% select(q,relation) %>% unique()
  
#reduce extraneous columns from data collected using old webapp
df_items <- rbind(df_items_fall17, df_items_spring18, df_items_fall21) %>% 
  mutate(rt_s = rt / 1000) %>% 
  select(subject, condition, term, mode, question, q, answer, correct, rt_s)
  
#reduce extraneous columns from data collected using new webapp
df_items_winter22 <- df_items_winter22 %>% 
  select(subject, condition, term, mode, question, q, answer, correct, rt_s)

#combine dataframes from old and new webapps
df_items <- rbind(df_items, df_items_winter22) %>% 
  mutate(
    subject = as.factor(subject),
    condition = as.factor(condition),
    term = as.factor(term),
    mode = as.factor(mode),
    question = as.factor(question),
    q = as.factor(q),
    answer = as.factor(answer)
  )

#separate 'free response' items into independent dataframe 
df_freeresponse <- df_items %>% filter(q == 16)
df_items <- df_items %>% filter (q != 16)

#clean up
rm(df_items_fall17,df_items_fall21, df_items_spring18, df_items_winter22, map_relations)
```

# INTRODUCTION

The _graph comprehension task_ of study SGC 3A presents readers with a graph, a question, and a series of checkboxes. Participants are instructed to use the graph to answer the question, and respond by selecting all the checkboxes that apply, where each checkbox corresponds to a datapoint in the graph.

![**Figure 1. Sample Graph Comprehension (Question # 6)**](static/sample_graphComprehensionTask.png)

In the psychological and education literatures on Tests \& Measures, the format of this type of question is referred to as _Multiple Choice Multiple Answer_ (MCMA) or _Multiple Answer Multiple Choice_ (MAMC).  It has a number of properties that make it different from traditional _Single Answer Multiple Choice_ (SAMC) questions, where the respondent marks a single response from a number of options In particular, there are a number of very different ways that MAMC questions can be _scored_.  

Traditionally in SAMC questions, one point is given for selecting the option designated as correct, and zero points given for marking any of the alternative (i.e. distractor) options.  In MAMC however, it is not obvious how to allocate points when the respondent marks a true-correct option (i.e. options that _should_ be selected), as well as one or more false-correct options (i.e. options that _should not_ be selected).  Should partial credit be awarded? 

Schmidt et. al (2021) performed a systematic literature review of publications proposing MAMC (or equivalent) scoring schemes, ultimately synthesizing over 80sources into 27 distinct scoring approaches. Upon reviewing the benefits of tradeoffs of each approach, for this study we choose utilize two of the schemes: **dichotomous scoring** (Schmidt. et. al scheme #1), and **partial scoring $[-1/n,0, +1/n]$** (Schmidt et. al. scheme #17). 

## MAMC to MTF response encoding

First, we acknowledge that the question type evaluated by Schmidt et. al. (2021) is referred to as _Multiple True-False_ (MTF), a variant of MAMC where respondents are presented with a question (stem) and series of response options with True/False (e.g. radio buttons) for each.  Depending on the implementation of the underlying instrument, it may or may not be possible for respondents to _not respond_ to a particular option (i.e. leave the item 'blank').  Although MTF questions have a different underlying implementation (and potentially different psychometric properties) they are identical in their mathematical properties; that is, responses to a MAMC question of 'select all that apply' can be coded as a series of T/F responses to each response option  

![**Figure 2. SAMC (vs) MAMC (vs) MTF **](static/MAMC-MTF.png)

In this example (Figure 2), we see an example of a question with four response options ($n=4$) in each question type.  In the **SAMC** approach (at left), there are four possible responses, given explictly by the response options (respondent can select only one) ($po_{responses} = n$). With only four options, we cannot entirely discriminate between all of the response variants we might be interested in, and must always choose an 'ideal subset' of possible distractors to present as response options.  In the MAMC (middle) and MTF (at right), the _same number of response options_ ($n=4$) yield a far greater range of responses ($po_{responses} = 2^{n}$).  We can also see the equivalence between a MAMC and MTF format questions with the same response options. Options the respondent _selects_ in MAMC are can be coded as **T**, and options they leave _unselected_ can be coded as **F**.  Thus, for response options (ABCD), a response of [AB] can be encoded as [TTFF].

_In our analysis, we will transform the MAMC response string recorded for the participant (given in column TODO), to an MTF encoding._

## Dichotomous Scoring

**Dichotomous Scoring** is the strictest scoring scheme, where a response only receives points if it is _exactly_ correct, meaning the respondent includes _only true-correct_ options, and does select any additional (i.e. true_incorrect) options that should not be selected. This is also known as _all or nothing scoring_, and importantly, it ignores any partial knowledge that a participant may be expressing through their choice of options. They may select some but not all of the true-correct options, and one or more but not all of the false-correct items, but receive the same score as a respondent selects none of the true-correct options, or all of the false-correct options. In this sense, dichotomous scoring tells us only about perfect knowledge, and ignores any indication of partial knowledge the respondent may be indicating. 

**In Dichotomous Scoring**  
- question score is either 0 or 1  
- full credit is only given if all responses are correct; otherwise; no credit  
- does not account for _partial knowledge_. 
- with increasing number of response options, scoring becomes stricter as each statement must be marked correctly. 

## Partial Scoring

**Partial Scoring** refers to a class or scoring schemes that award the respondent partial credit depending on pattern of options they select.  Schmidt et. al. identify twenty-six different partial credit scoring schemes in the literature, varying in the range of possible scores, and the relative weighting of incorrectly selected (vs) incorrectly unselected options. A particularly effective (and elegant!) approach to partial scoring is referred to as the $[-1/n, +1/n]$ approach (Schmidt. et. al 2021 #17).  This approach is particularly appealing in the context of SGC3A, because it: (1) takes into account all information provided by the respondent: the pattern of what the select, and choose not to select; and (2) weights an unsure/blank/non-response as _superior_ to an incorrect response. This scoring is more consistent with the motivating theory that Triangular Graph readers start out with an incorrect (i.e. orthongal, cartesian) interpretation of the coordinate system, and transition to a correct (i.e. triangular) interpretation. But the first step in making this transition is realizing the cartesian interpration _is incorrect_, which may yield blank responses where the respondent is essentially saying, 'there is no correct answer to this question'.  Schmidt. et. al (2021) describe the $\text{Partial}_{[-1/n, +1/n]}$ scoring scheme as the _only_ scoring method (of the 27 described) where respondents' scoring results can be interpreted as a percentage of their true knowledge. 

**In Partial Scoring $[-1/n, +1/n]$:**   
- Scores range from [-1, +1]  
- One point is awarded if all options are _correct_
- One point point is subtracted if all options are _incorrect_.   
- Intermediate results are credited as fractions accordingly ($+1/n$ for each correct, $-1/n$ for each incorrect)  
- This results in _at chance performance_ (i.e. half of the given options marked correctly), being awarded 0 points are awarded  
- No response (blanks) yields 0 points

## Scoring Algorithms and Properties

For each scoring scheme we can define an algorithm and examine the scheme's statistical properties. 

In the sections that follow, we use the terminology:  

 $f =$ resulting score  
 $n =$ number of response options  
 $i =$ number of _correct responses_ by respondent $(0 ≤ i ≤ n)$  
        (items correctly selected, or correctly not selected)  


The algorithm for **dichotomous scoring** is given by:  

$$ 
\begin{gather*}
f = 
\begin{cases}
  1, \text{if } i = n \\    
  0, \text{otherwise}    
\end{cases}
\end{gather*}
\text{where } 0 \le i \le n
$$
```{r}
f_dichom <- function(i, n) {
 
 if (n == 0 ) {
   "ERROR: n should not be 0"
 } 
  else if (i > n) {
   "ERROR: i CANNOT BE GREATER THAN n"
 } 
  else if(i == n){1}
  else {0}
}
```


The algorithm for **partial scoring $[-1/n, +1/n]$** is given by:  

$$
\begin{align}
f &= (1/n * i) - (1/n * (n-i)) \\
 &= (2i - n)/{n} 
\tag{2}
\end{align}
$$

```{r}
f_partial <- function(i, n) {
 
 if (n == 0 ) {
   "ERROR: n should not be 0"
 } 
  else if (i > n) {
   "ERROR: i CANNOT BE GREATER THAN n"
 } 
  else {
    (2*i - n) / n
  }
}
```

The **Expected Chance Score** of Multiple True-False (MTF) questions is calculated by the sum of the product of the binomial ($p = 0.5$) probabilities of each statement marked _correctly_ with the corresponding score for that number of correctly marked statements. ( Schmidt et. al. 2021, Albanese \& Sabers (1988)). Importantly, $i$ is _not_ the number of selected _options_, but rather the number of _correctly indicated_ items, where [T = correctly selected || correctly not selected] and [F = incorrectly selected || incorrectly not selected]_.  The $f_i$ refers to the 


$$
\begin{align}
f_{chance} &= \sum_{i = 0}^{n} \binom{n}{i} * (0.5)^i * (1-0.5)^{n-i} * f_i \\  
&= \sum_{i = 0}^{n} \binom{n}{i} * (0.5)^n * f_i \\
\tag{3}
\end{align}
$$

_where_ $n =$ number of options in MTF question (data points that can be selected)  
$i =$ number of options marked correctly  
$f_i =$ score for $i$ options marked correctly


```{r}
f_chance <- function(n, scheme) {
 
  if (n < 0) {"ERROR: n must be greater than 0"} 
  if (!scheme %in% c("d","p")) {"ERROR: unknown scoring scheme"}
  else {
    #sum from i=0 to i=n
    s = 0; #starting value
    for (x in 0:n) {
      
      #binomial coefficient n choose x
      binom = choose(n,x)
      
      #binomial probability of n statements marked correctly
      bprob = 0.5^n

      #score for x correctly marked items
      if (scheme == "d"){
        f = f_dichom(x,n)
      } else if (scheme == "p") {
        f = f_partial(x,n)
      }
      
      chance_at_x = (binom * bprob*f)
      s = s + chance_at_x
    }
  }
  s
}
```

### Comparison of Scoring Schemes 

```{r}
title <- "Properties of each scoring scheme for question with $n=4$ response options"
schemes <- c("Dichotomous", " Partial$_{[-1/n, +1/n]}$")
f_0 <- c(f_dichom(0,4), f_partial(0,4))
f_1 <- c(f_dichom(1,4), f_partial(1,4))
f_2 <- c(f_dichom(2,4), f_partial(2,4))
f_3 <- c(f_dichom(3,4), f_partial(3,4))
f_4 <- c(f_dichom(4,4), f_partial(4,4))
ecs <- c(f_chance(4,"d"),f_chance(4,"p"))

names = c("Scoring Scheme",
              "$f_0$",
              "$f_1$",
              "$f_2$",
              "$f_3$",
              "$f_4$",
              "Expected Score at Chance")

dt <- cbind(schemes,f_0,f_1,f_2,f_3,f_4,ecs)

kbl(dt, col.names = names, caption = title)%>%
  kable_classic() %>%
  add_header_above(c(" " = 1, "score for $i$ of 4 correctly marked options" = 5, " "=1))

```
```{r}
title <- "Properties of each scoring scheme for question with $n=15$ response options (SGC3A Q1 - Q5)"
schemes <- c("Dichotomous", " Partial$_{[-1/n, +1/n]}$")
f_0 <- c(f_dichom(0,15), round(f_partial(0,15),2))
f_1 <- c(f_dichom(1,15), round(f_partial(1,15),2))
f_2 <- c(f_dichom(2,15), round(f_partial(2,15),2))
f_3 <- c(f_dichom(3,15), round(f_partial(3,15),2))
f_4 <- c(f_dichom(4,15), round(f_partial(4,15),2))
f_e <- c("...","...")
f_15 <- c(f_dichom(15,15), round(f_partial(15,15),2))
ecs <- c(f_chance(15,"d"),f_chance(15,"p"))

names = c("Scoring Scheme",
              "$f_0$",
              "$f_1$",
              "$f_2$",
              "$f_3$",
              "$f_4$",
              "$...$",
              "$f_{15}$",
              "Expected Score at Chance")

dt <- cbind(schemes,f_0,f_1,f_2,f_3,f_4,f_e,f_15,ecs)

kbl(dt, col.names = names, caption = title)%>%
  kable_classic() %>%
  add_header_above(c(" " = 1, "score for $i$ of 15 correctly marked options" = 7, " "=1))
```
```{r}
title <- "Properties of each scoring scheme for question with $n=18$ response options (SGC3A Q6 - Q15)"
schemes <- c("Dichotomous", " Partial$_{[-1/n, +1/n]}$")
f_0 <- c(f_dichom(0,18), round(f_partial(0,18),2))
f_1 <- c(f_dichom(1,18), round(f_partial(1,18),2))
f_2 <- c(f_dichom(2,18), round(f_partial(2,18),2))
f_3 <- c(f_dichom(3,18), round(f_partial(3,18),2))
f_4 <- c(f_dichom(4,18), round(f_partial(4,18),2))
f_e <- c("...","...")
f_18 <- c(f_dichom(18,18), round(f_partial(18,18),2))
ecs <- c(f_chance(18,"d"),f_chance(18,"p"))

names = c("Scoring Scheme",
              "$f_0$",
              "$f_1$",
              "$f_2$",
              "$f_3$",
              "$f_4$",
              "$...$",
              "$f_{18}$",
              "Expected Score at Chance")

dt <- cbind(schemes,f_0,f_1,f_2,f_3,f_4,f_e,f_18,ecs)

kbl(dt, col.names = names, caption = title)%>%
  kable_classic() %>%
  add_header_above(c(" " = 1, "score for $i$ of 18 correctly marked options" = 7, " "=1))
```
# RESCORING 

In SGC_3A we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus).  The _graph comprehension task_ asks them to select the data points in the graph that meet the criteria posed in the question. This is known as a _first order graph reading_ (i.e. extracting values from a graph).  

To assess a particpant's performance, for each question (q=15) we will calculate the following scores: 

_An overall, strict score:_  
1. **Absolute Score** : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)  

_Subscores, for each observed graph interpretation_  
2. **Triangular Score ** :  using partial scoring [+1/n,-1/n] referencing true (Triangular) answer.  
3. **Orthongal Score ** :  using partial scoring [+1/n,-1/n] referencing (incorrect Orthogonal) answer.  
**TODO: should I subscore for Tversky & Satisficing as well?**

_A single composite score scaled to reward correct and penalize incorrect interpretation_  
4. **Discriminant Score**: using partial scoring [+1/n,-1/n] with theoretically motivated distinction of correct/incorrect/partial correct responses


## Encode MAMC as MTF

To calculate partial scores, first we need re-encode participant responses  which are currently captured in the `answer` column of the `df_items` dataframe. In the present encoding, the letter corresponding to each response item (corresponding to a data point in the stimulus graph) the subject selected on the task interface, is concatenated and stored in `answer`.

For example, if the respondent selected data points A and B, `answer = AB`. We need to transform this into a single column for each possible response option, encoding whether or not the option was selected `A = 1, B = 1, C = 0 ...`

```{r ENCODE-MTF}


#split items into 3 dataframes, one for each phase + nondiscriminanting questions
#in each new dataframe, split the answer column into binary for each reponse option

#SCAFFOLD PHASE ITEMS
#setup responses data frame for scaffolding phase questions(Q1-5)
item_responses_scaffold <- df_items %>% 
  #filter only q < 6, the 'scaffold' items
  filter(as.integer(q)<6) %>% 
  #select only the cols we need 
  select(subject,condition, correct, q, answer) %>% 
  #rename 'answers' to 'response' and remove commas
  mutate(response = str_remove_all(as.character(answer), ",")) %>% 
  #split response to TF columns
  mutate(
    r_A = as.integer(str_detect(response,"A")), #is there a A?
    r_X = as.integer(str_detect(response,"X")), #is there a X?
    r_C = as.integer(str_detect(response,"C")), #is there a C?
    r_O = as.integer(str_detect(response,"O")), #is there a O?
    r_I = as.integer(str_detect(response,"I")), #is there a I?
    r_J = as.integer(str_detect(response,"J")), #is there a J?
    r_H = as.integer(str_detect(response,"H")), #is there a H?
    r_F = as.integer(str_detect(response,"F")), #is there a F?
    r_K = as.integer(str_detect(response,"K")), #is there a K?
    r_D = as.integer(str_detect(response,"D")), #is there a D?
    r_U = as.integer(str_detect(response,"U")), #is there a U?
    r_E = as.integer(str_detect(response,"E")), #is there a E?
    r_G = as.integer(str_detect(response,"G")), #is there a G?
    r_B = as.integer(str_detect(response,"B")), #is there a B?
    r_Z = as.integer(str_detect(response,"Z"))  #is there a Z?
) %>% select (-answer)

#TEST PHASE ITEMS
#setup responses data frame for scaffolding phase questions(Q6-16)
item_responses_test <- df_items %>% 
  #filter only q >5, the 'test' items, discluding Q6 and Q9
  filter(as.integer(q) >5 & !as.integer(q) %in% c(6,9)) %>%  
  #select only the cols we need 
  select(subject,condition, correct, q, answer) %>% 
  #rename 'answers' to 'response' and remove commas
  mutate(response = str_remove_all(as.character(answer), ",")) %>% 
  #split response to TF columns
  mutate(
    r_A = as.integer(str_detect(response,"A")), #is there a A?
    r_B = as.integer(str_detect(response,"B")), #is there a B?
    r_C = as.integer(str_detect(response,"C")), #is there a C?
    r_D = as.integer(str_detect(response,"D")), #is there a D?
    r_E = as.integer(str_detect(response,"E")), #is there a E?
    r_F = as.integer(str_detect(response,"F")), #is there a F?
    r_G = as.integer(str_detect(response,"G")), #is there a G?
    r_H = as.integer(str_detect(response,"H")), #is there a H?
    r_I = as.integer(str_detect(response,"I")), #is there a I?
    r_J = as.integer(str_detect(response,"J")), #is there a J?
    r_K = as.integer(str_detect(response,"K")), #is there a K?
    r_L = as.integer(str_detect(response,"L")), #is there a L?
    r_M = as.integer(str_detect(response,"M")), #is there a M?
    r_N = as.integer(str_detect(response,"N")), #is there a N?
    r_O = as.integer(str_detect(response,"O")), #is there a O?
    r_P = as.integer(str_detect(response,"P")), #is there a P?
    r_Z = as.integer(str_detect(response,"Z")), #is there a Z?
    r_X = as.integer(str_detect(response,"X"))  #is there a X?
) %>% select (-answer)

#NONDISCRIMINANT ITEMS
#setup responses data frame for questions where answers are the same across strategies (Q6, Q9)
item_responses_indiscriminant <- df_items %>% 
  #filter only q == 6 or 9, the 'indiscriminant' items
  filter(as.integer(q) %in% c(6,9)) %>% 
  #select only the cols we need 
  select(subject,condition, correct, q, answer) %>% 
  #rename 'answers' to 'response' and remove commas
  mutate(response = str_remove_all(as.character(answer), ",")) %>% 
  #split response to TF columns
  mutate(
    r_A = as.integer(str_detect(response,"A")), #is there a A?
    r_B = as.integer(str_detect(response,"B")), #is there a B?
    r_C = as.integer(str_detect(response,"C")), #is there a C?
    r_D = as.integer(str_detect(response,"D")), #is there a D?
    r_E = as.integer(str_detect(response,"E")), #is there a E?
    r_F = as.integer(str_detect(response,"F")), #is there a F?
    r_G = as.integer(str_detect(response,"G")), #is there a G?
    r_H = as.integer(str_detect(response,"H")), #is there a H?
    r_I = as.integer(str_detect(response,"I")), #is there a I?
    r_J = as.integer(str_detect(response,"J")), #is there a J?
    r_K = as.integer(str_detect(response,"K")), #is there a K?
    r_L = as.integer(str_detect(response,"L")), #is there a L?
    r_M = as.integer(str_detect(response,"M")), #is there a M?
    r_N = as.integer(str_detect(response,"N")), #is there a N?
    r_O = as.integer(str_detect(response,"O")), #is there a O?
    r_P = as.integer(str_detect(response,"P")), #is there a P?
    r_Z = as.integer(str_detect(response,"Z")), #is there a Z?
    r_X = as.integer(str_detect(response,"X"))  #is there a X?
) %>% select (-answer)

#veryify that each item made it into a subdataframe
nrow(df_items) == (
  nrow(item_responses_indiscriminant) + 
  nrow(item_responses_scaffold) + 
  nrow(item_responses_test) )
```
Now we have three dataframes (one for each group of questions: scaffold phase, test phase, nondiscriminant) with subjects' response encoded as a series of T/F [1,0] states across all response options (represented as colums prefaced with `r_`)

## Create Answer Keys

Next, we read the answer keys for the question sets. Note that there is an answer key unique to each experimental condition, because the experimental manipulation (impasse vs. control) is established by changing the pattern of the underlying dataset, which in turn yields different 'correct' answers. The divergence in answers across conditions only holds for the first five questions (the scaffold manipulation), while the following 10 questions are displayed with the same dataset (and thus have the same answers, regardless of condition).

```{r ANSWER-KEYS}
key_111 <- read_csv('static/SGC3A_111_key.csv', show_col_types = FALSE)
key_121 <- read_csv('static/SGC3A_121_key.csv', show_col_types = FALSE)
```

Next, we split the answer key encoding into MTF encoding, just as we did for the response data. 

```{r ENCODE-KEYS-MTF}

#SCAFFOLD KEY CONDITION 111
key_scaffolded_c111 <- key_111 %>% 
  #filter only q < 6, the 'scaffold' items
  filter(Q<6) %>% 
  #create "triangle correct" columns
  mutate(
    tri_A = as.integer(str_detect(TRIANGULAR,"A")), #is there a A in TRIANGULAR?
    tri_X = as.integer(str_detect(TRIANGULAR,"X")), #is there a X in TRIANGULAR?
    tri_C = as.integer(str_detect(TRIANGULAR,"C")), #is there a C in TRIANGULAR?
    tri_O = as.integer(str_detect(TRIANGULAR,"O")), #is there a O in TRIANGULAR?
    tri_I = as.integer(str_detect(TRIANGULAR,"I")), #is there a I in TRIANGULAR?
    tri_J = as.integer(str_detect(TRIANGULAR,"J")), #is there a J in TRIANGULAR?
    tri_H = as.integer(str_detect(TRIANGULAR,"H")), #is there a H in TRIANGULAR?
    tri_F = as.integer(str_detect(TRIANGULAR,"F")), #is there a F in TRIANGULAR?
    tri_K = as.integer(str_detect(TRIANGULAR,"K")), #is there a K in TRIANGULAR?
    tri_D = as.integer(str_detect(TRIANGULAR,"D")), #is there a D in TRIANGULAR?
    tri_U = as.integer(str_detect(TRIANGULAR,"U")), #is there a U in TRIANGULAR?
    tri_E = as.integer(str_detect(TRIANGULAR,"E")), #is there a E in TRIANGULAR?
    tri_G = as.integer(str_detect(TRIANGULAR,"G")), #is there a G in TRIANGULAR?
    tri_B = as.integer(str_detect(TRIANGULAR,"B")), #is there a B in TRIANGULAR?
    tri_Z = as.integer(str_detect(TRIANGULAR,"Z"))  #is there a Z in TRIANGULAR?
   ) %>% 
  #create "orthogonal correct" columns
  mutate(
    orth_A = as.integer(str_detect(ORTHOGONAL,"A")), #is there a A in ORTHOGONAL?
    orth_X = as.integer(str_detect(ORTHOGONAL,"X")), #is there a X in ORTHOGONAL?
    orth_C = as.integer(str_detect(ORTHOGONAL,"C")), #is there a C in ORTHOGONAL?
    orth_O = as.integer(str_detect(ORTHOGONAL,"O")), #is there a O in ORTHOGONAL?
    orth_I = as.integer(str_detect(ORTHOGONAL,"I")), #is there a I in ORTHOGONAL?
    orth_J = as.integer(str_detect(ORTHOGONAL,"J")), #is there a J in ORTHOGONAL?
    orth_H = as.integer(str_detect(ORTHOGONAL,"H")), #is there a H in ORTHOGONAL?
    orth_F = as.integer(str_detect(ORTHOGONAL,"F")), #is there a F in ORTHOGONAL?
    orth_K = as.integer(str_detect(ORTHOGONAL,"K")), #is there a K in ORTHOGONAL?
    orth_D = as.integer(str_detect(ORTHOGONAL,"D")), #is there a D in ORTHOGONAL?
    orth_U = as.integer(str_detect(ORTHOGONAL,"U")), #is there a U in ORTHOGONAL?
    orth_E = as.integer(str_detect(ORTHOGONAL,"E")), #is there a E in ORTHOGONAL?
    orth_G = as.integer(str_detect(ORTHOGONAL,"G")), #is there a G in ORTHOGONAL?
    orth_B = as.integer(str_detect(ORTHOGONAL,"B")), #is there a B in ORTHOGONAL?
    orth_Z = as.integer(str_detect(ORTHOGONAL,"Z"))  #is there a Z in ORTHOGONAL?
   ) 



# key_scaffolded_c121
# key_test
# key_nondiscriminant

#TODO, CREATE REST OF THEM
```

Next, we calculate the $i$, number of correctly indicated options, based on the answer key for each question. 


```{r RESCORE-SCAFFOLDED}

#------------------------------------------------------------------
#calculate tri_i: number of triangularly-correct indicated options
#NOTE: THIS IS SUPER BRITTLE 
#relies on fact that columns were ordered the same across the dataframes!
#should refactor in less imperative mode (more R like!)
#------------------------------------------------------------------
tri_i <- function(responses,key){
  print(responses)
  print(key)
  assessment <- responses == key
  sum(assessment)
}


test <- item_responses_scaffold %>% filter(condition==111) 

#rescore scaffolded items in condition 111
for (x in 1:nrow(test)) {
  #get question number
  q = test[x,"q"]
  #get subjects response vector for this question
  response <- as_tibble(test[x,] %>% select(starts_with("r_")))
  #get key vector for this question
  key = as_tibble(key_scaffolded_c111 %>% filter(Q==q) %>% select(starts_with("tri_")))
  #calculate number of triangular-correct-options
  test[x,"tri_i"] <- tri_i(response,key)
}

```






# RESOURCES

## References

Schmidt, D., Raupach, T., Wiegand, A., Herrmann, M., & Kanzow, P. (2021). Relation between examinees’ true knowledge and examination scores: Systematic review and exemplary calculations on Multiple-True-False items. Educational Research Review, 34, 100409. https://doi.org/10.1016/j.edurev.2021.100409

Albanese, M. A., & Sabers, D. L. (1988). Multiple True-False Items: A Study of Inter-item Correlations, Scoring Alternatives, and Reliability Estimation. Journal of Educational Measurement, 25(2), 111–123. https://doi.org/10.1111/j.1745-3984.1988.tb00296.x

## Resources

_on kable tables_  
https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html