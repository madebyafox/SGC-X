---
title: "SGC3_A"
output: 
  # rmdformats::robobook:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
---

*The purpose of this notebook is to analyze data collected for study SGC-3: The Insight Hypothesis *

```{r SETUP, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#IMPORT LIBRARIES
library(rmdformats)
library(dplyr) #tidyverse data handling
# library(tables) # pretty tables
library(pastecs) #stat.desc
library(mosaic) #simple descriptives [favstats]

library(ggplot2) #graphs
library(car) #ANOVA, qqplot
library(effectsize) #effect size
library(pwr) #power analysis

```
  
```{r IMPORT-DATA}  
#IMPORT DATA from fall and spring files
fall_participants <- "data/fall_sgc3a_participants.csv"
spring_partcipants <- "data/spring_sgc3a_participants.csv"

df_fall <- read.csv(fall_participants)
df_spring <- read.csv(spring_partcipants)

#Create combined data frame
df_subjects <- rbind(df_fall, df_spring)
df_subjects$tt_min <- df_subjects$triangular_time / 1000 / 60

#Create factors 
df_subjects <- df_subjects %>% mutate(
  subject = as.factor(subject),
  session = as.factor(session),
  term = as.factor(term),
  condition = as.factor(condition),
  explicit = as.factor(explicit),
  impasse = as.factor(impasse),
  axis = as.factor(axis)
)

```

# INTRODUCTION
Data were collected across two terms: Fall 2017, and Spring 2018. Small changes were made to the experimental platform in Spring 2018 to change the size of multiple-choice input buttons, and to add an additional free-response question following the main task block. 

<span style="color: red;">TODO: Add condition images.</span>.

```{r TERMS}
#MANUALLY INSPECT TERMS
df_subjects %>% group_by(term) %>% 
  summarize(n=n())
```

SGC3-A is a 2-condition (111-control VS 121-impasse) between-subjects experiment with `r nrow(df_subjects)` participants. 

```{r CONDITIONS}

#The data collection framework was designed such that  that stimuli the participant experienced were triggered based on the input of a particular 'condition code' by the participant. This allowed us to collect data for a number of studies simultaneously in the same data collection session (i.e. multiple students in the computer lab at the same time).  Thus, the **final_participants.json** file contains data for a number of studies, identified by parsing the **condition** variable.

#First Digit    | explicit scaffolding
# ------------- |-------------
# 1      | control (no-scaffold)
# 2      | text/image (static)
# 3      | interactive
#
#Second Digit    | implicit scaffolding
# ------------- |-------------
#1      | control (no-scaffold)
#2      | impasse (no orthogonal answer)
#
#Third Digit    | grid format
#------------- |-------------
#1 | full orthogonal
#2 | partial orthogonal
#3 | diagonal

#MANUALLY INSPECT conditions
df_subjects %>% group_by(condition) %>% 
  summarize(n=n())
```

# DESCRIPTIVES

## Participant Demographics 

```{r PARTICIPANTS}
#Describe participants
favstats(~age, data = df_subjects)
```

`r nrow(df_subjects)` participants (`r round(sum(df_subjects$sex == "Female")/nrow(df_subjects),1) * 100` % female ) undergraduate STEM majors at a public American University participated in exchange for course credit (age: `r min(df_subjects$age)` - `r max(df_subjects$age)` years).  Participants were randomly assigned to one of two experimental groups. 


## Response Accuracy

Response accuracy refers to how many questions the subject answers with a correct-triangular interpretation.  

```{r ACCURACY}

#DESCRIBE distribution of triangular-correct scores
score.stats <- favstats(df_subjects$triangular_score)
score.stats
stat.desc(df_subjects$triangular_score)

#VISUALIZE distribution of response accuracy
gf_dhistogram(~triangular_score, data = df_subjects) %>%
  gf_vline(xintercept = score.stats$mean, color = "red")
```
<br>
Accuracy scores (n = `r nrow(df_subjects)`) range from `r round(score.stats$min,2)` to `r round(score.stats$max,2)` with a mean score of (M = `r round(score.stats$mean,2)`, SD = `r round(score.stats$sd,2)`). 

```{r ACCURACY-NORMAL}
qqPlot(df_subjects$triangular_score)
```


## Response Latency

### TODO: Investigate response times
<span style="color: red;">TODO: Investigate super high and super low response times.</span>.

```{r LATENCY}

#DESCRIBE distribution of triangular-correct scores
time.stats <- favstats(df_subjects$tt_min)
time.stats
stat.desc(df_subjects$tt_min)

#VISUALIZE distribution of response accuracy
gf_dhistogram(~tt_min, data = df_subjects) %>%
  gf_vline(xintercept = time.stats$mean, color = "red")
```
Response latency (for test blocks) range from `r round(time.stats$min,2)` minutes to `r round(time.stats$max,2)` minutes with a mean time of (M = `r round(time.stats$mean,2)`, SD = `r round(time.stats$sd,2)`). 

```{r LATENCY-NORMAL}
qqPlot(df_subjects$tt_min)
```




# HYPOTHESIS TESTING

## Response Accuracy by Condition
The experimental hypothesis (H1) is that structuring the data to pose an impasse (condition 121) will produce significantly better performance than non-impasse (condition 111). The null hypothesis (H0) is that there will be no difference in performance between conditions.  A linear model predicting triangular_score from condition indicates a statistically significant difference between conditions, explaning 7% of the model variance. 

### [EXPLORE]

```{r ACCURACY-BY-CONDITION}

#DESCRIBE scores by condition
score.cond.stats <- favstats(triangular_score ~ condition, data = df_subjects)
score.cond.stats

#VISUALIZE scores by condition
gf_dhistogram( ~triangular_score, fill= ~condition, data = df_subjects) %>% 
  gf_facet_grid(condition~.) %>% 
  gf_vline(xintercept = ~mean, data = score.cond.stats, color = "blue") 

#VISUALIZE scores by condition
gf_boxplot(triangular_score ~ condition, data=df_subjects) %>% 
  gf_jitter(color=~condition, alpha=0.5)

```
Accuracy scores (n = `r nrow(df_subjects)`) range from `r round(score.stats$min,2)` to `r round(score.stats$max,2)` with a mean score of (M = `r round(score.stats$mean,2)`, SD = `r round(score.stats$sd,2)`). On average, participants in the impasse group had higher scores (M = `r round(score.cond.stats[2,]$mean,2)` SD = `r round(score.cond.stats[2,]$sd,2)`) than those in #the non-impasse control group (M = `r round(score.cond.stats[1,]$mean,2)`, SD = `r round(score.cond.stats[1,]$sd,2)`)** 


### [MODEL]

```{r MODEL-SCORE}

#MODEL triangular score by condition with a linear model
m1 = lm(triangular_score ~ condition, data = df_subjects)
m1

#the first co-efficient represents the model estimate for the first group, the second, the difference for the second group

# partition variance with anova
# anova(m1)
# supernova(m1) #runs but doesn't knit?

#calculate effect size
eta_squared(m1, partial = FALSE)

```

A linear model predicting triangular score accuracy by condition explains approximately 7% of the variance in triangular score F(1,124) = 9.32, p < 0.05)

# POST-HOC EXPLORATION 

## 1. Power Analysis
What is the appropriate sample size to detect a moderate-sized effect (f = 0.25) at a 0.05 alpha level? 

```{r POWER}

#K = 2 groups, f = 0.25 is moderate effect size
pwr.anova.test(k=2,f=0.25 ,sig.level=.05, power=.8)

```
The studies should aim to have at least 60 subjects to detect a moderate sized main effect between two conditions. 

## 2. TERM-LEVEL analysis

Data from the first term only (Fall 2017) were analyzed and presented as a paper at CogSci 2019, in support of the experimental hypothesis (H1: impasse is an effective scaffold). Combined across terms, the prior analyses ALSO support the experimental hypothesis. Do the spring-only data also support the H1 hypothesis?

```{r MODELbyTerm}

#simple linear model predicting triangle score from condition for spring data only
mspring = lm(triangular_score ~ condition, data = df_subjects %>% filter(term=='spring18'))

#partition variance
anova(mspring)# supernova(m2)
confint(mspring)
```
A linear model predicting triangular_score from conditio for **only** data collected in the Spring term does **not** reveal a significant difference between conditions. However, the previous power analysis suggests that this test may be underpowered, as it includes data from only 72 subjects (total), instead of the recommended 120 (60 / group). It seems more appropriate to combine data from fall and Spring. 

## 3. Combining Fall and Spring
```{r VISbyTerm}

#VISUALIZE scores by condition
gf_dhistogram( ~triangular_score, fill= ~condition, data = df_subjects) %>%
  gf_facet_grid(condition~term) 
# 
# #VISUALIZE scores by condition
gf_boxplot(triangular_score ~ condition, data=df_subjects) %>%
  gf_facet_grid(~term) %>% 
  gf_jitter(color=~condition, alpha=0.5)
```

By visual inspection, the distribution of accuracy scores across condition appear similiar across terms. 

**It is therefore reasonable to conclude that data collected in Spring 2018 can be combined with Fall 2017.**