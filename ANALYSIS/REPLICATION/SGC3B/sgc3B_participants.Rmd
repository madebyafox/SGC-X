---
title: "SGC3_B Participants"
output: 
  # rmdformats::robobook:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
---

*The purpose of this notebook is to analyze _participant-level data_ collected for study SGC-3B: Implicit (vs) Explicit Scaffolding*


```{r SETUP, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#IMPORT LIBRARIES
library(rmdformats)
library(dplyr) #tidyverse data handling
# library(tables) # pretty tables
library(pastecs) #stat.desc
library(mosaic) #simple descriptives [favstats]

library(ggplot2) #graphs
library(car) #ANOVA, qqplot
library(effectsize)

#SET working directory
setwd("~/Sites/RESEARCH/SGCâ€”Scaffolding Graph Comprehension/SGCx/ANALYSIS/SGC3_B")
```
  
```{r IMPORT-DATA}  
#IMPORT DATA from fall and spring files
fall_participants <- "data/fall_sgc3b_participants.csv"
spring_partcipants <- "data/spring_sgc3b_participants.csv"

df_fall <- read.csv(fall_participants)
df_spring <- read.csv(spring_partcipants)

#Create combined data frame
df_subjects <- rbind(df_fall, df_spring)
df_subjects$tri_min <- df_subjects$triangular_time / 1000 / 60
df_subjects$test_min <- df_subjects$tt_t / 1000 / 60
df_subjects$learn_min <- df_subjects$ts_t / 1000 / 60

#Create factors 
df_subjects <- df_subjects %>% mutate(
  subject = as.factor(subject),
  session = as.factor(session),
  term = as.factor(term),
  condition = as.factor(condition),
  explicit = as.factor(explicit),
  impasse = as.factor(impasse),
  axis = as.factor(axis)
)

```

# INTRODUCTION

Data were collected across two terms: Fall 2017, and Spring 2018. Small changes were made to the experimental platform in Spring 2018 to change the size of multiple-choice input buttons, and to add an additional free-response question following the main task block. 

<span style="color: red;">TODO: Add condition images.</span>.

```{r TERMS}
#MANUALLY INSPECT TERMS
df_subjects %>% group_by(term) %>% 
  summarize(n=n())
```

SGC3-B is a 6-condition full factorial experiment: 3(explicit-scaffold: none, static, interactive) X 2 (insight-scaffold: none, impasse)`r nrow(df_subjects)` participants. 

```{r CONDITIONS}

#The data collection framework was designed such that  that stimuli the participant experienced were triggered based on the input of a particular 'condition code' by the participant. This allowed us to collect data for a number of studies simultaneously in the same data collection session (i.e. multiple students in the computer lab at the same time).  Thus, the **final_participants.json** file contains data for a number of studies, identified by parsing the **condition** variable.

#First Digit    | explicit scaffolding
# ------------- |-------------
# 1      | control (no-scaffold)
# 2      | text/image (static)
# 3      | interactive
#
#Second Digit    | implicit scaffolding
# ------------- |-------------
#1      | control (no-scaffold)
#2      | impasse (no orthogonal answer)
#
#Third Digit    | grid format
#------------- |-------------
#1 | full orthogonal
#2 | partial orthogonal
#3 | diagonal

#MANUALLY INSPECT conditions
df_subjects %>% group_by(condition) %>% 
  summarize(n=n())
```

<span style="color: red;">TODO: Investigate CONDITION totals. Is this right? Are there 2s and 3s missing?</span>.

# DESCRIPTIVES
The dependent measures in SGC3B are response accuracy (how many questions the subject answers with a correct-triangular interpretation), and response latency.  

## Participant Demographics 

```{r PARTICIPANTS}

#Describe participants
favstats(~age, data = df_subjects)
```

`r nrow(df_subjects)` participants (`r round(sum(df_subjects$sex == "Female")/nrow(df_subjects),1) * 100` % female ) undergraduate STEM majors at a public American University participated in exchange for course credit (age: `r min(df_subjects$age)` - `r max(df_subjects$age)` years).  Participants were randomly assigned to one of two experimental groups. 

<br>

## Response Accuracy

Response accuracy refers to how many questions the subject answers with a correct-triangular interpretation.  

```{r ACCURACY}

#DESCRIBE distribution of triangular-correct scores
score.stats <- favstats(df_subjects$triangular_score)
score.stats
stat.desc(df_subjects$triangular_score)

#VISUALIZE distribution of response accuracy
gf_dhistogram(~triangular_score, data = df_subjects) %>%
  gf_vline(xintercept = score.stats$mean, color = "red")
```

<br>
Accuracy scores (n = `r nrow(df_subjects)`) range from `r round(score.stats$min,2)` to `r round(score.stats$max,2)` with a mean score of (M = `r round(score.stats$mean,2)`, SD = `r round(score.stats$sd,2)`). 

```{r ACCURACY-NORMAL}
qqPlot(df_subjects$triangular_score)
```


## Response Latency

### TODO: Investigate response times
<span style="color: red;">TODO: Investigate super high and super low response times.</span>.

```{r LATENCY}

#DESCRIBE distribution of triangular-correct scores
time.stats <- favstats(df_subjects$tri_min)
time.stats
stat.desc(df_subjects$tri_min)

#VISUALIZE distribution of response accuracy
gf_dhistogram(~tri_min, data = df_subjects) %>%
  gf_vline(xintercept = time.stats$mean, color = "red")

shapiro.test(df_subjects$tri_min) 
```
Response latency (for test blocks) range from `r round(time.stats$min,2)` minutes to `r round(time.stats$max,2)` minutes with a mean time of (M = `r round(time.stats$mean,2)`, SD = `r round(time.stats$sd,2)`). 

```{r LATENCY-NORMAL}
qqPlot(df_subjects$tri_min)
```




# HYPOTHESIS TESTING

## Response Accuracy by Condition
The experimental hypothesis (H1) is that explicit and implicit scaffolding will combine to improve performance, but implicit performance will have a greater impact on accuracy.  The null hypothesis (H0) is that there will be no difference in performance between conditions. 

### [EXPLORE]

```{r ACCURACY-BY-CONDITION}

#DESCRIBE scores by condition
score.cond.stats <- favstats(triangular_score ~ condition, data = df_subjects)
score.cond.stats

#VISUALIZE scores by condition
gf_dhistogram( ~triangular_score, fill= ~condition, data = df_subjects) %>% 
  gf_facet_grid(condition~.) %>% 
  gf_vline(xintercept = ~mean, data = score.cond.stats, color = "blue") 

#VISUALIZE scores by condition
gf_boxplot(triangular_score ~ condition, fill= ~impasse, data=df_subjects) %>% 
  gf_jitter(color=~condition, alpha=0.5) 

```
Accuracy scores (n = `r nrow(df_subjects)`) range from `r round(score.stats$min,2)` to `r round(score.stats$max,2)` with a mean score of (M = `r round(score.stats$mean,2)`, SD = `r round(score.stats$sd,2)`). On average, participants in the impasse group had higher scores (M = `r round(score.cond.stats[2,]$mean,2)` SD = `r round(score.cond.stats[2,]$sd,2)`) than those in #the non-impasse control group (M = `r round(score.cond.stats[1,]$mean,2)`, SD = `r round(score.cond.stats[1,]$sd,2)`)** 


### [MODEL]

```{r MODEL-SCORE}

#MODEL triangular score by condition with a linear model
m1 = lm(triangular_score ~ impasse*explicit, data = df_subjects)
m1

# partition variance with anova
anova(m1)
# supernova(m1) #runs but doesn't knit?

#calculate effect size
eta_squared(m1, partial = FALSE)

```

A linear model predicting triangular score by implicit and explicit scaffold condition (including interaction) explains 38% variance in triangular score.  The model includes a significant main effect for impasse scaffold F(1,284) = 12.12, p < 0.001, a main effect for explicit scaffold F(2,284) = 55.53, p < 0.001, with no interaction. The main effect of implicit scaffold explained approximately 4% of variance in score accuracy, while explicit scaffold explained 28%. 

```{r INTERACTION-PLOT}

score_summary <- df_subjects %>% group_by(condition) %>% summarise(mean = mean(triangular_score), se = sd(triangular_score)/sqrt(nrow(df_subjects)))
score_summary$impasse <- c('none','impasse','none','impasse','none','impasse')
score_summary$explicit <- c('none','none','static','static','interactive','interactive')
score_summary$impasse <- factor(score_summary$impasse, levels = c("none", "impasse"))
score_summary$explicit <- factor(score_summary$explicit, levels = c("none", "static", "interactive"))
score_summary

ggplot(data = score_summary) +
  aes(x = impasse, color = explicit, group = explicit, y = mean) +
  stat_summary(fun.y = mean, geom = "point") +
  stat_summary(fun.y = mean, geom = "line") + 
  geom_errorbar(aes(ymin=mean+se, ymax=mean-se), width=.1)

ggplot(data = score_summary) +
  aes(x = explicit, color = impasse, group = impasse, y = mean) +
  stat_summary(fun.y = mean, geom = "point") +
  stat_summary(fun.y = mean, geom = "line") + 
  geom_errorbar(aes(ymin=mean+se, ymax=mean-se), width=.1)

```



# POST-HOC EXPLORATION 


## 1. Response Accuracy
## 1.1 Learning VS Testing 
In SGC3B, participants are provided with 15 questions. For the first 5 questions, they experience a 'scaffolded' version the stimuli (control condition = 111, impasse scaffold = 121). 

What if we **only** consider the final 10 test items? Assume students have 5 questions worth of scaffolding to figure out the graph for themselves, and we only assess accuracy for the final ten items? Is our experimental hypothesis supported?

```{r TESTING}
#DESCRIBE scores by condition
test.stats <- favstats(tt_n ~ condition, data = df_subjects)

#VISUALIZE scores by condition
gf_dhistogram( ~tt_n, fill= ~condition, data = df_subjects) %>% 
  gf_facet_grid(condition~.) %>% 
  gf_vline(xintercept = ~mean, data = test.stats, color = "blue") 

#VISUALIZE scores by condition
gf_boxplot(tt_n ~ condition, data=df_subjects) %>% 
  gf_jitter(color=~condition, alpha=0.5)

```

```{r TEST-model}

mTest = lm(tt_n ~ impasse*explicit, data = df_subjects)
summary(mTest)

anova(mTest)
# supernova(mTest)
```

A linear regression model predicting score (test blocks only) from condition explained 36% of variance, F(5,284) = 31.37, p < 0.001. A factorial ANOVA on the model reveals a main effect of implicit scaffold, F(1,284) = 9.05, p < 0.01 (explaining 3% of variance) a main effect of explicit scaffold F(2,284) = 51.06, p < 0.001 (explaining 26% of variance), but no interaction. 



## 2. Response Latency

### 2.1 Response Latency vs. Accuracy

Are response time (on the triangular blocks (test and learn)) and response accuracy correlated? <br>

```{r time-accuracy}

#VISUALIZE relationship between response time and accuracy
gf_jitter(triangular_score ~ tri_min, data = df_subjects) %>% 
  gf_lm()

#VISUALIZE relationship between response time and accuracy by condition
gf_jitter(triangular_score ~ tri_min, color = ~condition, data = df_subjects) %>% 
  gf_facet_grid(~condition) %>% 
  gf_lm()

```
<br> From visual inspection, it does not *appear* that score and time and strongly correlated. 

```{r model-time-accuracy}

#simple linear model predicting triangular score from triangular time
m1 = lm(triangular_score ~ tri_min, data = df_subjects)

summary(m1, correlation=TRUE)

cor.test(df_subjects$triangular_score, df_subjects$tri_min)
```
A linear regression indicates a significant (though small) negative correlation between score accuracy and response time r = -0.33, t(288) = -5.92, p < 0.001.  

### 2.2 Is Response Latency predictive?

Does adding response time to the model help predict score accuracy?

```{r}

#simple  model predicting triangular score from triangular time
m1 = lm(triangular_score ~ impasse*explicit, data = df_subjects)
m2 = lm(triangular_score ~ impasse*explicit+ tri_min, data = df_subjects)
summary(m1)
summary(m2)
anova(m1) # supernova(m1)
anova(m2) # supernova(m2)
```
The model adding response latency as a predictor explains 1% more variance than impasse*explicit conditions alone (39% vs 37%), with the response latency producing a signficant main effect F(1,283) = 7.31, p < 0.05, explaining 2.5% variance. 

<span style="color: red;">TODO: What is the appropriate inference to draw from this model comparison?</span>.

### 2.3. Response Latency on Learn (v) Test

```{r message=FALSE}

#DESCRIBE distribution of learning block times scores
learn.time.stats <- favstats(df_subjects$learn_min)
learn.time.stats

#VISUALIZE distribution of learning block time
gf_dhistogram(~learn_min, data = df_subjects) %>%
  gf_vline(xintercept = mean(df_subjects$learn_min), color = "red") %>% 
  gf_labs(title = "Distribution of LEARN block response time ")

```
Total learning block times ranged from `r round(learn.time.stats$min,2)` min to `r round(learn.time.stats$max, 2)`, with M = `r round(learn.time.stats$mean, 2)`, SD = `r round(learn.time.stats$sd, 2)`. 


```{r message=FALSE}

#DESCRIBE distribution of learning block times scores
test.time.stats <- favstats(df_subjects$test_min)
test.time.stats

#VISUALIZE distribution of learning block time
gf_dhistogram(~test_min, data = df_subjects) %>%
  gf_vline(xintercept = mean(df_subjects$test_min), color = "red") %>% 
  gf_labs(title = "Distribution of TEST block response time ")

```

Total testing block times ranged from `r round(test.time.stats$min,2)` min to `r round(test.time.stats$max, 2)`, with M = `r round(test.time.stats$mean, 2)`, SD = `r round(test.time.stats$sd, 2)`. 

Is time spent on learning (v) testing blocks correlated?

```{r}

#VISUALIZE test v learning block time
gf_point(test_min ~ learn_min, color = ~condition, data = df_subjects) %>% 
  gf_facet_grid(~condition) %>% 
  gf_lm()

```



```{r}

m1 <- lm( test_min ~ learn_min , data = df_subjects)
summary(m1)
anova(m1) #supernova(m1)

cor.test(df_subjects$test_min, df_subjects$learn_min)

```
A linear model predicting test block time from learning block time reveals that learning block time explains 15% variance in testing block time (F(1,288) = 50.7, p < 0.001), with an 23s increase in testing time for every 1 minute increase in learning time. The two variables have a small, significant correlation r = 0.39, t(288) = 7.12, p < 0.001, 95% CI [0.28, 0.48]. 


