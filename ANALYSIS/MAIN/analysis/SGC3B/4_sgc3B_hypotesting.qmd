---
subtitle: 'Study SGC3B | (Impasse * Explicit) Hypothesis Testing'
# YAML FOR generating modelsummary tables
# uncomment to run those  cells only 
# \usepackage{booktabs}
# \usepackage{siunitx}
# \newcolumntype{d}{S[input-symbols = ()]}
---

\newpage

# SGC3B (Impasse \* Explicit) Hypothesis Testing {#sec-SGC3B-hypotesting}

*The purpose of this notebook is test the hypotheses that determined the design of the SGC3B factorial study.*

```{r}
#| label: SETUP
#| warning : false
#| message : false


#UTILITIES
library(Hmisc) # %nin% operator
library(broom) #tidy model output
library(broom.mixed) #tidy mixed models
library(mosaic) #favstats
library(svglite) #saving plots as svg
library(distributional)

#VISUALIZATION
# library(ggpubr) #arrange plots
library(cowplot) #arrange shift function plots
library(ggformula) #easy graphs
library(scales) 
# # library(vcd) #mosaic plots
# # library(vcdExtra) #mosaic plots
library(kableExtra) #printing tables
library(sjPlot) #visualize model coefficients
library(ggdist) #uncertainty viz
library(modelr) #needed for ggdist
library(gghalves) # plots. in half
library(ggbeeswarm) # violin plot stuffs
library(statsExpressions)
library(ggstatsplot) #plots with stats
library(modelsummary) #latex tables for models!

#MODELLING
library(ARTool) #nonparametric anova
# library(rstatix) #helpful testing functions incl wilcoxon, etc
library(report) #easystats reporting
library(see) #easystats visualization
library(performance) #easystats model diagnostics
library(parameters) #easystats model summary and vis
library(insight)
# library(qqplotr) #confint on qq plot
# library(gmodels) #contingency table and CHISQR
# library(equatiomatic) #extract model equation
# library(pscl) #zeroinfl / hurdle models 
library(lme4) #mixed effects models
library(lmerTest) #for CIs in glmer
library(merTools) #predictInterval
# library(ggeffects) #visualization log regr models
#MULTINOMIAL 
library(nnet) #multinomial logistic regression [not mixed] #no p values
library(mclogit) #frequentist mixed multinomial logistic regression [mblogit] #gives p values
#BAYESIAN
library(cmdstanr) #executing stan
library(brms) #bayesian mixed multinomials [+ other bayesian reg models]
library(bayestestR) 
library(tidybayes)

library(tidyverse) #ALL THE THINGS

#OUTPUT OPTIONS
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
options(ggplot2.summarise.inform = FALSE)
options(scipen=1, digits=3)

#GRAPH THEMEING
# theme_set(theme_minimal()) 

# Custom ggplot theme to make pretty plots
# Get the font at https://fonts.google.com/specimen/Barlow+Semi+Condensed
theme_clean <- function() {
  theme_minimal(base_family = "Barlow Semi Condensed") +
    theme(panel.grid.minor = element_blank(),
          plot.title = element_text(family = "BarlowSemiCondensed-Bold"),
          axis.title = element_text(family = "BarlowSemiCondensed-Medium"),
          strip.text = element_text(family = "BarlowSemiCondensed-Bold",
                                    size = rel(1), hjust = 0),
          strip.background = element_rect(fill = "grey80", color = NA))
}

set_theme(base = theme_clean())

##IMPORT CUSTOM COLOR PALETTES
source("analysis/utils/foxy_palettes.R")

set.seed(12345)

```

```{r}
#| label: IMPORT-DATA
#| warning : false
#| message : false

#IMPORT DATA 

df_subjects <- read_rds('analysis/SGC3B/data/2-scored-data/sgc3b_scored_participants.rds') %>% 
  # filter(mode == "lab-synch") %>% #filter out online sgc3as 
  mutate(
  task_percent = DV_percent_NABS
) %>% droplevels()

#SEPARATE OUT EXTRA ONLINE FROM SGC3A
factorial <- df_subjects %>% filter(condition %nin% c(111,121))
mains <- df_subjects %>% filter(condition %in% c(111,121) & mode == "lab-synch")
df_subjects <- rbind(factorial,mains) %>% 
  mutate(
    EXPLICIT = as.factor(str_sub(condition, 1,1)),
    IMPLICIT = as.factor(str_sub(condition, 2,2)),
    EXPLICIT = recode_factor(EXPLICIT, "1" = "none", "2"="img", "3"="ixn"),
    IMPLICIT = recode_factor(IMPLICIT, "1" = "control", "2"="impasse")
  ) %>% 
  mutate( pretty_condition = recode_factor(condition, 
          "111" = "none-control", "121" =  "none-impasse",
          "211" = "img-control", "221" =  "img-impasse",
          "311" = "ixv-control", "321" =  "ixv-impasse")
  ) 

df_items <- read_rds('analysis/SGC3B/data/2-scored-data/sgc3b_scored_items.rds') %>% 
   mutate (
    q = as.factor(q), 
    subject = as.factor(subject),
    accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct"),
    # CODES TVERSKY AS TRI-LIKE
    # state = recode_factor(score_SCALED, #for ordinal
    #                      "-1" = "orth-like",
    #                      "-0.5" = "unknown",
    #                      "0" = "unknown",
    #                      "0.5" = "tri-like",
    #                      "1" = "tri-like"),
    # CODES TVERSKY AS OTHER
    state = recode_factor(score_SCALED, #for ordinal
                         "-1" = "orthogonal",
                         "-0.5" = "other",
                         "0" = "other",
                         "0.5" = "angular",
                         "1" = "triangular"),
    state = as.ordered(state),
    EXPLICIT = as.factor(str_sub(condition, 1,1)),
  IMPLICIT = as.factor(str_sub(condition, 2,2)),
  EXPLICIT = recode_factor(EXPLICIT, "1" = "none", "2"="img", "3"="ixn"),
  IMPLICIT = recode_factor(IMPLICIT, "1" = "control", "2"="impasse")) %>% 
    mutate( pretty_condition = recode_factor(condition, 
          "111" = "none-control", "121" =  "none-impasse",
          "211" = "img-control", "221" =  "img-impasse",
          "311" = "ixv-control", "321" =  "ixv-impasse")
  )
 

```

## SAMPLE

### Data Collection

```{r}
#| label : DESC-DATA-COLLECTION

title = "Participants by Condition and Data Collection Period"
cols = c("Control Condition","Impasse Condition", "Control+Img", "Impasse + Img", "Control + IXN", "Impasse + IXN", "Total for Period")
cont <- table(df_subjects$term, df_subjects$pretty_condition)
cont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()


table(df_subjects$IMPLICIT, df_subjects$EXPLICIT)
```

### Participants

```{r}
#| label: DESC-PARTICIPANTS

#Describe participants
subject.stats <-df_subjects %>% dplyr::select(age) %>% unlist() %>% favstats()
subject.stats$percent.male <- ((df_subjects %>% filter(gender=="Male") %>% count())/count(df_subjects))$n
subject.stats$percent.female <- ((df_subjects %>% filter(gender=="Female") %>% count())/count(df_subjects))$n
subject.stats$percent.other <- ((df_subjects %>% filter(gender %nin% c("Female","Male")) %>% count())/count(df_subjects))$n


title = "Descriptive Statistics of Participant Age and Gender"
subject.stats %>% kbl (caption = title) %>% kable_classic()%>% 
  footnote(general = "Age in Years", 
           general_title = "Note: ",footnote_as_chunk = T) 
```

**REPORTED**

**Overall** `r subject.stats$n` participants (`r round((subject.stats$percent.male),2) * 100` % male, `r round((subject.stats$percent.female),2) * 100` % female, `r round((subject.stats$percent.other),2) * 100` % other) undergraduate STEM majors at a public American University participated in exchange for course credit (age: `r (subject.stats$min)` - `r (subject.stats$max)` years).

## OVERALL ACCURACY

#### Setup

```{r}
#| label: SETUP-ACC

df_i = df_items %>% filter(q %nin% c(6,9)) %>% 
  dplyr::select(pretty_condition, condition, accuracy, subject,q, IMPLICIT, EXPLICIT)

df_s <- df_subjects %>% 
  dplyr::select(pretty_condition, condition, task_percent, IMPLICIT, EXPLICIT)

```

#### Visualize

##### Exploratory

```{r}
#| label: DESC-ACC-FACTORIAL

#:::::::: STACKED PROPORTIONAL BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = IMPLICIT,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  facet_grid(~EXPLICIT) + 
  # scale_fill_foxy(discrete = TRUE, "accuracy")  +
  scale_fill_brewer(palette = "Set1")  +
   labs(title = "Overall Accuracy",
       x = "Condition",
       fill = "",
       subtitle="IMPLICIT and EXPLICIT scaffolds appear to have an additive effect")

#:::::::: STACKED BAR CHART BY QUESTION
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_foxy(discrete = TRUE, "accuracy")  +
  facet_wrap(~ q ) +
   labs(title = "Accuracy by Question",
       x = "Condition",
       fill = "",
       subtitle="Q6 and Q9 are non-discriminative")

#:::::::: FACETED HISTOGRAM
stats = df_s %>% group_by(IMPLICIT, EXPLICIT) %>% dplyr::summarise(mean = mean(task_percent))
gf_props(~task_percent,
         fill = ~pretty_condition, data = df_s) %>%
  gf_facet_grid(IMPLICIT~EXPLICIT) %>%
  gf_vline(data = stats, xintercept = ~mean, color = "red") +
  labs(x = "% Correct",
       y = "proportion of subjects",
       title = "Overall Absolute Score (% Correct)",
       subtitle = "") + theme(legend.position = "blank")

```

##### Present


```{r}
##ALTERNATIVE FORM 

p <- grouped_ggbetweenstats(data = df_s,
                       y = task_percent, x = EXPLICIT, grouping.var = IMPLICIT,
               plot.type = "box", 
               type = "nonparametric", var.equal = FALSE,
               centrality.type = "parametric",
               package = "RColorBrewer",
               palette = "PRGn",
               centrality.point.args = list(color="black", size = 3, shape = 1),
               results.subtitle = TRUE,
               # point.args = list(alpha=0), #suppress points
               ggplot.component = ## modify further with `{ggplot2}` functions
                list(
                  # aes(color = IMPLICIT, fill = IMPLICIT),
                  # scale_colour_manual(values = paletteer::paletteer_c("viridis::viridis", 3)),
                  # scale_fill_manual(values = paletteer::paletteer_c("viridis::viridis", 3))
                  # theme(axis.text.x = element_text(angle = 90)
                )) 

p1 <- p[[1]] + coord_flip()
p2 <- p[[2]] + coord_flip()

(t <- cowplot::plot_grid(p1,  p2, ncol=1))

```


```{r}
#| warning: false
#| message: false


p <- grouped_ggbetweenstats(data = df_s,
                       y = task_percent, x = IMPLICIT, grouping.var = EXPLICIT,
               plot.type = "box", type = "nonparametric", var.equal = FALSE,
               centrality.type = "parametric",
               package = "RColorBrewer",
               palette = "PRGn",
               centrality.point.args = list(color="black", size = 3, shape = 1),
               results.subtitle = FALSE,
               # point.args = list(alpha=0), #suppress points
               ggplot.component = ## modify further with `{ggplot2}` functions
                list(
                  # aes(color = IMPLICIT, fill = IMPLICIT),
                  # scale_colour_manual(values = paletteer::paletteer_c("viridis::viridis", 3)),
                  # scale_fill_manual(values = paletteer::paletteer_c("viridis::viridis", 3))
                  # theme(axis.text.x = element_text(angle = 90)
                )) 

p1 <- p[[1]]+coord_flip() + theme_clean() + 
  ggeasy::easy_remove_x_axis() + ggeasy::easy_remove_legend() +
  labs(title = "EXPLICIT: no-image")
p2 <- p[[2]]+coord_flip() + theme_clean() +
  ggeasy::easy_remove_x_axis() + ggeasy::easy_remove_legend() +
  labs(title = "EXPLICIT: static-image")
p3 <- p[[3]]+coord_flip() + theme_clean() + 
  ggeasy::easy_remove_legend() +
  labs(title = "EXPLICIT: interactive-image",
                       y = "Percentage of correct responses across task")


(t <- cowplot::plot_grid(p1,  p2, p3, ncol=1))

# ggsave(t, filename = "figures/SGC3B_totalscore.png", width = 6, height =4)

# # ggplot(data = df_s, aes( x = pretty_condition, y = task_percent)) + 
#   ggdist::stat_halfeye(
#     alpha = 0.7,
#     point_colour = NA,
#     adjust = .5,
#     width = .5, .width = 0,
#     justification = -.5) +
#   geom_boxplot(
#     alpha = 0.1,
#     width = .2,
#     outlier.shape = NA
#   ) +
#   geom_point(
#     size = 2,
#     alpha = .5,
#     position = position_jitter(
#       seed = 1, width = .05, height = .02
#     )
#   ) 
# coord_flip() + theme_clean() + theme(legend.position = "blank")
# p$layers[[3]]=NULL #remove default boxplot
# e <- statsExpressions::two_sample_test(y = task_percent, x = pretty_condition, data = df_s,
#                                 type = "nonparametric", alternative = "less",
#                                 var.equal = FALSE)
# #labels are layer 4
# p + labs(title = "Distribution of Total Accuracy",
#          y = "Proportion of correct responses across task", x = "",
#          subtitle = "Impasse condition yields higher scores and greater variance",
#          caption=e$expression[[1]])

```

```{r}


#:::::::: STACKED PROPORTIONAL BAR CHART
x <- df_i %>% mutate(
  pretty_explicit = recode_factor(EXPLICIT,
                                  "none" = "no image",
                                  "img" = "static image",
                                  "ixn" = "interactive image"
  )
)

g <- x %>% 
  ggplot(data = .,
         mapping = aes(x = IMPLICIT,
                       fill = accuracy)) +
  geom_bar(position = "fill", width = 0.75 ) + 
  facet_wrap(~pretty_explicit)+
  scale_fill_foxy(discrete = TRUE, "accuracy")  +
  # scale_fill_manual(values = paletteer::paletteer_d("lisa::FridaKahlo", 2))+
  # facet_wrap(~pretty_mode) + 
  # coord_flip() +
  theme(legend.position="bottom")+
   labs(title = "Study 3B | DISTRIBUTION of Question Accuracy",
       x = "Condition",
       y = "Proportion of Questions",
       fill = "",
       subtitle="") 
  
g 
# ggsave(g, filename = "figures/SGC3B_accuracy.png", width = 6, height =4)

```

### Describe

```{r}
#| label: DESC2-ACC

title = "Descriptive Statistics of Response Accuracy (Total % Correct)"
tbl1 <- mosaic::favstats(~task_percent, data = df_s) 
tbl1 %>% kbl (caption = title) %>% kable_classic()


title = "Descriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION"
tbl2 <- mosaic::favstats(task_percent ~ pretty_condition, data = df_s) 
tbl2 %>% kbl (caption = title) %>% kable_classic()

```


**REPORTED**

Across conditions, overall accuracy on the task ranges from `r tbl1$min *100` to `r tbl1$max *100` with a mean of `r tbl1$mean * 100`. We see that the distribution of this outcome variable is clearly bimodal: with modes near the floor (0% correct) and ceiling (100% correct) of the scale. This bimodality is sensical considering the nature of the task, where each item in the task indexes a different information extraction operation over the same coordinate system.

A score of 100% indicates that the participant correctly interpreted the interval-coordinate system throughout the task, *starting at the first question*. A score of 0% indicates the individual *never* correctly interpreted the coordinate system. A score somewhere inbetween indicates that an individual deciphered the coordinate system *sometime over the course the task*.

#### TESTS

##### Aligned Ranks Transformation

<!-- https://rcompanion.org/handbook/F_16.html -->

```{r}


m.art = art(task_percent ~ IMPLICIT*EXPLICIT, data = df_s)
anova(m.art)

```

##### Kruskal Wallis Test

```{r}
#| label: TEST-ACC

(k <- kruskal.test(df_s$task_percent ~ df_s$pretty_condition))

```

##### Visualize

```{r}
#| label: TEST-VIZ-ACC

#:::::::: STATSPLOT | VIOLIN
grouped_ggbetweenstats(y = task_percent, x = EXPLICIT, grouping.var = IMPLICIT,  
               data = df_s, type = "nonparametric")

grouped_ggbetweenstats(y = task_percent, x = IMPLICIT, grouping.var = EXPLICIT,  
               data = df_s, type = "nonparametric")

temp <- df_i %>% mutate(accuracy = fct_rev(accuracy))
grouped_ggbarstats(y = IMPLICIT, x = accuracy, grouping.var = EXPLICIT, 
               data = temp, type = "nonparametric")

```

#### MIXED LOGISTIC REGRESSION

*Fit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.*

##### Fit Model

```{r}
#| label: MODEL-FIT-ACC

## 0 | SETUP
#confirm 13 items [all discriminating items]
nrow(df_i) / nrow(df_s) == 13
#confirm all factors 
is.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$pretty_condition) && is.factor(df_i$accuracy) && is.factor(df_i$ospan_split)

## 1 | SETUP RANDOM EFFECTS

#:: RANDOM INTERCEPT SUBJECT + ITEM
print("Subject Intercept + Item intercept random model")
mm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = "binomial")
#summary(mm.rSQ)

## 2 | ADD MAIN EFFECTS 

print("MAIN FIXED  implicit + explicit + Subject & Item random intercepts")
mm.I.ErSQ <- glmer(accuracy ~ IMPLICIT + EXPLICIT + (1|subject) + (1|q) ,
                data = df_i, family = "binomial")
# car::Anova(mm.IErSQ, type = 3)

paste("AIC decreases w/ new model", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.I.ErSQ)) )
test_lrt(mm.rSQ,mm.I.ErSQ) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rSQ,mm.I.ErSQ))$p[2])

## 2 | ADD IXN FIXED EFFECT

print("MAIN IXN implicit*explicit + Subject & Item random intercepts")
mm.IErSQ <- glmer(accuracy ~ IMPLICIT*EXPLICIT + (1|subject) + (1|q) ,
                data = df_i, family = "binomial")
# car::Anova(mm.IErSQ, type = 3)

paste("AIC decreases w/ new model", AIC(logLik(mm.IErSQ)) > AIC(logLik(mm.I.ErSQ)) )
test_lrt(mm.I.ErSQ,mm.IErSQ) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.I.ErSQ,mm.IErSQ))$p[2])


paste("Final model better than random model?")
test_lrt(mm.IErSQ,mm.rSQ) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.IErSQ,mm.rSQ))$p[2])

paste("Model Performance")
performance(mm.IErSQ)

```

**REPORTED**

To quantify the effect of IMPLICIT and EXPLICIT scaffolding techniques on ACCURACY, we fit a mixed effects logistic regression model with random intercepts for subjects and questions, and IMPLICIT, EXPLICIT and their interaction term as fixed effects. A likelihood ratio test indicates that a model including both fixed effect and their interaction term is a significantly better fit to the data than random-intercepts only baseline model $chi^2(3,8) = 161.93, p < 0.001$

A likelihood ratio test indicates that a model including both fixed effects and their interaction term is a significantly better fit to the data than a simpler main-effects (with random-intercepts) model ($\chi^2(6,8) = 8.20, p < 0.05$)


The explanatory power of the final model is substantial (conditional R2 = 0.83) and the part related to the fixed effects EXPLICIT and IMPLICIT scaffolding (marginal R2) explains 32% of variance.


##### Describe

```{r}
#| label: MODEL-DESC-ACC

# best model
m <- mm.IErSQ
# m %>% write_rds(file = "analysis/SGC3B/models/sgc3b_glmer_acc_mm.IErSQ.rds")
m <- read_rds(file = "analysis/SGC3B/models/sgc3b_glmer_acc_mm.IErSQ.rds" )

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m)


print("SIGNIFICANCE TEST [non directional]")
car::Anova(m, type=3) #TYPE 3 SS FOR IXNS

#:::::::: INTERPRET COEFFICIENTS

paste("LOG ODDS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald")

paste("ODDS RATIOS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald", exponentiate = TRUE)

paste("PROBABILITIES")
#probability control = plogis(intercept)
#probability impasse = plogis(intercept + coefficient)

#FROM predict()
# newdata <- df_i %>% dplyr::select(pretty_condition, subject, q)
# preds <- predict(m, newdata = newdata, type = "response")
# preds <- cbind(newdata, preds)
# p <- preds %>% 
#   dplyr::select(pretty_condition, preds) %>% 
#   group_by(pretty_condition) %>% 
#   summarise(
#     median = median(preds),
#     se = sd(preds)/sqrt(n()),
#     lwr = median - 1.96*se,
#     upr = median + 1.96*se)
    
  
#FROM merTools
#setup df 
newdata <- df_i %>% dplyr::select(IMPLICIT, EXPLICIT, subject, q)
#make predictions
preds <- predictInterval(m, newdata = newdata,
                              which = "fixed", #full, fixed or random for those only
                              type = "probability", #linear.prediction
                              stat = "median",
                              n.sims = 1000,
                              level = 0.80) #width of prediction interval
#join predictions to the new dataframe
preds <- cbind(newdata, preds)
#summarize
(summ_preds <- preds %>% 
  dplyr::select(IMPLICIT, EXPLICIT, fit, lwr, upr) %>% 
  group_by(IMPLICIT,EXPLICIT) %>% 
  summarise(
    median = median(fit),
    lower = median(lwr),
    upper = median(upr)
  )) 

```

##### INFERENCE 

**REPORTED**
Wald Chi-Square tests indicate both significant main effects for IMPLICIT ($\chi^2 (1) = 19.3, p < 0.001$) and EXPLICIT ($\chi^2 (1) = 95.2, p < 0.001$) scaffolds , as well as a significant interaction ($\chi^2 (1) = 8.1, p < 0.05$).  [SEE POSTHOCS BELOW]

The (unstandardized) regression coefficients indicate that across explicit scaffolds, the impasse manipulation increases the odds of a correct response by a factor of 17.5, ($e^{\beta_1} = 17.5, SE = 11.5, p < 0.001$). The effect of the explicit scaffolds were much more pronounced, however, with the static-image increasing odds of a correct response by a factor of 103, ($e^{\beta_1} = 103, SE = 68.7,  381], p < 0.001$) and the interactive-image increasing odds of a correct response by a factor of 910,  ($e^{\beta_1} = 910, SE = 643, p < 0.001$). 

##### Interactions

```{r}

# https://cran.r-project.org/web/packages/emmeans/vignettes/interactions.html#simple
# https://stats.oarc.ucla.edu/stata/faq/how-can-i-understand-a-categorical-by-categorical-interaction-in-logistic-regression-stata-12/
# https://stats.oarc.ucla.edu/stata/seminars/deciphering-interactions-in-logistic-regression/
  
library(emmeans)

#sanity check reference grid
ref_grid(m)

#PRINT ESTIMATED MARGINAL MEANS
#should be same as summ_preds
emmeans(m,  ~ IMPLICIT + EXPLICIT, type = "response")

##POST-HOC COMPARISONS
print("POSTHOC COMPARISONS")
emmeans(m,  pairwise ~ IMPLICIT + EXPLICIT, 
        type = "response" , adjust = "tukey") #sidak, tukey


##PLOT INTERACTION
#equivalent to plot_model, type = "int"
emmip(m, IMPLICIT ~ EXPLICIT,
      type = "response",
      CIs = TRUE,
      linearg = list(linetype = "dashed"),
      engine = "ggplot") 

#conditioned on fixed effects only
plot_model(m, type = "int",pred.type = "fe" )
#conditioned on random effects
plot_model(m, type = "int",pred.type = "re" )


##PLOT PROBABILITY
plot(ref_grid(m), by = "EXPLICIT", type = "response")

```
**REPORTED**

Posthoc paired comparisons (with Tukey method correction) reveal that across each the no-image control and static image conditions, posing a mental impasse significantly improved performance over the non-impasse control.  The source of the interaction effect is driven by accuracy reaching ceiling across both non-impasse and impasse conditions (OR = 0.76, z = -0.43, p = 0.99). That is to say, the explicit interactive-image scaffold is so effective, very few participants are in need of additional implicit scaffolding. This is in contrast with the no-explicit scaffold conditions (no images), where the addition of the mental impasse significantly increased the probability of a correct response (OR = 0.06, z = -4.39, p </.0001) 


##### Print

```{r}
#| label: MODEL-TBL-ACC

#SJPLOT | MODEL | TABLE
# tab_model(m)

# #MODEL SUMMARY | save latex table

# models <- list("odds ratios" = m, "(log odds)" = m)
# title = "Study 3B (EXPLICIT vs IMPLICIT) | Question Accuracy | Mixed Logistic Regression"
# notes = list("* p < 0.05, ** p < 0.01, *** p < 0.001",
#              paste("n = ", n_obs(m), "R^2(Conditional) =", round(r2(m)[[1]],2),
#                    "R^2(Marginal) =", round(r2(m)[[2]],2)),
#              "Accuracy  ~ Condition * OSPAN +  (1 | subject) + (1 | q)")
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c(
#                "IMPLICITimpasse" = "Implicit[impasse]",
#                "EXPLICITimg" = "Explicit[img]",
#                "EXPLICITixn" = "Explicit[ixn]",
#                "IMPLICITimpasseEXPLICITimg" = "Implicit[impasse] : Explicit[img]",
#                "IMPLICITimpasseEXPLICITixn" = "Implicit[impasse] : Explicit[ixn]"
#                ),
#              title = title,
#              notes = notes,
#              output = "tables/SGC3B_GLMER_OverallAccuracy.tex")
             # coef_omit = "Intercept"

# extract_eq(m, use_coefs = TRUE, wrap = TRUE)


```

##### Visualize

```{r}
#| label: MODEL-VIS-ACC

## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
plot_model(m, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Estimate | Odds Ratio",
       subtitle = "",
       x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
result <- model_parameters(m, exponentiate = TRUE, component = "all")
plot(result)

## | PLOT TESTS

result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)

## | PLOT TESTS
result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)

#ONLY FOR BAYESIAN VERSION
# result <- rope(m)
# plot(result)
# 
# result <- pd(m)
# plot(result)


## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
plot_model(m, type="int",
           show.p = TRUE,
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Prediction | Probability of Accurate Response",
           axis.title = c("Condition","Probability of Accurate Response"))

#PLOT MODEL PREDICTION
plot_model(m, type = "pred")  
plot_model(m, type = "eff")  

  # ylim(0,1) + 
  # labs(
  #   title = "Model Prediction | Probability of Accurate Response",
  #   subtitle = "Impasse increases Probability of Correct Response"
  # )
```

###### Present
```{r}
##PLOT INTERACTION ANOTHER WAY
library(ggeffects)
p <- ggpredict(m, terms = c("EXPLICIT","IMPLICIT")) %>% 
  plot(connect.lines = TRUE) + 

  scale_color_manual(values = fct_rev(paletteer::paletteer_d("nbapalettes::bulls")))+
  # scale_color_manual(values = paletteer::paletteer_d("awtools::gpalette",3))+
  # scale_color_manual(values = paletteer::paletteer_d("lisa::FridaKahlo", 3))+
  # scale_colour_manual(values = paletteer::paletteer_c("viridis::viridis", 2)) + 
  theme_clean() + labs(
  title = "MODEL | Predicted probability of correct response",
  x = "EXPLICIT scaffold",y="Probability of correct response"
) + theme(legend.position = "bottom")
p
# ggsave(p, filename = "figures/SGC3B_accuracy_prediction.png", width = 6, height =4)



dat <- ggpredict(m, terms = c("EXPLICIT","IMPLICIT")) 
p <- ggplot(dat, aes(x, predicted, x, colour = group, group = group)) +
  geom_point(position = position_dodge(.2), width = 0.1) +
  geom_errorbar(
    aes(ymin = conf.low, ymax = conf.high),
    position = position_dodge(.2), width = 0.2
  ) +
  geom_line(position = position_dodge(0.2))+
  # scale_x_discrete(breaks = 1:3, labels = get_x_labels(dat))+
scale_color_manual(values = fct_rev(paletteer::paletteer_d("nbapalettes::bulls"))) + theme_clean() +
   labs(
  title = "MODEL | Predicted probability of correct response",
  x = "EXPLICIT scaffold",y="Probability of correct response"
) + theme(legend.position = "bottom")

# ggsave(p, filename = "figures/SGC3B_accuracy_prediction.png", width = 6, height =4)


```

##### Diagnostics

```{r}
#| label: MODEL-DIAG-ACC
#| warning: false
#| message: false

# print("SANITY CHECK REPORTING")
# report(m)

# print("MODEL PERFORMANCE")
# performance(m)

print("DIAGNOSTICS")
check_model(m)

```

## OVERALL INTERPRETATION STATE

-   **"orthogonal"** \[reference category\] includes orthogonal and satisficing responses ==\> indicates a primarily *orthogonal* state of coordinate system understanding

-   **"other"** includes: blank, reference point, responses that can't be classified (including selecting all datapoints), =\> indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly *not* orthogonal nor triangular

-   **"angular"** includes 'lines connecting' responses as well as responses that include both orthogonal *and* triangular answers =\> indicates some degree of angular/triangular coordinate understanding

-   **"triangular"** includes correct triangular =\> indicates a correct triangular coordinate understanding

#### Setup

```{r}
#| label: SETUP-STATE

df_i = df_items %>% filter(q %nin% c(6,9)) %>% dplyr::select(pretty_condition, q,subject,state,IMPLICIT, EXPLICIT) %>% droplevels()

```

### Visualize

```{r}
#:::::::: STACKED BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = EXPLICIT,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~IMPLICIT) +
   labs(title = "Interpretation across all Questions",
       x = "Condition",
       fill = "",
       subtitle="")

#:::::::: STACKED BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = IMPLICIT,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~EXPLICIT) +
   labs(title = "Interpretation across all Questions",
       x = "Condition",
       fill = "",
       subtitle="")

#:::::::: STACKED BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = EXPLICIT,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(q ~ IMPLICIT) +
   labs(title = "Interpretation by Question",
       x = "Condition",
       fill = "",
       subtitle="")

```
```{r}

#:::::::: STACKED BAR CHART

x <- df_i %>% mutate(
  pretty_explicit = recode_factor(EXPLICIT,
                                  "none" = "no image",
                                  "img" = "static image",
                                  "ixn" = "interactive image"
  )
)

p <- x %>% 
  ggplot(data = .,
         mapping = aes(x = IMPLICIT,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
   scale_fill_foxy(discrete = TRUE, "state")  +
  facet_wrap(~pretty_explicit) +
   labs(title = "DISTRIBUTION | Question Interpretation",
       x = "Condition", y = "Proportion of Questions",
       fill = "",
       subtitle="") + theme_clean()

ggsave(p, filename = "figures/SGC3B_interpretation.png", width = 6, height =4)

```
```{r}

#::::::::::::DESCRIPTIVES

table(df_i$state, df_i$pretty_condition) %>% addmargins(2) %>% #display sum for row
      prop.table(margin=2) %>%  #return proportion (of column)
      addmargins(1) #sanity check sum of columns


(t <- table(df_i$state, df_i$EXPLICIT, df_i$IMPLICIT) %>% addmargins(2) %>% #display sum for row
      addmargins(1)) #sanity check sum of columns

```

#### MIXED MULTINOMIAL REGRESSION

*Does condition affect the response state of of items across the task?*

*Fit a MIXED logistic regression predicting interpretation state (k=3) by condition(k = 2).*

##### Fit Model \[brms\]

```{r}
#| label: FIT-BRMS-STATE

# to set priors... create model with default priors
# then run prior_summary to see what the defaults are and syntax for coefficients
# prior_summary(flat)
# then create lists with the new priors, and create a new model with the priors 

inf_priors <- c(
  #prior on INTERCEPTS 
  #25% chance of each answer in control, scale = from 0.01 to 62%
  prior(normal(-1.1, 1.5),  class = "Intercept", dpar = "muangular"),
  prior(normal(-1.1, 1.5),  class = "Intercept", dpar = "muother"),
  prior(normal(-1.1, 1.5),  class = "Intercept", dpar = "mutriangular"),
  
  #prior on CONDITION COEFFICIENT
  #likely to change odds between 0 and 2.4
    
  prior(normal(0, 2.42), class = b, coef="pretty_conditionimgMcontrol", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionimgMimpasse", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionixvMcontrol", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionixvMimpasse", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionnoneMimpasse", dpar = "muother"),
  
  prior(normal(0, 2.42), class = b, coef="pretty_conditionimgMcontrol", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionimgMimpasse", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionixvMcontrol", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionixvMimpasse", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionnoneMimpasse", dpar = "muangular"),
  
  prior(normal(0, 2.42), class = b, coef="pretty_conditionimgMcontrol", dpar = "mutriangular"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionimgMimpasse", dpar = "mutriangular"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionixvMcontrol", dpar = "mutriangular"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionixvMimpasse", dpar = "mutriangular"),
  prior(normal(0, 2.42), class = b, coef="pretty_conditionnoneMimpasse", dpar = "mutriangular")
)

me_priors <- c(
  #prior on INTERCEPTS 
  #25% chance of each answer in control, scale = from 0.01 to 62%
  prior(normal(-1.1, 1.5),  class = "Intercept", dpar = "muangular"),
  prior(normal(-1.1, 1.5),  class = "Intercept", dpar = "muother"),
  prior(normal(-1.1, 1.5),  class = "Intercept", dpar = "mutriangular"),
  
  #prior on IMPLICIT COEFFICIENT
  #likely to change odds between 0 and 2.4
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse", dpar = "mutriangular"),
  
  #prior on EXPLICIT COEFFICIENT
  #likely to change odds between 0 and 2.4
  prior(normal(0, 2.42), class = b, coef="EXPLICITimg", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="EXPLICITimg", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="EXPLICITimg", dpar = "mutriangular"),
  
  prior(normal(0, 2.42), class = b, coef="EXPLICITixn", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="EXPLICITixn", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="EXPLICITixn", dpar = "mutriangular")
)

ixn_priors <- c(
  #prior on INTERCEPTS 
  #25% chance of each answer in control, scale = from 0.01 to 62%
  prior(normal(-1.1, 1.5),  class = "Intercept", dpar = "muangular"),
  prior(normal(-1.1, 1.5),  class = "Intercept", dpar = "muother"),
  prior(normal(-1.1, 1.5),  class = "Intercept", dpar = "mutriangular"),
  
  #prior on IMPLICIT COEFFICIENT
  #likely to change odds between 0 and 2.4
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse", dpar = "mutriangular"),
  
  #prior on EXPLICIT COEFFICIENT
  #likely to change odds between 0 and 2.4
  prior(normal(0, 2.42), class = b, coef="EXPLICITimg", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="EXPLICITimg", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="EXPLICITimg", dpar = "mutriangular"),
  
  prior(normal(0, 2.42), class = b, coef="EXPLICITixn", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="EXPLICITixn", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="EXPLICITixn", dpar = "mutriangular"),
  
  #prior on IXN COEFFICIENT
  #likely to change odds between 0 and 2.4
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse:EXPLICITimg", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse:EXPLICITimg", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse:EXPLICITimg", dpar = "mutriangular"),
  
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse:EXPLICITixn", dpar = "muangular"),
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse:EXPLICITixn", dpar = "muother"),
  prior(normal(0, 2.42), class = b, coef="IMPLICITimpasse:EXPLICITixn", dpar = "mutriangular")
)

#BAYESIAN RANDOM ONLY
Bmm.cat.rSQ <- brm( state ~ 1 + (1|subject) + (1|q), 
                 data = df_i, 
                 family = "categorical",
                 chains = 4, iter = 2500, warmup = 1000,
                 cores = 4, seed = 1234,
                 save_pars = save_pars(all = TRUE),
                 # backend = "cmdstanr",
                 file ="analysis/SGC3B/models/sgc3b_brms_state_Bmm.cat.rSQ.rds")

# CONDITION (unravelled, not fatorial) EQUAL ONLY MODEL
Bmm.cat.CrSQ <- brm( state ~ pretty_condition + (1|subject) + (1|q), 
                 data = df_i, 
                 prior = inf_priors,
                 family = "categorical",
                 chains = 4, iter = 2000, warmup = 1000,
                 cores = 4, seed = 1234,
                 save_pars = save_pars(all = TRUE),
                 control = list(adapt_delta = 0.98),  # to deal with divergent transitions
                 backend = "cmdstanr",
                 file ="analysis/SGC3B/models/sgc3b_brms_state_Bmm.cat.CrSQ.rds")

##MODEL COMPARISON
# print("MODEL COMPARISON: random effects (vs) CONDITION")
# bayesfactor(Bmm.cat.rSQ, Bmm.cat.CrSQ)
#substantial evidence in favor of conditon model over random only BF 1.64e+16

# CONDITION * OSPAN MODEL
Bmm.cat.I.ErSQ  <- brm( state ~ IMPLICIT + EXPLICIT + (1|subject) + (1|q), 
                 data = df_i, 
                 prior = me_priors,
                 family = "categorical",
                 chains = 4, iter = 2000, warmup = 1000,
                 cores = 4, seed = 1234,
                 save_pars = save_pars(all = TRUE),
                 control = list(adapt_delta = 0.98),  # to deal with divergent transitions
                 # backend = "cmdstanr",
                 file ="analysis/SGC3B/models/sgc3b_brms_state_Bmm.cat.I.ErSQ.rds")



# CONDITION * OSPAN MODEL
Bmm.cat.IErSQ  <- brm( state ~ IMPLICIT*EXPLICIT + (1|subject) + (1|q), 
                 data = df_i, 
                 prior = ixn_priors,
                 family = "categorical",
                 chains = 4, iter = 2500, warmup = 1000,
                 cores = 4, seed = 1234,
                 save_pars = save_pars(all = TRUE),
                 control = list(adapt_delta = 0.98),  # to deal with divergent transitions
                 # backend = "cmdstanr",
                 file ="analysis/SGC3B/models/sgc3b_brms_state_Bmm.cat.IErSQ.rds")

```

```{r}
##MODEL COMPARISON
print("IS FACTORIAL better than MAIN EFFECTS model?")
(b2 <- bayesfactor(Bmm.cat.IErSQ,Bmm.cat.I.ErSQ))
compare_models(Bmm.cat.I.ErSQ, Bmm.cat.IErSQ)


##MODEL COMPARISON
print("IS FACTORIAL better than RANDOM EFFECTS model?")
(b <- bayesfactor(Bmm.cat.I.ErSQ,Bmm.cat.rSQ))


```

**Reported** 

To quantify the effect of IMPLICIT and EXPLICIT scaffolding on INTERPRETATION, we fit a (bayesian) mixed multinomial regression model with random intercepts for subjects and questions. 

A Bayes Factor analysis comparing a main effects only model vs a more complex model including the interaction term between IMPLICIT and EXPLICIT scaffold indicates extreme evidence in favor of the simpler main effects only model (BF = 4.04 e+122).


(old, removed)
A Bayes Factor model comparison (against a random intercepts only model) indicates extreme evidence for the final model including fixed effects of IMPLICIT, EXPLICIT and their interaction term (BF = 6.89e+125)

(note, the BF may change as it is estimated via simulation each time)

Thus, we proceed with the simpler, main effects only model

##### Describe

```{r}
#| label: DESC-BRMS-STATE

# best model
m <- Bmm.cat.I.ErSQ

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m)
(d <- describe_posterior(ci=.95, m))

print("BAYES FACTOR [comparison to RANDOM ONLY model]")
#think of this like the anova(model) to get p values for each predictor
#has to recompile the models with rstan. total drag
#already calculated bayes factor above, this just interprets it
effectsize::interpret_bf(exp(b$log_BF), include_value = TRUE)


print("DESCRIBE POSTERIOR")

#:::::::: INTERPRET COEFFICIENTS

paste("LOG ODDS")
(l <- describe_posterior(m))
# (tm <- tidy(m,   conf.int = TRUE))

paste("ODDS RATIOS")
(e <- model_parameters(m, exponentiate = TRUE))

paste("PROBABILITIES")

#PREDICT METHOD
newdata <- df_i %>% dplyr::select(IMPLICIT, EXPLICIT, subject, q)
preds <- predict(m, newdata = newdata, type = "response")
preds <- cbind(newdata, preds)
# lengthen data frame to handle multinomial
preds <- preds %>%
  dplyr::select(-subject, -q) %>% #marginalize over subject and q
  pivot_longer(
  cols = ! IMPLICIT & !EXPLICIT,
  values_to = "preds",
  names_to = "state",
)

(p <- preds %>%
  group_by(IMPLICIT, EXPLICIT , state ) %>%
  summarise(
    median = median(preds),
    se = sd(preds)/sqrt(n()),
    lwr = median - 1.96*se,
    upr = median + 1.96*se))

##DRAWS METHOD
# GENERATE draws from model
# draws <- df_i %>%
#   data_grid(pretty_condition, subject, q) %>% 
#   add_fitted_draws(Bmm.cat.CrSQ,
#                    # n = 100,
#                    # dpar = TRUE,
#                    # transform = TRUE, #gives prob%, otherwise OR
#                    re_formula = NA)
# # draws %>% write_rds(file = "analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds")
# 
# #OR load from file
# # draws <- read_rds(file = "analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds")
# 
# # SUMMARIZE draws from model
# (k <- kable(draws %>%
#   dplyr::select(pretty_condition, .category, .value) %>%
#   group_by(pretty_condition, .category) %>%
#   median_hdci(.value), digits = 4, col.names =
#     c("Condition","Category", "Probability","Lower Cred.I","Upper Cred.I", "CI Width", "Point Type", "Interval Type")) %>%
#   kable_styling())

```

##### INFERENCE

Consistent with our findings for question ACCURACY, we see similarly increasing probability of triangular responses across the explicit scaffold factors such that no image < static image < interactive image.  Within each explicit scaffold, we see that the impasse factor yields more angular and other responses relative to control, consistent with our findings in Study 3A.  Much like with ACCURACY, we see near ceiling performance across both non-impasse and impasse conditions with the interactive image. 

The model parameter estimates (see appendix TODO) indicate reliable evidence for main effects of both impasse and explicit. Specifically, the static image and interactive image conditions increase the probability of TRIANGULAR, ANGULAR and OTHER answers, (relative to orthogonal answers, in the no-image condition). The impasse factor increases the probability of triangular, angular and other answers in the no-image condition,   but does not substantially increase the proportion of angular or other answers relative to the non-impasse control in the static image and interactive image conditions.  It seems that the explicit scaffold has the strongest effect. Importantly, impasse factor increases the probability of triangular responses (relative to non-impasse) across an explicit scaffolds, except the interactive image, where both the non-impasse control and impasse condition are nearly at ceiling. 

##### Bayes Factor
- https://easystats.github.io/bayestestR/articles/bayes_factors.html

Compute bayes factor for model parameters

```{r}

#define null on the log odds scale as the range from -0.4 to 0.4, a 1.5X increase or decrease in odds 
(coef_bf <- bayesfactor(m, null = c(-0.4, 0.4)))

#plot bayes factors
plot(coef_bf)

#interpretation guidelines
effectsize::interpret_bf(exp(coef_bf$log_BF), include_value = TRUE)

```




##### Interactions

```{r}

#EMMEANS doesn't support multinomial brms models. BUMMER
#BUT brms has some built in stuffs. 

# FOR MAIN EFFECTS PLOTS 
print("MAIN EFFECTS")
conditional_effects(m, categorical = TRUE)


print("INTERACTION PLOT")
conditions <- make_conditions(m, vars = c("EXPLICIT"))
c <- conditional_effects(m, "IMPLICIT", conditions = conditions,
                    categorical = TRUE)
ugly_plot <- plot(c, plot = FALSE)[[1]]                    
ugly_plot + 
  # geom_line(aes(group = EXPLICIT))+
  scale_color_foxy(discrete = TRUE, "state")  +
  theme_clean() 

                    

print("INTERACTION PLOT")
plot_model(m, type="int")

## MARGINAL EFFECTS

# from easystats 
# library(modelbased)
# uses emmeans, doesn't support brms categorical 
# estimate_contrasts(m, test = "bf", bf_prior = m)

# library(marginaleffects)
# https://vincentarelbundock.github.io/marginaleffects/articles/brms.html
## takes a really long time... maybe hangs machine??
# (mfx <- marginaleffects::marginaleffects(m))
# summary(mfx)

```

```{r}
library(ggeffects)
g <- ggpredict(m, terms = c("EXPLICIT","IMPLICIT")) %>% 
  plot(connect.lines = TRUE) + 
  scale_color_manual(values = fct_rev(paletteer::paletteer_d("nbapalettes::bulls")))+
  # scale_color_manual(values = paletteer::paletteer_d("awtools::gpalette",3))+
  # scale_color_manual(values = paletteer::paletteer_d("lisa::FridaKahlo", 3))+
  # scale_colour_manual(values = paletteer::paletteer_c("viridis::viridis", 3)) +
  theme_clean() + labs(
    title = "Model Prediction | Probability of Interpretation",
    x = "EXPLICIT scaffold", y = "Probability of Interpretation"
) + theme(legend.position = "bottom")

g 
ggsave(g, filename = "figures/SGC3B_interpretation_prediction.png", width = 6, height =4)
```

##### Visualize

```{r}
#| label: VIS-BRMS-STATE

## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
# plot_model(m, vline.color = "red",
#            show.intercept = TRUE,
#            show.values = TRUE,
#            p.threshold = 0.1, #manually adjust to account for directional test
#            ci.lvl = 0.90 ) + #manually adjusted for directional test
#   labs(title = "Model Estimate | Odds Ratio",
#        subtitle = "",
#        x = "Condition")


#EASYSTATS | MODEL | LOG ODDS RATIO
result <- model_parameters(m, exponentiate = FALSE, component = "all")
plot(result, show_intercept = TRUE, show_labels = TRUE) 

#EASYSTATS | MODEL | ODDS RATIO
result <- model_parameters(m, exponentiate = TRUE, component = "all")
plot(result, show_intercept = TRUE, show_labels = TRUE) 
# + theme_clean()

## | PLOT TESTS

result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)

result <- rope(m)
plot(result)

result <- pd(m) 
plot(result)

## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
plot_model(m, type="int",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Prediction | Probability of Accurate Response",
           axis.title = c("Condition","Probability of Accurate Response"))

#PLOT MODEL PREDICTION
# plot_model(m, type = "pred")  
# plot_model(m, type = "eff")  
  # ylim(0,1) + 
  # labs(
  #   title = "Model Prediction | Probability of Accurate Response",
  #   subtitle = "Impasse increases Probability of Correct Response"
  # )

```

```{r}
#::::: GGDIST POSTERIOR PROBABILITY OF RESPONSE
##WORKING
# https://mjskay.github.io/ggdist/reference/stat_slab.html
## VIS probability of correct response
#TAKES A REALLY LONG TIME

#1 | get draws
# draws <- df_i %>%
#   data_grid(pretty_condition, ospan_split, subject, q) %>%
#   add_epred_draws(m,
#                    # ndraws = 100, # n = 100,
#                    # dpar = TRUE,
#                    transform = TRUE, #gives prob%, otherwise OR
#                    re_formula = NA)
# draws %>% write_rds(file = "analysis/SGC3A/models/draws/draws_Bmm.catCOrSQ_OPSAN.rds")

#OR load from file
# draws <- read_rds(file = "analysis/SGC3A/models/draws/draws_Bmm.catCOrSQ_OPSAN.rds")

#2| VISUALIZE PREDICTIONS | GGDIST
##TODO figure out height normalization.
##do it with much smaller number of draws 
#TODO adjust bandwidth/smoothing? + put on same line + 
#TAKES A REAAALY LONG TIME
# d <- 

# d <- draws %>% sample_n(10) %>% 
#   ggplot(aes(x = .epred,  y = pretty_condition, fill = ospan_split)) +
#   stat_slab(width = c(.95), alpha = 0.5, normalize="xy") +
#   facet_wrap(~.category) +
#   #   #normalize = all, panels, xy, groups, none
#   xlim(0,1) + labs(
#     title = "Model Predicted Probability of Correct Response",
#     x = "probability of correct response",
#     y = "Interpretation"
#   ) +  theme_clean() #+ ggeasy::easy_remove_legend() + ggeasy::easy_remove_y_axis()
# # # #TO PLOT ON THE SAME LINE, INCLUDE Y = 0 in aes and ggeasy::remove_y_axis()
# # # 
# # # ggsave(d, filename = "figures/sgc3a_BBm.cat.CrSQ_lab_posterior.svg", width = 6, height =4)
# d


#USING PREDS

preds %>% 
  ggplot(aes (x = preds, y = EXPLICIT, fill = IMPLICIT)) +
  stat_slab( normalize = "xy", alpha = 0.5) +
  facet_grid(~ state)

```

##### Print 
```{r}
#| label: MODEL-TBL-STATE

#SJPLOT | MODEL | TABLE
# tab_model(m)

# #MODEL SUMMARY | save latex table
#model summary doesn't work for brms multinomial

# DOESN'T WORK FOR BRMS
# extract_eq(m, use_coefs = TRUE, wrap = TRUE)
# 
# 
# #GET MODEL ESTIMATES
t <- as.data.frame(model_parameters(m, exponentiate = TRUE))

# # #REFORMAT
x <- t %>%
  mutate(
    Parameter = str_remove_all(Parameter,"_pretty"),
    Parameter = str_remove_all(Parameter,"b_mu"),
    Interpretation = word(Parameter, 1, sep = "_"),
    Interpretation = fct_relevel(Interpretation, levels = c("other","angular","triangular")),
    Factor = word(Parameter, 2, sep = "_"),
    Factor = recode_factor(Factor,
                         "Intercept" = "(Intercept)",
                         "IMPLICITimpasse" = "Implicit[impasse]",
                         "EXPLICITimg" = "Explicit[image]",
                         "EXPLICITixn" = "Explicit[interactive]"),
    Median = round(Median,2),
    CI_low = round(CI_low,2),
    CI_high = round(CI_high,2),
    pd = round(pd,2),
    ROPE_Percentage = round(ROPE_Percentage,2)) %>%
  arrange(Interpretation) %>%
  dplyr::select(-CI, -Rhat, -ESS) %>%
  rename( "%_in_ROPE"="ROPE_Percentage",
  "(Odds Ratio)" = "Median") %>%
  dplyr::select(Interpretation, Factor, `(Odds Ratio)`, CI_low, CI_high, pd, `%_in_ROPE`)

# # #KNIT
title = "Study 3B (Explicit (vs) Implicit) | Question Interpretation "
tab <- kbl(x, format = "latex", caption = title,
           booktabs = FALSE) %>% kable_classic() 
# tab
# footnote(general = paste("Model Interpretation ~ ",b$Model[2], "Bayes Factor ", "format( exp(b$log_BF[2]), digits =2 )" ), footnote_as_chunk = T, general_title = "")
writeLines(tab, "tables/SGC3B_BRMS_state.tex")


```
##### Diagnostics

```{r}

#CHECK Fit of posterior predictive to data
pp_check(m, ndraws=1000)

#CHECK posterior vs. priors
result <- estimate_density(m)
plot(result, stack = FALSE, priors= TRUE)

#CHECK model
plot(m)

```

## H1B \| Q1 ACCURACY

#### Setup

```{r}
#| label: SETUP-Q1ACC

df <- df_items %>% filter(q==1) %>% dplyr::select(accuracy, pretty_condition, IMPLICIT, EXPLICIT)

```

#### Describe

```{r}
#| label: VIS-Q1ACC

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
   labs(#y = "Correct Response on Q 1",
       title = "Q1 Accuracy",
       x = "Condition",
       fill = "",
       subtitle="")

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = IMPLICIT,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~EXPLICIT) +
   labs(#y = "Correct Response on Q 1",
       title = "Q1 Accuracy",
       x = "Condition",
       fill = "",
       subtitle="")

```

```{r}
#| label: DESC-Q1ACC

#::::::::::::DESCRIPTIVES

paste("Proportions of Correct Responses by Condition")
table(df$accuracy, df$pretty_condition) %>% addmargins(2) %>% #display sum for row
      prop.table(margin=2) %>%  #return proportion (of column)
      addmargins(1) #sanity check sum of columns

paste("Number of Correct Responses by Condition")
table(df$accuracy, df$IMPLICIT, df$EXPLICIT) %>% addmargins(2) %>% #display sum for row
      addmargins(1) #sanity check sum of columns
```

#### TESTS

```{r}
#| label: TESTS-Q1ACC

#CHI SQUARE ON ACCURACY X OSPAN-SPLIT in LOW WORKING MEMORY
# df_low <- df %>% filter(ospan_split == "low-memory")
# # table(df_low$pretty_condition, df_low$accuracy)
# chisq.test( x = df_low$pretty_condition, y = df_low$accuracy, correct = TRUE)
# 
# #CHI SQUARE ON ACCURACY X OSPAN-SPLIT in HIGH WORKING MEMORY
# df_high <- df %>% filter(ospan_split == "high-memory")
# #table(df_high$pretty_condition, df_high$accuracy)
# chisq.test( x = df_high$pretty_condition, y = df_high$accuracy,correct = TRUE)
# #significant if correct = FALSE
```

```{r}

# INTERACTION (IMPLICIT (VS) EXPLICIT)
grouped_ggbarstats( data = df, x = IMPLICIT, y = accuracy, 
                    grouping.var = EXPLICIT,
                    type = "nonparametric")

# MAIN EFFECT CONDITION (yes)
ggbarstats( data = df, x = accuracy, y = pretty_condition,
                    type = "nonparametric")

# MAIN EFFECT OSPAN (none)
# ggbarstats( data = df, x = accuracy, y = ospan_split, 
#                     type = "nonparametric")

```

#### LOGISTIC REGRESSION (MAIN EFFECT CONDITION)

Fit a logistic regression predicting accuracy (absolute score) (n = `r nrow(df)`) by condition (k = 2).\

-   Parameter estimate: $\beta_{0}$ = Log Odds of (correct) responses in CONTROL condition
-   $e^{\beta_{0}}$ = ODDS of correct response in CONTROL condition
-   Parameter estimate: $\beta_{1}$ = $\beta_{1impasse}$ Log Odds (Log OR; change in odds for correct response in impasse (vs) control \[log scale\])
-   $e^{\beta_{1}}$ = ODDS RATIO of correct response in IMPASSE (vs) CONTROL
-   **Null hypothesis**:$\beta_{impasse} \le 0$ the odds for a correct response does not change, or decreases
-   **Alternative hypothesis:** $\beta_{impasse} \gt 0$ the odds of a correct response increases

##### Fit CONDITION Model

*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-MODEL-Q1ACC
#| warning: false
#| message: false

# MODEL FITTING ::::::::

#: 1 EMPTY MODEL baseline glm model intercept only
m.0 = glm(accuracy ~ 1, data = df, family = "binomial")
# print("EMPTY MODEL")
# summary(m0)

#: 2 CONDITION model
m.C <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
# print("PREDICTOR MODEL")
# summary(m1)

#: 2 TEST SUPERIOR FIT
paste("AIC wth predictor is lower than empty model?", m.0$aic > m.C$aic)
test_lrt(m.0,m.C) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m.0,m.C))$p[2])
# summary(m.C)


#: 4 FACTORIAL model
m.IE <- glm( accuracy ~ IMPLICIT*EXPLICIT, data = df, family = "binomial")
# print("PREDICTOR MODEL")
# summary(m.IE)
# car::Anova(m.IE, type=3)

#: 4 TEST SUPERIOR FIT
paste("AIC with FACTORIAL lower than FLAT CONDITION only model?", m.C$aic > m.IE$aic)
#can't do lrt ... not nested models


```

##### Describe

```{r}
#| label: DESC-MODEL-Q1ACC

#set model
m <- m.IE

# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: 

print("PREDICTOR MODEL [default two-tailed sig test]")
summary(m)

print("SIGNIFIGANCE TEST")
car::Anova(m, type=3)

# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: 

# one-sided (right tail) z test for B COEFFICIENT
#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients

#SANITY CHECK 2-tailed test should match the model output
# tt <- 2*pnorm(summary(m)$coefficients[2:4], lower.tail = F)
# paste("p value for two-tailed test, null B = 0 : ",round(tt,3))
# ot <- pnorm(summary(m)$coefficients[2:4], lower.tail = F)
# paste("BUT we want a one tailed directional, null: B <= 0: ",round(ot,3))
# paste("adjusted confint for directional hypothesis")
# (dcint <- confint(m, level = 0.90)) # get 90% for right side))
# # https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte

#:::::::: INTERPRET COEFFICIENTS

# print("Confidence Interval —- LOG ODDS")
# confint(m1) #not adjusted for 1-tailed
# print("Coefficients —- ODDS RATIOS")
# (e <- cbind( exp(coef(m)), exp(confint(m)))) #exponentiated, not adjusted
# (e <- cbind( exp(coef(m)), exp(dcint))) #exponentiated, adjusted

#TODO INTERACTIONS & ESTIMATED MARGINAL MEANS 
# print("MODEL PREDICTIONS")
# Retrieve predictions as probabilities 
# (for each level of the predictor)
# pred.control <- predict(m,data.frame(pretty_condition="control"),type="response")
#this should match : plogis(intercept coefficient)
# paste("Probability of success in control,", pred.control)
# pred.impasse <- predict(m,data.frame(pretty_condition="impasse"),type="response")
#this should match : plogis(intercept coefficient + predictor coeff)
# paste("Probability of success in impasse,", pred.impasse)
```

##### Inference

##### Visualize

```{r}
#| label: MODEL-VIS-Q1ACC
#| message: false
#| warning : false


## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
plot_model(m, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Estimate | Odds Ratio",
       subtitle = "",
       x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
# result <- model_parameters(m, exponentiate = TRUE, component = "all")
# plot(result)



## | PLOT TESTS
result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)

#ONLY FOR BAYESIAN VERSION
# result <- rope(m)
# plot(result)
# 
# result <- pd(m)
# plot(result)


## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
plot_model(m, type="int",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Prediction | Probability of Accurate Response",
           axis.title = c("Condition","Probability of Accurate Response"))

#PLOT MODEL PREDICTION
plot_model(m, type = "pred")  
plot_model(m, type = "eff")  
  # ylim(0,1) + 
  # labs(
  #   title = "Model Prediction | Probability of Accurate Response",
  #   subtitle = "Impasse increases Probability of Correct Response"
  # )


```

```{r}
#| label: TBL-MODEL-Q1ACC

#SJPLOT | MODEL | TABLE
tab_model(m)

# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m1, "(log odds)" = m1)
# notes = list("* p < 0.05, ** p < 0.01, *** p < 0.001",
#              '$sigma^{2}$ = 3.29" N(subject) = 126 $\tau_{00}$(subject) = 22.22 N(question) = 13 $\tau_{00}$(question) = 0.31'
#                )
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"),
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', 
#              notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex")
# #              # coef_omit = "Intercept",

```

##### Diagnostics

```{r}
#| label: DIAG-MODEL-Q1ACC
#| message: false
#| warning: false

# print("SANITY CHECK REPORTING")
# report::report(m)

#print("MODEL PERFORMANCE")
# performance(m)

print("MODEL DIAGNOSTICS")
check_model(m)
```

## H1B \| Q1 INTERPRETATION STATE

-   **"orthogonal"** \[reference category\] includes orthogonal and satisficing responses ==\> indicates a primarily *orthogonal* state of coordinate system understanding

-   **"other"** includes: blank, reference point, responses that can't be classified (including selecting all datapoints), =\> indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly *not* orthogonal nor triangular

-   **"angular"** includes 'lines connecting' responses as well as responses that include both orthogonal *and* triangular answers =\> indicates some degree of angular/triangular coordinate understanding

-   **"triangular"** includes correct triangular and 'lines connecting' responses as well as responses that include both orthogonal *and* triangular answers =\> indicates some degree of angular/triangular coordinate understanding

+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Does Ss in the impasse condition produce less orthogonal responses on the first question?                                                                                                                                                                                                                                                                 |
+=======================+===========================================================================================================================================================================================================================================================================================================================================================+
| **Hypothesis**        | H1A \| Ss in the IMPASSE condition will have a higher likelihood of producing unknown and and triangle-like response states, relative to orthogonal response states, on the first question                                                                                                                                                                |
+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | -   data: `df_items` where `q == 1`                                                                                                                                                                                                                                                                                                                       |
|                       | -   outcome: `state` ( 4 level factor from 5 level high_interpretation )                                                                                                                                                                                                                                                                                  |
|                       | -   predictor: `condition` \[between-subjects factor\]                                                                                                                                                                                                                                                                                                    |
+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  Multinomial (Logistic) Regression on state predicted by condition                                                                                                                                                                                                                                                                                     |
|                       |                                                                                                                                                                                                                                                                                                                                                           |
|                       | Alternative:                                                                                                                                                                                                                                                                                                                                              |
|                       |                                                                                                                                                                                                                                                                                                                                                           |
|                       | -   Ordinal regression on state; but model doesn't satisfy proportional odds assumption (parallel slopes)                                                                                                                                                                                                                                                 |
|                       | -   Multinomial or Ordinal regression on high_interpretation (5 category interpretation state which distinguishes between uncertain (blank, reference) unclassifiable, triangle-like and true triangular.) There are some cells with zeros, however (no uncertain responses in control) which means the model can't accurately estimate those comparisons |
+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

#### Setup

```{r}
#| label: SETUP-Q1-STATE

df <- df_items %>% filter(q==1) %>% 
  dplyr::select(pretty_condition, IMPLICIT, EXPLICIT, state)

```

#### Describe

```{r}
#| label: VIS-Q1STATE

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
   labs(#y = "Correct Response on Q 1",
       title = "Q1 Interpretation",
       x = "Condition",
       fill = "",
       subtitle="")


#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = IMPLICIT,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~EXPLICIT) +
   labs(#y = "Correct Response on Q 1",
       title = "Q1 Interpretation",
       x = "Condition",
       fill = "",
       subtitle="")

```

```{r}
#| label: DESC-Q1STATE

#::::::::::::DESCRIPTIVES

table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row
      prop.table(margin=2) %>%  #return proportion (of column)
      addmargins(1) #sanity check sum of columns


(t <- table(df$state, df$IMPLICIT, df$EXPLICIT) %>% addmargins(2) %>% #display sum for row
      addmargins(1)) #sanity check sum of columns

```

#### MULTINOMIAL REGRESSION

*Does condition affect the response state of Q1?*

*Fit a logistic regression predicting interpretation state (k=3) by condition(k = 2).*

-   3 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) \[essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing \[reference category\] vs \[this category\])

-   For *each* equation:

    -   $\beta_{0}$ *= Log Odds of \[this category type vs. reference category type) response in CONTROL condition*
    -   $e^{\beta_{0}}$ *= ODDS of \[this category type vs. reference category type\] response in CONTROL condition*
    -   $\beta_{1}$ *=* $\beta_{1impasse}$ *Log Odds (Log OR; change in odds for \[this category\] type response in impasse (vs) control \[log scale\])*
    -   $e^{\beta_{1}}$ *= ODDS RATIO of \[this. vs reference category type\] response in IMPASSE (vs) CONTROL*
    -   Two-tailed NHST *Null hypothesis:* $\beta_{impasse} = 0$ *the odds for \[this category of response vs. reference\] are not different for IMPASSE condition*
    -   *Alternative hypothesis:* $\beta_{impasse} \ne 0$ *the odds of \[this category of response vs. reference\] increases or decreases for IMPASSE condition*

##### Fit CONDITION Model

```{r}
#| label: FIT-Q1-INTERPRETATION

#check reference level 
print("Categories (first is reference)")
levels(df$state)

#FIT EMPTY MODEL
print("EMPTY MODEL")
catm.0 <- multinom(state ~ 1, data = df)
# summary(catm.0)

#FIT PREDICTOR MODEL
print("CONDITION MODEL")
catm.C <- multinom(formula = state ~ pretty_condition, data = df, model = TRUE)
# summary(catm.C)

#COMPARE MODEL FIT
paste("AIC wth CONDITION predictor is lower than empty model?", catm.0$AIC > catm.C$AIC)
test_lrt(catm.0, catm.C)

#FIT PREDICTOR MODEL
print("FACTORIAL MODEL")
catm.IE <- multinom(formula = state ~ IMPLICIT*EXPLICIT, data = df, model = TRUE)
# summary(catm.IE)
# car::Anova(catm.IE)

#COMPARE MODEL FIT
paste("AIC wth FACTORIAL predictor is lower than empty model?", catm.IE$AIC > catm.0$AIC)
test_lrt(catm.0, catm.IE)


```

*AIC in predictor model is less than empty model, and likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.*

##### Describe

```{r}
#| label: DESC-Q1-INTERPRETATION

#set model
m <- catm.IE

#::::::::INTERPRETATION
paste("MODEL SUMMARY")
summary(m)
car::Anova(m, type =3) #always type 3 for ixns 

paste("LOG ODDS")
tidy(m)

paste("ODDS RATIO")
tidy(m, exponentiate = TRUE)

```

##### INFERENCE

##### Visualize

```{r}
#| label: VIS-Q1INTERPRETATION-LAB

#:::::::: PLOT

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m, type = "est",
           vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) +  #manually adjusted for directional test   
  # scale_y_continuous() + #remove to put on log scale x axis 
  # scale_x_discrete(labels=c("control","impasse"))+
  labs(title = "MODEL ESTIMATE  | Q1 Accuracy ~ condition",
       subtitle = "Impasse increases odds of correct response on Q1",
       x = "Condition") + theme_clean()

  
#SJPLOT | MODEL | PROBABILITIES
plot_model(m, type = "int") 
plot_model(m, type="eff", ci.lvl = 0.95) 
# +  ylim(0,1) +
#   labs(title = "MODEL PREDICTION  | Q1 State ~ condition",
#        subtitle = "Impasse increases probability of more accurate response states Q1",
#        x = "Condition") + theme_clean()

#TODO ESTIMAED MARGINALS AND IXN PLOTS 
# https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html


```

```{r}

#SJPLOT | MODEL | TABLE
tab_model(m)

# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m1, "(log odds)" = m1)
# notes = list("* p < 0.05, ** p < 0.01, *** p < 0.001",
#              '$sigma^{2}$ = 3.29" N(subject) = 126 $\tau_{00}$(subject) = 22.22 N(question) = 13 $\tau_{00}$(question) = 0.31'
#                )
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"),
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', 
#              notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex")
# #              # coef_omit = "Intercept",

# modelsummary(mixcat.1, s)
#TODO OUTPUT TABLE 
#https://arelbundock.com/posts/modelsummary_multinomial_logit/


```

##### Diagnostics

```{r}

#EXAMINE PREDICTIONS
#create sample data frame
# test <- data.frame(pretty_condition = c("control", "impasse"))
# pred <- predict(m, newdata = test, "probs")
# paste("Predicted Probability of Being in Each State")
# ( x <- cbind(test, pred))
# 
# print("MODEL PERFORMANCE")
# performance(m)
# DescTools::PseudoR2(m, which = c("McFadden", "CoxSnell", "Nagelkerke"))

```

## EXPLORE specific question

```{r}
# 
# df <- df_items %>% filter(q==10)
# grouped_ggbarstats( data = df, x = accuracy, y = pretty_condition, grouping.var = ospan_split)

```
