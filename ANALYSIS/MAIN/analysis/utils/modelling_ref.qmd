---
subtitle: 'SGCX | Modelling Reference'
---

\newpage

# Modelling Reference {#modelling .unnumbered}

**In this notebook we use data from study SGC3A \[in person, LAB\] to explore different modelling techniques and assess their suitability for the bimodal accuracy distributions.**

```{r}
#| label: SETUP
#| warning : false
#| message : false

library(Hmisc) # %nin% operator
library(jtools) #misc helpers 

library(ggpubr) #arrange plots
library(cowplot) #arrange shift function plots
library(ggformula) #easy graphs
library(vcd) #mosaic plots
library(vcdExtra) #mosaic plots
library(kableExtra) #printing tables 
library(sjPlot) #visualize model coefficients
library(ggeasy) #the way it should be
library(statsExpressions) #expressions to add to plots
library(ggstatsplot) #plots with expressions

#plot model estimates with uncertainty
library(ggdist)
library(broom)
library(modelr)
library(distributional)

#models and performance
library(WRS2) #robust and nonparametric tests 
library(lmerTest) #for CIs in glmer 
library(ggstatsplot) #plots w/ embedded stats
library(report) #easystats reporting
library(see) #easystats visualization
library(performance) #easystats model diagnostics
library(qqplotr) #confint on qq plot
library(gmodels) #contingency table and CHISQR
library(equatiomatic) #extract model equation
library(pscl) #zeroinfl / hurdle models 
library(lme4) #mixed effects models 
library(ggeffects) #visualization log regr models
library(ordinal) #ordinal regression
library(MASS) # polyr ordinal regression
library(brant) #brant test for ordinal regression

library(tidyverse) #ALL THE THINGS

#OUTPUT OPTIONS
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
options(ggplot2.summarise.inform = FALSE)
options(scipen=1, digits=3)

#GRAPH THEMEING
theme_set(theme_minimal()) 

```

```{r}
#| label: IMPORT-DATA
#| warning : false
#| message : false

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
#mbp = "/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
#setwd(mbp)

#IMPORT DATA 
df_items <- read_rds('analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.rds')
df_subjects <- read_rds('analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds')

#PREP DATA 
df_subjects <- df_subjects %>% mutate(
  test_score = item_test_NABS,
  raw_condition = condition,
  condition = pretty_condition
)
  
df_items <- df_items %>% mutate(
  accuracy = as.factor(score_niceABS),
  scaled = recode_factor(score_SCALED, "-1" = "orth", "-0.5"="unknown","0"="uncertain","0.5"="lines","1"="tri"),
  scaled = as.ordered(scaled),
  raw_condition = condition,
  condition = pretty_condition,
  state = recode_factor(score_SCALED, #for ordinal
                         "-1" = "orth-like",
                         "-0.5" = "unknown",
                         "0" = "unknown",
                         "0.5" = "tri-like",
                         "1" = "tri-like"),
  state = as.ordered(state)
)

```

# INDEPENDENT SAMPLES

## INDEPENDENT --- (Continuous)

**Does CONDITION have an effect on TEST PHASE ABSOLUTE SCORE? (# questions) \[in lab\]** *Number of questions correct on test phase of task. (Can also be transformed to proportion or percentage).*

```{r}
#| label: SETUP-IND-TESTSCORE

#::::::::::::SETUP DATA
df = df_subjects %>% filter(mode == "lab-synch")

#::::::::::::DESCRIPTIVES
mosaic::favstats(test_score ~ condition, data = df)

#::::::::::::VISUALIZE DISTRIBUTIONS

# #GGFORMULA | FACETED HISTOGRAM
# stats = df %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))
# gf_props(~item_test_NABS, 
#          fill = ~pretty_condition, data = df) %>% 
#   gf_facet_grid(~pretty_condition) %>% 
#   gf_vline(data = stats, xintercept = ~mean, color = "red") +
#   labs(x = "# Correct",
#        y = "proportion of subjects",
#        title = "Test Phase Absolute Score (# Correct)",
#        subtitle = "") + theme(legend.position = "blank")

##GGPUBR | HIST+DENSITY SCORE 
p <- gghistogram(df, x = "test_score", binwidth = 0.5,
   add = "mean", rug = TRUE,
   fill = "pretty_condition", #, palette = c("#00AFBB", "#E7B800"),
   add_density = TRUE)
facet(p, facet.by=c("condition")) +
  labs( title = "Distribution of TEST Absolute Score",
        subtitle ="Pattern of response is similar across data collection modes but differs by condition",
        x = "Total Absolute Score (Test Phase)", y = "number of subjects") +
  theme_minimal() + theme(legend.position = "blank")

##VERTICAL RAINCLOUD USING GGDISTR
ggplot(df, aes(x = condition, y = test_score,
                        fill = condition) ) + 
  ggdist::stat_halfeye(
    side = "left",
    justification = 1.1,
    width = 1, 
    point_colour = NA
   ) + 
  geom_boxplot(
    inherit.aes = FALSE, #supress fill
    mapping = aes(x=condition, y = test_score),
    width = .15, 
    outlier.shape = NA
  ) + 
  geom_point(
    inherit.aes = FALSE, #supress fill
    mapping = aes(x=condition, y = test_score, color = condition),
    size = 1.3,
    alpha = .3,
    position = position_jitter( 
      seed = 1, width = .05
  )) + labs( 
    title = "Distribution of TEST Absolute Score ",
    x = "Condition", y = "Total Absolute Score (Test Phase)") +
  theme(legend.position = "blank") + 
  coord_cartesian(xlim = c(0.5, NA), clip = "off")


```

### Independent Samples T-Test (Student's T)

-   Tests null hypothesis that true difference in population mean is == 0
-   Assumes normally distributed variables
-   Assumes equal variance of samples (homogeneity of variance)
-   ***OUR DATA VIOLATE BOTH NORMALITY AND HOMOGENEITY OF VARIANCE***

```{r}
#| label: STUDENTS-T

(t <- t.test( test_score ~ condition,data = df,
              paired = FALSE, var.equal = TRUE, alternative = c("two.sided"))) # less, greater for one sided tests
report(t)

```

```{r}
#STATSPLOT | VIOLIN

#one tailed test must be done manually by extracting results expression and adding as subtitle
#default is two tailed test
# results <- two_sample_test( data = df, x = pretty_condition, 
#                             y = item_test_NABS,
#                             alternative = "g")

ggbetweenstats(y = test_score, x = condition, data = df,
               type = "parametric", var.equal = TRUE,
               alternative  = "g",
               pairwide.display = "significant", ) 
# + labs(subtitle = results$expression[[1]])

```

### Independent Samples T-Test (Welch's T)

-   Tests null hypothesis that true difference in population mean is == 0
-   Assumes normally distributed variables
-   **Does not** assumes equal variance of samples (homogeneity of variance)
-   ***OUR DATA VIOLATE BOTH NORMALITY ASSUMPTION***

```{r}
#| label: WELCHES-T

(t <- t.test( test_score ~ condition,data = df,
              paired = FALSE, var.equal = FALSE))
report(t)

```

```{r}
#STATSPLOT | VIOLIN
ggbetweenstats(y = test_score, x = condition, data = df,
               type = "parametric", var.equal = FALSE,
               pairwide.display = "significant", )

```

### Yuen's T-Test (Trimmed Means)

-   **Robust alternative to** to t-test is Yuen's t-test which uses trimmed means
-   Trimmed means are not desireable for this research scenario because they trim data from the extremes, and in this study these are true, interesting values
-   https://garstats.wordpress.com/2017/11/28/trimmed-means/

```{r}
#| label: YUENS-T

(y <- yuenbt( formula = test_score ~ condition, data = df,
              side = TRUE, EQVAR = FALSE))

```

```{r}
#STATSPLOT | VIOLIN
ggbetweenstats(y = test_score, x = condition, data = df,
               type = "robust", var.equal = FALSE,
               pairwide.display = "significant", )

```

### Wilcoxon Rank Sum (Mann-Whitney Test)

-   **Non parametric alternative** to t-test; compares median rather than mean by ranking data
-   Does not assume normality
-   Does not assume equal variance of samples (homogeneity of variance)

```{r}
#| label: WILCOXON RANK SUM

(w <- wilcox.test(df$test_score ~ df$condition,
                 pared = FALSE, alternative = "two.sided")) #less, greater
report(w)


```

```{r}
#STATSPLOT | VIOLIN
ggbetweenstats(y = test_score, x = condition, data = df,
               type = "nonparametric", var.equal = FALSE,
               pairwide.display = "significant", )

```

### Floor/Ceiling Corrections + Wilcoxon Rank Sum

-   The bimodal distribution of the subject-level score data do not meet the requirements for t-tests.
-   However, a non-parametric alternative is available (Wilcoxon rank sum test / Man-Whitney test)
-   Additional corrections are available for data with 'floor' and/or 'ceiling' effects via the 'DACR' package
-   https://link.springer.com/article/10.3758/s13428-020-01407-2#Sec14 see also https://qmliu.shinyapps.io/DACFE/

For comparison we run a standard followed by a Wilcoxon rank-sum (Mann-Whitney) test that is a nonparametric alternative for non-normally distributed data.

```{r}

(t <- wilcox.test(df$test_score ~ df$condition))

```

Next, we calculate the t-test and ANOVA (F-test) based on a series of corrections provided for data with floor and/or ceiling effects.

-   https://link.springer.com/article/10.3758/s13428-020-01407-2#Sec14

Using properties from truncated normal distributions, the authors propose an easy-to-use method for the *t*-test and ANOVA with ceiling/floor data.

*The proposed method calculates the degrees of freedom based on the after-truncation sample sizes (where l = number of floor observations, and r = number of ceiling observations). The rationale was that the proposed method utilizes full information only from data points of n − r − l participants and partial information from data points of r + l participants of a group for the mean and variance estimation. Specifically, the corrected mean and variance estimates (Eqs. 14 and 15) are functions of mean and variance estimates using after-truncation data (n − r − l participants) and the standardized floor and ceiling threshold estimates. The thresholds are estimated using the ceiling and floor percentage estimates based on data points of n − r and n − l participants, respectively. This is a relatively conservative approach for calculating the degrees of freedom, which can help control the type I error rate. This feature can be beneficial, especially given the "replication crisis" in psychological and behavioral research.*

```{r}

#FLOOR-CEILING ADJUSTED T TESTS
library(DACF) #tests for data with floor and ceiling 
# https://www.rdocumentation.org/packages/DACF/versions/1.0.0

#prepare data [vector of scores for each group]
score_111 <- df %>% filter(condition == "control") %>% dplyr::select(test_score) %>% pull()
score_121 <- df %>% filter(condition == "impasse") %>% dplyr::select(test_score) %>% pull()

# recover the mean and variance for ceiling/floor data
a <- rec.mean.var(score_111) %>% unlist()
# recover the mean and variance for ceiling/floor data
b <- rec.mean.var(score_121) %>% unlist()
r <- as.data.frame(rbind("control"=a ,"impasse"=b))
r

# method "a" uses original sample size
# method "b" uses after-truncation sample size

# perform adjusted t test
lw.t.test(score_111,score_121,"b")

#FLOOR-CEILING ADJUSTED F* TEST ANOVA
lw.f.star(df,test_score~condition,"b")
# method "a" uses original sample size
# method "b" uses after-truncation sample size


```

The control condition has 11% of data at ceiling and 71% at floor, with corrected mean of -7 and variance of 207. The impasse condition has 17% at ceiling, and only 42% at floor, with with corrected mean of 2.39 and variance respectively as of 58.

A corrected t-test t statistic is -4.94, p = 0.05. The estimated Cohen's d is -0.89 with a confidence interval 0f \[-14.22, -5.97\].

A correct F-test (ANOVA) has a corrected F-statstic of 5.85, p \< 0.05, and Fsquared effect size of 0.195

### Linear Regression

-   Assumes homogeneity of variance
-   Assumes normally distributed residuals
-   assuming dummy coding... intercept = predicted mean of first group, predictor coefficient = difference to mean of second group

```{r}
#SCORE predicted by CONDITION
lm.1 <- lm(test_score ~ condition, data = df)
paste("Model")
summary(lm.1)
paste("Partition Variance")
anova(lm.1)
paste("Confidence Interval on Parameter Estimates")
confint(lm.1)
report(lm.1) #sanity check
check_model(lm.1)
```

```{r}
#| label: PLOT-MODEL-COEFSS-GGDIST

#MODEL ESTIMATES WITH UNCERTAINTY

#setup references
m <- lm.1
df <- df
call <- m$call %>% as.character()

# uncertainty model visualization
df  %>%
  modelr::data_grid(condition) %>%
  augment(lm.1, newdata = ., se_fit = TRUE) %>%
  ggplot(aes(y = condition, color = condition)) +
  stat_halfeye( scale = .5,
      aes(
        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),
        fill = stat(cut_cdf_qi(cdf,
                .width = c(.90, .95),
                labels = scales::percent_format())))) +
  scale_fill_brewer(direction = -1) +
  labs (title = "(LAB) Test Phase Accuracy ~ Condition",
        x = "model predicted mean (% correct)", y = "Condition", fill = "Interval",
        subtitle = paste("lm(",call[2],")")
  ) + theme(legend.position = "blank")

```

```{r}
#| label: PLOT-MODEL-PRED-SJPLOT

#sjPlot
p1 <- plot_model(lm.1,  type = "pred", 
           show.data = TRUE, jitter = TRUE,
           show.p = TRUE) 

#BS TO MANUALLY ADD LABEL
library(equatiomatic)
library(latex2exp)
(x <- extract_eq(lm.1, use_coefs = TRUE, ital_vars=TRUE, coef_digits = 1, raw_tex = FALSE))
b = TeX(x)
p1[["condition"]][["labels"]][["subtitle"]]= expression( paste(widehat(accuracy), " = ", 1.7, " + ", "1.6", "*", condition[impasse]) )
p1

```

### Censored (Tobit) Regression

https://stats.oarc.ucla.edu/r/dae/tobit-models/

**For censored data (i.e. truncated axis).** The tobit model, also called a censored regression model, is designed to estimate linear relationships between variables when there is either left- or right-censoring in the dependent variable (also known as censoring from below and above, respectively). Censoring from above takes place when cases with a value at or above some threshold, all take on the value of that threshold, so that the true value might be equal to the threshold, but it might also be higher. In the case of censoring from below, values those that fall at or below some threshold are censored.

-   censored vs truncated : There is sometimes confusion about the difference between truncated data and censored data. With censored variables, all of the observations are in the dataset, but we don't know the "true" values of some of them. With truncation some of the observations are not included in the analysis because of the value of the variable. When a variable is censored, regression models for truncated data provide inconsistent estimates of the parameters. See Long (1997, chapter 7) for a more detailed discussion of problems of using regression models for truncated data to analyze censored data.

```{r}

#set censoring values 
lo = 0
hi = 8 
range(df$test_score)
print("Lo and Hi should equate to upper and lower bounds of the # Qs ")

library(VGAM)

#FIT MODEL
m1<- vglm(test_score ~ condition, tobit(Lower = lo, Upper =hi ), data = df)
summary(m1)

#CONFIDENCE INTERVALS
b <- coef(m1)
se <- sqrt(diag(vcov(m1)))
cbind(LL = b - qnorm(0.975) * se, UL = b + qnorm(0.975) * se)

#TEST FIT
#We can test the significance of condition overall by fitting an empty model and using a likelihood ratio test.
m0 <- vglm(test_score ~ 1, tobit(Lower = lo, Upper = hi), data = df)
(p <- pchisq(2 * (logLik(m1) - logLik(m0)), df = 2, lower.tail = FALSE))
paste("P value of likelihood ratio test less than alpha = 0.05? ", p <0.05)
compare_performance(m0,m1)

#DIAGNOSTICS
plot(m1)

df$yhat <- fitted(m1)[,1]
df$rr <- resid(m1, type = "response")
df$rp <- resid(m1, type = "pearson")[,1]

par(mfcol = c(2, 3))

with(df, {
  plot(yhat, rr, main = "Fitted vs Residuals")
  qqnorm(rr)
  plot(yhat, rp, main = "Fitted vs Pearson Residuals")
  qqnorm(rp)
  plot(test_score, rp, main = "Actual vs Pearson Residuals")
  plot(test_score, yhat, main = "Actual vs Fitted")
})

#VARIANCE ACCOUNTED FOR
print("VARIANCE ACCOUNTED FOR")
# correlation
(r <- with(df, cor(yhat, test_score)))
# variance accounted for
r^2
performance(m1)


#NOTE: censReg package also does Tobit regression [including mixed models]
```

-   The coefficient labeled "(Intercept):1" is the intercept or constant for the model.
-   The coefficient labeled "(Intercept):2" is an ancillary statistic. If we exponentiate this value, we get a statistic that is analogous to the square root of the residual variance in OLS regression. logSigma is the variance of the model (logarithmized) (same as the second intercept in the VGAM output)
-   The predicted value of test_phase_score is 5.75 points *higher* for students in the impasse condition than for students in the control condition. (72% improvement in score!)

**Using censReg package** - https://cran.r-project.org/web/packages/censReg/vignettes/censReg.pdf

```{r}
library(censReg) #censored regression

#FIT MODEL
c1 <- censReg( test_score ~ condition, left=lo, right=hi, data = df )
summary(c1)

#CONFIDENCE INTERVALS
b <- coef(c1)
se <- sqrt(diag(vcov(c1)))
cbind(LL = b - qnorm(0.975) * se, UL = b + qnorm(0.975) * se)

#TEST FIT
#We can test the significance of condition overall by fitting an empty model and using a likelihood ratio test.

c0 <- censReg( test_score ~ 1, left=lo, right=hi, data = df )
(p <- pchisq(2 * (logLik(c1) - logLik(c0)), df = 2, lower.tail = FALSE))
paste("P value of likelihood ratio test less than alpha = 0.05? ", p <0.05)


performance(c1)
```

-   logSigma is the variance of the model (logarithmized) (same as the second intercept in the VGAM output)
-   output should match that of VGAM

## TODO FIX INDEPENDENT --- (Count)

### Poisson Regression

https://stats.oarc.ucla.edu/r/dae/poisson-regression/

The outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can consider it a type of *count*, (ie. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function).

```{r}
#POISSON

#SCORE predicted by CONDITION --> POISSON DISTRIBUTION
p.1 <- glm(item_test_NABS ~ pretty_condition, data = df_subjects, family = "poisson")
paste("Model")
summary(p.1)
paste("Partition Variance")
anova(p.1)
paste("Confidence Interval on Parameter Estimates")
confint(p.1)
report(p.1) #sanity check
check_model(p.1)

```

### Zero Inflated Poisson

https://stats.oarc.ucla.edu/r/dae/zip/\
Poisson count process with excess zeros

```{r}
#ZERO INFLATED POISSON

zinfp.1 <- zeroinfl(item_test_NABS ~  item_q1_rt| pretty_condition , data = df_subjects)
summary(zinfp.1)
report(zinfp.1)
performance(zinfp.1)
# check_model(zinfp.1)

```

### Negative Binomial Regression

https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/ - overdispersed count data (variance much greater than mean)

```{r}
#NEGATIVE BIONOMIAL REGRESSION
# - https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/
# - Overdispersed Count variables

library(MASS)

nb.1 <- glm.nb(item_test_NABS ~ pretty_condition, data = df_subjects)
summary(nb.1)
report(nb.1)
check_model(nb.1)

#check model assumption
#assumes conditional means are not equal to conditional variances
#conduct likelihood ration test to compare and test [need poisson]
m3 <- glm(item_test_NABS ~ pretty_condition, family = "poisson", data = df_subjects)
pchisq(2 * (logLik(nb.1) - logLik(m3)), df = 1, lower.tail = FALSE)
#A large (+) log likelihood suggests that the negative binomial is more appropriate than the Poisson model


#EXPONENTIATE PARAMETER ESTIMATES
est <- cbind(Estimate = coef(nb.1), confint(nb.1))
#exponentiate parameter estimates
print("Exponentiated Estimates")
exp(est)
```

The variable condition has a coefficient of 0.67, (p \< 0.005). This means that for the impasse condition, the expected log count \# of questions increases by 0.67. By exponentiating the estimate we see that \# question correct rate for the impasse condition is nearly 2x that of the control condition.

**Diagnostics** ??

### Zero Inflated Negative Binomial Regression

https://stats.oarc.ucla.edu/r/dae/zinb/ count data that are overdispersed and have excess zeros

Zero-inflated negative binomial regression is for modelling count variables with excessive zeros, and especially when the count data are overdispersed (mean is much larger than variance). It can help account for situations where theory suggests that excess zeros are generated by 2 separate processes, one that includes the other count values, and the other that is just the zeros, and thus that the *excess* zeros can be modelled independently.

Total Absolute Score (# items correct) may fit this situation, as the data are overdispersed (variance much greater than the mean) and there are are very large number of zeros. It is theoretically plausible that these excess zeros (no answers correct) are the result of a different 'process' ... (i.e) little understanding and/or resistance to restructuring understanding of the coordinate system. However, I am not certain if it is plausible to suggest that the zeros themselves are the result of two different processes: (ie. perhaps trying to understand, and not trying to understand?) \<- this could maybe be disentangled by first question latency?

The model includes: - A logistic model to model which of the two processes the zero outcome is associated with - A negative binomial model to model the count process

```{r}
#| label: MODEL-TESTABS-ZINFNEGBINOM

library(pscl) #  for zeroinfl negbinomial

#ZERO INFLATED NEGATIVE BINOMIAL
zinb.1 <- zeroinfl(item_test_NABS ~ pretty_condition | pretty_condition , data = df_subjects, dist = "negbin")
#before the | is the count part, after the | is the logit model
paste("Model")
summary(zinb.1)
report(zinb.1)
performance(zinb.1)

#   rootogram(zinb.1)



# #EXPONENTIATE PARAMETER ESTIMATES
# est <- cbind(Estimate = coef(zinb.1), confint(zinb.1))
# #exponentiate parameter estimates
# print("Exponentiated Estimates")
# exp(est)

```

In the count model, the coefficient for the condition is very small, and not significant (suggesting it does not contribute to the count yielding process?).

In the zero-inflation model, the coefficient for the condition variable is -1.056 and statistically significant. This suggests that the log odds of being an excessive zero decrease by 1.06 if you are in the impasse condition (exponentiate it?)

**TODO come back to this and discuss further**\

## TODO FIX INDEPENDENT --- (Mixture/HURDLE) REGRESSION

### WIP \| HURDLE MODEL

-   https://data.library.virginia.edu/getting-started-with-hurdle-models/\
-   https://en.wikipedia.org/wiki/Hurdle_model#:\~:text=A%20hurdle%20model%20is%20a,of%20the%20non%2Dzero%20values.

class of models for count data with both overdispersion and excess zeros;\
different from zero-inflated models where the excess zeros are theorized to arise from two different processes; in the hurdle model, there is a model for P(x=0) and a separate model for P(x!=0)

The model includes: - A binary logit model to model whether the observation takes a positive count or not. - a truncated Poisson or Negative binomial model that only fits positive counts

This allows us to model: (1) Does the student get *any* questions right? (2) How many questions does the student get right?

```{r}
library(pscl) #zero-inf and hurdle models 
library(countreg) #rootogram
#install.packages("countreg", repos="http://R-Forge.R-project.org")

#SYNTAX OUTCOME ~ count model predictor | hurdle predictor

h.1 <- pscl::hurdle(item_test_NABS ~ condition | condition , data = df_subjects,
              zero.dist = "binomial", dist = "poisson", size = 8)

h.2 <- pscl::hurdle(item_test_NABS ~ condition | condition , data = df_subjects,
              zero.dist = "binomial", dist = "negbin", size = 8)

summary(h.1)
summary(h.2)


rootogram(h.1)
rootogram(h.2)
compare_performance(h.1,h.2)


```

### Beta Regression (% Correct)

Beta regression on % correct (with standard transformation for including \[0,1\]) https://stats.stackexchange.com/questions/63350/how-to-interpret-the-coefficients-from-a-beta-regression

```{r}
# 
library(betareg)

#RESCLAE VARIABLE
#beta reg can't handle 0s and 1s 
sub <- df_subjects %>% dplyr::select(condition, DV_percent_NABS)
n = nrow(sub) %>% unlist()
sub$dv_transformed = (sub$DV_percent_NABS * (n-1) + 0.5)/n
 
#VISUALIZE VARIABLES
histogram(sub$dv_transformed)
gf_histogram(~dv_transformed, fill = ~condition, data = sub) %>% gf_facet_wrap(~condition)

#FIT MODEL
mb <- betareg(dv_transformed ~ condition, data = sub)
summary(mb)
plot(mb)

```

### BETA HURDLE Regression

https://github.com/markhwhiteii/beta_hurdle/blob/master/manuscript/beta_hurdle.pdf

```{r}
library(gamlss)

#CREATE SAMPLE DATA 
n <- 5000 
mu <- 0.40 
sigma <- 0.60 
p0 <- 0.13 
p1 <- 0.17 
p2 <- 1- p0- p1
a <- mu * (1- sigma ^ 2) / (sigma ^ 2) 
b <- a * (1- mu) / mu

#CREATE DIST
set.seed(1839) 
y <- rbeta(n, a, b) 
cat <- sample(1:3, n, prob = c(p0, p2, p1), replace = TRUE) 
y[cat == 1] <- 0 
y[cat == 3] <- 1

#VISUALIZE DISTRIBUTION
x <- as.data.frame(y)
gf_histogram(~x$y)
#this looks not unlike my distribution! 

#CREATE AN EMPTY MODEL
fit <- gamlss( formula = y ~ 1, # formula for mu 
               formula.sigma = ~ 1, # formula for sigma 
               formula.nu = ~ 1, # formula for nu 
               formula.tau = ~ 1, # formula for tau 
               family = BEINF() )

summary(fit)
plot(fit)

#TRANSFORM PARAMETRS BACK 
inv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function
fit_mu <- inv_logit(fit$mu.coefficients) 
paste("MU: ",fit_mu)
fit_sigma <- inv_logit(fit$sigma.coefficients) 
paste("SIGMA: ",fit_sigma)
fit_nu <- exp(fit$nu.coefficients) 
fit_tau <- exp(fit$tau.coefficients) 
fit_p0 <- fit_nu / (1 + fit_nu + fit_tau) 
paste("P0: ",fit_p0)
fit_p1 <- fit_tau / (1 + fit_nu + fit_tau)
paste("P1: ",fit_p1)

```

**BETA HURDLE INTERPRETATION** - beta component\
- MU "location" (mean)\
- SIGMA "scale" (positively related to variance; variance = sigma.squared *mean* (1-mean)\
- Rigby, Stasinopoulos, Heller, and De Bastiani (2017) "reparameterized" the beta distribution so that the two parameters determining the shape of the distribution would be more useful in a regression framework (see Ferrari & Cribari-Neto, 2004 for a different parameterization)

**ZERO-ONE HURDLE COMPONENT**\
- The two additional parameters, ν NU and τTAU , are related to p0 and p1, respectively.\
- p0 is the probability that a case equals 0,\
- p1 is the probability that a case equals 1,\
- p2 (i.e., 1 −p0 −p1) is the probability that the case comes from the beta distribution

```{r}


#SETUP DATA 

min = 0 #min possible value of scale
max = 8 #max possible value of scale

library(mosaic) #for shuffling
#1. Rescale accuracy using 
# recommended adjustment 
#rescaled = value-min/(max-min)
df <- df_subjects %>% mutate(
  accuracy = item_test_NABS,
  R_acc = (accuracy-min)/(max-min), #as %
  T_acc = (accuracy * (nrow(df)-1) + 0.5)/nrow(df)/8, #transform for no 0 and 1
  perm = shuffle(condition),
  scaffold_rt = item_scaffold_rt
) %>% dplyr::select(accuracy,R_acc, T_acc, condition, perm,scaffold_rt)

#VISUALIZE DISTRIBUTION
gf_histogram(~R_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = "Histogram of accuracy")

#VISUALIZE DISTRIBUTION
gf_histogram(~T_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = "Histogram of [rescaled] accuracy")

gf_histogram(~R_acc, fill = ~perm, data = df) %>% gf_facet_wrap(~perm) + labs(title = "Histogram of shuffled accuracy")

#SUMMARIZE SAMPLE
paste("Grand mean", mean(df$R_acc))

library(mosaic)
stats = favstats(df$R_acc ~ df$condition)
stats$mean <- mean(df$R_acc ~ df$condition)
stats$var <- var(df$R_acc ~ df$condition)
print("Grand stats")
stats 
print("P0")
nrow(df %>% filter(R_acc ==0))/nrow(df)
print("P1")
nrow(df %>% filter(R_acc ==1))/nrow(df)

#CREATE MODEL

#CREATE AN EMPTY MODEL
m0 <- gamlss( formula = R_acc ~ 1, # formula for mu 
              formula.sigma =  ~ 1, # formula for sigma 
              formula.nu =  ~ 1, # formula for nu 
              formula.tau =  ~ 1, # formula for tau 
              family = BEINF(), data = df )

m0 <- gamlss(R_acc ~ 1, ~ 1, ~ 1, ~ 1, 
            data = df, family = BEINF())
summary(m0)
plot(m0)

#TRANSFORM PARAMETRS BACK 
inv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function
m0_mu <- inv_logit(m0$mu.coefficients) 
paste("MU: ",m0_mu)
m0_sigma <- inv_logit(m0$sigma.coefficients) 
paste("SIGMA: ",m0_sigma)
m0_nu <- exp(m0$nu.coefficients) 
paste("NU: ",m0_nu)
m0_tau <- exp(m0$tau.coefficients) 
paste("TAU: ",m0_tau)
m0_p0 <- fit_nu / (1 + fit_nu + fit_tau) 
paste("P0: ",m0_p0)
m0_p1 <- fit_tau / (1 + fit_nu + fit_tau)
paste("P1: ",m0_p1)



#CREATE PREDICTOR MODEL
m1 <- gamlss(R_acc ~ condition, ~ condition, ~ condition, ~ condition, 
            data = df, family = BEINF())
summary(m1)

#LOOKING PREDICTOR MODEL
m <- gamlss(R_acc ~ condition , 
            ~ condition , 
            ~ condition , 
            ~ condition , 
            data = df, family = BEINF())
summary(m)


#CREATE PREDICTOR MODEL ON SHUFFLED [PERMUTATION TEST]
mperm <- gamlss(R_acc ~ perm, ~ perm, ~ perm, ~ perm, 
            data = df, family = BEINF())
summary(mperm)

#sanity check with scaled outcome, no zeros ones
m3 <- gamlss(T_acc ~ condition, ~ condition, ~ condition, ~ condition, 
            data = df, family = BEINF())
summary(m3)
#m3 shouldn't show condition as significant for nu and tau, because T_acc was scaled to not include any 0s and 1s

#investigate beta negative binomial distribution
#https://en.wikipedia.org/wiki/Beta_negative_binomial_distribution

#TRANSFORM PARAMETRS BACK 
inv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function
m1_mu <- inv_logit(m1$mu.coefficients) 
paste("MU: ",m1_mu)
m1_sigma <- inv_logit(m0$sigma.coefficients) 
paste("SIGMA: ",m1_sigma)
m1_nu <- exp(m1$nu.coefficients) 
paste("NU: ",m1_nu)
m1_tau <- exp(m1$tau.coefficients) 
paste("TAU: ",m1_tau)

summary(m)
plot(m)


```

-   MU tells if mean is different by condition\
-   SIGMA tells if variance is different by condition\
-   NU coefficient tells if condition yields different probability at floor
-   TAU coefficient tells if condition yields different probability at ceiling

## TODO RETROFIT INDEPENDENT --- (Binomial) REGRESSION

```{r}
#| label: SETUP-Q1-ACCURACY

#PREPARE DATA 
df <- df_items %>% filter(q ==1) %>% filter(mode == "lab-synch")
# %>% mutate(
#   accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct"),
#   scaled = as.ordered(score_SCALED),
#   q = as.factor(q),
#   high_interpretation = as.factor(high_interpretation)
# )

#GROUPED PROPORTIONAL BAR CHART
gf_props(~accuracy, fill = ~pretty_condition, x =~pretty_condition,
       position = position_dodge(), data = df) %>%
  gf_facet_grid(~pretty_mode) +
   labs(x = "Question 1 Accuracy",
       title = "Accuracy on Q1",
       subtitle="")

#STACKED PROPORTIONAL BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
   labs(#y = "",
       title = "Accuracy on Test Phase",
       x = "Condition",
       fill = "",
       subtitle="Impasse Condition yields a greater proportion of correct responses")

```

### CHI SQUARE

**Does CONDITION affect Q1 ACCURACY?**

```{r}

CrossTable( x = df$condition, y = df$accuracy, 
             fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)
```

```{r}
#| label : VIS-CHISQR-Q1

#MOSAIC PLOT
#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis
mosaicplot(main="Accuracy on First Question by Condition",
            data = df, pretty_condition ~ accuracy, 
            shade = T)
#SJTABLE
df %>%
  sjtab(fun = "xtab", var.labels=c("accuracy", "pretty_condition"),
        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,
        statistics = "fisher")

#::::::::::::WITH STATISTICS
ggbarstats(data = df, x = condition, y = accuracy,
           type = "nonparametric")
```

**For (In Person) data collection** (n=126) the Pearson's Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition that is not significant at the alpha level 0.05, $\chi^2$ (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. In this particular data sample, the odds ratio (Odds Ratio = 2.18, p = 0.055, 95% CI \[0.982, +Inf\]) indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition .

### LOGISTIC REGRESSION

*Fit a logistic regression (at the subject-item level), predicting Q1 accuracy (absolute score) by condition.*\
*note: this example uses the combined dataset rather than lab-only, as learning notes were done with the combined and I don't want to recalcualte all the marginal probabilities by hand for learning purposes.*

#### Fit Model

*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-Q1ACC-LOG-combined
#| warning: false
#| message: false

#combined dataset, not lab only
df <- df_items %>% filter(q==1) %>% mutate(
  accuracy = as.factor(score_niceABS)
)

# FREQUENCY TABLE
# my.table <- table(df$accuracy, df$pretty_condition)
# addmargins(my.table) #counts
# addmargins(prop.table(my.table)) #props

# MODEL FITTING:::::::::::::::::::::::::::::::::::::

#: 1 EMPTY MODEL baseline glm model intercept only
m0 = glm(accuracy ~ 1, data = df, family = "binomial")
print("EMPTY MODEL")
summary(m0)

#: 2 CONDITION model
m1 <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
print("PREDICTOR MODEL")
summary(m1)

#: 3 TEST SUPERIOR FIT
paste("AIC wth predictor is lower than empty model?", m0$aic > m1$aic)
test_lrt(m0,m1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,m1))$p[2])
```

*The Condition predictor significantly improves model fit.*

#### Learning Notes

```{r}
#| label: MODEL-Q1ACC-LOG-combined

# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: 

print("PREDICTOR MODEL")
summary(m1)

#: INTERPRET COEFFICIENTS

print("Coefficients —- LOG ODDS")
confint(m1)
print("Coefficients —- ODDS RATIOS")
e <- cbind( exp(coef(m1)), exp(confint(m1))) #exponentiate
e
```

**Understanding the logistic regression model**

*The logistic regression intercept gives the log odds of the outcome for the reference level of the predictor variable*

*The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.*

**\[the empty model**

-   The intercept of an empty model (glm(accuracy \~ 1) is equal to log(p/(1-p)), where p = the overall probability of a correct response (df\$accuracy ==1 ).
-   In SGC3A Q1 accuracy this = 71 correct / 330 = 0.215 -\> log(0.215 / (1-0.215)) = -1.29.
-   In other words, the intercept from the model with no predictor variables is the estimated log odds of a correct response for the whole sample.
-   We can also transform the log of the odds back to a probability: p = ODDS/ (1+ODDS) = exp(-1.29)/(1+exp(-1.29)) = 0.215. This should matched the prediction of the empty model

**\[a dichotomous predictor\]**

natural log (odds of +) = -1.822 + 0.901(x1) ; x1 = 0 for control, 1 for impasse

-   INTERCEPT: log odds of (+ response) in control condition
    -   log odds of (+) in control : -1.822 + 0.9(0) = -1.822
    -   convert to odds by exponentiating the coefficients\
        log odds of (+) in control = exp(-1.822) = 0.162 odds
    -   convert to probability by formula =\>\
        p(+) = odds / (1+odds) = 0.162 / (1 + 0.162) = 0.139\
        probability of (+) in control = \~14%
-   B1 COEFFICIENT: DIFFERENCE in log odds of (+) in impasse vs. control
    -   log odds of (+) in impasse: -1.822 + 0.901 = -0.921
    -   convert to odds by exponentiating log odds\
        log odds (+) in impasse = exp(-0.921) = 0.398
    -   convert to probability by formula =\>\
        p(+) = odds / (1 + odds) = 0.398 / (1+0.398) = 0.285\
        probaility of (+) in impasse = \~ 29%
-   ODDS RATIO : exponentiated B1 COEFFICIENT
    -   B1 = (slope of logit model = difference in log odds = log odds ratio

    -   B1 = 0.901 is log odds ratio of (+) in impasse vs control

    -   exp(b1) = exp(0.901) = 2.46

    -   Ratio of odds in impasse are 2.46 times higher than in control. Bein in the impasse condition yields odds athat are 2.46 X higher than in control.

+:----------------------------------------------------------------------+
| MARGINAL\                                                             |
| total = 330 success : 71, failure : 259\                              |
| p(+) = 71 / 330 = 0.215 = 22%\                                        |
| odds(+) = 71 / 259 = 0.274                                            |
+-----------------------------------------------------------------------+
| CONTROL total = 158 success = 22; failure = 136\                      |
| p(+) = 22/158 = 0.139 = 14%\                                          |
| odds(+) = 22/136 = 0.162                                              |
+-----------------------------------------------------------------------+
| IMPASSE total = 172 success = 49; failure = 123\                      |
| p(+) = 49/172 = 0.285 = 29%\                                          |
| odds(+) = 49/123 = 0.398                                              |
+-----------------------------------------------------------------------+

#### Visualize

```{r}
print("MODEL PERFORMANCE")
performance(m1)
print("SANITY CHECK REPORTING")
report(m1)

print("MODEL PREDICTIONS")
# Retrieve predictions as probabilities 
# (for each level of the predictor)
p.control <- predict(m1,data.frame(pretty_condition="control"),type="response")
paste("Probability of success in control,", p.control)
p.impasse <- predict(m1,data.frame(pretty_condition="impasse"),type="response")
paste("Probability of success in impasse,", p.impasse)

#: PLOT

#GGSTATS | MODEL | LOG ODDS 
# library(ggstatsplot)
# ggcoefstats(m1, output = "plot") + labs(x = "Log Odds Estimate")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m1, type="std2", vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE) +  
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
# plot_model(m1, type="pred",
#            show.intercept = TRUE, 
#            show.values = TRUE,
#            title = "Model Predicted Probability of Accuracy",
#            axis.title = c("Condition","Probability of Accurate Response"))

#GGEFFECTS | MODEL | PROBABILITIES
# library(ggeffects)
ggeffect(model = m1) %>% plot()


#SANITY CHECK SJPLOT
# library(effects)
# plot(allEffects(m))

```

#### Diagnostics

```{r}
check_model(m1)
binned_residuals(m1)
```

#### Inference

We fit a logistic regression model to analyze the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 3.16, p = 0.0016). The model predicts that the odds of a correct response on the first question in the *impasse condition* increase by 146% ($e^{beta_1}$ = 2.46, 95% CI \[1.42, 4.37\]) over the *control condition*.

*Equivalent statements:*

-   being in impasse condition increases log odds of correct response by 0.901 (over control)
-   being in impasse increases odds of correct response in impasse over control increases by factor of 2.46
-   probability of correct response in control predicted as 28.5%, vs only 14% in control condition

```{r}
#PRETTY TABLE SJPLOT
tab_model(m1)
```

## INDEPENDENT --- (Ordinal) REGRESSION

**Does CONDITION affect the Q1 [ordered] type of response given?**

```{r}
#| label : SETUP-INDEPENDENT-ORDINAL

#SETUP DATA
df <- df_items %>% filter(q==1)  %>% filter(mode == "lab-synch") 
#scaled has already been set as an ordered factor of score_SCALED

#::::::::::::DESCRIPTIVES
mosaic::favstats(scaled ~ condition, data = df)

#::::::::::::VISUALIZE DISTRIBUTIONS
gf_props(~scaled, fill= ~condition, data = df) %>% 
  gf_facet_grid(condition ~ .) + easy_remove_legend()

#withstats 
ggbarstats(
  data = df,
  x = condition,
  y = scaled,
  results.subtitle = FALSE,
  subtitle = "(misleading, because bars are scaled to 100%)"
)
```
**Does CONDITION affect the Q1 [ordered] state of understanding?**

```{r}
#| label : SETUP-INDEPENDENT-ORDINAL-STATE

#SETUP DATA
df <- df_items %>% filter(q==1)  %>% filter(mode == "lab-synch") 
#scaled has already been set as an ordered factor of score_SCALED

#::::::::::::DESCRIPTIVES
mosaic::favstats(state ~ condition, data = df)

#::::::::::::VISUALIZE DISTRIBUTIONS
gf_props(~state, fill= ~condition, data = df) %>% 
  gf_facet_grid(condition ~ .) + easy_remove_legend()

#withstats 
ggbarstats(
  data = df,
  x = condition,
  y = state,
  results.subtitle = FALSE,
  subtitle = "(misleading, because bars are scaled to 100%)"
)
```

### ORDINAL REGRESSION --- Cumulative Link; Proportional Odds 

*Fit an ordinal logistic regression (at the subject level), predicting Q1 interpretation by condition.*

-   https://stats.oarc.ucla.edu/r/faq/ologit-coefficients/
-   https://journals.sagepub.com/doi/full/10.1177/2515245918823199
-   todo see ordinal regression video: https://www.youtube.com/watch?v=rPcMcW25PPA&ab_channel=NCRMUK
-   https://peopleanalytics-regression-book.org/ord-reg.html
-   https://medium.com/evangelinelee/brant-test-for-proportional-odds-in-r-b0b373a93aa2
-   https://github.com/runehaubo/ordinal/blob/master/old_vignettes/clm_tutorial.pdf

**Learning Notes** - proportional odds regression models effectively act as a series of stratified binomial models under the assumption that the 'slope' of the logistic function of each stratified model is the same. - thus need to verify proportional odds assumption - ordinal regression requires an **proportional odds assumption** (the same slope holds for each equation) - this is required because the model simultaneously estimates k-1 equations, but each equation has the same *slope*, with different intercepts.\
- conversely, a multinomial (categorical) model will have different slopes as well as intercepts - the intercepts are always ordered in size alpha 1 \< alpha 2 \< alpha k-1...

**TODO** - see difference between the three types of orindal models TOODO check Agresti book

#### Fit Model

```{r}

#::::::::::::ORDINAL REGRESSION MODELS

#EMPTY MODEL
paste("EMPTY Ordinal regression of q1 SCALED score (ordered interpretation)")
om.0 <- clm(scaled ~ 1 , data = df)
# summary(om.0)

#PREDICTOR MODEL
paste("Ordinal regression of q1 SCALED score (ordered interpretation)")
om <- clm(scaled ~ condition, data = df)
# summary(om)

#COMPARE EMPTY AND PREDICTOR
test_lrt(om.0, om)


#::::::::: EQUIVALENT APPROACH USING POLYR 

# #MODEL
m <- polr(state ~ condition , data = df, Hess=TRUE)
# summary(m)

#exponentiate coefficients and CIs
# (ctable <- coef(summary(m)))
# (p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2)
# (ctable <- cbind(ctable, "p value" = p))
# (ci <- confint(m))
# (e <- coef(m))

```

*Likelihood ratio test suggests the predictor model is a better fit than the empty (intercept only) model.*

#### Inference

```{r}

paste("SUMMARY")
summary(om)

#LOG ODDS
paste("IN LOG ODDS")
(ctable <- coef(summary(om)))
(ci <- confint(om)) 

paste("IN ODDS RATIOS")
#ODDS RATIOS
exp(coef(om))
exp(ci)

```

**Overall, participants in the impasse condition had higher odds (6.41 X as likely) to offer *more correct* interpretations than those in the control condition (z = 4.65, p \< 0.001).**

-   we see the estimates for the 4 intercepts, which are sometimes called cutpoints.
-   The intercepts indicate where the latent variable is cut to make the three groups that we observe in our data.
-   Note that this latent variable is continuous. In general, these are not used in the interpretation of the results.
-   The cutpoints are closely related to thresholds, which are reported by other statistical packages.
-   for k groups there will be k-1 intercepts (cutpoints)
-   confirm that the CI does not include 0 (the units are ordered logits \[ordered log odds\])
-   as with logistic regression we exponentiate the coefficients and confints to get odds ratio

#### Visualize Model

```{r}
# sjPlot::tab_model(om)
sjPlot::plot_model(om)
sjPlot::plot_model(om, type = "eff")
                   # show.data = TRUE, jitter = TRUE)

```

#### Diagnostics

```{r}

#:: ASSESS FIT
performance(om)

# #test proporitional odds assumption 
brant(m) #only works for polyr type model not clm type model
# # A p-value of less than 0.05 on this test—particularly on the Omnibus plus at least one of the variables—should be interpreted as a failure of the proportional odds assumption.

#test proportional odds assumption
library(pomcheckr)
# https://cran.r-project.org/web/packages/pomcheckr/pomcheckr.pdf
(p <- pomcheck( scaled ~ condition , data = df))
plot(p)

```

**The output of the graphical test for proportional odds assumption suggests that the proportional odds assumption may be unreasonable for this dataset. Also, inspecting the output table, we see the coefficients for each level of the scaled variable are quite different.**

Thus, an alternative approach may be more appropriate:

-   Baseline logistic model. This model is the same as the multinomial regression model covered in the previous chapter, using the lowest ordinal value as the reference.

-   Adjacent-category logistic model. This model compares each level of the ordinal variable to the next highest level, and it is a constrained version of the baseline logistic model. The brglm2 package in R offers a function bracl() for calculating an adjacent category logistic model.

-   Continuation-ratio logistic model. This model compares each level of the ordinal variable to all lower levels. This can be modeled using binary logistic regression techniques, but new variables need to be constructed from the data set to allow this. The R package rms has a function cr.setup() which is a utility for preparing an outcome variable for a continuation ratio model.

*Note: for multiple regression, the ordinal package offers a parameter (nominal = ~predictors) that allow you to designate some predictors as nominal rather than ordinal. But this is not appropriate for this use case._

_The ordinal package offers a parameter to set certain predictors as nominal)



## INDEPENDENT --- (Categorical) REGRESSION

### CHI SQUARE

**Does CONDITION affect the Q1 [categorical] type of response given?**

```{r}
#| label : SETUP-INDEPENDENT-CATEGORICAL

#SETUP DATA
df <- df_items %>% filter(q==1) #%>% filter(mode == "lab-synch") 
#high_interpretation has already been set as a factor

#::::::::::::DESCRIPTIVES
mosaic::favstats(interpretation ~ condition, data = df)

#::::::::::::VISUALIZE DISTRIBUTIONS
gf_props(~scaled, fill= ~condition, data = df) %>% 
  gf_facet_grid(condition ~ .) + easy_remove_legend()

#withstats 
ggbarstats(
  data = df,
  x = condition,
  y = scaled,
  results.subtitle = FALSE,
  subtitle = "(misleading, because bars are scaled to 100%)"
)
```
\*\* Does CONDITION affect Q1 INTERPRETATION?\*\*

```{r}
#| label : SETUP-INDEPENDENT-ORDINAL

#SETUP DATA
df <- df_items %>% filter(q==1) %>% filter(mode == "lab-synch") 
#scaled has already been set as an ordered factor of score_SCALED

#::::::::::::DESCRIPTIVES
mosaic::favstats(scaled ~ condition, data = df)

#::::::::::::VISUALIZE DISTRIBUTIONS
gf_props(~scaled, fill= ~condition, data = df) %>% 
  gf_facet_grid(condition ~ .) + easy_remove_legend()

#withstats 
ggbarstats(
  data = df,
  x = condition,
  y = scaled,
  results.subtitle = FALSE,
  subtitle = "(misleading, because bars are scaled to 100%)"
)
```

```{r}
#| label : CHISQR-interpreation

#MOSAIC PLOT
#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis
mosaicplot(main="Response Type on First Question by Condition",
            data = df, pretty_condition ~ high_interpretation, 
            shade = T)

CrossTable( x = df$pretty_condition, y = df$high_interpretation, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)


df %>%
  sjtab(fun = "xtab", var.labels=c("high_interpretation", "pretty_condition"),
        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,
        statistics = "fisher")
```

**TODO INFERENCE**

### TODO WIP MULTINOMIAL REGRESSION \[INTERPRETATION\]

\*\*But what does affect of condition really mean relative to base rate of the reference category??

*Does condition affect the type of response given on Q1?*

-   <https://peopleanalytics-regression-book.org/multinomial-logistic-regression-for-nominal-category-outcomes.html#running-a-multinomial-regression-model>
-   https://www.youtube.com/watch?v=JcCBIPqcwFo&list=PLzv58M2GAfm50X_Twskr1aXaV5qMuIszx&ab_channel=NCRMUK


#### Fit Model

```{r}
library(nnet)
df <- df_items %>% filter(q==1)
#check reference level 
levels(df$high_interpretation)

#FIT MODEL
mm <- multinom(formula = high_interpretation ~ condition, data = df)
summary(mm)

# calculate z-statistics of coefficients
z_stats <- summary(mm)$coefficients/summary(mm)$standard.errors
# convert to p-values
p_values <- (1 - pnorm(abs(z_stats)))*2
# display p-values in transposed data frame
p_values <- data.frame(p = (p_values))
# display odds ratios in transposed data frame
paste("ODDS RATIOS")
odds_ratios <- data.frame(OR = exp(summary(mm)$coefficients))

options(scipen = 5)
(results <- cbind(odds_ratios, p_values))
```

#### Inference

Being in the control condition does not change the odds of a negative transition or neutral category over the orthogonal. But it does increase the odds:

-   of giving a positive transition response (vs) orthogonal by 17.2 (?%)
-   of giving a triangular response (vs) orthogonal by 6.38 (?%)

#### Visualize

```{r}
plot_model(mm, type = "pred")
plot_model(mm)
```

#### Diagnostics

```{r}

performance(mm)
library(generalhoslem)
logitgof(df$high_interpretation, exp = mm$fitted.values, ord=FALSE)

```

# MIXED (Repeated Measures)

```{r}
#| label: SETUP-TEST-ACC-ITEM

#PREPARE DATA 
n_items = 8 #number of items in test

#item level
df = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct"),
  q = as.factor(q)
)

#FACETED HISTOGRAM
stats = df_subjects %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))
gf_props(~item_test_NABS, 
         fill = ~pretty_condition, data = df_subjects) %>% 
  gf_facet_grid(pretty_condition ~ pretty_mode) %>% 
  gf_vline(data = stats, xintercept = ~mean, color = "red") +
  labs(x = "# Correct",
       y = "proportion of subjects",
       title = "Test Phase Absolute Score (# Correct)",
       subtitle = "") + theme(legend.position = "blank")

#GROUPED PROPORTIONAL BAR CHART
gf_props(~accuracy, fill = ~pretty_condition, x =~pretty_condition,
       position = position_dodge(), data = df) %>%
  gf_facet_grid(~pretty_mode) +
   labs(x = "Correct Responses in Test Phase",
       title = "Accuracy on Task by Condition",
       subtitle="Impasse Condition yields a greater proportion of correct responses")

#STACKED PROPORTIONAL BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~pretty_mode) + 
   labs(#y = "",
       title = "Accuracy on Test Phase",
       x = "Condition",
       fill = "",
       subtitle="Impasse Condition yields a greater proportion of correct responses")

```

## MIXED --- CONTINUOUS

## MIXED --- BINOMIAL

## MIXED --- ORDINAL

## MIXED --- CATEGORICAL

# ADVANCED (Other Distributions)

## SINGLE ITEM LEVEL --- INTERPRETATION

## REPEATED ITEM LEVEL --- ACCURACY

**Test Phase Accuracy (absolute score)**

#### Mixed Logistic Regression

*Fit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on test phase questions by condition; accounting for random effects of subject.*

##### Fit Model

```{r}

#SETUP DATA 
#PREPARE DATA 
n_items = 8 #number of items in test

#item level
df_test = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(
  accuracy = as.factor(score_niceABS),
  q = as.factor(q)
)

df <- df_test

library(lmerTest) #for CIs in glmer 

## 1 | SETUP RANDOM EFFECT

#:: EMPTY MODEL (baseline, no random effect)
m0 = glm(accuracy ~ 1, family = "binomial", data = df) 

#:: RANDOM INTERCEPT SUBJECT
mm.rS <- glmer(accuracy ~ (1|subject), data = df,family = "binomial")

# :: TEST random effect
paste("AIC with random effect is lower than glm empty model?", m0$aic > AIC(logLik(mm.rS)))
test_lrt(m0,mm.rS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,mm.rS))$p[2])

## 2 | ADD FIXED EFFECT

# SUBJECT INTERCEPT | FIXED CONDITION 
mm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject), 
                data = df,family = "binomial")

# :: TEST fixed factor 
paste("AIC with random effect is lower than glm empty model?", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )
test_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rS,mm.CrS))$p[2])
```

##### Visualize

```{r}

#: PRINT MODEL 
print("PREDICTOR MODEL")
summary(mm.CrS)

#: INTERPRET COEFFICIENTS

print("MODEL PERFORMANCE")
performance(mm.CrS)
print("SANITY CHECK REPORTING")
report(mm.CrS)


#: PLOT

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(mm.CrS, type="std2", vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE) +  
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(mm.CrS, type="pred",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Predicted Probability of Accuracy",
           axis.title = c("Condition","Probability of Accurate Response"))

#GGEFFECTS | MODEL | PROBABILITIES
# library(ggeffects)
# ggeffect(model = mm.CrS) %>% plot()

#SANITY CHECK SJPLOT
# library(effects)
# plot(allEffects(mm.CrS))

```

##### Diagnostics

```{r}
check_model(mm.CrS)
binned_residuals(mm.CrS)
```

##### Inference

We fit a mixed-effect binomial logistic regression model with random intercepts for subjects to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(3): 4.98, p \< 0.05). Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 5 over the control condition $e^{\beta_1}$ = 5.11, 95% CI \[1.17,22,36\], p \< 0.05.

```{r}
# PRETTY TABLE SJPLOT
tab_model(mm.CrS)
```

#### TODO Mixed Ordinal Regression

## WIP UNKNOWN

### Cummulative Ordinal (Bayesian)

https://journals.sagepub.com/doi/full/10.1177/2515245918823199

```{r}
# library(brms)


# #DEFINE DATA 
# df <- df_items %>% mutate(
#   scaled = factor(score_SCALED, ordered = TRUE,  #ordered factor
#                     levels = c("-1", "-0.5", "0", "0.5","1"))
# )
# 
# ord_cum <- brm( formula = scaled ~ condition,
#                data = df,
#                family = cumulative("probit"),
#                file = "analysis/SGC3A/models/m_items_ord.cum.rds" # cache model (can be removed)  
# 
# )
# 
# summary(ord_cum)
# conditional_effects(ord_cum, "condition", categorical = TRUE)
# 
# #SJPLOT
# library(sjPlot)
# plot_model(ord_cum)
# 
# # m %>%
# #   spread_draws(b_Intercept, r_condition[condition,]) %>%
# #   mutate(condition_mean = b_Intercept + r_condition) %>%
# #   ggplot(aes(y = condition, x = condition_mean)) +
# #   stat_halfeye()
# 
# # performance(ord_cum)
# # plot(ord_cum)
```

### Adjacent-Category Ordinal (Bayesian)

```{r}
# 
# #DEFINE DATA 
# df <- df_items %>% mutate(
#   scaled = factor(score_SCALED, ordered = TRUE,  #ordered factor
#                     levels = c("-1", "-0.5", "0", "0.5","1"))
# )
# 
# 
# # To specify an adjacent-category model, we use family = acat() instead of family = cumulative() as an argument to the brm() function. Then, to model condition with possible category-specific effects, we wrap this variable in cs() in the model’s formula:
# 
# ord_acat <- brm( formula = scaled ~ cs(condition),
#                data = df,
#                family = acat("probit"),
#                file = "analysis/SGC3A/models/m_items_ord.acat.rds" # cache model (can be removed)  
# )
# 
# summary(ord_acat)
# conditional_effects(ord_cum, "condition", categorical = TRUE)
# conditional_effects(ord_acat, "condition", categorical = TRUE)
# 
# #TIDYBAYES VISUALIZATION
# library(tidybayes)
# ord_acat %>%
#   spread_draws(b_Intercept, r_condition[condition,]) %>%
#   mutate(condition_mean = b_Intercept + r_condition) %>%
#   ggplot(aes(y = condition, x = condition_mean)) +
#   stat_halfeye()
# 


```
