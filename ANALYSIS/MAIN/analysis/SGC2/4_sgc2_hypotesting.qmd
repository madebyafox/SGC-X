---
subtitle: 'Study SGC2 | Hypothesis Testing'
---

\newpage

# Hypothesis Testing {#sec-SGC2-hypotesting}

## TODO

Q3 looks nondiscriminant perhaps remove this from analysis

*The purpose of this notebook is test the hypotheses that determined the design of the SGC2 study.*

(The need for scaffolding: )
 - (H1) Scaffolding will \textit{not} affect performance on the LM graph, because readers already understand its conventional coordinate system. 

 - (H2) Because the TM graph is unconventional, it will require scaffolding. In its absence, readers will perform significantly better with the LM than the TM.

The efficacy of scaffolding: 
- (H3) Learners with (any form of) scaffolding will perform better with the TM than LM (replication of Qiang et. al)

Order as scaffold 

- (H4) Learners who solve problems with the LM graph _first_ will perform better on the TM (relative to TM-first learners) as their attention will be drawn to the salient differences between the graph types.
    
  

```{r}
#| label: SETUP
#| warning : false
#| message : false


#UTILITIES
library(Hmisc) # %nin% operator
library(broom) #tidy model output
library(broom.mixed) #tidy mixed  models
library(mosaic) #favstats
library(svglite) #saving plots as svg
library(distributional)

#VISUALIZATION
# library(ggpubr) #arrange plots
# library(cowplot) #arrange shift function plots
library(ggformula) #easy graphs
# # library(vcd) #mosaic plots
# # library(vcdExtra) #mosaic plots
library(kableExtra) #printing tables
library(sjPlot) #visualize model coefficients
library(ggdist) #uncertainty viz
library(modelr) #needed for ggdist
library(gghalves) # plots. in half
library(ggbeeswarm) # violin plot stuffs
library(statsExpressions)
library(ggstatsplot) #plots with stats
library(modelsummary) #latex tables for models!
library(ggeasy) #shortcuts 

#MODELLING
# library(rstatix) #helpful testing functions incl wilcoxon, etc
library(report) #easystats reporting
library(see) #easystats visualization
library(performance) #easystats model diagnostics
library(parameters) #easystats model summary and vis
# library(qqplotr) #confint on qq plot
# library(gmodels) #contingency table and CHISQR
# library(equatiomatic) #extract model equation
# library(pscl) #zeroinfl / hurdle models 
library(lme4) #mixed effects models
library(lmerTest) #for CIs in glmer
library(merTools) #predictInterval
# library(ggeffects) #visualization log regr models
#MULTINOMIAL 
library(nnet) #multinomial logistic regression [not mixed] #no p values
library(mclogit) #frequentist mixed multinomial logistic regression [mblogit] #gives p values
#BAYESIAN
library(cmdstanr) #executing stan
library(brms) #bayesian mixed multinomials [+ other bayesian reg models]
library(bayestestR) 
library(tidybayes)
library(posterior)

library(tidyverse) #ALL THE THINGS

#OUTPUT OPTIONS
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
options(ggplot2.summarise.inform = FALSE)
options(scipen=1, digits=3)

#GRAPH THEMEING
# theme_set(theme_minimal()) 

# Custom ggplot theme to make pretty plots
# Get the font at https://fonts.google.com/specimen/Barlow+Semi+Condensed
theme_clean <- function() {
  theme_minimal(base_family = "Barlow Semi Condensed") +
    theme(panel.grid.minor = element_blank(),
          plot.title = element_text(family = "BarlowSemiCondensed-Bold"),
          axis.title = element_text(family = "BarlowSemiCondensed-Medium"),
          strip.text = element_text(family = "BarlowSemiCondensed-Bold",
                                    size = rel(1), hjust = 0),
          strip.background = element_rect(fill = "grey80", color = NA))
}

set_theme(base = theme_clean())

```

**Research Questions**

In SGC2 we compare learner performance on the linear and triangular model graphs by testing the effectiveness of 4 scaffolds and by seeking to replicate the Qiang et.al (2014) finding that after 20 minutes of video training, students perform faster and more accurately with the unconventional TM than the conventional Linear Model (LM). Will our participants show similar performance on the TM with scaffolds rather than formal instruction? Further, will engagement with the TM in a reading task be sufficient for students to reproduce the graph in a subsequent drawing task?

**Hypotheses**

1.  Learners without scaffolding (control) will perform better with the LM than TM
2.  Learners with (any form of) scaffolding will perform better with the TM than LM (replication of \[12\]).
3.  Based on observations in Study One we expect that graph-order will act as a scaffold. Learners who solve problems with the LM graph first will perform better on the TM (relative to TM-first learners) as their attention will be drawn to the salient differences between the graphs.

```{r}
#| label: IMPORT-DATA
#| warning : false
#| message : false

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
# mbp = "/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
# setwd(mbp)

#IMPORT DATA 
df_items <- read_rds('analysis/SGC2/data/2-scored-data/sgc2_items.rds') %>% dplyr::select(
  subject,
  pretty_condition,
  order,
  scenario,
  graph,
  rt_sec,
  block,
  q_order, 
  q,
  score
) %>% 
  mutate( subject = as.factor(subject),
          accuracy = as.factor(score),
          order = recode_factor(order, "TM-First" = "TM-First", "LM-First"="LM-First")) %>% 
  filter(block %nin% c("drawingTest")) #discard drawing task questions (only two, not analyzing)


df_subjects <- read_rds('analysis/SGC2/data/2-scored-data/sgc2_participants.rds') %>% dplyr::select(
  AGE, GENDER, 
  subject,
  pretty_condition,
  order, #order of graphs counterbalanced
  lm_scenarios, #scenario for lm graph
  tm_scenarios, #scenario for tm graph — counterbalanced with graphs
  linear_score,
  triangular_score,
  score_diff, #triangular - linear 
  time_diff, #triangular time  - linear time
  LM_T_M, #linear model time
  TM_T_M, #traingular model time
  DT_T_M, #drawing task time
  draw_type,
  ls_n, #linear scaffold phase score
  lt_n, #linear test phase score
  ts_n, #tri scaffold phase score
  tt_n  #tri test phase score
) %>%  mutate( subject = as.factor(subject),
               order = recode_factor(order, "TM-First" = "TM-First", "LM-First"="LM-First"),
               total = linear_score + triangular_score) 


```

## SAMPLE

### Data Collection

Data was collected in Spring 2017.

```{r}
#| label : DESC-DATA-COLLECTION

title = "Participants by Condition and Data Collection Period"
cols = c("Control","Text:What", "Text:How", "Img:Static", "Img:Ixv")
table(df_subjects$pretty_condition) 

```

### Participants

```{r}
#| label: DESC-PARTICIPANTS

#Describe participants
subject.stats <-df_subjects %>% dplyr::select(AGE) %>% unlist() %>% favstats()
subject.stats$percent.male <- ((df_subjects %>% filter(GENDER=="Male") %>% count())/count(df_subjects))$n
subject.stats$percent.female <- ((df_subjects %>% filter(GENDER=="Female") %>% count())/count(df_subjects))$n
subject.stats$percent.other <- ((df_subjects %>% filter(GENDER %nin% c("Female","Male")) %>% count())/count(df_subjects))$n


title = "Descriptive Statistics of Participant Age and Gender"
subject.stats %>% kbl (caption = title) %>% kable_classic()%>% 
  footnote(general = "Age in Years", 
           general_title = "Note: ",footnote_as_chunk = T) 
```

**Overall** `r subject.stats$n` participants (`r round((subject.stats$percent.male),2) * 100` % male, `r round((subject.stats$percent.female),2) * 100` % female, `r round((subject.stats$percent.other),2) * 100` % other) undergraduate STEM majors at a public American University participated in exchange for course credit (age: `r (subject.stats$min)` - `r (subject.stats$max)` years).



## H1 \| The Utility of Scaffolding (aka, Linear Graph x Condition)

Does ACCURACY differ between *scaffold conditions* for the *linear graph*? 

**Hypothesis:**
Scaffolding does not improve performance overall, rather, it is only necessary to facilitate discovery of the triangular coordinate system. Thus, performance with the LM graph won't differ by condition. 

#### Setup

```{r}
#| label: SETUP-H1

#FILTER THE DATASET
#only control condition 
df_s <- df_subjects 

df_i <- df_items %>% filter(graph == "linear") 
```

#### Visualize

```{r}
#| label: VIS-H1

#:::::::: STACKED PROPORTIONAL BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(order~graph) + 
   labs(title = "Linear Graph Accuracy by Condition and Order",
       x = "Condition",
       subtitle="Across graph orders, Accuracy is comparable across conditions.")

#:::::::: STACKED PROPORTIONAL BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(scenario ~ graph) + 
   labs(title = "Accuracy by Graph and Scenario",
       x = "Condition",
       subtitle="Across graph scenarios, Accuracy is comparable across conditions.")


#:::::::: STACKED BAR CHART BY QUESTION
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = score)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap( graph ~ q) +
   labs(title = "Accuracy by Question",
       x = "Condition",
       fill = "",
       subtitle="Accuracy differs by question, but is comparable across conditions")

#:::::::: FACETED HISTOGRAM
gf_props(~linear_score,
         fill = ~pretty_condition, data = df_subjects) %>%
  gf_facet_grid(pretty_condition ~ order) +
  labs(x = "# Correct",
       y = "proportion of subjects",
       title = "Linear Graph Score by Condition and Order",
       subtitle = "comprable across factors") + 
  theme(legend.position = "blank")


#:::::::: FACETED HISTOGRAM
gf_props(~linear_score,
         fill = ~pretty_condition, data = df_subjects) %>%
  gf_facet_grid(pretty_condition~tm_scenarios) +
  labs(x = "# Correct",
       y = "proportion of subjects",
       title = "Linear Graph Score by Condition and Scenario",
       subtitle = "comparable across factors") + 
  theme(legend.position = "blank")

# ##VERTICAL RAINCLOUD USING GGDISTR
ggplot(df_subjects, aes(x = pretty_condition, y = linear_score,
                        fill = pretty_condition) ) +
  ggdist::stat_halfeye(
    side = "left",
    justification = 1.1,
    width = 1,
    point_colour = NA
   ) +
  geom_boxplot(
    inherit.aes = TRUE, #supress fill
    # mapping = aes(x=pretty_condition, y = score),
    width = .15,
    outlier.shape = NA
  ) +
  geom_point(
    inherit.aes = TRUE, #supress fill
    # mapping = aes(x=graph, y = score, color = graph),
    size = 1.3,
    alpha = .3,
    position = position_jitter(
      seed = 1, width = .05
  )) + labs(
    title = "Distribution of scores on LINEAR graph",
    x = "Condition", y = "Score (# correct)") +
  theme(legend.position = "blank") +
  coord_cartesian(xlim = c(0.5, NA), clip = "off")

```

*Visualizations indicate it is likely that scores in the LM graph scores are stable across conditions (and orders and scenarios)*

#### Describe

```{r}
favstats (linear_score ~ pretty_condition, data = df_subjects)
```

#### Present


```{r}
#PLOT PAIRED DATA
g <- ggbetweenstats(
  data = df_subjects,
  x    = pretty_condition,
  y    = linear_score, 
  type  = "nonparametric", 
  plot.type = "boxviolin",
  title = "Study 2 | Linear Graph Accuracy is consistent across conditions",
  xlab = "Scaffold Condition", ylab = "Linear Graph Score (# questions correct)",
  centrality.point.args = list(fill="black", size = 3)
) + scale_color_manual(values = paletteer::paletteer_d("awtools::a_palette", 5)) + 
  theme_clean() + theme(legend.position = "none")

g
# ggsave(g, filename = "figures/SGC2_linearscores.png", width = 6, height = 4)
#ggsave(g, filename = "figures/SGC2_linearscores.svg", width = 6, height = 4)
```
#### Test


```{r}

kruskal.test(df_s$linear_score, df_s$pretty_condition)

```

**Reported** 

To test hypothesis that scaffolding is not universally helpful, but rather is only necessary to facilitate discovery of the triangular coordinate system, we compare performance across explicit scaffold conditions on just the linear model graph (one of the two experimental blocks). A Kruskal-Wallis rank sum test supports this hypothesis, indicating the median linear graph score did not differ by scaffold condition ($\chi(4) = 0.4, p > 0.1$).


_Next we check that this conclusion holds across the graph-order and scenario-order counterbalanced factors._

```{r}
#PLOT PAIRED DATA
grouped_ggbetweenstats(
  data = df_subjects,
  grouping.var = order,
  x    = pretty_condition,
  y    = linear_score, 
  type  = "parametric", #parametric, robust, bayes
  annotation.args = list(title = "Linear score is consistent across conditions and graph order"),
)

#PLOT PAIRED DATA
grouped_ggbetweenstats(
  data = df_subjects,
  grouping.var = tm_scenarios,
  x    = pretty_condition,
  y    = linear_score, 
  type  = "parametric", #parametric, robust, bayes
  annotation.args = list(title = "Linear score is consistent across conditions and graph order"),
)

```

_Across both scenario and graph orders, there is no differential effect of condition on linear graph score._

### Logistic Regression

##### Visualize

```{r}
#:::::::: STACKED PROPORTIONAL BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
   labs(title = "Linear Graph Accuracy by Condition",
       x = "Condition",
       subtitle="Probability of correct response consistent across conditions")

```

##### Fit Model
```{r}

#empty model
m.0 <- glm(accuracy ~ 1, family = "binomial", data = df_i)

#condition model
m.C <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q), family = "binomial", data = df_i)
# summary(m.C)
# car::Anova(m.C)
```


##### Describe

```{r}

# best model
m <- m.C

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m)

print("SIGNIFICANCE TEST [non directional]")
car::Anova(m, type=2) #TYPE 3 SS FOR IXNS

#:::::::: INTERPRET COEFFICIENTS

paste("LOG ODDS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald")

paste("ODDS RATIOS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald", exponentiate = TRUE)

paste("PROBABILITIES")
#probability control = plogis(intercept)
#probability impasse = plogis(intercept + coefficient)

#FROM predict()
newdata <- df_i %>% dplyr::select(pretty_condition, subject, q)
preds <- predict(m, newdata = newdata, type = "response")
preds <- cbind(newdata, preds)
(p <- preds %>% 
  dplyr::select(pretty_condition, preds) %>%
  group_by(pretty_condition) %>%
  summarise(
    median = median(preds),
    se = sd(preds)/sqrt(n()),
    lwr = median - 1.96*se,
    upr = median + 1.96*se))
    
```

##### INFERENCE

A Mixed Logistic Regression model including CONDITION as a fixed effect and SUBJECT and QUESTION as random effects indicates that the odds of a correct response in the linear graph task does not differ by scaffold condition, $\chi(4) = 0.35, p > 0.1$.


##### Visualize

```{r}

## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
plot_model(m, vline.color = "red", 
           show.intercept = FALSE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Estimate | Odds Ratio",
       subtitle = "",
       x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
# result <- model_parameters(m, exponentiate = TRUE, component = "all")
# plot(result)


## | PLOT TESTS

result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)


#ONLY FOR BAYESIAN VERSION
# result <- rope(m)
# plot(result)
# 
# result <- pd(m)
# plot(result)


## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
# plot_model(m, type="int",
#            show.intercept = TRUE,
#            show.values = TRUE,
#            title = "Model Prediction | Probability of Accurate Response",
#            axis.title = c("Condition","Probability of Accurate Response"))

#PLOT MODEL PREDICTION
plot_model(m, type = "pred")  
# plot_model(m, type = "eff")  
  # ylim(0,1) + 
  # labs(
  #   title = "Model Prediction | Probability of Accurate Response",
  #   subtitle = "Impasse increases Probability of Correct Response"
  # )

```

```{r}
#SJPLOT | MODEL | TABLE
tab_model(m)

# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m, "(log odds)" = m)
# notes = list('"* p < 0.05, ** p < 0.01, *** p < 0.001"',
#                'N(subject) = 133 $\tau_{00}$(subject) = 34.85',
#              'N(question) = 13 $\tau_{00}$(question) = 1.14')
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"),
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',
#              notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_online.tex")
# # #              # coef_omit = "Intercept",
```

##### Diagnostics

```{r}
#| warning: false
#| message: false

# print("SANITY CHECK REPORTING")
# report(m)

# print("MODEL PERFORMANCE")
# performance(m)

print("DIAGNOSTICS")
check_model(m)

```




## H2 \| The Need for Scaffolding (aka, Linear VS Triangular in CONTROL)

Does ACCURACY differ between *linear* and *triangular* graphs in the control condition?

**Hypothesis** The TM graph is not *discoverable* and requires scaffolding for correct interpretation. We predict that learners without scaffolding (the control condition) will perform better with the LM than TM

#### Setup

```{r}


#FILTER THE DATASET
#only control condition 
df_s <- df_subjects %>% filter(pretty_condition == "control")

df_long <- df_s %>% dplyr::select(subject,pretty_condition,order,lm_scenarios,tm_scenarios,linear_score, triangular_score) %>% pivot_longer(
  cols = ends_with("score"),
  names_to = "graph",
  values_to = "score"
)

df_i <- df_items %>% filter(pretty_condition == "control") %>% filter(q %nin% c(3))
```

#### Visualize

```{r}


#:::::::: STACKED PROPORTIONAL BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(order~graph) + 
   labs(title = "Accuracy by Graph and Order",
       x = "Condition",
       subtitle="Across graph orders, Accuracy is higher on the LINEAR than TRIANGULAR graph.")

#:::::::: STACKED PROPORTIONAL BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(scenario ~ graph) + 
   labs(title = "Accuracy by Graph and Scenario",
       x = "Condition",
       subtitle="Accuracy is higher on the LINEAR than TRIANGULAR graph; much lower for TM = AXIS scenario.")



#:::::::: STACKED BAR CHART BY QUESTION
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = score)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap( graph ~ q) +
   labs(title = "Accuracy by Question",
       x = "Condition",
       fill = "",
       subtitle="")

#:::::::: FACETED HISTOGRAM
gf_props(~score,
         fill = ~graph, data = df_long) %>%
  gf_facet_grid(order ~ graph) +
  labs(x = "# Correct",
       y = "proportion of subjects",
       title = "Overall Absolute Score by Graph and Order",
       subtitle = "") + 
  theme(legend.position = "blank")


#:::::::: FACETED HISTOGRAM
gf_props(~score,
         fill = ~graph, data = df_long) %>%
  gf_facet_grid(tm_scenarios ~ graph) +
  labs(x = "# Correct",
       y = "proportion of subjects",
       title = "Overall Absolute Score by Graph and TM Scenario",
       subtitle = "TM Scores are different by Scenario") + 
  theme(legend.position = "blank")

# ##VERTICAL RAINCLOUD USING GGDISTR
# ggplot(df_long, aes(x = graph, y = score,
#                         fill = graph) ) + 
#   ggdist::stat_halfeye(
#     side = "left",
#     justification = 1.1,
#     width = 1, 
#     point_colour = NA
#    ) + 
#   geom_boxplot(
#     inherit.aes = FALSE, #supress fill
#     mapping = aes(x=graph, y = score),
#     width = .15, 
#     outlier.shape = NA
#   ) + 
#   geom_point(
#     inherit.aes = FALSE, #supress fill
#     mapping = aes(x=graph, y = score, color = graph),
#     size = 1.3,
#     alpha = .3,
#     position = position_jitter( 
#       seed = 1, width = .05
#   )) + labs( 
#     title = "Distribution of scores in CONTROL condition", 
#     x = "Condition", y = "Score (# correct)") +
#   theme(legend.position = "blank") + 
#   coord_cartesian(xlim = c(0.5, NA), clip = "off")


#DISTRIBUTION OF SCORE
gf_dhistogram(~score_diff, data = df_s) %>% 
  gf_facet_grid(tm_scenarios ~ order) +
  xlim(-13,5) + 
  labs(title = "Distribution of paired score differences in CONTROL condition") + 
  easy_remove_legend() 

```

*Visualizations indicate it is likely that scores in the LM are infact higher than the TM in the control condition.*

#### Describe

```{r}


title = "Descriptive Statistics of Response Accuracy by Block (CONTROL Condition)"
abs.stats <- rbind(
  "linear.block"= df_s %>% dplyr::select(linear_score) %>% unlist() %>% favstats(),
  "triangular.block" = df_s %>% dplyr::select(triangular_score) %>% unlist() %>% favstats(),
  "block.differences" = df_s %>% dplyr::select(score_diff) %>% unlist() %>% favstats()
)

abs.stats %>% kbl (caption = title) %>% kable_classic() %>%
  footnote(general = "block # questions correct [0,15]; DIFF = triangular - linear",
           general_title = "Note: ",footnote_as_chunk = T)
```

For participants in the CONTROL condition, total absolute scores for the LINEAR graph (n = `r abs.stats["linear.block",]$n`) range from `r round(abs.stats["linear.block",]$min,2)` to `r round(abs.stats["linear.block",]$max,2)` with a mean score of (M = `r round(abs.stats["linear.block",]$mean,2)`, SD = `r round(abs.stats["linear.block",]$sd,2)`).

For participants in the CONTROL condition, total absolute scores for the TRIANGULAR graph (n = `r abs.stats["triangular.block",]$n`) range from `r round(abs.stats["triangular.block",]$min,2)` to `r round(abs.stats["triangular.block",]$max,2)` with a mean score of (M = `r round(abs.stats["triangular.block",]$mean,2)`, SD = `r round(abs.stats["triangular.block",]$sd,2)`).

Visual inspection of the distribution of scores for each block reveal that scores in on the triangular task were more variant than those in the linear graph. On average, scores on the triangular block were lower than those on the linear block.

#### Test

```{r}
#PLOT PAIRED DATA
g <- ggwithinstats(
  data = df_long,
  x    = graph,
  y    = score, 
  type  = "nonparametric", 
  plot.type = "box",
  title = "Study 2 | Without scaffolding, higher accuracy with Linear Model",
  xlab = "Graph", ylab = "Graph Reading Score (# questions correct)",
  centrality.point.args = list(fill="black", size = 3)
) + scale_color_manual(values = paletteer::paletteer_d("awtools::a_palette", 2)) + 
  theme_clean() + theme(legend.position = "none")

g

# ggsave(g, filename = "figures/SGC2_graphdifference.png", width = 6, height = 4)
# ggsave(g, filename = "figures/SGC2_graphdifference.svg", width = 6, height = 4)
```

-   WILCOXON RANK SUM (Mann-Whitney Test) **Non parametric alternative** to t-test; compares median rather than mean by ranking data
-   Does not assume normality
-   Does not assume equal variance of samples (homogeneity of variance)

```{r}

#WILCOXON RANK SUM PAIRED T-TEST
w <- wilcox.test(df_s$linear_score, df_s$triangular_score, 
            paired = TRUE)
            # alternative = "greater", conf.int = TRUE)
w
report(w)
```



```{r}

#PLOT PAIRED DATA
grouped_ggwithinstats(
  grouping.var = order,
  data = df_long,
  x    = graph,
  y    = score, 
  annotation.args = list(title = "Linear is better than Triangular regardless of Order"),
  type  = "nonparametric" #parametric, robust, bayes
) 

#PLOT PAIRED DATA
grouped_ggwithinstats(
  grouping.var = tm_scenarios,
  data = df_long,
  x    = graph,
  y    = score, 
  type  = "nonparametric", #parametric, robust, bayes
  annotation.args = list(title = "Linear is better than Triangular regardless of [triangle model] Scenario"),
)

```

The Wilcoxon signed rank test confirms that (for subjects in the control condition) scores on the **triangle graph** were significantly lower than those in the **linear graph** block. This provides evidence in support of our hypothesis that the Triangular model graph (though computationally efficient) is in fact unconventional and lacking in discoverablility. It needs to be augmented with scaffolding in order to be correctly interpreted by novice readers.

#### MIXED EFFECTS LOGISTIC REGRESSION

*Fit mixed effects logistic regression model to test effect of GRAPH, ORDER, and SCENARIO on probability of correct response \[just in the control condition\]*

##### Fit Model

```{r}
#| label: MODEL-FIT-CACC

## 0 | SETUP
#confirm 13 items [all discriminating items]
nrow(df_i) / nrow(df_s) == 28 #removed nondiscrim Q3
#confirm all factors 
is.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$graph) && is.factor(df_i$score) && is.factor(df_i$order) && is.factor(df_i$scenario)


#RECODE TRIANGULAR AS REFERENCE LEVEL 
df_i <- df_i %>% mutate(
  graph = recode_factor(graph, "triangular"="triangular","linear"="linear")
) 

## 1 | SETUP RANDOM INTERCEPT SUBJECT

#:: EMPTY MODEL (baseline, no random effect)
print("Empty fixed model")
m0 = glm(accuracy ~ 1, family = "binomial", data = df_i) 
# summary(m0)

#:: RANDOM INTERCEPT SUBJECT
print("Subject intercept random model")
mm.rS <- glmer(accuracy ~ (1|subject), data = df_i, family = "binomial")
# summary(mm.rS)

# :: TEST random effect
paste("AIC decreases w/ new model?", m0$aic > AIC(logLik(mm.rS)))
test_lrt(m0,mm.rS) #same as anova(m0, m1, test = "Chi")


#:: RANDOM INTERCEPT SUBJECT + ITEM
print("Subject Intercept + Item intercept random model")
mm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = "binomial")
#summary(mm.rSQ)

# :: TEST random effect
paste("AIC decreases w/ new model?", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)))
test_lrt(mm.rS, mm.rSQ) #same as anova(m0, m1, test = "Chi")

## 2 | ADD FIXED EFFECT GRAPH

print("FIXED Condition + Subject & Item random intercepts")
mm.GrSQ <- glmer(accuracy ~ graph + (1|subject) + (1|q) ,
                data = df_i, family = "binomial")
# car::Anova(mm.GrSQ) #main effect graph

paste("AIC decreases w/ new model", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.GrSQ)) )
test_lrt(mm.rSQ,mm.GrSQ) #same as anova(m0, m1, test = "Chi")


# NOT RELEVANT TO HYPOTHESIS 
# 
# ## 2 | ADD IXN GRAPH & ORDER EFFECT GRAPH ++ ORDER
# 
# print("FIXED Graph + ORDER + Subject & Item random intercepts")
# mm.GOrSQ <- glmer(accuracy ~ graph * order + (1|subject) + (1|q) ,
#                 data = df_i, family = "binomial")
# # summary(mm.GOrSQ)
# # car::Anova(mm.GOrSQ)
# 
# paste("AIC decreases w/ new model", AIC(logLik(mm.GrSQ)) > AIC(logLik(mm.GOrSQ)) )
# test_lrt(mm.GrSQ,mm.GOrSQ) #same as anova(m0, m1, test = "Chi")
# 
# 
# ## 2 | ADD FIXED EFFECT GRAPH + ORDER+ SCENARIO
# 
# print("FIXED Graph * SCENARIO * ORDER + Subject & Item random intercepts")
# mm.GSOrSQ <- glmer(accuracy ~ graph*scenario*order + (1|subject) + (1|q) ,
#                 data = df_i, family = "binomial",
#                 control=glmerControl(optimizer="bobyqa",
#                                  optCtrl=list(maxfun=2e5)))
# # summary(mm.GSOrSQ)
# #car::Anova(mm.GSOrSQ)
# 
# paste("AIC decreases w/ new model", AIC(logLik(mm.GOrSQ)) > AIC(logLik(mm.GSOrSQ)) )
# test_lrt(mm.GOrSQ,mm.GSOrSQ) #same as anova(m0, m1, test = "Chi")


```


##### Describe

```{r}
#| label: MODEL-DESC-CACC

# best model
m <- mm.GrSQ

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m)

print("SIGNIFICANCE TEST [non directional]")
car::Anova(m, type=2) #TYPE 3 SS FOR IXNS

#:::::::: INTERPRET COEFFICIENTS

paste("LOG ODDS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald")

paste("ODDS RATIOS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald", exponentiate = TRUE)

paste("PROBABILITIES")
#probability control = plogis(intercept)
#probability impasse = plogis(intercept + coefficient)

#FROM predict()
newdata <- df_i %>% dplyr::select(graph, subject, q)
preds <- predict(m, newdata = newdata, type = "response")
preds <- cbind(newdata, preds)
(p <- preds %>%
  dplyr::select(graph, preds) %>%
  group_by(graph) %>%
  summarise(
    median = median(preds),
    se = sd(preds)/sqrt(n()),
    lwr = median - 1.96*se,
    upr = median + 1.96*se))
    
  
# #FROM merTools
# #setup df 
# newdata <- df_i %>% dplyr::select(graph, order, scenario, subject, q)
# #make predictions
# preds <- predictInterval(m, newdata = newdata,
#                               which = "fixed", #full, fixed or random for those only
#                               type = "probability", #linear.prediction
#                               stat = "median",
#                               n.sims = 1000,
#                               level = 0.80) #width of prediction interval
# #join predictions to the new dataframe
# preds <- cbind(newdata, preds)
# #summarize
# (summ_preds <- preds %>% 
#   dplyr::select(graph, scenario, order, fit, lwr, upr) %>% 
#   group_by(graph, scenario, order) %>% 
#   dplyr::summarise(
#     median = median(fit),
#     lower = median(lwr),
#     upper = median(upr)
#   )) 

```

##### Inference TODO UPDATE

We fit a mixed logistic regression model predicting question-level accuracy by graph for only participants in the control condition (n = 61). The model included random intercepts for subject and question, and a fixed effect of GRAPH. The model implies that (with no scaffolding; in the control condition) the odds of a correct response on a linear-graph question are more than 4 times higher than the triangle graph, ($e^\beta_1 = 4.34, 95 \% CI [3.47, 5.44], p < 0.001$) 

```{r}
#SJPLOT | MODEL | TABLE
tab_model(m)

# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m1, "(log odds)" = m1)
# notes = list("* p < 0.05, ** p < 0.01, *** p < 0.001",
#              '$sigma^{2}$ = 3.29" N(subject) = 126 $\tau_{00}$(subject) = 22.22 N(question) = 13 $\tau_{00}$(question) = 0.31'
#                )
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"),
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', 
#              notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex")
# #              # coef_omit = "Intercept",
# 
# 
# 

```

##### Visualize

```{r}
#| label: MODEL-VIS-CACC

## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
plot_model(m, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Estimate | Odds Ratio",
       subtitle = "",
       x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
# result <- model_parameters(m, exponentiate = TRUE, component = "all")
# plot(result)


## | PLOT TESTS

result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)


#ONLY FOR BAYESIAN VERSION
# result <- rope(m)
# plot(result)
# 
# result <- pd(m)
# plot(result)


## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
# plot_model(m, type="int",
#            show.intercept = TRUE,
#            show.values = TRUE,
#            title = "Model Prediction | Probability of Accurate Response",
#            axis.title = c("Condition","Probability of Accurate Response"))

#PLOT MODEL PREDICTION
plot_model(m, type = "pred")  
plot_model(m, type = "eff")  
  # ylim(0,1) + 
  # labs(
  #   title = "Model Prediction | Probability of Accurate Response",
  #   subtitle = "Impasse increases Probability of Correct Response"
  # )

#TODO EMMEANS for the estimated marginal means OR USE IXN PLOT


```

```{r}
#SJPLOT | MODEL | TABLE
tab_model(m)

# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m, "(log odds)" = m)
# notes = list('"* p < 0.05, ** p < 0.01, *** p < 0.001"',
#                'N(subject) = 133 $\tau_{00}$(subject) = 34.85',
#              'N(question) = 13 $\tau_{00}$(question) = 1.14')
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"),
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',
#              notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_online.tex")
# # #              # coef_omit = "Intercept",
```

##### Diagnostics

```{r}
#| label: MODEL-DIAG-ACC
#| warning: false
#| message: false

# print("SANITY CHECK REPORTING")
# report(m)

# print("MODEL PERFORMANCE")
# performance(m)

print("DIAGNOSTICS")
check_model(m)

```




## H3-1 ( Replication Hypothesis — DIFFERENCE SCORE )

**Hypothesis** Qiang et. al found that the TM graph was more computationally efficient than the LM graph. We expect that for learners that *do* correctly interpret the graph, they will have lower response times for the TM vs. LM graph.

### DIFFERENCE Score

#### Setup

```{r}
#| label: SETUP-ACC

#FILTER THE DATASET
df_s <- df_subjects %>% dplyr::select(total, triangular_score, score_diff, time_diff, subject, order, tm_scenarios, pretty_condition)
df_i <- df_items %>% filter(q %nin% c(3))
```

#### Visualize

```{r}

## Is score difference normally distributed?
gf_histogram(~df_s$score_diff)


# Box plots
# ++++++++++++++++++++
# Plot weight by group and color by group
library("ggpubr")
ggboxplot(df_s, x = "pretty_condition", y = "score_diff", 
          color = "pretty_condition", 
          # palette = c("#00AFBB", "#E7B800", "#FC4E07"),
          # order = c("ctrl", "trt1", "trt2"),
          title = "DIFFERENCE IN GRAPH scores by Condition",
          ylab = "TRI - LINEAR Score [-15,15]", xlab = "Condition") +
  geom_hline(yintercept = 0, color = "black") + 
  geom_jitter( aes(color = pretty_condition), width = 0.15, alpha = 0.5) +
  ylim(-15,15) + 
  # facet_wrap(order~ tm_scenarios) +
  easy_remove_legend() + theme_clean()

```

```{r}

favstats(score_diff ~ pretty_condition, data = df_s)

```


**Reported**
To test the hypothesis that any form of scaffolding will replicate the findings of Qiang et. al (significantly better accuracy on TM than LM graph), we calculated a performance increase (Triangular Accuracy - Linear Accuracy). Positive values indicate better performance with the TM than LM, with a range from -15 to +15.  In all but interactive image condition, the median difference score were less than 0, indicating that only the interactive image condition yielded more accurate performance on the TM graph than the LM graph. However, the performance increase was very small; a one-sample t-test indicates that the average performance increase in the interactive image condition was not significantly different than 0 ($t(67) = -0.6, p = 0.6$). 

#### Test

```{r}

#one sample t-test
x <- df_s %>% filter(pretty_condition == "img:ixv")
t_test(x$score_diff)
```



#### LINEAR REGRESSION [ SCORE diff ~ condition]

##### Fit Model

```{r}
# empty model 
m0 <- lm( score_diff ~ 1, data = df_s)

# FIXED EFFECTS model 
m.C.O.S <- lm( score_diff ~ pretty_condition + order + tm_scenarios, data = df_s)
test_lrt(m0,m.C.O.S)
car::Anova(m.C.O.S) #main effects condition
#main effects of each 

# INTERACTION MODEL
m.COS <- lm( score_diff ~ pretty_condition*order*tm_scenarios, data = df_s)
test_lrt(m.C.O.S,m.COS)
car::Anova(m.COS, type = 3) 
#main effects condition, scenario

```

The best fitting model includes main effects CONDITION, ORDER and SCENARIO. 


##### Describe

```{r}

# best model
m <- m.COS

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m)

print("SIGNIFICANCE TEST [non directional]")
car::Anova(m, type=2) #TYPE 3 SS FOR IXNS

#:::::::: MANUAL ONE-SIDED SIGTEST 
#note: anova and chi square are always one-tailed, but that is independent of being one-sided
#https://www.ibm.com/support/pages/can-one-get-one-tailed-tests-logistic-regression-dividing-significance-levels-half

# one-sided (right tail) z test for B COEFFICIENT
#SANITY CHECK 2-tailed test should match the model output
#NOTE ... NEED TO DO THIS FOR _EACH_ COEFFICIENT
# tt <- 2*pnorm(summary(m)$coefficients[2,3], lower.tail = F)
# paste("p value for two-tailed test, null B = 0 : ",round(tt,5))
# ot <- pnorm(summary(m)$coefficients[2,3], lower.tail = F)
# paste("BUT we want a one  directional, null: B <= 0: ",round(ot,5))

#:::::::: INTERPRET COEFFICIENTS
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald")

paste("PROBABILITIES")
#probability control = plogis(intercept)
#probability impasse = plogis(intercept + coefficient)

#FROM predict()
newdata <- df_s %>% dplyr::select(pretty_condition, order, tm_scenarios, subject)
preds <- predict(m, newdata = newdata, type = "response")
preds <- cbind(newdata, preds)
(p <- preds %>%
  group_by(pretty_condition, order, tm_scenarios) %>%
  dplyr::summarise(
    median = median(preds),
    se = sd(preds)/sqrt(n()),
    lwr = median - 1.96*se,
    upr = median + 1.96*se))

```

##### INFERENCE

To test the hypothesis that SCAFFOLD condition affects triangular score performance, we fit a linear regression model predicting difference score (triangular - linear; where + scores indicate better performance on triangular vs linear) by SCAFFOLD, ORDER and SCENARIO.  

In this model, there were significant main effects of SCAFFOLD, ORDER and SCENARIO, but no interactions.  The model estimates that each condition significantly improves performance on the triangular vs linear graph, increasing the difference in scores (score diff <0 means higher linear, scorediff > 0 higher triangular).
The most effective scaffold was the interactive image, increasing the score difference by 4 points. Taking the LM before the TM had a small but significant effect increasing the differential by 1 point, while scenario increased the score by 2.5 points. 



##### Visualize

```{r}


## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
plot_model(m, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Estimate | Score Difference (TRI - LIN)",
       subtitle = "",
       x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
# result <- model_parameters(m, exponentiate = TRUE, component = "all")
# plot(result)


## | PLOT TESTS

result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)

## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
plot_model(m, type="eff",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Prediction | Probability of Accurate Response")

#PLOT MODEL PREDICTION
# plot_model(m, type = "pred")  
# plot_model(m, type = "eff")  
  # ylim(0,1) + 
  # labs(
  #   title = "Model Prediction | Probability of Accurate Response",
  #   subtitle = "Impasse increases Probability of Correct Response"
  # )

## | PLOT INTERACTIONS
# plot_model(m.C.O.S, type="int",
#            show.intercept = TRUE,
#            show.values = TRUE,
#            title = "Model Prediction | Probability of Accurate Response")
# 

```

```{r}
#SJPLOT | MODEL | TABLE
tab_model(m)

# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m, "(log odds)" = m)
# notes = list('"* p < 0.05, ** p < 0.01, *** p < 0.001"',
#                'N(subject) = 133 $\tau_{00}$(subject) = 34.85',
#              'N(question) = 13 $\tau_{00}$(question) = 1.14')
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"),
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',
#              notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_online.tex")
# # #              # coef_omit = "Intercept",
```



##### Diagnostics

```{r}
#DIAGNOSTICS
check_model(m)
```



## H3 \| The Effectiveness of Scaffolding — TRIANGULAR SCORE


### TRIANGULAR SCORE

#### Setup

```{r}

#FILTER THE DATASET
df_s <- df_subjects %>% dplyr::select(triangular_score, score_diff, subject, order, tm_scenarios, pretty_condition)
df_i <- df_items %>% filter(q %nin% c(3)) %>% filter(graph == "triangular")
```

#### Visualize

```{r}

# Box plots
# ++++++++++++++++++++
# Plot weight by group and color by group
library("ggpubr")
ggboxplot(df_s, x = "pretty_condition", y = "triangular_score", 
          color = "pretty_condition", 
          # palette = c("#00AFBB", "#E7B800", "#FC4E07"),
          # order = c("ctrl", "trt1", "trt2"),
          title = "TRIANGULAR SCORE by Condition",
          ylab = "Triangular Score [0,15]", xlab = "Condition") +
  # geom_hline(yintercept = 0, color = "black") + 
  geom_jitter( aes(color = pretty_condition), width = 0.15, alpha = 0.5) +
  # ylim(-15,15) + 
  # facet_wrap(order~ tm_scenarios) +
  easy_remove_legend() + theme_clean()


gf_histogram(~triangular_score, data = df_s)
```

```{r}

favstats(triangular_score ~ pretty_condition, data = df_s)

```


#### [no] MIXED EFFECTS LOGISTIC REGRESSION --\> RAW SCORE

*Fit mixed effects logistic regression model to test effect of CONDITION ORDER, and SCENARIO on probability of correct response just in TRIANGULAR graph*

##### Fit Model

```{r}


## 0 | SETUP
#confirm 13 items [all discriminating items]
nrow(df_i) / nrow(df_s)  #removed nondiscrim Q3

#confirm all factors 
is.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$graph) && is.factor(df_i$score) && is.factor(df_i$order) && is.factor(df_i$scenario)


#:: RANDOM INTERCEPT SUBJECT + ITEM
print("Subject Intercept + Item intercept random model")
mm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = "binomial")
#summary(mm.rSQ)


## 2 | ADD FIXED EFFECT CONDITION

print("Condition + Subject & Item random intercepts")
mm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q) ,
                data = df_i, family = "binomial", 
                # control=glmerControl(optimizer="bobyqa",
                #                  optCtrl=list(maxfun=2e5))
                )
# car::Anova(mm.CrSQ) #main effects graph
test_lrt(mm.rSQ,mm.CrSQ) 
#adding condition improves fit

# 2 | ADD SIMPLE FIXED EFFECTS 
print("FIXED CONDITION + ORDER + SCENARIO + Subject & Item random intercepts")
mm.C.O.SrSQ <- glmer(accuracy ~ pretty_condition + order + scenario + (1|subject) + (1|q),
                   data = df_i, family = "binomial", 
                   control=glmerControl(optimizer="bobyqa",
                                 optCtrl=list(maxfun=2e5)))
# car::Anova(mm.C.O.SrSQ, type = 2) #main effect condition
test_lrt(mm.CrSQ,mm.C.O.SrSQ) 
#adding main effects DOES improve fit 


# 2 | ADD IXNS 
print("FIXED CONDITION * ORDER * SCENARIO  + Subject & Item random intercepts")
mm.COSrSQ <- glmer(accuracy ~ pretty_condition*order*scenario + (1|subject) + (1|q),
                   data = df_i, family = "binomial", 
                   control=glmerControl(optimizer="bobyqa",
                                 optCtrl=list(maxfun=2e5)))
car::Anova(mm.COSrSQ, type = 3) #ME condition, scenario
test_lrt(mm.COSrSQ,mm.C.O.SrSQ)
# adding interactions did not improve model fit

#PERFORMANCE
performance(mm.C.O.SrSQ)

```

**REPORTED**
To quantify the effect of explicit scaffold condition on triangular graph performance, we fit a mixed logistic regression model on item-level accuracy (correct/incorrect) for only the triangular model block. We included random intercepts for subjects and questions, and fixed effects for explicit scaffold condition, graph-order, and scenario-order.  A likelihood ratio test comparing a model with main effects to a second model including the interaction between fixed factors indicates that the interaction term does not improve model fit (Chi2(22,9) = 11.87, p = 0.539). The explanatory power of the final model is moderate (conditional R2 = 0.474) with the part related to fixed effects explaining 9% of variance. 


##### Describe

```{r}

# best model
m <- mm.C.O.SrSQ

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m)

print("SIGNIFICANCE TEST [non directional]")
car::Anova(m, type=2) #TYPE 3 SS FOR IXNS

#:::::::: INTERPRET COEFFICIENTS

paste("LOG ODDS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald")

paste("ODDS RATIOS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald", exponentiate = TRUE)

paste("PROBABILITIES")
#probability control = plogis(intercept)
#probability impasse = plogis(intercept + coefficient)

#FROM merTools
#setup empty df 
newdata <- df_i %>% dplyr::select(graph, pretty_condition, order, scenario, subject, q)
#make predictions
preds <- predictInterval(m, newdata = newdata,
                              which = "fixed", #full, fixed or random for those only
                              type = "probability", #linear.prediction
                              stat = "median",
                              n.sims = 1000,
                              level = 0.80) #width of prediction interval
#join predictions to the new dataframe
preds <- cbind(newdata, preds)
#summarize
(summ_preds <- preds %>% 
  dplyr::select(graph, pretty_condition, scenario, order, fit, lwr, upr) %>% 
  group_by(graph, pretty_condition, scenario, order) %>% 
  dplyr::summarise(
    median = median(fit),
    lower = median(lwr),
    upper = median(upr)
  )) 


 
library(emmeans)

##POST-HOC COMPARISONS
print("POSTHOC COMPARISONS :: CONDITION")
emmeans(m,  pairwise ~ pretty_condition, 
        type = "response" , adjust = "tukey") #sidak, tukey



##PLOT PROBABILITY
# plot(ref_grid(m), by = "pretty_condition", type = "response")

```

##### INFERENCE

**REPORTED**


Consistent with our hypothesis, Wald Chi-Square tests revealed a significant main effect for \textbf{explicit scaffold} condition ($\chi^2(4) = 32.12,   p < 0.001$). The (unstandardized) regression coefficients indicate that each explicit scaffolds, significantly increases the odds of a correct response in the triangular graph task relative to the non-scaffold control.  The two text conditions each roughly double the odds of a correct response,($e^{\beta_1[conceptual]} = 2.42, SE = 0.744, p < 0.001 ;e^{\beta_1[procedural]} = 2.25, SE = 0.67, p < 0.01$). The static image condition also doubles the odds of a correct response,($e^{\beta_1[static image]} = 2.23, SE = 0.66, p < 0.01$). Alternatively, the interactive image condition increased the odds of a correct response by over a factor of 5, ($e^{\beta_1[interactive image]} = 5.41, SE = 1.61, p < 0.01$).


Unexpectedly, Wald Chi-Square tests revealed a significant main effect for \textbf{scenario-order}  ($\chi^2(1) = 32.30,   p < 0.001$).  Specifically, pairing the event-scheduling scenario B with the TM graph task increased the odds of a correct response by a factor of 3, ($e^{\beta_1[scenarioB]} = 2.97, SE = 0.57, p < 0.001$)


Counter to our hypothesis, Wald Chi-Square tests did not indicate a significant main effect for \textbf{graph-order}  ($\chi^2(1) = 1.92,   p = 0.17$). Completing the TM graph block after the LM graph task did not improve performance on the TM graph as we expected.






##### Visualize

```{r}

## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
plot_model(m, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Estimate | Odds Ratio",
       subtitle = "",
       x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
# result <- model_parameters(m, exponentiate = TRUE, component = "all")
# plot(result)


## | PLOT TESTS

result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)

## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
# plot_model(m, type="int",
#            show.intercept = TRUE,
#            show.values = TRUE,
#            title = "Model Prediction | Probability of Accurate Response")

#PLOT MODEL PREDICTION
# plot_model(m, type = "pred")  
# plot_model(m, type = "eff")  
  # ylim(0,1) + 
  # labs(
  #   title = "Model Prediction | Probability of Accurate Response",
  #   subtitle = "Impasse increases Probability of Correct Response"
  # )

#TODO EMMEANS for the estimated marginal means OR USE IXN PLOT


```

```{r}
#SJPLOT | MODEL | TABLE
tab_model(m)

# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m, "(log odds)" = m)
# notes = list('"* p < 0.05, ** p < 0.01, *** p < 0.001"',
#                'N(subject) = 133 $\tau_{00}$(subject) = 34.85',
#              'N(question) = 13 $\tau_{00}$(question) = 1.14')
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"),
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',
#              notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_online.tex")
# # #              # coef_omit = "Intercept",
```

##### Diagnostics

```{r}
#| warning: false
#| message: false

# print("SANITY CHECK REPORTING")
# report(m)

# print("MODEL PERFORMANCE")
# performance(m)

print("DIAGNOSTICS")
check_model(m)

```


#### [no] LINEAR REGRESSION [ TRI SCORE ~ condition]

[violates normality of residuals, and has issues with colinearity]
##### Fit Model

```{r}
# empty model 
m0 <- lm( triangular_score ~ 1, data = df_s)

# CONDITION model 
m.C <- lm( triangular_score ~ pretty_condition, data = df_s)
paste("Condition model better fit than empty?")
test_lrt(m0,m.C)
# car::Anova(m.C) #main effects condition

# CONDITION + ORDER + SCENARIO model 
m.COS <- lm( triangular_score ~ pretty_condition*tm_scenarios*order, data = df_s)
paste("Condition*Order*Scenario model better fit than interactions model?")
test_lrt(m.COS,m.C)
car::Anova(m.COS, type = 3) #main effects condition, order scenario, scenario
#no interactions were significant; weak main effects

# #MAIN EFFECTS ONLY MODEL
# m.COS <- lm( triangular_score ~ pretty_condition+order+tm_scenarios, data = df_s)
# paste("Condition+Order+Scenario model better fit than interaction model?")
# test_lrt(m.C.O.S,m.COS)



```

The best fitting model includes main effects CONDITION, ORDER and SCENARIO. 


##### Describe

```{r}

# best model
m <- m.COS

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m)

print("SIGNIFICANCE TEST [non directional]")
car::Anova(m, type=3) #TYPE 3 SS FOR IXNS

#:::::::: MANUAL ONE-SIDED SIGTEST 
#note: anova and chi square are always one-tailed, but that is independent of being one-sided
#https://www.ibm.com/support/pages/can-one-get-one-tailed-tests-logistic-regression-dividing-significance-levels-half

# one-sided (right tail) z test for B COEFFICIENT
#SANITY CHECK 2-tailed test should match the model output
#NOTE ... NEED TO DO THIS FOR _EACH_ COEFFICIENT
# tt <- 2*pnorm(summary(m)$coefficients[2,3], lower.tail = F)
# paste("p value for two-tailed test, null B = 0 : ",round(tt,5))
# ot <- pnorm(summary(m)$coefficients[2,3], lower.tail = F)
# paste("BUT we want a one  directional, null: B <= 0: ",round(ot,5))

#:::::::: INTERPRET COEFFICIENTS
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald")

paste("PROBABILITIES")
#probability control = plogis(intercept)
#probability impasse = plogis(intercept + coefficient)

#FROM predict()
newdata <- df_s %>% dplyr::select(pretty_condition, order, tm_scenarios, subject)
preds <- predict(m, newdata = newdata, type = "response")
preds <- cbind(newdata, preds)
(p <- preds %>%
  group_by(pretty_condition, order, tm_scenarios) %>%
  dplyr::summarise(
    median = median(preds),
    se = sd(preds)/sqrt(n()),
    lwr = median - 1.96*se,
    upr = median + 1.96*se))

```


##### Visualize

```{r}


## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
plot_model(m, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Estimate | Score Difference (TRI - LIN)",
       subtitle = "",
       x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
# result <- model_parameters(m, exponentiate = TRUE, component = "all")
# plot(result)


## | PLOT TESTS

result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)

## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
plot_model(m, type="eff",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Prediction | Probability of Accurate Response")

#PLOT MODEL PREDICTION
# plot_model(m, type = "pred")  
# plot_model(m, type = "eff")  
  # ylim(0,1) + 
  # labs(
  #   title = "Model Prediction | Probability of Accurate Response",
  #   subtitle = "Impasse increases Probability of Correct Response"
  # )

## | PLOT INTERACTIONS
# plot_model(m.C.O.S, type="int",
#            show.intercept = TRUE,
#            show.values = TRUE,
#            title = "Model Prediction | Probability of Accurate Response")
# 

```

```{r}
#SJPLOT | MODEL | TABLE
tab_model(m)

# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m, "(log odds)" = m)
# notes = list('"* p < 0.05, ** p < 0.01, *** p < 0.001"',
#                'N(subject) = 133 $\tau_{00}$(subject) = 34.85',
#              'N(question) = 13 $\tau_{00}$(question) = 1.14')
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"),
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',
#              notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_online.tex")
# # #              # coef_omit = "Intercept",
```



#### Diagnostics

```{r}
#DIAGNOSTICS
check_model(m)
```













### H2 \| Realizing Computational Efficiency ------ TIME EFFICIENCY

#### Setup

```{r}

df_s <- df_subjects 
df_i <- df_items 

```



#### Visualize

```{r}

library(ggpubr)
ggboxplot(df_s, x = "pretty_condition", y = "time_diff", 
          color = "pretty_condition", 
          # palette = c("#00AFBB", "#E7B800", "#FC4E07"),
          # order = c("ctrl", "trt1", "trt2"),
          title = "DIFFERENCE IN TIME by Condition",
          ylab = "TRI - LINEAR TIME (seconds)", xlab = "Condition") +
  geom_hline(yintercept = 0, color = "black") + 
  geom_jitter( aes(color = pretty_condition), width = 0.15, alpha = 0.5) +
  # facet_wrap(order~ tm_scenarios) +
  easy_remove_legend() + theme_clean()

```

#### Model Response time for CORRECT answers

**Is the TRIANGULAR graph FASTER for correct answers? (i.e. computational efficiency)**

```{r}

#GET ONLY CORRECT ITEMS in TEST phase 
df <- df_items %>% filter(q %nin% c(1,2,3,4,5)) %>% filter(score == 1)

table (df$pretty_condition)

# Box plots
# ++++++++++++++++++++
ggplot( data = df, aes( x = pretty_condition, y = rt_sec, color = graph )) + 
  geom_boxplot(position = position_dodge()) 
  # geom_point( alpha = 0.2, position = position_dodge(width = 2))


## Does GRAPH predict rt_sec for correct items?
m <- lmer( log(rt_sec) ~ graph + (1|q), data = df)
summary(m)
car::Anova(m)

plot_model(m, type = "pred")


```

A mixed log-linear model predicting response time by GRAPH for only CORRECT items in the test phase (i.e. without scaffolding text or images) indicates that there is not a significant difference between response times for linear and orthogonal graphs.

#### Model Response time for by ACCURACY on TRIANGLE graph (Test Phase)

**Are *correct* answers FASTER than *incorrect* answers for the TRIANGULAR graph? (i.e. computational inefficiency of mental projection/transformation)**

```{r}

#GET ONLY CORRECT ITEMS in TEST phase 
df <- df_items %>% filter(q %nin% c(1,2,3,4,5)) %>% filter(graph=="triangular")

table (df$pretty_condition)

# ++++++++++++++++++++
ggplot( data = df, aes( x = pretty_condition, y = rt_sec, color = score )) + 
  geom_boxplot(position = position_dodge()) 
  # geom_point( alpha = 0.2, position = position_dodge(width = 2))


gf_histogram(~log(rt_sec), data = df) %>% 
  gf_facet_grid(score ~ .)



## Does ACCURACY predict rt_sec for correct items?
m <- lmer( log(rt_sec) ~ accuracy + (1|q), data = df)
summary(m)
car::Anova(m)

plot_model(m, type = "pred")



```

A mixed log-linear model predicting response time by GRAPH for only CORRECT items in the test phase (i.e. without scaffolding text or images) indicates that there is not a significant difference between response times for linear and orthogonal graphs.

## DRAWING TASK

### Setup

```{r}
 
df_s <- df_subjects 

```

### Visualize

```{r}

ggbarstats( x = draw_type, y =pretty_condition , data = df_s,
            title = "Drawing Type by Condition")


#drawing type by order 
grouped_ggbarstats( x = draw_type, y =pretty_condition , 
                    grouping.var = order, data = df_s,
                    annotation.args = list(
                      title = "Drawing type BY ORDER"
                    ))

#drawing type by tm-scenario 
grouped_ggbarstats( x = draw_type, y =pretty_condition , 
                    grouping.var = tm_scenarios, data = df_s,
                    annotation.args = list(
                      title = "Drawing type BY tm-scenario"
                    ))
```

### MULTINOMIAL REGRESSION

*Does condition affect the type of drawing produced?*

##### Fit Model \[multinom\]

```{r}
#| label: FIT-MBLOGIT-STATE

#https://www.elff.eu/software/mclogit/manual/mblogit/
#"baseline category logit" model matches multinom()

#check reference level 
print("Categories (first is reference)")
levels(df_s$draw_type)

#FIT EMPTY MODEL
print("EMPTY MODEL")
mm.cat.rSQ <- multinom(draw_type ~ 1 , data = df_s)
# summary(mm.cat.rSQ)

#FIT PREDICTOR MODEL
print("CONDITION MODEL")
mm.cat.CrSQ <- multinom(draw_type ~ pretty_condition , data = df_s)
mb <- mblogit(draw_type ~ pretty_condition , data = df_s)
summary(mb)

# print("CONDITION + ORDER MODEL")
mm.cat.C.0rSQ <- multinom(draw_type ~ pretty_condition + order , data = df_s)
mb2 <- mblogit(draw_type ~ pretty_condition + order , data = df_s)
summary(mb2)

#COMPARE MODEL FIT
paste("AIC wth predictor is lower than empty model?", AIC(mm.cat.rSQ) > AIC(mm.cat.CrSQ))
test_lrt(mm.cat.rSQ, mm.cat.CrSQ)
```

##### Describe

```{r}
#| label: DESC-BRMS-STATE

# best model
m <- mm.cat.CrSQ

#DESCRIBE MODEL
summary(mb) #use mblogit version to see estimate level p values

#INTERPRET COEFFICIENTS
paste("LOG ODDS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald")
paste("ODDS RATIOS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald", exponentiate = TRUE)


#PERFORMANCE
performance(m)


paste("PROBABILITIES")

#PREDICT METHOD
newdata <- df_i %>% dplyr::select(pretty_condition, subject)
preds <- predict(m, newdata = newdata, type = "probs")
preds <- cbind(newdata, preds)
#lengthen data frame to handle multinomial
preds <- preds %>%
  dplyr::select(-subject) %>% #marginalize over subject and q
  pivot_longer(
  cols = !pretty_condition,
  values_to = "preds",
  names_to = "drawing_type",
)

(p <- preds %>%
  group_by(pretty_condition, drawing_type ) %>%
  dplyr::summarise(
    median = median(preds),
    se = sd(preds)/sqrt(n()),
    lwr = median - 1.96*se,
    upr = median + 1.96*se))

##DRAWS METHOD
# GENERATE draws from model
# draws <- df_i %>%
#   data_grid(pretty_condition, subject, q) %>% 
#   add_fitted_draws(Bmm.cat.CrSQ,
#                    # n = 100,
#                    # dpar = TRUE,
#                    # transform = TRUE, #gives prob%, otherwise OR
#                    re_formula = NA)
# # draws %>% write_rds(file = "analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds")
# 
# #OR load from file
# # draws <- read_rds(file = "analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds")
# 
# # SUMMARIZE draws from model
# (k <- kable(draws %>%
#   dplyr::select(pretty_condition, .category, .value) %>%
#   group_by(pretty_condition, .category) %>%
#   median_hdci(.value), digits = 4, col.names =
#     c("Condition","Category", "Probability","Lower Cred.I","Upper Cred.I", "CI Width", "Point Type", "Interval Type")) %>%
#   kable_styling())

```

##### INFERENCE

##### Visualize

```{r}
#| label: VIS-BRMS-STATE



## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
# plot_model(m, vline.color = "red", 
#            show.intercept = TRUE, 
#            show.values = TRUE,
#            p.threshold = 0.1, #manually adjust to account for directional test
#            ci.lvl = 0.90 ) + #manually adjusted for directional test   
#   labs(title = "Model Estimate | Odds Ratio",
#        subtitle = "",
#        x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
result <- model_parameters(m, exponentiate = TRUE, component = "all")
plot(result, show_intercept = TRUE, show_labels = TRUE) 
# + theme_clean()


## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
plot_model(m, type="eff",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Prediction | Probability of Drawing Type",
           axis.title = c("Condition","Probability of Drawing Type"))



```

```{r}
#::::: GGDIST POSTERIOR PROBABILITY OF RESPONSE
##WORKING
# https://mjskay.github.io/ggdist/reference/stat_slab.html
## VIS probability of correct response
#TAKES A REALLY LONG TIME

#1 | get draws
# draws <- df_i %>%
#   data_grid(pretty_condition, ospan_split, subject, q) %>%
#   add_epred_draws(m,
#                    # ndraws = 100, # n = 100,
#                    # dpar = TRUE,
#                    transform = TRUE, #gives prob%, otherwise OR
#                    re_formula = NA)
# # draws %>% write_rds(file = "analysis/SGC3A/models/draws/draws_Bmm.catCOrSQ_OPSAN.rds")
# 
# #OR load from file
# # draws <- read_rds(file = "analysis/SGC3A/models/draws/draws_Bmm.catCOrSQ_OPSAN.rds")
# 
# #2| VISUALIZE PREDICTIONS | GGDIST
# ##TODO figure out height normalization.
# ##do it with much smaller number of draws 
# #TODO adjust bandwidth/smoothing? + put on same line + 
# #TAKES A REAAALY LONG TIME
# # d <- 
# 
# d <- draws %>% sample_n(10) %>% 
#   ggplot(aes(x = .epred,  y = pretty_condition, fill = ospan_split)) +
#   stat_slab(width = c(.95), alpha = 0.5, normalize="xy") +
#   facet_wrap(~.category) +
#   #   #normalize = all, panels, xy, groups, none
#   xlim(0,1) + labs(
#     title = "Model Predicted Probability of Correct Response",
#     x = "probability of correct response",
#     y = "Interpretation"
#   ) +  theme_clean() #+ ggeasy::easy_remove_legend() + ggeasy::easy_remove_y_axis()
# # # #TO PLOT ON THE SAME LINE, INCLUDE Y = 0 in aes and ggeasy::remove_y_axis()
# # # 
# # # ggsave(d, filename = "figures/sgc3a_BBm.cat.CrSQ_lab_posterior.svg", width = 6, height =4)
# d
```

```{r}
#| label: TBL-BRMS-STATE

#DISPLAY MODEL AS TABLE
tab_model(m)

# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m, "(log odds)" = m)
# notes = list('"* p < 0.05, ** p < 0.01, *** p < 0.001"',
#                'N(subject) = 133 $\tau_{00}$(subject) = 34.85',
#              'N(question) = 13 $\tau_{00}$(question) = 1.14')
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"),
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',
#              notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_online.tex")
# # #              # coef_omit = "Intercept",
#TODO OUTPUT TABLE 

#https://arelbundock.com/posts/modelsummary_multinomial_logit/
# modelsummary(m)



```



## H0 | ACCURACY FULL MODEL

Full model (with specific interactions; full model with 4 way interaction does not converge). Don't move forward with this strategy, as it is too convoluted to explain. 

#### Setup

```{r}

#FILTER THE DATASET

df_s <- df_subjects 

df_long <- df_s %>% dplyr::select(subject,pretty_condition,order,lm_scenarios,tm_scenarios,linear_score, triangular_score) %>% pivot_longer(
  cols = ends_with("score"),
  names_to = "graph",
  values_to = "score"
)

df_i <- df_items %>% #filter(q %nin% c(3)) %>%  #3 is nondiscrim
 mutate(
   order = fct_rev(order)
 )

#check factors
is.factor(df_i$subject) & is.factor(df_i$pretty_condition) &
  is.factor(df_i$q) & is.factor(df_i$order) & is.factor(df_i$scenario)

table(df_subjects$pretty_condition)
```

#### Describe

```{r}
#| label: DESCR-H1
#| 
title = "Descriptive Statistics of Response Accuracy by Block (CONTROL Condition)"
abs.stats <- rbind(
  "linear.block"= df_s %>% dplyr::select(linear_score) %>% unlist() %>% favstats(),
  "triangular.block" = df_s %>% dplyr::select(triangular_score) %>% unlist() %>% favstats(),
  "block.differences" = df_s %>% dplyr::select(score_diff) %>% unlist() %>% favstats()
)

abs.stats %>% kbl (caption = title) %>% kable_classic() %>%
  footnote(general = "block # questions correct [0,15]; DIFF = triangular - linear",
           general_title = "Note: ",footnote_as_chunk = T)
```

For participants in the CONTROL condition, total absolute scores for the LINEAR graph (n = `r abs.stats["linear.block",]$n`) range from `r round(abs.stats["linear.block",]$min,2)` to `r round(abs.stats["linear.block",]$max,2)` with a mean score of (M = `r round(abs.stats["linear.block",]$mean,2)`, SD = `r round(abs.stats["linear.block",]$sd,2)`).

For participants in the CONTROL condition, total absolute scores for the TRIANGULAR graph (n = `r abs.stats["triangular.block",]$n`) range from `r round(abs.stats["triangular.block",]$min,2)` to `r round(abs.stats["triangular.block",]$max,2)` with a mean score of (M = `r round(abs.stats["triangular.block",]$mean,2)`, SD = `r round(abs.stats["triangular.block",]$sd,2)`).

#### Visualize

```{r}

#:::::::: STACKED PROPORTIONAL BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(order~graph) + 
   labs(title = "Accuracy by GRAPH, CONDITION and ORDER",
       x = "Condition",
       subtitle="Accuracy appears to differ only on the triangular graph.")

#:::::::: STACKED PROPORTIONAL BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(scenario ~ graph) + 
   labs(title = "Accuracy by GRAPH, CONDITION and SCENARIO",
       x = "Condition",
       subtitle="Triangular accuracy appears to differ by scenario")


#:::::::: STACKED BAR CHART BY QUESTION
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = score)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_grid( graph ~ q) +
   labs(title = "Accuracy by GRAPH and QUESTION",
       x = "Condition",
       fill = "",
       subtitle="")

# ##VERTICAL RAINCLOUD USING GGDISTR
ggplot(df_subjects, aes(x = pretty_condition, y = linear_score,
                        fill = pretty_condition) ) +
  ggdist::stat_halfeye(
    side = "left",
    justification = 1.1,
    width = 1,
    point_colour = NA
   ) +
  geom_boxplot(
    inherit.aes = TRUE, #supress fill
    # mapping = aes(x=pretty_condition, y = score),
    width = .15,
    outlier.shape = NA
  ) +
  geom_point(
    inherit.aes = TRUE, #supress fill
    # mapping = aes(x=graph, y = score, color = graph),
    size = 1.3,
    alpha = .3,
    position = position_jitter(
      seed = 1, width = .05
  )) + labs(
    title = "Distribution of scores on LINEAR graph",
    x = "Condition", y = "Score (# correct)") +
  theme(legend.position = "blank") +
  coord_cartesian(xlim = c(0.5, NA), clip = "off")


ggplot(df_subjects, aes(x = pretty_condition, y = triangular_score,
                        fill = pretty_condition) ) +
  ggdist::stat_halfeye(
    side = "left",
    justification = 1.1,
    width = 1,
    point_colour = NA
   ) +
  geom_boxplot(
    inherit.aes = TRUE, #supress fill
    # mapping = aes(x=pretty_condition, y = score),
    width = .15,
    outlier.shape = NA
  ) +
  geom_point(
    inherit.aes = TRUE, #supress fill
    # mapping = aes(x=graph, y = score, color = graph),
    size = 1.3,
    alpha = .3,
    position = position_jitter(
      seed = 1, width = .05
  )) + labs(
    title = "Distribution of scores on TRIANGULAR graph",
    x = "Condition", y = "Score (# correct)") +
  theme(legend.position = "blank") +
  coord_cartesian(xlim = c(0.5, NA), clip = "off")

```


```{r}
#:::::::: STACKED PROPORTIONAL BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  facet_grid(order ~ scenario ~ graph) + 
  scale_fill_brewer(palette = "Set1")  +
   labs(title = "ACCURACY by Condition, GRAPH SCENARIO and ORDER",
       x = "Condition",
       subtitle="")

```




### MAYBE USE THIS GRAPH
```{r}

#PLOT PAIRED DATA
grouped_ggwithinstats(
  grouping.var = pretty_condition,
  data = df_long,
  x    = graph,
  y    = score, 
  type  = "nonparametric", #parametric, robust, bayes
  annotation.args = list(title = "Effect of Condition"),
)

```

#### MIXED EFFECTS LOGISTIC REGRESSION



##### Fit Model

```{r}
#| label: MODEL-FIT-ACC

## 0 | SETUP
#confirm 13 items [all discriminating items]
nrow(df_i) / nrow(df_s)  #28; removed nondiscrim Q3

#confirm all factors 
is.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$graph) && is.factor(df_i$score) && is.factor(df_i$order) && is.factor(df_i$scenario)

## 1 | SETUP RANDOM INTERCEPT SUBJECT


#:: RANDOM INTERCEPT SUBJECT + ITEM
print("Subject Intercept + Item intercept random model")
mm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = "binomial")
#summary(mm.rSQ)

## 2 | ADD FIXED EFFECT GRAPH

print("FIXED GRAPH + Subject & Item random intercepts")
mm.GrSQ <- glmer(accuracy ~ graph + (1|subject) + (1|q) ,
                data = df_i, family = "binomial")
# car::Anova(mm.GrSQ) #main effect graph
test_lrt(mm.rSQ,mm.GrSQ) #same as anova(m0, m1, test = "Chi")
#adding GRAPH improves fit 

## 2 | ADD FIXED EFFECT CONDITION

print("IXN GRAPH* Condition + Subject & Item random intercepts")
mm.GCrSQ <- glmer(accuracy ~ graph*pretty_condition + (1|subject) + (1|q) ,
                data = df_i, family = "binomial", 
                control=glmerControl(optimizer="bobyqa",
                                 optCtrl=list(maxfun=2e5)))
#car::Anova(mm.GCrSQ) #main effects graph + IXN graph*condition

test_lrt(mm.rSQ,mm.GCrSQ) #same as anova(m0, m1, test = "Chi")
#adding condition improves fit

# 2 | ADD IXN ORDER
print("FIXED Graph + ORDER + Subject & Item random intercepts")
mm.GC.COrSQ <- glmer(accuracy ~ graph*pretty_condition + graph*order + (1|subject) + (1|q),
                   data = df_i, family = "binomial", 
                   control=glmerControl(optimizer="bobyqa",
                                 optCtrl=list(maxfun=2e5)))
# summary(mm.GC.COrSQ)
#car::Anova(mm.GC.COrSQ, type = 3) #IXN graphX condition, GRAPH*order
test_lrt(mm.GCrSQ,mm.GC.COrSQ) 


# 3 | ADD FIXED EFFECT SCENARIO
print("FIXED Graph * SCENARIO * ORDER + Subject & Item random intercepts")
mm.GC.CO.GSrSQ <- glmer(accuracy ~ graph*pretty_condition + graph*order + graph*scenario +  (1|subject) + (1|q) ,
                data = df_i, family = "binomial",
                control=glmerControl(optimizer="bobyqa",
                                 optCtrl=list(maxfun=2e5)))
#summary(mm.GC.CO.GSrSQ)
#car::Anova(mm.GC.CO.GSrSQ, type = 3) #IXN graphXscenario, graphXorder, graphXcondition, 
test_lrt(mm.GC.COrSQ,mm.GC.CO.GSrSQ) #same as anova(m0, m1, test = "Chi")


simple <- glmer(accuracy ~ graph+pretty_condition +order + scenario +  (1|subject) + (1|q) ,
                data = df_i, family = "binomial",
                control=glmerControl(optimizer="bobyqa",
                                 optCtrl=list(maxfun=2e5)))


##full model won't converge, try to run again later
# full <- glmer(accuracy ~ graph*pretty_condition*order*scenario +  (1|subject) + (1|q) ,
#                 data = df_i, family = "binomial" )
# # ,
#                 control=glmerControl(optimizer="bobyqa",
#                                  optCtrl=list(maxfun=2e5)))


paste("ideal model better than simple model?")
test_lrt(simple, mm.GC.CO.GSrSQ)

paste("performance")
performance(m)
```


To test our experimental hypotheses, we fit a series of logistic mixed effects models predicting question-level accuracy (correct, incorrect) The best fitting model included interactions for graph X condition, graph X order, and graph X scenario, and likelihood ratio tests indicate this is a significantly better fit than a model including only main effects ($chi^2(10,16) = 112.79, p < 0.001$), or a more complex model with an interaction between all fixed factors TODO ($chi^2(22,9) = 11.87, p = 0.539$)



##### Describe

```{r}

# best model
m <- mm.GC.CO.GSrSQ

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m)

print("SIGNIFICANCE TEST [non directional]")
car::Anova(m, type=3) #TYPE 3 SS FOR IXNS

#:::::::: INTERPRET COEFFICIENTS

paste("LOG ODDS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald")

paste("ODDS RATIOS")
tidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = "Wald", exponentiate = TRUE)

paste("PROBABILITIES")
#probability control = plogis(intercept)
#probability impasse = plogis(intercept + coefficient)

#FROM merTools
#setup empty df 
newdata <- df_i %>% dplyr::select(graph, pretty_condition, order, scenario, subject, q)
#make predictions
preds <- predictInterval(m, newdata = newdata,
                              which = "fixed", #full, fixed or random for those only
                              type = "probability", #linear.prediction
                              stat = "median",
                              n.sims = 1000,
                              level = 0.80) #width of prediction interval
#join predictions to the new dataframe
preds <- cbind(newdata, preds)
#summarize
(summ_preds <- preds %>% 
  dplyr::select(graph, pretty_condition, scenario, order, fit, lwr, upr) %>% 
  group_by(graph, pretty_condition, scenario, order) %>% 
  dplyr::summarise(
    median = median(fit),
    lower = median(lwr),
    upper = median(upr)
  )) 

```


##### POSTHOCS

```{r}

library(emmeans)

##POST-HOC COMPARISONS
print("POSTHOC COMPARISONS :: GRAPH X CONDITION")
emmeans(m,  pairwise ~ graph*pretty_condition, 
        type = "response" , adjust = "tukey") #sidak, tukey


print("POSTHOC COMPARISONS :: GRAPH X ORDER")
emmeans(m,  pairwise ~ graph*order, 
        type = "response" , adjust = "tukey") #sidak, tukey
#POST HOC COMPARING TM graph with lm vs tm first order is NOT significant


print("POSTHOC COMPARISONS :: GRAPH X SCENARIO")
emmeans(m,  pairwise ~ graph*scenario, 
        type = "response" , adjust = "tukey") #sidak, tukey
#POST HOC COMPARING TM graph with lm vs tm first order is NOT significant


##PLOT PROBABILITY
# plot(ref_grid(m), by = "pretty_condition", type = "response")
```
##### Visualize

```{r}

## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
plot_model(m, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Estimate | Odds Ratio",
       subtitle = "",
       x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
# result <- model_parameters(m, exponentiate = TRUE, component = "all")
# plot(result)


## | PLOT TESTS

result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)

## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
p <- plot_model(m, type="int",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Prediction | Probability of Accurate Response")
p[[1]] + p[[2]] + p[[3]]

#PLOT MODEL PREDICTION
# plot_model(m, type = "pred")
# plot_model(m, type = "eff")
  # ylim(0,1) + 
  # labs(
  #   title = "Model Prediction | Probability of Accurate Response",
  #   subtitle = "Impasse increases Probability of Correct Response"
  # )



```

```{r}
#SJPLOT | MODEL | TABLE
tab_model(m)

# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m, "(log odds)" = m)
# notes = list('"* p < 0.05, ** p < 0.01, *** p < 0.001"',
#                'N(subject) = 133 $\tau_{00}$(subject) = 34.85',
#              'N(question) = 13 $\tau_{00}$(question) = 1.14')
# 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              gof_map = c("AIC", "sigma"),
#              gof_omit = 'RMSE|ICC|BIC',
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"),
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',
#              notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_online.tex")
# # #              # coef_omit = "Intercept",
```

##### Diagnostics

```{r}
#| warning: false
#| message: false

# print("SANITY CHECK REPORTING")
# report(m)

# print("MODEL PERFORMANCE")
# performance(m)

print("DIAGNOSTICS")
check_model(m)

```
















