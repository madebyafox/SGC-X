---
subtitle: 'Study SGC2 | Hypothesis Testing'
---

\newpage

# Hypothesis Testing {#sec-SGC2-hypotesting}

*The purpose of this notebook is test the hypotheses that determined the design of the SGC2 study.*

```{r}
#| label: SETUP
#| warning : false
#| message : false

library(Hmisc) # %nin% operator

library(ggpubr) #arrange plots
library(cowplot) #arrange shift function plots
library(ggformula) #easy graphs
library(vcd) #mosaic plots
library(vcdExtra) #mosaic plots
library(kableExtra) #printing tables 
library(sjPlot) #visualize model coefficients

#plot model estimates with uncertainty
library(ggdist)
library(broom)
library(modelr)
library(distributional)

#models and performance
library(lmerTest) #for CIs in glmer 
library(ggstatsplot) #plots w/ embedded stats
library(report) #easystats reporting
library(see) #easystats visualization
library(performance) #easystats model diagnostics
library(qqplotr) #confint on qq plot
library(gmodels) #contingency table and CHISQR
# library(equatiomatic) #extract model equation
# library(pscl) #zeroinfl / hurdle models 
library(lme4) #mixed effects models 
library(ggeffects) #visualization log regr models

library(tidyverse) #ALL THE THINGS

#OUTPUT OPTIONS
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
options(ggplot2.summarise.inform = FALSE)
options(scipen=1, digits=3)

#GRAPH THEMEING
theme_set(theme_minimal()) 

```

**Research Questions**

In SGC2 we compare learner performance on the linear and triangular model graphs by testing the effectiveness of 4 scaffolds and by seeking to replicate the Qiang et.al (2014) finding that after 20 minutes of video training, students perform faster and more accurately with the unconventional TM than the conventional Linear Model (LM). Will our participants show similar performance on the TM with scaffolds rather than formal instruction? Further, will engagement with the TM in a reading task be sufficient for students to reproduce the graph in a subsequent drawing task?

**Hypotheses**

1.  Learners without scaffolding (control) will perform better with the LM than TM
2.  Learners with (any form of) scaffolding will perform better with the TM than LM (replication of \[12\]).
3.  Based on observations in Study One we expect that graph-order will act as a scaffold. Learners who solve problems with the LM graph first will perform better on the TM (relative to TM-first learners) as their attention will be drawn to the salient differences between the graphs.

```{r}
#| label: IMPORT-DATA
#| warning : false
#| message : false

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
mbp = "/Users/amyfox/Sites/RESEARCH/SGCâ€”Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
setwd(mbp)

#IMPORT DATA 
df_items <- read_rds('analysis/SGC2/data/2-scored-data/sgc2_items.rds')
df_subjects <- read_rds('analysis/SGC2/data/2-scored-data/sgc2_participants.rds') 

```

## MODELLING

```{r}

df_items <- df_items %>% filter(graph != "none") %>% 
  mutate(graph = as.factor(graph)) #only include linear and triangular items

##FIT LINEAR MODEL
library(lme4)
m <- lm(triangular_score ~ pretty_condition + order + tm_scenarios+ linear_score, data = df_subjects)
summary(m)
check_model(m)

#+ linear_score
m <- lm(score ~  pretty_condition*graph, data = long_scores)
summary(m)
check_model(m)


##FI MIXED MODEL
library(lmerTest)
#on subject, score total per block
m0 <- lm( score ~ 1, data = long_scores)
summary(m0)

m.rS <- lmer(score ~ (1|subject), data = long_scores)
summary(m.rS)

#compare fit
compare_performance(m0,m.rS)
test_lrt(m0, m.rS)
#validates that mixed mod is justified 

mC.rS <- lmer(score ~ pretty_condition + (1|subject), data = long_scores)
summary(mC.rS)

#compare fit
compare_performance(m.rS,mC.rS)
test_lrt(m.rS, mC.rS)
#validates that fixed effect significantly improves fit 


##ITEM LEVEL MIXED MODEL

#empty model
m <- glm(correct ~ 1, data = df_items, family = "binomial")
summary(m)

#condition model
m1 <- glm(correct ~ condition, data = df_items, family = "binomial")
summary(m1)

compare_performance(m,m1)
test_lrt(m,m1)
#condition model is better fit than null

#graph model
m2 <- glm(correct ~ condition + graph + graph*condition, data = df_items, family = "binomial" )
summary(m2)

library(effects)
plot(allEffects(m2))

compare_performance(m1,m2)
test_lrt(m1,m2)
#condition model is better fit than null


#scenario model
m3 <- glm(correct ~ condition + graph + graph*condition + order + scenario, data = df_items, family = "binomial" )
summary(m3)

library(effects)
plot(allEffects(m2))

compare_performance(m1,m2)
test_lrt(m1,m2)
#condition model is better fit than null



#IGNORE THE ONES ABOVE 

#full model
m <- glm(correct ~ condition + order + scenario + condition*order + condition*scenario + order*scenario + condition*order*scenario, data = df_items %>% filter(graph=="triangular"), family = "binomial" )
summary(m)

library(effects)
plot(allEffects(m2))

compare_performance(m1,m2)
test_lrt(m1,m2)
#condition model is better fit than null

```

## H1 \| The Need for Scaffolding

**Hypothesis** The TM graph is not *discoverable* and requires scaffolding for correct interpretation. We predict that learners without scaffolding (the control condition) will perform better with the LM than TM

+-----------------------+-------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Do Ss in the CONTROL condition perform better on the LINEAR graph than the TRIANGULAR graph?                            |
+=======================+=========================================================================================================================+
| **Hypothesis**        | Ss in the CONTROL condition will have higher scores on the LINEAR graph than the TRIANGULAR graph                       |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------+
| **Data**              | data: `df_subjects` where `condition == 1`                                                                              |
|                       |                                                                                                                         |
|                       | outcome:                                                                                                                |
|                       |                                                                                                                         |
|                       | -   *linear graph accuracy* `linear_score` \[absolute score\]                                                           |
|                       | -   *triangular graph accuracy* `triangular_score`                                                                      |
|                       |                                                                                                                         |
|                       | predictor: `graph` (block) \[within-subjects factor\]                                                                   |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  Paired Samples T-Test                                                                                               |
|                       |     -   compare average accuracy score in linear vs triangular block                                                    |
|                       |     -   either T-Test or Wilcoxon Rank Sum (paired sample) alternative if difference scores are is non normal           |
|                       |                                                                                                                         |
|                       | *Alternative*                                                                                                           |
|                       |                                                                                                                         |
|                       | 1.  Linear Mixed Effects Model \[violates normality of residuals\]                                                      |
|                       |     -   predict subject `score` \[0-15\] by `graph` with random intercept for `subject`                                 |
|                       |     -   demonstrate that `score` is independent of `order` \[nested\] and `scenario` \[nested\]                         |
|                       | 2.  Logistic Mixed Effects Model                                                                                        |
|                       |     -   predict item `score` \[0,1\] by `graph` with random intercept for `subject` and random intercept for `question` |
|                       |                                                                                                                         |
|                       |     -   demonstrate than `score` is independent of `order` and `scenario`                                               |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------+
| **Notes**             |                                                                                                                         |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------+

```{r}
#| label: SETUP-H1

#FILTER THE DATASET
df <- df_subjects %>% filter(condition == 1)

df_long <- df %>% dplyr::select(subject,pretty_condition,order,lm_scenarios,tm_scenarios,linear_score, triangular_score) %>% pivot_longer(
  cols = ends_with("score"),
  names_to = "graph",
  values_to = "score"
)
  

title = "Descriptive Statistics of Response Accuracy by Block (CONTROL Condition)"
abs.stats <- rbind(
  "linear.block"= df %>% dplyr::select(linear_score) %>% unlist() %>% favstats(),
  "triangular.block" = df %>% dplyr::select(triangular_score) %>% unlist() %>% favstats(),
  "block.differences" = df %>% dplyr::select(score_diff) %>% unlist() %>% favstats()
)

abs.stats %>% kbl (caption = title) %>% kable_classic() %>%
  footnote(general = "block # questions correct [0,15]; DIFF = triangular - linear",
           general_title = "Note: ",footnote_as_chunk = T)

#DISTRIBUTION OF SCORE
gf_dhistogram(~score, fill = ~graph, data = df_long) %>% gf_facet_wrap(~graph)+
  labs(title = "Distribution of scores in CONTROL condition") + 
  easy_remove_legend()

##VERTICAL RAINCLOUD USING GGDISTR
ggplot(df_long, aes(x = graph, y = score,
                        fill = graph) ) + 
  ggdist::stat_halfeye(
    side = "left",
    justification = 1.1,
    width = 1, 
    point_colour = NA
   ) + 
  geom_boxplot(
    inherit.aes = FALSE, #supress fill
    mapping = aes(x=graph, y = score),
    width = .15, 
    outlier.shape = NA
  ) + 
  geom_point(
    inherit.aes = FALSE, #supress fill
    mapping = aes(x=graph, y = score, color = graph),
    size = 1.3,
    alpha = .3,
    position = position_jitter( 
      seed = 1, width = .05
  )) + labs( 
    title = "Distribution of scores in CONTROL condition", 
    x = "Condition", y = "Score (# correct)") +
  theme(legend.position = "blank") + 
  coord_cartesian(xlim = c(0.5, NA), clip = "off")


#DISTRIBUTION OF SCORE
gf_dhistogram(~score_diff, data = df) %>% 
  gf_fitdistr(~score_diff) + 
  labs(title = "Distribution of paired score differences in CONTROL condition") + 
  easy_remove_legend() 



```

For participants in the CONTROL condition, total absolute scores for the LINEAR graph (n = `r abs.stats["linear.block",]$n`) range from `r round(abs.stats["linear.block",]$min,2)` to `r round(abs.stats["linear.block",]$max,2)` with a mean score of (M = `r round(abs.stats["linear.block",]$mean,2)`, SD = `r round(abs.stats["linear.block",]$sd,2)`).

For participants in the CONTROL condition, total absolute scores for the TRIANGULAR graph (n = `r abs.stats["triangular.block",]$n`) range from `r round(abs.stats["triangular.block",]$min,2)` to `r round(abs.stats["triangular.block",]$max,2)` with a mean score of (M = `r round(abs.stats["triangular.block",]$mean,2)`, SD = `r round(abs.stats["triangular.block",]$sd,2)`).

Visual inspection of the distribution of scores for each block reveal that scores in on the triangular task were more variant than those in the linear graph. On average, scores on the triangular block were lower than those on the linear block.

### PAIRED SAMPLES T-TEST

#### Check Assumptions

```{r}

#PAIRED T TEST ASSUMPTIONS

# 1| PAIRED?
paste("1| Data are paired? ", "YES, block is crossed within subjects")
paste("2| Sample size? ", "YES, sample size ",nrow(df), "> 30")
paste("3| Paired differences are normally distributed? accept null [normal] at p > 0.05")
shapiro.test(df$score_diff) 

```
_Because the difference scores are not normally distributed, we don't meet the assumptions of a standard paired t-test. Instead, we should use the alternative test Wilcoxon Rank Sum [paired] designed for non-normal distributions. 

#### Visualize

```{r}

#PLOT PAIRED DATA
#subset linear
linear <- subset(df_long,  graph == "linear_score", score,
                 drop = TRUE)
# subset triangular
triangular <- subset(df_long,  graph == "triangular_score", score,
                 drop = TRUE)
# Plot paired data
library(PairedData)
pd <- paired(linear, triangular)
plot(pd, type = "profile") + theme_bw() + labs(title = "Paired Data | Control Condition scores by block")

```

#### Run Test (Wilcoxon Paired Rank Sum) 

```{r}

#WILCOXON RANK SUM PAIRED T-TEST
w <- wilcox.test(df$linear_score, df$triangular_score, 
            paired = TRUE, alternative = "greater", conf.int = TRUE)
w
report(w)
```
#### Inference

The Wilcoxon signed rank test confirms that (for subjects in the control condition) scores on the **triangle graph** were significantly lower than those in the **linear graph** block.  This provides evidence in support of our hypothesis that the Triangular model graph (though computationally efficient) is in fact unconventional and lacking in discoverablility. It needs to be augmented with scaffolding in order to be correctly interpreted by novice readers. 


## H2 \| The Effectiveness of Scaffolding

**Hypothesis** All of the designs offered by participants in Study 1 are promising. We expect that only a small amount of scaffolding (a little nudge) will be required to help readers correctly interpret the graph. We predict that learners with (any form of) scaffolding will perform better with the TM than LM (a replication of Qiang et. al's finding on the computational efficiency of the TM graph).


### MIXED EFFECTS LINEAR REGRESSION

*Fit a linear mixed effects model (at the subject level), predicting block accuracy (absolute score) by graph, with a random intercept for subject.*

#### Fit Model

*First, we fit a linear regression with graph as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-CONTROLSCORE-LM
#| warning: false
#| message: false

# MODEL FITTING:::::::::::::::::::::::::::::::::::::

#: 1 EMPTY MODEL [grand mean as intercept]
m0 = lmer(score ~ (1|subject), data = df_long)
print("EMPTY MODEL")
summary(m0)

#: 2 GRAPH as predictor
m1 <- lmer( score ~ graph + (1|subject), data = df_long)
print("PREDICTOR MODEL")
summary(m1)

#: 3 TEST SUPERIOR FIT
test_lrt(m0,m1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,m1))$p[2])


#: 4 add additional predictors [shouldn't be significant]
m2 <- lmer( score ~ graph + order + tm_scenarios + (1|subject), data = df_long)
print("PREDICTOR MODEL")
summary(m2)

check_model(m2)
```

*The GRAPH predictor significantly improves model fit.*

#### Visualize

```{r}
#| label: VIS-CONTROLSCORE-LM

# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: 

#: 4 ASSESS PERFORMANCE
print("MODEL PERFORMANCE")
performance(m1)
print("SANITY CHECK REPORTING")
report(m1)

#: PLOT

#GGSTATS | MODEL | LOG ODDS 
# library(ggstatsplot)
# ggcoefstats(m1, output = "plot") + labs(x = "Log Odds Estimate")

#SJPLOT | MODEL | RANDOM EFFECTS
#library(sjPlot)
# plot_model(m1, type = "re") +
# labs(title = "Model Random Effects")

#SJPLOT | MODEL | PROBABILITIES
plot_model(m1, type="pred",
           title = "Model Predicted Score",
           axis.title = c("Graph Block","Score [0,15]"))

```

#### Diagnostics

```{r}
check_model(m1)
binned_residuals(m1)
```

#### Inference

We fit a linear mixed effects model to analyze the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 3.16, p = 0.0016). The model predicts that the odds of a correct response on the first question in the *impasse condition* increase by 146% ($e^{beta_1}$ = 2.46, 95% CI \[1.42, 4.37\]) over the *control condition*.

*Equivalent statements:*

-   being in impasse condition increases log odds of correct response by 0.901 (over control)
-   being in impasse increases odds of correct response in impasse over control increases by factor of 2.46
-   probability of correct response in control predicted as 28.5%, vs only 14% in control condition

```{r}
#PRETTY TABLE SJPLOT
tab_model(m1)
```

#### TODO

-   Are these residuals OK? I didn't think normally distributed residuals were an assumption for logistic regression.
-   Interpretation/reporting of model fit?
-   sanity check correct interpretation of coefficients & reporting

### LOGISTIC REGRESSION

*Fit a logistic regression (at the subject level), predicting Q1 accuracy (absolute score) by condition.*

#### Fit Model

*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-Q1ACC-LOG-combined
#| warning: false
#| message: false

#combined
df <- df_q1 

# FREQUENCY TABLE
# my.table <- table(df$accuracy, df$pretty_condition)
# addmargins(my.table) #counts
# addmargins(prop.table(my.table)) #props

# MODEL FITTING:::::::::::::::::::::::::::::::::::::

#: 1 EMPTY MODEL baseline glm model intercept only
m0 = glm(accuracy ~ 1, data = df, family = "binomial")
print("EMPTY MODEL")
summary(m0)

#: 2 CONDITION model
m1 <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
print("PREDICTOR MODEL")
summary(m1)

#: 3 TEST SUPERIOR FIT
paste("AIC wth predictor is lower than empty model?", m0$aic > m1$aic)
test_lrt(m0,m1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,m1))$p[2])
```

*The Condition predictor significantly improves model fit.*

#### Visualize

```{r}
#| label: MODEL-Q1ACC-LOG-combined

# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: 

print("PREDICTOR MODEL")
# summary(m1)

#: INTERPRET COEFFICIENTS

print("Coefficients â€”- LOG ODDS")
confint(m1)
print("Coefficients â€”- ODDS RATIOS")
e <- cbind( exp(coef(m1)), exp(confint(m1))) #exponentiate
e

print("MODEL PERFORMANCE")
performance(m1)
print("SANITY CHECK REPORTING")
report(m1)

print("MODEL PREDICTIONS")
# Retrieve predictions as probabilities 
# (for each level of the predictor)
p.control <- predict(m1,data.frame(pretty_condition="control"),type="response")
paste("Probability of success in control,", p.control)
p.impasse <- predict(m1,data.frame(pretty_condition="impasse"),type="response")
paste("Probability of success in impasse,", p.impasse)

#: PLOT

#GGSTATS | MODEL | LOG ODDS 
# library(ggstatsplot)
# ggcoefstats(m1, output = "plot") + labs(x = "Log Odds Estimate")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m1, type="std2", vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE) +  
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
# plot_model(m1, type="pred",
#            show.intercept = TRUE, 
#            show.values = TRUE,
#            title = "Model Predicted Probability of Accuracy",
#            axis.title = c("Condition","Probability of Accurate Response"))

#GGEFFECTS | MODEL | PROBABILITIES
# library(ggeffects)
ggeffect(model = m1) %>% plot()


#SANITY CHECK SJPLOT
# library(effects)
# plot(allEffects(m))

```

#### Diagnostics

```{r}
check_model(m1)
binned_residuals(m1)
```

#### Inference

We fit a logistic regression model to analyze the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 3.16, p = 0.0016). The model predicts that the odds of a correct response on the first question in the *impasse condition* increase by 146% ($e^{beta_1}$ = 2.46, 95% CI \[1.42, 4.37\]) over the *control condition*.

*Equivalent statements:*

-   being in impasse condition increases log odds of correct response by 0.901 (over control)
-   being in impasse increases odds of correct response in impasse over control increases by factor of 2.46
-   probability of correct response in control predicted as 28.5%, vs only 14% in control condition

```{r}
#PRETTY TABLE SJPLOT
tab_model(m1)
```

#### TODO

-   Are these residuals OK? I didn't think normally distributed residuals were an assumption for logistic regression.
-   Interpretation/reporting of model fit?
-   sanity check correct interpretation of coefficients & reporting



## H3 \| Computational Efficiency

**Hypothesis** Qiang et. al found that the TM graph was more computationally efficient than the LM graph. We expect that for learners that *do* correctly interpret the graph, they will have lower response times for the TM vs. LM graph.\*\*

## H4 \| Graph Order as Scaffold

**Hypothesis** Based on observations in Study One we expect that graph-order will act as a scaffold. Learners who solve problems with the LM graph first will perform better on the TM (relative to TM-first learners) as their attention will be drawn to the salient differences between the graphs.

## DIAGRAMS 2018 PUBLICATION ANALYSIS

## RESOURCES

reset plot margins par(mar=c(1,1,1,1))

### Logistic Regression Notes

*The logistic regression intercept gives the log odds of the outcome for the reference level of the predictor variable*

*The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.*
