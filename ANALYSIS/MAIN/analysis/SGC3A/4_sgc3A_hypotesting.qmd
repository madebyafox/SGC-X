---
subtitle: 'Study SGC3A | 4 Hypothesis Testing'
---

\newpage

# Hypothesis Testing {#sec-SGC3A-hypotesting}

**TODO**

-   HURDLE MODEL? (mixture model w/ 0 + count)
-   review models already created in ARCHIVE?
-   separate combined (vs) lab (vs) online replication \[once analysis strategy locked down\]

*The purpose of this notebook is test the hypotheses that determined the design of the SGC3A study.*

+---------------------+
| Pre-Requisite       |
+=====================+
| 2_sgc3A_scoring.qmd |
+---------------------+

```{r}
#| label: SETUP
#| warning : false
#| message : false

library(Hmisc) # %nin% operator

library(ggpubr) #arrange plots
library(cowplot) #arrange shift function plots
library(ggformula) #easy graphs
library(vcd) #mosaic plots
library(vcdExtra) #mosaic plots
library(kableExtra) #printing tables 
library(sjPlot) #visualize model coefficients

#plot model estimates with uncertainty
library(ggdist)
library(broom)
library(modelr)
library(distributional)

#models and performance
library(ggstatsplot) #plots w/ embedded stats
library(report) #easystats reporting
library(see) #easystats visualization
library(performance) #easystats model diagnostics
library(qqplotr) #confint on qq plot
library(gmodels) #contingency table and CHISQR
library(equatiomatic) #extract model equation
library(pscl) #zeroinfl / hurdle models 
library(lme4) #mixed effects models 

library(tidyverse) #ALL THE THINGS

#OUTPUT OPTIONS
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
options(ggplot2.summarise.inform = FALSE)
options(scipen=1, digits=3)

#GRAPH THEMEING
theme_set(theme_minimal()) 

```

**Research Questions**

In SGC3A we set out to answer the following question: Does posing a mental impasse improve performance on the graph comprehension task?

**Experimental Hypothesis**\
*Learners posed with scenario designed to evoke a mental impasse will be more likely to correct interpret the graph.*

-   H1A \| Learners in the IMPASSE condition will be more likely to correctly answer the first question than learners in CONTROL.
-   H1B \| Learners in the IMPASSE condition will score higher on the TEST Phase than learners in CONTROL.

**Null Hypothesis**\
*No significant differences in performance will exist between learners in the IMPASSE and CONTROL conditions.*

```{r}
#| label: IMPORT-DATA
#| warning : false
#| message : false

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
mbp = "/Users/amyfox/Sites/RESEARCH/SGCâ€”Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
setwd(mbp)

#IMPORT DATA 
df_items <- read_rds('analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.rds')
df_subjects <- read_rds('analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds')

#PREP DATA 
df_lab <- df_subjects %>% filter(pretty_mode == "laboratory")
df_online <- df_subjects %>% filter(pretty_mode == "online-replication")

```

## H1A \| Q1 ACCURACY

**Do Ss in the IMPASSE condition have a higher likelihood of producing a correct response to the first question?**

The graph comprehension tasks includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their *first exposure* to the unconventional triangular coordinate system.

### Accuracy of Q1 by Condition

+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Does the frequency of correct (vs) incorrect responses on the first question differ by condition? \[Is response accuracy independent of condition?\]                                                                                   |
+=======================+========================================================================================================================================================================================================================================+
| **Hypothesis**        | H1A \| Ss in the IMPASSE condition will have a higher likelihood of correctly responding to the first question than those in the CONTROL condition                                                                                     |
+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | data: `df_items` where `q == 1`                                                                                                                                                                                                        |
|                       |                                                                                                                                                                                                                                        |
|                       | outcome:                                                                                                                                                                                                                               |
|                       |                                                                                                                                                                                                                                        |
|                       | -   *accuracy* ( factor(incorrect/correct) from `score_niceABS` \[absolute score\]                                                                                                                                                     |
|                       |                                                                                                                                                                                                                                        |
|                       | -   *interpretation* (ordered factor from `interpretation`)                                                                                                                                                                            |
|                       |                                                                                                                                                                                                                                        |
|                       | predictor: `condition` \[between-subjects factor\]                                                                                                                                                                                     |
+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  Logistic Regression on `accuracy` predicted by `condition`                                                                                                                                                                         |
|                       |     -   account for difference in odds of correct score by condition                                                                                                                                                                   |
|                       | 2.  Ordinal Regression on `interpretation` predicted by `condition`                                                                                                                                                                    |
|                       |     -   account for difference in (ordered correctness of interpretation) by condition                                                                                                                                                 |
|                       |                                                                                                                                                                                                                                        |
|                       | Alternative:                                                                                                                                                                                                                           |
|                       |                                                                                                                                                                                                                                        |
|                       | -   Chi-Square test of independence on outcome `score_niceABS` by `condition`                                                                                                                                                          |
+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Notes**             | -   CHIQ SQR is simplest method to examine independence of two categorical factors; LOGISTIC REGRESSION is recommended for binomial \~ continuous; though with regression we can quantify the size of the effect and overall model fit |
|                       | -   independence assumption : (CHI SQR) as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent                                                           |
|                       | -   cell frequency : (CHI SQR) expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)                                                                      |
+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

```{r}
#| label: SETUP-Q1ACC

#FILTER THE DATASET
df_q1 = df_items %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" = "incorrect", "1"="correct")
)
 
#GROUPED PROPORTIONAL BAR CHART
# gf_props(~accuracy, fill = ~pretty_condition, 
#        position = position_dodge(), data = df_q1) %>% 
#   gf_facet_grid(~pretty_mode) +
#    labs(x = "Correct Response on Q 1",
#        title = "Accuracy on First Question by Condition",
#        subtitle="Impasse Condition yields a greater proportion of correct responses") #theme(legend.position = "none")

#STACKED BAR CHART
df_q1 %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~pretty_mode) + 
   labs(#y = "Correct Response on Q 1",
       title = "Accuracy on First Question by Condition",
       x = "Condition",
       fill = "",
       subtitle="Impasse Condition yields a greater proportion of correct responses")

```

A proportional bar chart visualizing the proportion of incorrect (vs) correct responses in each condition for each data collection modality (left/right facet) reveals that the pattern of responses appear the same regardless of the data collection modality. In both data collection sessions, the proportion of incorrect responses is much greater than the proportion of correct responses, regardless of condition (marginal probability of incorrect). In the impasse condition, the difference in proportions is smaller than the control condition (conditional probability of success in impasse; (i.e) There are more correct responses in the impasse condition than the control condition).

#### LOGISTIC REGRESSION

##### Model

```{r}
#| label: MODEL-Q1ACC-LOG-combined
#| warning: false
#| message: false

#combined
df <- df_q1 %>% dplyr::select(accuracy,score_niceABS,pretty_condition,pretty_mode, rt_s)

# FREQUENCY TABLE
# my.table <- table(df$accuracy, df$pretty_condition)
# addmargins(my.table) #counts
# addmargins(prop.table(my.table)) #props

#:::::::::::::::::::::::::::::::::::::::
#CONDITION MODEL
print("LOGISTIC REGRESSION MODEL")
m <- glm( accuracy ~ pretty_condition, 
          data = df, family = "binomial")
summary(m)
print("Coefficients â€”- LOG ODDS")
confint(m)
print("Coefficients â€”- ODDS RATIOS")
e <- cbind( exp(coef(m)), exp(confint(m))) #exponentiate
e
print("MODEL PERFORMANCE")
performance(m)
print("SANITY CHECK REPORTING")
report(m)

print("MODEL PREDICTIONS")
# Retrieve predictions as probabilities 
# (for each level of the predictor)
p.control <- predict(m,data.frame(pretty_condition="control"),type="response")
paste("Probability of success in control,", p.control)
p.impasse <- predict(m,data.frame(pretty_condition="impasse"),type="response")
paste("Probability of success in impasse,", p.impasse)

#GGSTATS | MODEL | LOG ODDS 
# library(ggstatsplot)
ggcoefstats(m, output = "plot") + labs(x = "Log Odds Estimate")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m, type="std2", vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE) +  
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
# plot_model(m, type="pred",
#            show.intercept = TRUE, 
#            show.values = TRUE,
#            title = "Model Predicted Probability of Accuracy",
#            axis.title = c("Condition","Probability of Accurate Response"))

#GGEFFECTS | MODEL | PROBABILITIES
library(ggeffects)
ggeffect(model = m) %>% plot()


#SANITY CHECK SJPLOT
# library(effects)
# plot(allEffects(m))

```

##### Diagnostics

```{r}
check_model(m)
binned_residuals(m, n_bins=10)
```

##### Inference

We fit a logistic regression model to analyze the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 3.16, p = 0.0016). The model predicts that the odds of a correct response on the first question in the *impasse condition* increase by 146% ($e^{beta_1}$ = 2.46, 95% CI \[1.42, 4.37\]) over the *control condition*.

*Equivalent statements:*

-   being in impasse condition increases log odds of correct response by 0.901 (over control)
-   being in impasse increases odds of correct response in impasse over control increases by factor of 2.46
-   probability of correct response in control predicted as 28.5%, vs only 14% in control condition

##### TODO

-   Are these residuals OK? I didn't think normally distributed residuals were an assumption for logistic regression.
-   Interpretation/reporting of model fit?
-   sanity check correct interpretation of coefficients & reporting

#### TODO ORDINAL REGRESSION

-   https://stats.oarc.ucla.edu/r/faq/ologit-coefficients/
-   https://journals.sagepub.com/doi/full/10.1177/2515245918823199
-   todo see ordinal regression video

```{r}
# #CREATE DATAFRAME OF Q1
# df <- df_items %>% filter(q ==1) %>% mutate(scaled = as.factor(score_SCALED))
# 
# #MODEL
# m <- polr(scaled ~ condition , data = df, Hess=TRUE)
# summary(m)
# confint(m)
# performance(m)
# report(m)
# 
# #exponentiate coefficients and CIs 
# ci <- confint(m)
# ci
# e <- coef(m)
# e
# # exp(cbind(e,ci))
# 
# # Retrieve predictions as probabilities 
# # (for each level of the predictor)
# # p.control <- predict(m,data.frame(condition="111"),type="response")
# # paste("Probability of success in control,", p.control)
# # p.impasse <- predict(m,data.frame(condition="121"),type="response")
# # paste("Probability of success in impasse,", p.impasse)
# 
# # Plot Predicted data and original data points
# # ggplot(df, aes(x=condition, y=accuracy)) + 
# #   geom_point() +
# #   stat_smooth(method="glm", color="green", se=FALSE,
# #                 method.args = list(family=binomial))
#   
# #TO PLOT ALL EFFECTS
# library(effects)
# plot(allEffects(m))
# 
# #SJPLOT
# library(sjPlot)
# plot_model(m, )
# 
# 
# #CONVERT TO PROBABILITIES
# newdat <- data.frame(condition=c("111","121"))
# prob <- (phat <- predict(object = m, newdat, type="p"))
# prob
# 

```

## H1B \| TEST PHASE ACCURACY

**Do Ss in the IMPASSE condition have higher scores in the TEST Phase of the task?**

The graph comprehension tasks includes 13 interpretation-discriminant questions completed in sequence. The first five questions constitute the 'scaffold' phase, and the remaining 8 the test phase. By examining the effect of condition on test-phase scores, we can evaluate if the impasse scaffold has an effect after it it has been removed.

+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Do Ss in the IMPASSE condition score higher in the test phase than those in the CONTROL group.                                                          |
+=======================+=========================================================================================================================================================+
| **Hypothesis**        | (H1B) Participants in the IMPASSE condition will have higher test phase performance than those in the CONTROL condition.                                |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | **data**: `df_items` where `q in 7,8,10,11,12,13,14,15` (the 8 discriminating test phase questions), `df_subjects`                                      |
|                       |                                                                                                                                                         |
|                       | **outcome**:                                                                                                                                            |
|                       |                                                                                                                                                         |
|                       | -   \[at item level\]                                                                                                                                   |
|                       |     -   *accuracy* ( factor(incorrect/correct) from `score_niceABS` \[absolute score\]                                                                  |
|                       |     -   *interpretation* (ordered factor from `interpretation`)                                                                                         |
|                       | -   \[subject level\]                                                                                                                                   |
|                       |     -   p_accuracy (percent of correct responses)                                                                                                       |
|                       |                                                                                                                                                         |
|                       | **predictor**: `condition` \[between-subjects factor\]                                                                                                  |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  Shift in Modal Mass (descriptive)\                                                                                                                  |
|                       |     describe & visualize shift in deciles between conditions for `` `scaled_score` `` (at subject level)                                                |
|                       | 2.  Mixed Logistic Regression\                                                                                                                          |
|                       |     `accuracy` \~ `condition` + (1 \| `subject` )\                                                                                                      |
|                       |     model effect of condition on probability of correct response \[during test phase\] while accounting for subject (and item-level?) effects           |
|                       | 3.  Ordinal Mixed Logistic Regression\                                                                                                                  |
|                       |     `interpretation` \~ `condition` + (1 \| `subject` )\                                                                                                |
|                       |     model effect of condition on \[ordered correctness of interpretation\] \[during test phase\] while accounting for subject (and item-level?) effects |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Alternatives**      | -   OLS LINEAR REGRESSION                                                                                                                               |
|                       |     -   bimodal distribution at tails makes the mean a poor predictor; LMs violate assumptions of normally distributed residuals                        |
|                       |     -   lm `DV_percent_test_NABS` \~ `condition` (absolute scoring) OR lm `item_test_SCALED` \~ `condition` (scaled scoring)                            |
|                       |     -   both absolute and scaled scores yield non-normal residuals                                                                                      |
|                       |     -   no transformation of the outcome variables yield normal residuals                                                                               |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Notes**             | **Also exploring:**                                                                                                                                     |
|                       |                                                                                                                                                         |
|                       | -   Hurdle model (mixture model w/ binomial + \[poisson or negbinom count; 0s from 1 DGP)                                                               |
|                       | -   Zero Inflated model (mixture model w/ binomial + poisson or negbinom count; 0s from 2 DGPs)                                                         |
|                       | -   Beta regression hurdle model? (mixture with location and scale parameters \[mean, variance\] and hurdles for floor and ceiling effects)             |
|                       | -   Other way to account for the severe bimodality?                                                                                                     |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+

```{r}
#| label: SETUP-TEST-ACC

#PREPARE DATA 
n_items = 8 #number of items in test

#item level
df_test = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(
  accuracy = as.factor(score_niceABS),
  q = as.factor(q)
)

#subject level
df_Stest = df_subjects %>% mutate(
  p_accuracy = item_test_NABS/n_items
)

#STACKED PROPORTIONAL BAR CHART
df_test %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~pretty_mode) + 
   labs(#y = "Correct Response on Q 1",
       title = "Accuracy on Test Phase Questions",
       x = "Condition",
       fill = "",
       subtitle="Impasse Condition yields a greater proportion of correct responses")

#GROUPED PROPORTIONAL BAR CHART
gf_props(~accuracy, fill = ~pretty_condition, x =~pretty_condition,
       position = position_dodge(), data = df_test) %>% 
  gf_facet_grid(~pretty_mode) +
   labs(x = "Correct Responses in Test Phase",
       title = "Accuracy in Test Phase by Condition",
       subtitle="Impasse Condition yields a greater proportion of correct responses")

 
#FACETED HISTOGRAM
stats = df_Stest %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(p_accuracy))
gf_props(~p_accuracy, 
         fill = ~pretty_condition, data = df_Stest) %>% 
  gf_facet_grid(pretty_condition ~ pretty_mode) %>% 
  gf_vline(data = stats, xintercept = ~mean, color = "red") +
  labs(x = "% Correct",
       y = "proportion of subjects",
       title = "Test Phase Absolute Score (% Correct)",
       subtitle = "") + theme(legend.position = "blank")

```

### Test Phase Absolute Score

#### WIP \| MIXED LOGISTIC REGRESSION

##### (Combined)

```{r}
#| label: MODEL-MLOG-ABS-comb

#SETUP DATA 
df <- df_test 

# SUBJECT INTERCEPT 
mm.rS <- glmer(accuracy ~ (1|subject), data = df,family = "binomial")
summary(mm.rS)

# SUBJECT + ITEM INTERCEPT 
mm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df,family = "binomial")
summary(mm.rSQ)

# SUBJECT INTERCEPT | CONDITION FIXED
mm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject), data = df,family = "binomial")
summary(mm.CrS)

# SUBJECT INTERCEPT | ITEM FIXED
mm.QrS <- glmer(accuracy ~ q + (1|subject), data = df,
                family = "binomial",nAGQ=0)
summary(mm.QrS)

# SUBJECT INTERCEPT | CONDITION + ITEM FIXED
mm.CQrS <- glmer(accuracy ~ pretty_condition + q + (1|subject), data = df,
                family = "binomial",nAGQ=0)
summary(mm.CQrS)

# SUBJECT + ITEM INTERCEPT | CONDITION FIXED
mm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q), data = df,family = "binomial")
summary(mm.CrSQ)

compare_performance(mm.rS, mm.rSQ, mm.CrS, mm.QrS, mm.CQrS,mm.CrSQ)

#SJ PLOTS TABLES
tab_model(mm.rS, mm.rSQ, mm.CrS, mm.QrS, mm.CQrS,mm.CrSQ)
```

```{r}

#THEORETICALLY MOTIVATED MODEL
m <- mm.CrS
report(m)
check_model(m)

paste("MODEL FORMULA")
paste(m@call[["formula"]])

#SJ PLOTS
#table of effects
sjPlot:: tab_model(m)

#PLOT RANDOM EFFECTS
library(lattice)
randoms <- ranef(m)
dotplot(randoms)

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m, type="std2", vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE) +  
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(m, type="pred",
           show.intercept = TRUE, 
           show.values = TRUE,
           title = "Model Predicted Probability of Accuracy",
           axis.title = c("Condition","Probability of Accurate Response"))

#SANITY CHECK SJPLOT
library(effects)
plot(allEffects(m)) 

```

#### TODO \| Ordinal Regression on ITEM-Interpretation


##### SHIFT IN MODAL MASS

The Effect of Condition on Total Absolute Test Score can be described as a 'shift' in mass between the two modes of each distribution.

*First, we use the Kolmogorov-Smirnov test as a Robust alternative to the t-test to test if the two distributions likely come from different populations.*

```{r}
#| label: COMPARE-DIST-NABS-comb

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
mbp = "/Users/amyfox/Sites/RESEARCH/SGCâ€”Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
setwd(mbp)

#(requires shift function files loaded)
#LOAD MODAL SHIFT FUNCTION RESOURCES
source("analysis/utils/shift_function/Rallfun-v30.txt")
source("analysis/utils/shift_function/wilcox_modified.txt")
source("analysis/utils/shift_function/rgar_visualisation.txt")
source("analysis/utils/shift_function/rgar_utils.txt")
#NOTE: something in these breaks the stat_ecdf in ggplot2

#PREP DATA 
df <- df_subjects %>%
  dplyr::select(s_SCALED, pretty_condition) %>%
  mutate(
    data = as.numeric(s_SCALED),
    #flip order levels to correctly orient graph
    # gr = recode_factor(pretty_condition, "impasse" = "impasse", "control"="control")
    gr = as.character(pretty_condition)
  ) %>% dplyr::select(data,gr)


g1 <- df %>% filter(gr == "control") %>% dplyr::pull(data)
g2 <- df %>% filter(gr == "impasse") %>% dplyr::pull(data)


#COMPARE DISTRIBUTIONS WITH ROBUST TESTS

#What do common tests say about the difference?

# Kolmogorov-Smirnov test
#If y is numeric, a two-sample (Smirnov) test of the null hypothesis that x and y 
#were drawn from the same continuous distribution is performed. Alternatively, y ...

#null is X is drawn from CDF EQUAL TO Y
ks.test(g1,g2) 
print("SUGGESTS that impasse and control come from different population distributions")

# #null is X is NOT LESS THAN Y
ks.test(g1,g2, alternative = "greater") 
print("SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]")

#REGULAR T-TEST
t.test(g1,g2) # regular Welsh t-test

```

```{r}
#| label: SHIFT-FN-NABS-comb
#| warnings: false
#| messages: false

#IF THIS ERRORS, consider loadling plyr (older than dplyr)
# kernel density estimate + rug plot + superimposed deciles
kde <- plot.kde_rug_dec2(df)
# kde

# compute shift function
out <- shifthd( g1, g2, nboot=200)

# plot shift function
sf <- plot.sf(data=out) # function from rgar_visualisation.txt
# sf

# combine KDE + SF
cowplot::plot_grid(kde, sf, labels=c("A", "B"), ncol = 1, nrow = 2, rel_heights = c(1.5, 1),label_size = 18,hjust = -1,scale=.95)

```







#### Cummulative Ordinal (Bayesian)

https://journals.sagepub.com/doi/full/10.1177/2515245918823199

```{r}
# library(brms)


# #DEFINE DATA 
# df <- df_items %>% mutate(
#   scaled = factor(score_SCALED, ordered = TRUE,  #ordered factor
#                     levels = c("-1", "-0.5", "0", "0.5","1"))
# )
# 
# ord_cum <- brm( formula = scaled ~ condition,
#                data = df,
#                family = cumulative("probit"),
#                file = "analysis/SGC3A/models/m_items_ord.cum.rds" # cache model (can be removed)  
# 
# )
# 
# summary(ord_cum)
# conditional_effects(ord_cum, "condition", categorical = TRUE)
# 
# #SJPLOT
# library(sjPlot)
# plot_model(ord_cum)
# 
# # m %>%
# #   spread_draws(b_Intercept, r_condition[condition,]) %>%
# #   mutate(condition_mean = b_Intercept + r_condition) %>%
# #   ggplot(aes(y = condition, x = condition_mean)) +
# #   stat_halfeye()
# 
# # performance(ord_cum)
# # plot(ord_cum)
```

#### Adjacent-Category Ordinal (Bayesian)

```{r}
# 
# #DEFINE DATA 
# df <- df_items %>% mutate(
#   scaled = factor(score_SCALED, ordered = TRUE,  #ordered factor
#                     levels = c("-1", "-0.5", "0", "0.5","1"))
# )
# 
# 
# # To specify an adjacent-category model, we use familyâ€‰=â€‰acat() instead of familyâ€‰=â€‰cumulative() as an argument to the brm() function. Then, to model condition with possible category-specific effects, we wrap this variable in cs() in the modelâ€™s formula:
# 
# ord_acat <- brm( formula = scaled ~ cs(condition),
#                data = df,
#                family = acat("probit"),
#                file = "analysis/SGC3A/models/m_items_ord.acat.rds" # cache model (can be removed)  
# )
# 
# summary(ord_acat)
# conditional_effects(ord_cum, "condition", categorical = TRUE)
# conditional_effects(ord_acat, "condition", categorical = TRUE)
# 
# #TIDYBAYES VISUALIZATION
# library(tidybayes)
# ord_acat %>%
#   spread_draws(b_Intercept, r_condition[condition,]) %>%
#   mutate(condition_mean = b_Intercept + r_condition) %>%
#   ggplot(aes(y = condition, x = condition_mean)) +
#   stat_halfeye()
# 


```

## RESOURCES

reset plot margins par(mar=c(1,1,1,1))

### LOG REGRESISION NOTES

**Understanding the logistic regression model**

*The logistic regression intercept gives the log odds of the outcome for the reference level of the predictor variable*

*The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.*

**\[the empty model**

-   The intercept of an empty model (glm(accuracy \~ 1) is equal to log(p/(1-p)), where p = the overall probability of a correct response (df\$accuracy ==1 ).
-   In SGC3A Q1 accuracy this = 71 correct / 330 = 0.215 -\> log(0.215 / (1-0.215)) = -1.29.
-   In other words, the intercept from the model with no predictor variables is the estimated log odds of a correct response for the whole sample.
-   We can also transform the log of the odds back to a probability: p = ODDS/ (1+ODDS) = exp(-1.29)/(1+exp(-1.29)) = 0.215. This should matched the prediction of the empty model

**\[a dichotomous predictor\]**

natural log (odds of +) = -1.822 + 0.9(x1) ; x1 = 0 for control, 1 for impasse

-   INTERCEPT: log odds of (+ response) in control condition
    -   log odds of (+) in control : -1.822 + 0.9(0) = -1.822
    -   convert to odds by exponentiating the coefficients\
        log odds of (+) in control = exp(-1.822) = 0.162 odds
    -   convert to probability by formula =\>\
        p(+) = odds / (1+odds) = 0.162 / (1 + 0.162) = 0.139\
        probability of (+) in control = \~14%
-   B1 COEFFICIENT: DIFFERENCE in log odds of (+) in impasse vs. control
    -   log odds of (+) in impasse: -1.822 + 0.901 = -0.921
    -   convert to odds by exponentiating log odds\
        log odds (+) in impasse = exp(-0.921) = 0.398
    -   convert to probability by formula =\>\
        p(+) = odds / (1 + odds) = 0.398 / (1+0.398) = 0.285\
        probaility of (+) in impasse = \~ 29%
-   ODDS RATIO : exponentiated B1 COEFFICIENT
    -   B1 = (slope of logit model = difference in log odds = log odds ratio

    -   B1 = 0.901 is log odds ratio of (+) in impasse vs control

    -   exp(b1) = exp(0.901) = 2.46

    -   Ratio of odds in impasse are 2.46 times higher than in control. Bein in the impasse condition yields odds athat are 2.46 X higher than in control.

+:----------------------------------------------------------------------+
| MARGINAL\                                                             |
| total = 330 success : 71, failure : 259\                              |
| p(+) = 71 / 330 = 0.215 = 22%\                                        |
| odds(+) = 71 / 259 = 0.274                                            |
+-----------------------------------------------------------------------+
| CONTROL total = 158 success = 22; failure = 136\                      |
| p(+) = 22/158 = 0.139 = 14%\                                          |
| odds(+) = 22/136 = 0.162                                              |
+-----------------------------------------------------------------------+
| IMPASSE total = 172 success = 49; failure = 123\                      |
| p(+) = 49/172 = 0.285 = 29%\                                          |
| odds(+) = 49/123 = 0.398                                              |
+-----------------------------------------------------------------------+

## WIP EXPLORING

### Test Phase Accuracy

**Test Phase Absolute Score (# questions)**

#### Linear Regression

*LM on Test Phase absolute score **as number of questions**, rather than % correct.*

```{r}

#SCORE predicted by CONDITION
lm.1 <- lm(item_test_NABS ~ pretty_condition, data = df_subjects)
paste("Model")
summary(lm.1)
paste("Partition Variance")
anova(lm.1)
paste("Confidence Interval on Parameter Estimates")
confint(lm.1)
report(lm.1) #sanity check
check_model(lm.1)
```

```{r}
#| label: VISMODEL-TEST-ABS-LAB

#MODEL ESTIMATES WITH UNCERTAINTY

# #setup references 
# m <- lm.1
# df <- df_subjects
# call <- m$call %>% as.character()
# 
# # uncertainty model visualization
# df  %>%
#   data_grid(pretty_condition) %>%
#   augment(lm.1, newdata = ., se_fit = TRUE) %>% 
#   ggplot(aes(y = pretty_condition, color = pretty_condition)) 
# 
# +
#   stat_halfeye( scale = .5,
#       aes(
#         xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),
#         fill = stat(cut_cdf_qi(cdf, 
#                 .width = c(.90, .95),
#                 labels = scales::percent_format())))) +
#   scale_fill_brewer(direction = -1) + 
#   labs (title = "(LAB) Test Phase Accuracy ~ Condition", 
#         x = "model predicted mean (% correct)", y = "Condition", fill = "Interval",
#         subtitle = paste("lm(",call[2],")")
#   ) + theme(legend.position = "blank")

```

#### Poisson Regression TODO

https://stats.oarc.ucla.edu/r/dae/poisson-regression/

The outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can consider it a type of *count*, (ie. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function).

```{r}
#POISSON

#SCORE predicted by CONDITION --> POISSON DISTRIBUTION
p.1 <- glm(item_test_NABS ~ pretty_condition, data = df_subjects, family = "poisson")
paste("Model")
summary(p.1)
paste("Partition Variance")
anova(p.1)
paste("Confidence Interval on Parameter Estimates")
confint(p.1)
report(p.1) #sanity check
check_model(p.1)

```

#### Zero Inflated Poisson

https://stats.oarc.ucla.edu/r/dae/zip/\
Poisson count process with excess zeros

```{r}
#ZERO INFLATED POISSON

zinfp.1 <- zeroinfl(item_test_NABS ~  item_q1_rt| pretty_condition , data = df_subjects)
summary(zinfp.1)
report(zinfp.1)
performance(zinfp.1)
# check_model(zinfp.1)

```

#### Negative Binomial Regression

https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/ - overdispersed count data (variance much greater than mean)

```{r}
#NEGATIVE BIONOMIAL REGRESSION
# - https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/
# - Overdispersed Count variables

library(MASS)

nb.1 <- glm.nb(item_test_NABS ~ pretty_condition, data = df_subjects)
summary(nb.1)
report(nb.1)
check_model(nb.1)

#check model assumption
#assumes conditional means are not equal to conditional variances
#conduct likelihood ration test to compare and test [need poisson]
m3 <- glm(item_test_NABS ~ pretty_condition, family = "poisson", data = df_subjects)
pchisq(2 * (logLik(nb.1) - logLik(m3)), df = 1, lower.tail = FALSE)
#A large (+) log likelihood suggests that the negative binomial is more appropriate than the Poisson model


#EXPONENTIATE PARAMETER ESTIMATES
est <- cbind(Estimate = coef(nb.1), confint(nb.1))
#exponentiate parameter estimates
print("Exponentiated Estimates")
exp(est)
```

The variable condition has a coefficient of 0.67, (p \< 0.005). This means that for the impasse condition, the expected log count \# of questions increases by 0.67. By exponentiating the estimate we see that \# question correct rate for the impasse condition is nearly 2x that of the control condition.

**Diagnostics** ??

#### Zero Inflated Negative Binomial Regression

https://stats.oarc.ucla.edu/r/dae/zinb/ count data that are overdispersed and have excess zeros

Zero-inflated negative binomial regression is for modelling count variables with excessive zeros, and especially when the count data are overdispersed (mean is much larger than variance). It can help account for situations where theory suggests that excess zeros are generated by 2 separate processes, one that includes the other count values, and the other that is just the zeros, and thus that the *excess* zeros can be modelled independently.

Total Absolute Score (# items correct) may fit this situation, as the data are overdispersed (variance much greater than the mean) and there are are very large number of zeros. It is theoretically plausible that these excess zeros (no answers correct) are the result of a different 'process' ... (i.e) little understanding and/or resistance to restructuring understanding of the coordinate system. However, I am not certain if it is plausible to suggest that the zeros themselves are the result of two different processes: (ie. perhaps trying to understand, and not trying to understand?) \<- this could maybe be disentangled by first question latency?

The model includes: - A logistic model to model which of the two processes the zero outcome is associated with - A negative binomial model to model the count process

```{r}
#| label: MODEL-TESTABS-ZINFNEGBINOM

library(pscl) #  for zeroinfl negbinomial

#ZERO INFLATED NEGATIVE BINOMIAL
zinb.1 <- zeroinfl(item_test_NABS ~ pretty_condition | pretty_condition , data = df_subjects, dist = "negbin")
#before the | is the count part, after the | is the logit model
paste("Model")
summary(zinb.1)
report(zinb.1)
performance(zinb.1)

#   rootogram(zinb.1)



# #EXPONENTIATE PARAMETER ESTIMATES
# est <- cbind(Estimate = coef(zinb.1), confint(zinb.1))
# #exponentiate parameter estimates
# print("Exponentiated Estimates")
# exp(est)

```

In the count model, the coefficient for the condition is very small, and not significant (suggesting it does not contribute to the count yielding process?).

In the zero-inflation model, the coefficient for the condition variable is -1.056 and statistically significant. This suggests that the log odds of being an excessive zero decrease by 1.06 if you are in the impasse condition (exponentiate it?)

**TODO come back to this and discuss further**\

#### Model Comparison

```{r}
compare_performance(lm.1, p.1, nb.1, zinb.1)
```

For modelling test phase absolute score (# items correct) it seems that the zero inflated negative binomial model is the best fit according to R2 and AIC, however, I am not clear on the implications of the interpretation (non significant in count process, significant on logit process), and also not clear if \# items correct is truly a count process.

```{r}
#uncertainty model visualization
# df %>%
  # data_grid(pretty_condition) %>%
  # augment(m, newdata = ., se_fit = TRUE) %>%
  # ggplot(aes(y = pretty_condition)) +
  # stat_halfeye(
  #   aes(xdist = dist_student_t(df = df.residual(m), 
  #       mu = .fitted, sigma = .se.fit)), scale = .5) +
  # # add raw data in too (scale = .5 above adjusts the halfeye height so
  # # that the data fit in as well)
  # geom_jitter(aes(x = x), data = df, pch = "|", size = 2, 
  #             position =   position_nudge(y = -.15), alpha = 0.5) +  
  # labs (title = "Model Estimates with Uncertainty", x = "model coefficient") + 
  # theme_minimal()

```

#### Hurdle Beta Regression

https://github.com/markhwhiteii/beta_hurdle/blob/master/manuscript/beta_hurdle.pdf

```{r}
library(gamlss)

#CREATE SAMPLE DATA 
n <- 5000 
mu <- 0.40 
sigma <- 0.60 
p0 <- 0.13 
p1 <- 0.17 
p2 <- 1- p0- p1
a <- mu * (1- sigma ^ 2) / (sigma ^ 2) 
b <- a * (1- mu) / mu

#CREATE DIST
set.seed(1839) 
y <- rbeta(n, a, b) 
cat <- sample(1:3, n, prob = c(p0, p2, p1), replace = TRUE) 
y[cat == 1] <- 0 
y[cat == 3] <- 1

#VISUALIZE DISTRIBUTION
x <- as.data.frame(y)
gf_histogram(~x$y)
#this looks not unlike my distribution! 

#CREATE AN EMPTY MODEL
fit <- gamlss( formula = y ~ 1, # formula for mu 
               formula.sigma = ~ 1, # formula for sigma 
               formula.nu = ~ 1, # formula for nu 
               formula.tau = ~ 1, # formula for tau 
               family = BEINF() )

summary(fit)
plot(fit)

#TRANSFORM PARAMETRS BACK 
inv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function
fit_mu <- inv_logit(fit$mu.coefficients) 
paste("MU: ",fit_mu)
fit_sigma <- inv_logit(fit$sigma.coefficients) 
paste("SIGMA: ",fit_sigma)
fit_nu <- exp(fit$nu.coefficients) 
fit_tau <- exp(fit$tau.coefficients) 
fit_p0 <- fit_nu / (1 + fit_nu + fit_tau) 
paste("P0: ",fit_p0)
fit_p1 <- fit_tau / (1 + fit_nu + fit_tau)
paste("P1: ",fit_p1)

```

**BETA HURDLE INTERPRETATION** - beta component\
- MU "location" (mean)\
- SIGMA "scale" (positively related to variance; variance = sigma.squared *mean* (1-mean)\
- Rigby, Stasinopoulos, Heller, and De Bastiani (2017) "reparameterized" the beta distribution so that the two parameters determining the shape of the distribution would be more useful in a regression framework (see Ferrari & Cribari-Neto, 2004 for a different parameterization)

**ZERO-ONE HURDLE COMPONENT**\
- The two additional parameters, Î½ NU and Ï„TAU , are related to p0 and p1, respectively.\
- p0 is the probability that a case equals 0,\
- p1 is the probability that a case equals 1,\
- p2 (i.e., 1 âˆ’p0 âˆ’p1) is the probability that the case comes from the beta distribution

```{r}


#SETUP DATA 

min = 0 #min possible value of scale
max = 8 #max possible value of scale

library(mosaic) #for shuffling
#1. Rescale accuracy using 
# recommended adjustment 
#rescaled = value-min/(max-min)
df <- df_subjects %>% mutate(
  accuracy = item_test_NABS,
  R_acc = (accuracy-min)/(max-min), #as %
  T_acc = (accuracy * (nrow(df)-1) + 0.5)/nrow(df)/8, #transform for no 0 and 1
  perm = shuffle(condition),
  scaffold_rt = item_scaffold_rt
) %>% dplyr::select(accuracy,R_acc, T_acc, condition, perm,scaffold_rt)

#VISUALIZE DISTRIBUTION
gf_histogram(~R_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = "Histogram of accuracy")

#VISUALIZE DISTRIBUTION
gf_histogram(~T_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = "Histogram of [rescaled] accuracy")

gf_histogram(~R_acc, fill = ~perm, data = df) %>% gf_facet_wrap(~perm) + labs(title = "Histogram of shuffled accuracy")

#SUMMARIZE SAMPLE
paste("Grand mean", mean(df$R_acc))

library(mosaic)
stats = favstats(df$R_acc ~ df$condition)
stats$mean <- mean(df$R_acc ~ df$condition)
stats$var <- var(df$R_acc ~ df$condition)
print("Grand stats")
stats 
print("P0")
nrow(df %>% filter(R_acc ==0))/nrow(df)
print("P1")
nrow(df %>% filter(R_acc ==1))/nrow(df)

#CREATE MODEL

#CREATE AN EMPTY MODEL
m0 <- gamlss( formula = R_acc ~ 1, # formula for mu 
              formula.sigma =  ~ 1, # formula for sigma 
              formula.nu =  ~ 1, # formula for nu 
              formula.tau =  ~ 1, # formula for tau 
              family = BEINF(), data = df )

m0 <- gamlss(R_acc ~ 1, ~ 1, ~ 1, ~ 1, 
            data = df, family = BEINF())
summary(m0)
plot(m0)

#TRANSFORM PARAMETRS BACK 
inv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function
m0_mu <- inv_logit(m0$mu.coefficients) 
paste("MU: ",m0_mu)
m0_sigma <- inv_logit(m0$sigma.coefficients) 
paste("SIGMA: ",m0_sigma)
m0_nu <- exp(m0$nu.coefficients) 
paste("NU: ",m0_nu)
m0_tau <- exp(m0$tau.coefficients) 
paste("TAU: ",m0_tau)
m0_p0 <- fit_nu / (1 + fit_nu + fit_tau) 
paste("P0: ",m0_p0)
m0_p1 <- fit_tau / (1 + fit_nu + fit_tau)
paste("P1: ",m0_p1)



#CREATE PREDICTOR MODEL
m1 <- gamlss(R_acc ~ condition, ~ condition, ~ condition, ~ condition, 
            data = df, family = BEINF())
summary(m1)

#LOOKING PREDICTOR MODEL
m <- gamlss(R_acc ~ condition , 
            ~ condition , 
            ~ condition , 
            ~ condition , 
            data = df, family = BEINF())
summary(m)


#CREATE PREDICTOR MODEL ON SHUFFLED [PERMUTATION TEST]
mperm <- gamlss(R_acc ~ perm, ~ perm, ~ perm, ~ perm, 
            data = df, family = BEINF())
summary(mperm)

#sanity check with scaled outcome, no zeros ones
m3 <- gamlss(T_acc ~ condition, ~ condition, ~ condition, ~ condition, 
            data = df, family = BEINF())
summary(m3)
#m3 shouldn't show condition as significant for nu and tau, because T_acc was scaled to not include any 0s and 1s

#investigate beta negative binomial distribution
#https://en.wikipedia.org/wiki/Beta_negative_binomial_distribution

#TRANSFORM PARAMETRS BACK 
inv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function
m1_mu <- inv_logit(m1$mu.coefficients) 
paste("MU: ",m1_mu)
m1_sigma <- inv_logit(m0$sigma.coefficients) 
paste("SIGMA: ",m1_sigma)
m1_nu <- exp(m1$nu.coefficients) 
paste("NU: ",m1_nu)
m1_tau <- exp(m1$tau.coefficients) 
paste("TAU: ",m1_tau)

summary(m)
plot(m)


```

-   MU tells if mean is different by condition\
-   SIGMA tells if variance is different by condition\
-   NU coefficient tells if condition yields different probability at floor
-   TAU coefficient tells if condition yields different probability at ceiling

#### Beta Regression (% Correct)

Beta regression on % correct (with standard transformation for including \[0,1\]) https://stats.stackexchange.com/questions/63350/how-to-interpret-the-coefficients-from-a-beta-regression

```{r}
# 
library(betareg)

#RESCLAE VARIABLE
#beta reg can't handle 0s and 1s 
sub <- df_subjects %>% dplyr::select(condition, DV_percent_NABS)
n = nrow(sub) %>% unlist()
sub$dv_transformed = (sub$DV_percent_NABS * (n-1) + 0.5)/n
 
#VISUALIZE VARIABLES
histogram(sub$dv_transformed)
gf_histogram(~dv_transformed, fill = ~condition, data = sub) %>% gf_facet_wrap(~condition)

#FIT MODEL
mb <- betareg(dv_transformed ~ condition, data = sub)
summary(mb)
plot(mb)

```

#### TOBIT Regression

https://stats.oarc.ucla.edu/r/dae/tobit-models/

```{r}

#set up data 
df <- df_subjects %>% mutate(
  accuracy = s_NABS
)

library(VGAM)
t <- vglm(accuracy ~ condition, tobit(Upper = 13), data = df)
summary(t)
plot(t)
```

#### WIP \| HURDLE MODEL

-   https://data.library.virginia.edu/getting-started-with-hurdle-models/\
-   https://en.wikipedia.org/wiki/Hurdle_model#:\~:text=A%20hurdle%20model%20is%20a,of%20the%20non%2Dzero%20values.

class of models for count data with both overdispersion and excess zeros;\
different from zero-inflated models where the excess zeros are theorized to arise from two different processes; in the hurdle model, there is a model for P(x=0) and a separate model for P(x!=0)

The model includes: - A binary logit model to model whether the observation takes a positive count or not. - a truncated Poisson or Negative binomial model that only fits positive counts

This allows us to model: (1) Does the student get *any* questions right? (2) How many questions does the student get right?

```{r}
library(pscl) #zero-inf and hurdle models 
library(countreg) #rootogram
#install.packages("countreg", repos="http://R-Forge.R-project.org")

#SYNTAX OUTCOME ~ count model predictor | hurdle predictor

h.1 <- pscl::hurdle(item_test_NABS ~ condition | condition , data = df_subjects,
              zero.dist = "binomial", dist = "poisson", size = 8)

h.2 <- pscl::hurdle(item_test_NABS ~ condition | condition , data = df_subjects,
              zero.dist = "binomial", dist = "negbin", size = 8)

summary(h.1)
summary(h.2)


rootogram(h.1)
rootogram(h.2)
compare_performance(h.1,h.2)


```

### Q1 Accuracy

#### CHI SQUARE

##### (Combined)

```{r}
#| label : CHISQR-Q1TRI.by.COND-BOTH

#lab only
df <- df_q1 

#MOSAIC PLOT
#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis
mosaicplot(main="Accuracy on First Question by Condition",
            data = df, pretty_condition ~ accuracy, shade = T, color = 2)


CrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)


df %>%
  sjtab(fun = "xtab", var.labels=c("accuracy", "pretty_condition"),
        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,
        statistics = "fisher")

```

**Combining data across both sessions** (n=330), a Pearson's Chi-squared test suggests a statistically significant relationship between response accuracy on the first question and experimental condition, $\chi_2$ (1) = 10.3, p = 0.001. The sample odds ratio (2.46, p = 0.001, 95% CI \[1.37, 4.53\]) indicates that the odds of providing a correct response to the first question are 2.46 higher for subjects in the impasse condition than those in the control condition.

##### (In Person)

```{r}
#| label : CHISQR-Q1TRI.by.COND-LAB

#lab only
df <- df_q1 %>% filter(mode == "lab-synch")

#MOSAIC PLOT
#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis
mosaicplot(main="Accuracy on First Question by Condition",
            data = df, pretty_condition ~ accuracy, 
            shade = T)


CrossTable( x = df$condition, y = df$score_niceABS, 
            fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)


df %>%
  sjtab(fun = "xtab", var.labels=c("accuracy", "pretty_condition"),
        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,
        statistics = "fisher")
```

**For (In Person) data collection** (n=126) the Pearson's Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition that is not significant at the alpha level 0.05, $\chi^2$ (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. In this particular data sample, the odds ratio (Odds Ratio = 2.18, p = 0.055, 95% CI \[0.982, +Inf\]) indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition .

##### (Online Replication)

```{r}
#| label : CHISQR-Q1TRI.by.COND-ONLINE

#online only
df <- df_q1 %>% filter(mode == "asynch")

#MOSAIC PLOT
#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis
mosaicplot(main="Accuracy on First Question by Condition",
            data = df, pretty_condition ~ accuracy, shade = T)


CrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, 
            chisq=TRUE, expected = TRUE, sresid = TRUE)


df %>%
  sjtab(fun = "xtab", var.labels=c("accuracy", "pretty_condition"),
        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,
        statistics = "fisher")

```

**For online data collection** (n=204), a Pearson's Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, $\chi^2$ (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. The odds ratio (Odds Ratio = 2.68, p = 0.005, 95% CI \[1.37, +Inf\]) indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition .
