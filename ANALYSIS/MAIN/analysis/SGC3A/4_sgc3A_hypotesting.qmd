---
subtitle: 'Study SGC3A | Hypothesis Testing'
---

\newpage

# Hypothesis Testing {#sec-SGC3A-hypotesting}

**TODO**

-   HURDLE MODEL? (mixture model w/ 0 + count)
-   review models already created in ARCHIVE?

*The purpose of this notebook is test the hypotheses that determined the design of the SGC3A study.*

```{r}
#| label: SETUP
#| warning : false
#| message : false

#misc utilities
library(Hmisc) # %nin% operator
library(broom)
library(modelr)
library(distributional)
library(jtools)
library(pwr) #power analysis

#visualization
library(ggpubr) #arrange plots
library(cowplot) #arrange shift function plots
library(ggformula) #easy graphs
library(vcd) #mosaic plots
library(vcdExtra) #mosaic plots
library(kableExtra) #printing tables 
library(sjPlot) #visualize model coefficients
library(ggdist) #uncertainty viz 
library(ggstatsplot) #plots with stats

#models and performance
library(report) #easystats reporting
library(see) #easystats visualization
library(performance) #easystats model diagnostics
library(qqplotr) #confint on qq plot
library(gmodels) #contingency table and CHISQR
library(equatiomatic) #extract model equation
library(pscl) #zeroinfl / hurdle models 
library(lme4) #mixed effects models 
library(lmerTest) #for CIs in glmer 
library(ggeffects) #visualization log regr models
library(nnet) #multinomial logistic regression [not mixed]
library(mclogit) #frequentist mixed multinomial logistic regression [mblogit]
library(brms) #bayesian mixed multinomials [+ other bayesian reg models]

library(tidyverse) #ALL THE THINGS

#OUTPUT OPTIONS
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
options(ggplot2.summarise.inform = FALSE)
options(scipen=1, digits=3)

#GRAPH THEMEING
theme_set(theme_minimal()) 

```

**Research Questions**

In SGC3A we set out to answer the following question: Does posing a mental impasse improve performance on the interval graph comprehension task?

**Experimental Hypothesis**\
*Learners posed with scenario designed to evoke a mental impasse will be more likely to correct interpret the graph.*

-   H1A \| Learners in the IMPASSE condition will be more likely to correctly answer the first question than learners in CONTROL.
-   H1B \| Learners in the IMPASSE condition will score higher on the TEST Phase than learners in CONTROL.

**Null Hypothesis**\
*No significant differences in performance will exist between learners in the IMPASSE and CONTROL conditions.*

```{r}
#| label: IMPORT-DATA
#| warning : false
#| message : false

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
# mbp = "/Users/amyfox/Sites/RESEARCH/SGCâ€”Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
# setwd(mbp)

#IMPORT DATA 
df_subjects <- read_rds('analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds')
df_items <- read_rds('analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.rds') %>% 
   mutate (
    q = as.factor(q), #QUESTION
    state = recode_factor(score_SCALED, #for ordinal
                         "-1" = "orth-like",
                         "-0.5" = "unknown",
                         "0" = "unknown",
                         "0.5" = "tri-like",
                         "1" = "tri-like"),
    state = as.ordered(state))

```

## H1A \| Q1 ACCURACY

**Do Ss in the IMPASSE condition have a higher likelihood of producing a correct response to the first question?**

The graph comprehension tasks includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their *first exposure* to the unconventional triangular coordinate system.

+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Does the frequency of correct (vs) incorrect responses on the first question differ by condition? \[Is response accuracy independent of condition?\]                                                                                   |
+=======================+========================================================================================================================================================================================================================================+
| **Hypothesis**        | H1A \| Ss in the IMPASSE condition will have a higher likelihood of correctly responding to the first question than those in the CONTROL condition                                                                                     |
+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | -   data: `df_items` where `q == 1`                                                                                                                                                                                                    |
|                       | -   outcome: *accuracy* ( factor(incorrect/correct) from `score_niceABS` \[absolute score\]                                                                                                                                            |
|                       | -   predictor: `condition` \[between-subjects factor\]                                                                                                                                                                                 |
+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  Logistic Regression on `accuracy` predicted by `condition`                                                                                                                                                                         |
|                       |     -   account for difference in odds of correct score by condition                                                                                                                                                                   |
|                       |                                                                                                                                                                                                                                        |
|                       | Alternative:                                                                                                                                                                                                                           |
|                       |                                                                                                                                                                                                                                        |
|                       | -   Chi-Square test of independence on outcome `score_niceABS` by `condition`                                                                                                                                                          |
+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Notes**             | -   CHIQ SQR is simplest method to examine independence of two categorical factors; LOGISTIC REGRESSION is recommended for binomial \~ continuous; though with regression we can quantify the size of the effect and overall model fit |
|                       | -   independence assumption : (CHI SQR) as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent                                                           |
|                       | -   cell frequency : (CHI SQR) expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)                                                                      |
+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

```{r}
#| label: SETUP-Q1ACC

#:::::::: PREP DATA
df <- df_items %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

# #:::::::: GROUPED PROPORTIONAL BAR CHART
# gf_props(~accuracy, fill = ~pretty_condition,
#        position = position_dodge(), data = df) %>%
#   gf_facet_grid(~pretty_mode) +
#    labs(x = "Correct Response on Q 1",
#        title = "Accuracy on First Question by Condition",
#        subtitle="Impasse Condition yields a greater proportion of correct responses") #theme(legend.position = "none")

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~pretty_mode) + 
   labs(#y = "Correct Response on Q 1",
       title = "Accuracy on First Question by Condition",
       x = "Condition",
       fill = "",
       subtitle="Impasse Condition yields a greater proportion of correct responses")

```

A proportional bar chart visualizing the proportion of incorrect (vs) correct responses in each condition for each data collection modality (left/right facet) reveals that the pattern of responses appear the same regardless of the data collection modality. In both data collection sessions, the proportion of incorrect responses is much greater than the proportion of correct responses, regardless of condition (marginal probability of incorrect). In the impasse condition, the difference in proportions is smaller than the control condition (conditional probability of success in impasse; (i.e) There are more correct responses in the impasse condition than the control condition).

### (LAB)

```{r}

#:::::::: IN PERSON ONLY
df <- df_items %>% filter(mode == "lab-synch") %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

#::::::::::::DESCRIPTIVES

table(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row
      prop.table(margin=2) %>%  #return proportion (of column)
      addmargins(1) #sanity check sum of columns


table(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row
      addmargins(1) #sanity check sum of columns
```

#### LOGISTIC REGRESSION

Fit a logistic regression predicting accuracy (absolute score) (n = `r nrow(df)`) by condition. (k = 2).\

-   Parameter estimate: $\beta_{0}$ = Log Odds of (correct) responses in CONTROL condition
-   $e^{\beta_{0}}$ = ODDS of correct response in CONTROL condition
-   Parameter estimate: $\beta_{1}$ = $\beta_{1impasse}$ Log Odds (Log OR; change in odds for correct response in impasse (vs) control \[log scale\])
-   $e^{\beta_{1}}$ = ODDS RATIO of correct response in IMPASSE (vs) CONTROL
-   **Null hypothesis**:$\beta_{impasse} \le 0$ the odds for a correct response does not change, or decreases
-   **Alternative hypothesis:** $\beta_{impasse} \gt 0$ the odds of a correct response increases

##### Fit Model

*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-Q1ACC-LAB
#| warning: false
#| message: false

# MODEL FITTING ::::::::

#: 1 EMPTY MODEL baseline glm model intercept only
m0 = glm(accuracy ~ 1, data = df, family = "binomial")
# print("EMPTY MODEL")
# summary(m0)

#: 2 CONDITION model
m1 <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
# print("PREDICTOR MODEL")
# summary(m1)

#: 3 TEST SUPERIOR FIT
paste("AIC wth predictor is lower than empty model?", m0$aic > m1$aic)
test_lrt(m0,m1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,m1))$p[2])
```

*The Condition predictor decreases AIC, but the Likelihood Ratio Test is marginal. We proceed to examine the predictor model, as we plan to do a 1-tailed NHST .*

##### Describe

```{r}
#| label: MODEL-Q1ACC-LOG-LAB

# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: 

print("PREDICTOR MODEL [default two-tailed sig test]")
summary(m1)

# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: 

# one-sided (right tail) z test for B COEFFICIENT
#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients

#SANITY CHECK 2-tailed test should match the model output
tt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("p value for two-tailed test, null B = 0 : ",round(tt,3))
ot <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("BUT we want a one tailed directional, null: B <= 0: ",round(ot,3))
paste("adjusted confint for directional hypothesis")
(dcint <- confint(m1, level = 0.90)) # get 90% for right side))
# https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte


#:::::::: INTERPRET COEFFICIENTS

# print("Confidence Interval â€”- LOG ODDS")
# confint(m1) #not adjusted for 1-tailed
print("Coefficients â€”- ODDS RATIOS")
# (e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiated, not adjusted
(e <- cbind( exp(coef(m1)), exp(dcint))) #exponentiated, adjusted

print("MODEL PREDICTIONS")
# Retrieve predictions as probabilities 
# (for each level of the predictor)
pred.control <- predict(m1,data.frame(pretty_condition="control"),type="response")
paste("Probability of success in control,", pred.control)
pred.impasse <- predict(m1,data.frame(pretty_condition="impasse"),type="response")
paste("Probability of success in impasse,", pred.impasse)
```

##### Visualize

```{r}
#| label: VIS-Q1ACC-LOG-LAB

#:::::::: PLOT

#GGSTATS | MODEL | LOG ODDS 
ggcoefstats(m1, output = "plot", 
              conf.level = 0.90) + 
  labs(x = "Log Odds Estimate", 
       title = "LOGODDS | Q1 Accuracy ~ condition",
       subtitle = "(p is for two tailed test)")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) +  #manually adjusted for directional test   
  labs(title = "ODDS RATIO | Q1 Accuracy ~ condition",
       subtitle = "(p for one sided test)",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(m1, type="eff",
           show.data = TRUE, jitter = TRUE,
           title = "MODEL PREDICTION | Q1 Accuracy ~ condition",
           axis.title = c("Condition","Probability of Accurate Response"))

#SJPLOT | MODEL | TABLE
# tab_model(m1)

```

##### Diagnostics

```{r}
#| label: DIAG-Q1ACC-LOG-LAB

print("SANITY CHECK REPORTING")
report(m1)

print("MODEL PERFORMANCE")
performance(m1)

print("MODEL DIAGNOSTICS")
check_model(m1)
```

##### Inference

**We fit a logistic regression model to explore the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 1.79, p = 0.04, *one-tailed*). The model predicts that the odds of a correct response on the first question in the *impasse condition* increase by nearly 120% (**$e^{beta_{impasse}}$ **= 2.19, 95% CI \[1.08, 4.63\]) over the *control condition*. The intercept** $\beta_{0}$ **parameter is also significant, (**$e^{b_{0}}$ **= 0.19, p \< 0.001, 95% CI \[0.11, 0.33\]) indicating that the odds of a correct response in the control condition are significantly less than even (less than 50/50 chance of correct response in control condition).**

*Equivalent statements:*

-   being in impasse condition increases log odds of correct response by 0.79 (over control)
-   being in impasse increases odds of correct response in impasse over control increases by factor of 2.19
-   probability of correct response in impasse predicted as 30%, vs only 16% in control condition

##### TODO

-   Are these residuals OK? I didn't think normally distributed residuals were an assumption for logistic regression.
-   interpretation/reporting of model fit?
-   sanity check correct interpretation of coefficients & reporting
-   SANITY CHECK:: meaning of b0 estimate
    -   b0 parameter is odds of (+) response on reference \[control\]

    -   *significant* b0 indicates that odds of a (+) are (significantly) different from 1:1 (i.e. not an equal probability of correct vs incorrect responses in control)

### (ONLINE REPLICATION)

```{r}

#:::::::: ONLINE ONLY
df <- df_items %>% filter(mode == "asynch") %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

#::::::::::::DESCRIPTIVES

table(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row
      prop.table(margin=2) %>%  #return proportion (of column)
      addmargins(1) #sanity check sum of columns


table(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row
      addmargins(1) #sanity check sum of columns

```

#### LOGISTIC REGRESSION

*Fit a logistic regression predicting accuracy (absolute score) (n = r nrow(df)) by condition. (k = 2).*

-   *Parameter estimate:* $\beta_{0}$ *= Log Odds of (correct) responses in CONTROL condition*
-   $e^{\beta_{0}}$ *= ODDS of correct response in CONTROL condition Parameter estimate:*
-   $\beta_{1}$ *=* $\beta_{1impasse}$ *Log Odds (Log OR; change in odds for correct response in impasse (vs) control \[log scale\])*
-   $e^{\beta_{1}}$ *= ODDS RATIO of correct response in IMPASSE (vs) CONTROL*
-   *Null hypothesis:* $\beta_{impasse} \le 0$ *the odds for a correct response does not change, or decreases*
-   *Alternative hypothesis:* $\beta_{impasse} \gt 0$ *the odds of a correct response increases*

##### Fit Model

*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-Q1ACC-ONLINE
#| warning: false
#| message: false

# MODEL FITTING ::::::::

#: 1 EMPTY MODEL baseline glm model intercept only
m0 = glm(accuracy ~ 1, data = df, family = "binomial")
# print("EMPTY MODEL")
# summary(m0)

#: 2 CONDITION model
m1 <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
# print("PREDICTOR MODEL")
# summary(m1)

#: 3 TEST SUPERIOR FIT
paste("AIC wth predictor is lower than empty model?", m0$aic > m1$aic)
test_lrt(m0,m1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,m1))$p[2])
```

*The Condition predictor decreases AIC, and the Likelihood Ratio Test indicates the predictor model is a better fit.*

##### Describe

```{r}
#| label: MODEL-Q1ACC-LOG-ONLINE

# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: 

print("PREDICTOR MODEL [default two-tailed sig test]")
summary(m1)

# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: 

# one-sided (right tail) z test for B COEFFICIENT
#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients

#SANITY CHECK 2-tailed test should match the model output
tt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("p value for two-tailed test, null B = 0 : ",round(tt,3))
ot <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("BUT we want a one tailed directional, null: B <= 0: ",round(ot,3))
paste("adjusted confint for directional hypothesis")
(dcint <- confint(m1, level = 0.90)) # get 90% for right side))
# https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte


#:::::::: INTERPRET COEFFICIENTS

# print("Confidence Interval â€”- LOG ODDS")
# confint(m1) #not adjusted for 1-tailed
print("Coefficients â€”- ODDS RATIOS")
# (e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiated, not adjusted
(e <- cbind( exp(coef(m1)), exp(dcint))) #exponentiated, adjusted

print("MODEL PREDICTIONS")
# Retrieve predictions as probabilities 
# (for each level of the predictor)
pred.control <- predict(m1,data.frame(pretty_condition="control"),type="response")
paste("Probability of success in control,", pred.control)
pred.impasse <- predict(m1,data.frame(pretty_condition="impasse"),type="response")
paste("Probability of success in impasse,", pred.impasse)
```

##### Visualize

```{r}
#| label: VIS-Q1ACC-LOG-ONLINE

#:::::::: PLOT

#GGSTATS | MODEL | LOG ODDS 
ggcoefstats(m1, output = "plot", 
              conf.level = 0.90) + 
  labs(x = "Log Odds Estimate", 
       title = "LOGODDS | Q1 Accuracy ~ condition",
       subtitle = "(p is for two tailed test)")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) +  #manually adjusted for directional test   
  labs(title = "ODDS RATIO | Q1 Accuracy ~ condition",
       subtitle = "(p for one sided test)",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(m1, type="eff",
           show.data = TRUE, jitter = TRUE,
           title = "MODEL PREDICTION | Q1 Accuracy ~ condition",
           axis.title = c("Condition","Probability of Accurate Response"))

#SJPLOT | MODEL | TABLE
# tab_model(m1)

```

##### Diagnostics

```{r}
#| label: DIAG-Q1ACC-LOG-ONLINE

print("SANITY CHECK REPORTING")
report(m1)

print("MODEL PERFORMANCE")
performance(m1)

print("MODEL DIAGNOSTICS")
check_model(m1)
```

##### Inference

**We fit a logistic regression model to explore the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 2.63, p = 0.004, *one-tailed*). The model predicts that the odds of a correct response on the first question in the *impasse condition* increase by nearly 170% (**$e^{beta_{impasse}}$ = 2.69, 95% CI \[1.47, 5,11\]) over the *control condition*.**The intercept** $\beta_{0}$ **parameter is also significant, (**$e^{b_{0}}$ **= 0.14, p \< 0.001, 95% CI \[0.08, 0.23\]) indicating that the odds of a correct response in the control condition are significantly less than even (less than 50/50 chance of correct response in control condition).**

*Equivalent statements:*

-   being in impasse condition increases log odds of correct response by 0.99 (over control)
-   being in impasse increases odds of correct response in impasse over control increases by factor of 2.69
-   probability of correct response in impasse predicted as 28%, vs only 12% in control condition

##### 

## H1A \| Q1 INTERPRETATION STATE

**Do Ss in the IMPASSE condition have more-triangular interpretations on first question?**

While absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn't allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject's response, and determine if these interpretations differ by experimental condition. state is a 3-category variable that groups the following interpretations:

-   **"orthogonal-like"** \[reference category\] includes orthogonal and satisficing responses ==\> indicates a primarily *orthogonal* state of coordinate system understanding
-   **"unknown"** includes: blank, reference point, responses that can't be classified (including selecting all datapoints) =\> indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly *not* orthogonal nor triangular
-   **"triangle-like"** includes correct triangular and 'lines connecting' responses as well as responses that include both orthogonal *and* triangular answers =\> indicates some degree of angular/triangular coordinate understanding

+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Does Ss in the impasse condition produce less orthogonal responses on the first question?                                                                                                                                                                                                                                                                 |
+=======================+===========================================================================================================================================================================================================================================================================================================================================================+
| **Hypothesis**        | H1A \| Ss in the IMPASSE condition will have a higher likelihood of producing unknown and and triangle-like response states, relative to orthogonal response states, on the first question                                                                                                                                                                |
+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | -   data: `df_items` where `q == 1`                                                                                                                                                                                                                                                                                                                       |
|                       | -   outcome: `state` ( 3 level factor from high_interpretation )                                                                                                                                                                                                                                                                                          |
|                       | -   predictor: `condition` \[between-subjects factor\]                                                                                                                                                                                                                                                                                                    |
+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  Multinomial (Logistic) Regression on state predicted by condition                                                                                                                                                                                                                                                                                     |
|                       |                                                                                                                                                                                                                                                                                                                                                           |
|                       | Alternative:                                                                                                                                                                                                                                                                                                                                              |
|                       |                                                                                                                                                                                                                                                                                                                                                           |
|                       | -   Ordinal regression on state; but model doesn't satisfy proportional odds assumption (parallel slopes)                                                                                                                                                                                                                                                 |
|                       | -   Multinomial or Ordinal regression on high_interpretation (5 category interpretation state which distinguishes between uncertain (blank, reference) unclassifiable, triangle-like and true triangular.) There are some cells with zeros, however (no uncertain responses in control) which means the model can't accurately estimate those comparisons |
+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

```{r}
#| label: SETUP-Q1-INTERPRETATION

#:::::::: PREP DATA
df <- df_items %>% filter(q==1) 

# #:::::::: GROUPED PROPORTIONAL BAR CHART
# gf_props(~high_interpretation, fill = ~pretty_condition,
#        position = position_dodge(), data = df) %>%
#   gf_facet_grid(~pretty_mode) +
#    labs(x = "Correct Response on Q 1",
#        title = "Interpretation on First Question by Condition",
#        subtitle="") #theme(legend.position = "none")

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~pretty_mode) + 
   labs(#y = "Correct Response on Q 1",
       title = "Interpretation on First Question by Condition",
       x = "Condition",
       fill = "",
       subtitle="")

```

A proportional bar chart visualizing the proportion of each interpretations type by condition for each data collection modality (left/right facet) reveals that the pattern of responses are similar regardless of the data collection modality, by differ by condition. In the impasse condition, there are more triangular responses than in control. In the impasse condition, there are also more positive transition (i.e. triangle-like) and neutral (ie. blank or uncertain response types) than in the control condition. This suggests that the

### (LAB)

```{r}

#:::::::: IN PERSON ONLY
df <- df_items %>% filter(mode == "lab-synch") %>% filter(q==1) 

#::::::::::::DESCRIPTIVES

table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row
      prop.table(margin=2) %>%  #return proportion (of column)
      addmargins(1) #sanity check sum of columns


(t <- table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row
      addmargins(1)) #sanity check sum of columns

```

#### MULTINOMIAL REGRESSION

*Does condition affect the response state of Q1?*

*Fit a logistic regression predicting interpretation state (k=3) by condition(k = 2).*

-   2 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) \[essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing \[reference category\] vs \[this category\])

-   For *each* equation:

    -   $\beta_{0}$ *= Log Odds of \[this category type vs. reference category type) response in CONTROL condition*
    -   $e^{\beta_{0}}$ *= ODDS of \[this category type vs. reference category type\] response in CONTROL condition*
    -   $\beta_{1}$ *=* $\beta_{1impasse}$ *Log Odds (Log OR; change in odds for \[this category\] type response in impasse (vs) control \[log scale\])*
    -   $e^{\beta_{1}}$ *= ODDS RATIO of \[this. vs reference category type\] response in IMPASSE (vs) CONTROL*
    -   Two-tailed NHST *Null hypothesis:* $\beta_{impasse} = 0$ *the odds for \[this category of response vs. reference\] are not different for IMPASSE condition*
    -   *Alternative hypothesis:* $\beta_{impasse} \ne 0$ *the odds of \[this category of response vs. reference\] increases or decreases for IMPASSE condition*

##### Fit Model

```{r}
#| label: FIT-Q1-INTERPRETATION

#check reference level 
print("Categories (first is reference)")
levels(df$state)

#FIT EMPTY MODEL
# print("EMPTY MODEL")
catm.0 <- multinom(state ~ 1, data = df)
# summary(catm.0)

#FIT PREDICTOR MODEL
# print("PREDICTOR MODEL")
catm <- multinom(formula = state ~ pretty_condition, data = df, model = TRUE)
# summary(catm)

#COMPARE MODEL FIT
paste("AIC wth predictor is lower than empty model?", catm.0$AIC > catm$AIC)
test_lrt(catm.0, catm)

##compare bayesian version
#library(brms)
# bm1 <- brm( state ~ pretty_condition, data = df, family = "categorical")
# summary(bm1)
# plot_model(bm1)
# report(bm1)
# coefficient estimates are very simliar to catm. super cool!

##compare mclogit version
#"baseline-category logit model
# https://www.elff.eu/software/mclogit/manual/mblogit/
# blm1 <- mblogit(state ~ pretty_condition , data = df)
# summary(blm1)
#identical to catm. super cool!

```

*AIC in predictor model is less than empty model, and likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.*

##### Describe

```{r}
#| label: DESC-Q1-INTERPRETATION

#::::::::INTERPRETATION
paste("MODEL SUMMARY")
summary(catm)

# calculate z-statistics of coefficients
z_stats <- summary(catm)$coefficients/summary(catm)$standard.errors
# convert to p-values
p_values <- (1 - pnorm(abs(z_stats)))*2
# display p-values in transposed data frame
(p_values <- data.frame(p = (p_values)))
# display odds ratios in transposed data frame

paste("ODDS RATIOS")
odds_ratios <- data.frame(OR = exp(summary(catm)$coefficients))
options(scipen = 2)
(results <- cbind(odds_ratios, p_values))
```

```{r}

```

##### Inference

-   Being in the IMPASSE condition *increases* the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 45 (z = 3.58, p \< 0.001) . **Participants in the impasse condition were 45x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control**.

-   Being in the IMPASSE condition *increases* the odds of giving 'triangle-like' response rather than an orthogonal (or satisficing) response by a factor of 5.90 (z = 3.98, p \< 0.001 ). **Participants in the impasse condition were almost 6x as likely to give an a triangular response rather than an orthogonal response compared to participants in control**.

-   As with the (binary) logistic regression on accuracy \~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 *in the control condition.* (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)

-   \[need to to double check interpretation, but I *think* that the OR intercepts converted to probabilities equate to the marginal probability of being in each state in the control condition. which makes sense. I think.?\]

-   IF I change reference category for condition... then the intercepts should no longer be significant. The b1 coefficients should still be significant, but with changed sign (much less likely) \[Yup! this works!\]

##### Visualize

```{r}
#| label: VIS-Q1INTERPRETATION-LAB

#:::::::: PLOT

#GGSTATS | MODEL | LOG ODDS 
ggcoefstats(catm, output = "plot", 
              # conf.level = 0.90,
              exclude.intercept = FALSE) + 
  labs(x = "Log Odds Estimate", 
       title = "LOGODDS | Q1 State ~ condition",
       subtitle = "(p is for two tailed test)")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(catm, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.05, #can manually adjust to account for directional test
           ci.lvl = 0.95 ) +  #can manually adjusted for directional test   
  labs(title = "ODDS RATIO | Q1 State ~ condition",
       subtitle = "(p for one two test)",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(catm, type="eff",
           title = "MODEL PREDICTION | Q1 State ~ condition",
           axis.title = c("Condition","Probability of Response State"))

#SJPLOT | MODEL | TABLE
#tab_model(catm)

```

##### Diagnostics

```{r}

#EXAMINE PREDICTIONS
#create sample data frame
test <- data.frame(pretty_condition = c("control", "impasse"))
pred <- predict(catm, newdata = test, "probs")
paste("Predicted Probability of Being in Each State")
(cbind(test, pred))

print("MODEL PERFORMANCE")
performance(catm)
DescTools::PseudoR2(catm, which = c("McFadden", "CoxSnell", "Nagelkerke"))

#General Goodness of Fit
#library(generalhoslem)
#logitgof(df$state, catm$fitted.values, g = 3)
#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).
#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables
chisq.test(df$state,predict(catm)) #actual states VS predicted states
# The chi-square test tests the decrease in unexplained variance from the baseline model to the final model

# print("MODEL DIAGNOSTICS")
#check_model(catm) can't do overall diagnostics, have to do them on individual model equations

```

##### TODO

-   interpretation/reporting of model fit?
-   sanity check correct interpretation of coefficients & reporting
-   diagnostics on individual model equations

### (ONLINE REPLICATION)

```{r}

#:::::::: ONLINE ONLY
df <- df_items %>% filter(mode == "asynch") %>% filter(q==1) 

#::::::::::::DESCRIPTIVES

table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row
      prop.table(margin=2) %>%  #return proportion (of column)
      addmargins(1) #sanity check sum of columns


(t <- table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row
      addmargins(1)) #sanity check sum of columns

```

#### MULTINOMIAL REGRESSION

*Does condition affect the response state of Q1?*

*Fit a logistic regression predicting interpretation (k=3) by condition(k = 2).*

-   2 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) \[essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing \[reference category\] vs \[this category\])

-   For *each* equation:

    -   $\beta_{0}$ *= Log Odds of \[this category type vs. reference category type) response in CONTROL condition*
    -   $e^{\beta_{0}}$ *= ODDS of \[this category type vs. reference category type\] response in CONTROL condition*
    -   $\beta_{1}$ *=* $\beta_{1impasse}$ *Log Odds (Log OR; change in odds for \[this category\] type response in impasse (vs) control \[log scale\])*
    -   $e^{\beta_{1}}$ *= ODDS RATIO of \[this. vs reference category type\] response in IMPASSE (vs) CONTROL*
    -   Two-tailed NHST *Null hypothesis:* $\beta_{impasse} = 0$ *the odds for \[this category of response vs. reference\] does not change, or decreases*
    -   *Alternative hypothesis:* $\beta_{impasse} \ne 0$ *the odds of \[this category of response vs. reference\] increases*

##### Fit Model

```{r}
#| label: FIT-Q1-INTERPRETATION-ONLINE

#check reference level 
print("Categories (first is reference)")
levels(df$state)

#FIT EMPTY MODEL
# print("EMPTY MODEL")
catm.0 <- multinom(state ~ 1, data = df)
# summary(catm.0)

#FIT PREDICTOR MODEL
# print("PREDICTOR MODEL")
catm <- multinom(formula = state ~ pretty_condition, data = df, model = TRUE)
# summary(catm)

#COMPARE MODEL FIT
paste("AIC wth predictor is lower than empty model?", catm.0$AIC > catm$AIC)
test_lrt(catm.0, catm)

##compare bayesian version
#library(brms)
# m1 <- brm( state ~ condition, data = df, family = "categorical")
# summary(m1)
# plot_model(m1)
# report(m1)

```

*AIC in predictor model is less than empty model, and likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.*

##### Describe

```{r}
#| label: DESC-Q1-INTERPRETATION-ONLINE

#::::::::INTERPRETATION
paste("MODEL SUMMARY")
summary(catm)

# calculate z-statistics of coefficients
z_stats <- summary(catm)$coefficients/summary(catm)$standard.errors
# convert to p-values
p_values <- (1 - pnorm(abs(z_stats)))*2
# display p-values in transposed data frame
(p_values <- data.frame(p = (p_values)))
# display odds ratios in transposed data frame

paste("ODDS RATIOS")
odds_ratios <- data.frame(OR = exp(summary(catm)$coefficients))
options(scipen = 2)
(results <- cbind(odds_ratios, p_values))
```

##### Inference

-   Being in the IMPASSE condition *increases* the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 44 (z = 5.92, p \< 0.001) . **Participants in the impasse condition were 44x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control**.

-   Being in the IMPASSE condition *increases* the odds of giving 'triangle-like' response rather than an orthogonal (or satisficing) response by a factor of 9.25 (z = 5.77, p \< 0.001 ). **Participants in the impasse condition were moret than 9x as likely to give an a triangular response rather than an orthogonal response compared to participants in control**.

-   As with the (binary) logistic regression on accuracy \~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 *in the control condition.* (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)

##### Visualize

```{r}
#| label: VIS-Q1INTERPRETATION-ONLINE

#:::::::: PLOT

#GGSTATS | MODEL | LOG ODDS 
ggcoefstats(catm, output = "plot", 
              # conf.level = 0.90,
              exclude.intercept = FALSE) + 
  labs(x = "Log Odds Estimate", 
       title = "LOGODDS | Q1 State ~ condition",
       subtitle = "(p is for two tailed test)")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(catm, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.05, #can manually adjust to account for directional test
           ci.lvl = 0.95 ) +  #can manually adjusted for directional test   
  labs(title = "ODDS RATIO | Q1 State ~ condition",
       subtitle = "(p for one two test)",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(catm, type="eff",
           title = "MODEL PREDICTION | Q1 State ~ condition",
           axis.title = c("Condition","Probability of Response State"))

#SJPLOT | MODEL | TABLE
#tab_model(catm)

```

##### Diagnostics

```{r}

#EXAMINE PREDICTIONS
#create sample data frame
test <- data.frame(pretty_condition = c("control", "impasse"))
pred <- predict(catm, newdata = test, "probs")
paste("Predicted Probability of Being in Each State")
paste("[these should be consistent with effects plot, above")
(cbind(test, pred))

print("MODEL PERFORMANCE")
performance(catm)
DescTools::PseudoR2(catm, which = c("McFadden", "CoxSnell", "Nagelkerke"))

#General Goodness of Fit
#library(generalhoslem)
#logitgof(df$state, catm$fitted.values, g = 3)
#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).
#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables
chisq.test(df$state,predict(catm)) #actual states VS predicted states
# The chi-square test tests the decrease in unexplained variance from the baseline model to the final model

# print("MODEL DIAGNOSTICS")
#check_model(catm) can't do overall diagnostics, have to do them on individual model equations

```

## H1B \| TEST PHASE ACCURACY

+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Do Ss in the IMPASSE condition score higher across the entire task than those in the CONTROL group?                                                     |
+=======================+=========================================================================================================================================================+
| **Hypothesis**        | (H1B) Participants in the IMPASSE condition will have higher test phase performance than those in the CONTROL condition.                                |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | **data**: `df_items` where `q nin 1,2,3,4,5,6,9` (the 8 discriminating test phase Qs ), `df_subjects`                                                   |
|                       |                                                                                                                                                         |
|                       | **outcome**:                                                                                                                                            |
|                       |                                                                                                                                                         |
|                       | -   \[at item level\]                                                                                                                                   |
|                       |     -   *accuracy* ( factor(incorrect/correct) from `score_niceABS` \[absolute score\]                                                                  |
|                       |     -   *state* (3-level ordered factor from `interpretation`)                                                                                          |
|                       | -   \[subject level\]                                                                                                                                   |
|                       |     -   accuracy (number of test phase qs correct from total `s_NABS`)                                                                                  |
|                       |                                                                                                                                                         |
|                       | **predictor**: `condition` \[between-subjects factor\]                                                                                                  |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  Wilcoxon-Rank Sum (Mann-Whitney) test on subject-level total accuracy of test phase (`item_test_NABS`)                                              |
|                       | 2.  Mixed Logistic Regression\                                                                                                                          |
|                       |     `accuracy` \~ `condition` + (1 \| `subject` )\                                                                                                      |
|                       |     model effect of condition on probability of correct response \[during test phase\] while accounting for subject (and item-level?) effects           |
|                       | 3.  Ordinal Mixed Logistic Regression\                                                                                                                  |
|                       |     `interpretation` \~ `condition` + (1 \| `subject` )\                                                                                                |
|                       |     model effect of condition on \[ordered correctness of interpretation\] \[during test phase\] while accounting for subject (and item-level?) effects |
|                       | 4.  Shift in Modal Mass (descriptive)\                                                                                                                  |
|                       |     describe & visualize shift in deciles between conditions for `` `scaled_score` `` (at subject level)                                                |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Alternatives**      | -   OLS LINEAR REGRESSION                                                                                                                               |
|                       |     -   bimodal distribution at tails makes the mean a poor predictor; LMs violate assumptions of normally distributed residuals                        |
|                       |     -   lm `DV_percent_test_NABS` \~ `condition` (absolute scoring) OR lm `item_test_SCALED` \~ `condition` (scaled scoring)                            |
|                       |     -   both absolute and scaled scores yield non-normal residuals                                                                                      |
|                       |     -   no transformation of the outcome variables yield normal residuals                                                                               |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Notes**             | **Also exploring:**                                                                                                                                     |
|                       |                                                                                                                                                         |
|                       | -   Hurdle model (mixture model w/ binomial + \[poisson or negbinom count; 0s from 1 DGP)                                                               |
|                       | -   Zero Inflated model (mixture model w/ binomial + poisson or negbinom count; 0s from 2 DGPs)                                                         |
|                       | -   Beta regression hurdle model? (mixture with location and scale parameters \[mean, variance\] and hurdles for floor and ceiling effects)             |
|                       | -   Other way to account for the severe bimodality?                                                                                                     |
+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+

```{r}
#| label: SETUP-TEST-ACC


#item level
df = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct"),
  q = as.factor(q)
)

#:::::::: STACKED PROPORTIONAL BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~pretty_mode) + 
   labs(title = "Test Phase Accuracy",
       x = "Condition",
       fill = "",
       subtitle="Impasse Condition yields a greater proportion of correct responses")


#:::::::: FACETED HISTOGRAM
stats = df_subjects %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))
gf_props(~item_test_NABS, 
         fill = ~pretty_condition, data = df_subjects) %>% 
  gf_facet_grid(pretty_condition ~ pretty_mode) %>% 
  gf_vline(data = stats, xintercept = ~mean, color = "red") +
  labs(x = "# Correct",
       y = "proportion of subjects",
       title = "Test Phase Absolute Score (# Correct)",
       subtitle = "") + theme(legend.position = "blank")


```

### (LAB)

```{r}

#:::::::: IN PERSON ONLY
df_i <- df_items %>% filter(mode == "lab-synch") %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

df_s <- df_subjects %>% filter(mode == "lab-synch") %>% mutate(
  test_score = item_test_NABS
)


```

#### WILCOXON RANK SUM (Mann-Whitney Test)

-   **Non parametric alternative** to t-test; compares median rather than mean by ranking data
-   Does not assume normality
-   Does not assume equal variance of samples (homogeneity of variance)

##### Test

```{r}
#| label: WILCOX-LAB

(w <- wilcox.test(df_s$test_score ~ df_s$condition,
                 paired = FALSE, alternative = "less")) #less, greater
report(w)

```

##### Visualize

```{r}


#[manual one-sided test]
(results <- statsExpressions::two_sample_test(y = test_score, x = condition, data = df_s,
                                type = "nonparametric", alternative = "less",
                                var.equal = FALSE))


#:::::::: STATSPLOT | VIOLIN
ggbetweenstats(y = test_score, x = condition, data = df_s,
               results.subtitle = FALSE, #override default [two tailed] test dsiplay
               subtitle = results$expression[[1]]
              )

```

##### Inference

A Mann-Whitney (Wilcoxon Rank Sum) test evaluating the difference in median accuracy score in the test phase of the graph comprehension task indicates that performance was better in the impasse (vs) control condition. \[report stats\]

#### MIXED LOGISTIC REGRESSION

*Fit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.*

**TODO: Do I need to add a random effect for item? I have no reason to suspect that accuracy changes by item, only response time.**

##### Fit Model

```{r}
#| label: MODEL-MLOG-ABS-LAB

## 1 | SETUP RANDOM INTERCEPT SUBJECT

#:: EMPTY MODEL (baseline, no random effect)
print("Empty fixed model")
m0 = glm(accuracy ~ 1, family = "binomial", data = df_i) 
# summary(m0)

#:: RANDOM INTERCEPT SUBJECT
print("Subject intercept random model")
mm.rS <- glmer(accuracy ~ (1|subject), data = df_i, family = "binomial")
# summary(mm.rS)

# :: TEST random effect
paste("AIC decreases w/ new model?", m0$aic > AIC(logLik(mm.rS)))
test_lrt(m0,mm.rS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,mm.rS))$p[2])


## 2 | ADD RANDOM INTERCEPT ITEM?

#:: RANDOM INTERCEPT SUBJECT + INTERCEPT Q
# print("Subject & Question random intercepts")
# mm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = "binomial")
# # summary(mm.rSQ)
# 
# # :: TEST random effect
# paste("AIC decreases w/ new model?", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)) )
# test_lrt(mm.rS,mm.rSQ) #same as anova(m0, m1, test = "Chi")
# paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rS,mm.rSQ))$p[2])

## 3 | ADD FIXED EFFECT CONDITION

# print("FIXED Condition + Subject & Question random intercepts")
# mm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q), 
#                 data = df_i, family = "binomial")
# summary(mm.CrSQ)
# 
# paste("AIC decreases w/ new model", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.CrSQ)) )
# test_lrt(mm.rSQ,mm.CrSQ) #same as anova(m0, m1, test = "Chi")
# paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rSQ,mm.CrSQ))$p[2])

print("FIXED Condition + Subject random intercepts")
mm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject) ,
                data = df_i, family = "binomial")
# summary(mm.CrS)

paste("AIC decreases w/ new model", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )
test_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rS,mm.CrS))$p[2])

```

##### Describe

```{r}

# best model
m1 <- mm.CrS

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m1)

#:::::::: MANUAL ONE-SIDED SIGTEST 

# one-sided (right tail) z test for B COEFFICIENT
#SANITY CHECK 2-tailed test should match the model output
tt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("p value for two-tailed test, null B = 0 : ",round(tt,5))
ot <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("BUT we want a one tailed directional, null: B <= 0: ",round(ot,5))

#:::::::: INTERPRET COEFFICIENTS

se <- sqrt(diag(stats::vcov(m1)))
# table of estimates with 95% CI
paste("LOG ODDS")
(tab <- cbind(Est = fixef(m1), LL = fixef(m1) - 1.96 * se, UL = fixef(m1) + 1.96 *
    se))
paste("ODDS RATIOS")
(e <- exp(tab))

```

##### Inference

To investigate the effect of condition on test phase item accuracy, we fit a mixed-effect binomial logistic regression model with random intercepts for subjects. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (Ï‡2(4): 9.81, p \< 0.002). The explanatory power of the entire model is substantial (conditional R2 = 0.74) though the part related to the fixed effects (marginal R2) is 0.02. Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 56 over the control condition $e^{\beta_1}$ = 56, p \< 0.01, one-tailed, 90% CI \[2.14, 1473\].

##### Visualize

```{r}
#:::::::: PLOT

#GGSTATS | MODEL | LOG ODDS 
ggcoefstats(m1, output = "plot", 
              conf.level = 0.90) + 
  labs(x = "Log Odds Estimate", 
       subtitle = "p is for two tailed test")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(m1, type="eff",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Predicted Probability of Accuracy",
           axis.title = c("Condition","Probability of Accurate Response"))

#SJPLOT | MODEL | TABLE
#tab_model(m1)

```

##### Diagnostics

```{r}
print("SANITY CHECK REPORTING")
report(m1)

print("MODEL PERFORMANCE")
performance(m1)

print("SANITY CHECK REPORTING")
check_model(m1)
binned_residuals(m1)
```

##### TODO

-   sanity check interpretation
-   sanity check random effects structure : ITEM appropriate as random intercept? What does it mean to have two random intercepts?
-   DIAGNOSTICS: What in the world is happening with the normality of random effects plot? Do the fixed effects residuals need to be normally distributed?
-   Are there other plots or recommended diagnostics for mixed log regression
-   consider multiple regression with rt, sequence cluster, confidence, etc.
-   What else needs to be interpreted with respect to the item and subject random effects?
-   Double check: can't have condition by subject or item slope bc subjects are nested in conditions, not crossed

### TODO (ONLINE REPLICATION)

\*\*TODO after verify correctness of approach for the lab-based sample (above)

### SHIFT IN MODAL MASS

The Effect of Condition on Total Scaled Score can be described as a 'shift' in mass between the low and high modes of each distribution.

*First, we use the Kolmogorov-Smirnov test as a Robust alternative to the t-test to test if the two distributions likely come from different populations.*

```{r}
#| label: COMPARE-DIST-NABS-comb

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
# mbp = "/Users/amyfox/Sites/RESEARCH/SGCâ€”Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
# setwd(mbp)

#(requires shift function files loaded)
#LOAD MODAL SHIFT FUNCTION RESOURCES
source("analysis/utils/shift_function/Rallfun-v30.txt")
source("analysis/utils/shift_function/wilcox_modified.txt")
source("analysis/utils/shift_function/rgar_visualisation.txt")
source("analysis/utils/shift_function/rgar_utils.txt")
#NOTE: something in these breaks the stat_ecdf in ggplot2

#PREP DATA 
df <- df_subjects %>%
  dplyr::select(s_SCALED, pretty_condition) %>%
  mutate(
    data = as.numeric(s_SCALED),
    #flip order levels to correctly orient graph
    # gr = recode_factor(pretty_condition, "impasse" = "impasse", "control"="control")
    gr = as.character(pretty_condition)
  ) %>% dplyr::select(data,gr)


g1 <- df %>% filter(gr == "control") %>% dplyr::pull(data)
g2 <- df %>% filter(gr == "impasse") %>% dplyr::pull(data)


#COMPARE DISTRIBUTIONS WITH ROBUST TESTS

#What do common tests say about the difference?

# Kolmogorov-Smirnov test
#If y is numeric, a two-sample (Smirnov) test of the null hypothesis that x and y 
#were drawn from the same continuous distribution is performed. Alternatively, y ...

#null is X is drawn from CDF EQUAL TO Y
ks.test(g1,g2) 
print("SUGGESTS that impasse and control come from different population distributions")

# #null is X is NOT LESS THAN Y
ks.test(g1,g2, alternative = "greater") 
print("SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]")

#REGULAR T-TEST
t.test(g1,g2) # regular Welsh t-test

```

```{r}
#| label: SHIFT-FN-NABS-comb
#| warnings: false
#| messages: false

#IF THIS ERRORS, consider loadling plyr (older than dplyr)
# kernel density estimate + rug plot + superimposed deciles
kde <- plot.kde_rug_dec2(df)
# kde

# compute shift function
out <- shifthd( g1, g2, nboot=200)

# plot shift function
sf <- plot.sf(data=out) # function from rgar_visualisation.txt
# sf

# combine KDE + SF
cowplot::plot_grid(kde, sf, labels=c("A", "B"), ncol = 1, nrow = 2, rel_heights = c(1.5, 1),label_size = 18,hjust = -1,scale=.95)

```

## H1B \| TEST PHASE INTERPRETATION STATE

**Do Ss in the IMPASSE condition have more-triangular interpretations on across the test phase questions?**

While absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn't allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject's response, and determine if these interpretations differ by experimental condition. state is a 3-category variable that groups the following interpretations:

-   **"orthogonal-like"** \[reference category\] includes orthogonal and satisficing responses ==\> indicates a primarily *orthogonal* state of coordinate system understanding
-   **"unknown"** includes: blank, reference point, responses that can't be classified (including selecting all datapoints) =\> indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly *not* orthogonal nor triangular
-   **"triangle-like"** includes correct triangular and 'lines connecting' responses as well as responses that include both orthogonal *and* triangular answers =\> indicates some degree of angular/triangular coordinate understanding

+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Does Ss in the impasse condition produce less orthogonal responses across questions in the test phase?                                          |
+=======================+=================================================================================================================================================+
| **Hypothesis**        | H1A \| Ss in the IMPASSE condition will have a higher likelihood of producing unknown and triangle-like response states across test phase items |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | -   data: `df_items` where `q nin 1,2,3,4,5,6,9` (8 discriminant test phase items)                                                              |
|                       | -   outcome: `state` ( 3 level factor from high_interpretation )                                                                                |
|                       | -   predictor: `condition` \[between-subjects factor\]                                                                                          |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  MIXED Multinomial (Logistic) Regression on state predicted by condition                                                                     |
|                       |                                                                                                                                                 |
|                       | Alternative:                                                                                                                                    |
|                       |                                                                                                                                                 |
|                       | -   MIXED Ordinal regression on state                                                                                                           |
|                       | -   MIXED Multinomial or Ordinal regression on high_interpretation                                                                              |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+

```{r}
#| label: SETUP-TEST-INTERPRETATION

#:::::::: PREP DATA
df = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(
  q = as.factor(q),
  subject = as.factor(subject)
)

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~pretty_mode) + 
   labs(title = "Test Phase Interpretation",
       x = "Condition",
       fill = "",
       subtitle="")

```

A proportional bar chart visualizing the proportion of each interpretations type by condition for each data collection modality (left/right facet) reveals that the pattern of responses are similar regardless of the data collection modality, by differ by condition. In the impasse condition, there are more triangular responses than in control. In the impasse condition, there are also more positive transition (i.e. triangle-like) and neutral (ie. blank or uncertain response types) than in the control condition. This suggests that the

### (LAB)

```{r}

#:::::::: IN PERSON ONLY
df <- df_items %>% filter(mode == "lab-synch") %>% filter(q %nin% c(1,2,3,4,5,6,9)) 

#::::::::::::DESCRIPTIVES

table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row
      prop.table(margin=2) %>%  #return proportion (of column)
      addmargins(1) #sanity check sum of columns


(t <- table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row
      addmargins(1)) #sanity check sum of columns

```

#### MIXED MULTINOMIAL REGRESSION

*Does condition affect the response state of of items in the test phase?*

*Fit a MIXED logistic regression predicting interpretation state (k=3) by condition(k = 2).*

-   Have to use a bayesian approach b/c nlme, lme4 don't support mixed multinomial
-   Alternative would be to manually run 2 X binomal mixed models [should compare outcomes]

-   2 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) \[essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing \[reference category\] vs \[this category\])

-   For *each* equation:

    -   $\beta_{0}$ *= Log Odds of \[this category type vs. reference category type) response in CONTROL condition*
    -   $e^{\beta_{0}}$ *= ODDS of \[this category type vs. reference category type\] response in CONTROL condition*
    -   $\beta_{1}$ *=* $\beta_{1impasse}$ *Log Odds (Log OR; change in odds for \[this category\] type response in impasse (vs) control \[log scale\])*
    -   $e^{\beta_{1}}$ *= ODDS RATIO of \[this. vs reference category type\] response in IMPASSE (vs) CONTROL*
    -   Two-tailed NHST *Null hypothesis:* $\beta_{impasse} = 0$ *the odds for \[this category of response vs. reference\] are not different for IMPASSE condition*
    -   *Alternative hypothesis:* $\beta_{impasse} \ne 0$ *the odds of \[this category of response vs. reference\] increases or decreases for IMPASSE condition*
    
    
##### Fit Model [mblogit]

```{r}

#https://www.elff.eu/software/mclogit/manual/mblogit/
#"baseline category logit" model matches multinom()

#check reference level 
print("Categories (first is reference)")
levels(df$state)

#FIT EMPTY MODEL
# print("EMPTY MODEL")
m.mbl0 <- mblogit(state ~ pretty_condition ,  #no random intercepts; fixed only model 
                  data = df)
#summary(m.mbl0)

#FIT PREDICTOR MODEL
# print("PREDICTOR MODEL")
m.mbl1 <- mblogit(state ~ pretty_condition , 
                  random = ~ 1 | subject , 
                  data = df)
# summary(m.mbl1)

#COMPARE MODEL FIT
paste("AIC wth predictor is lower than empty model?", AIC(m.mbl0) > AIC(m.mbl1))
test_lrt(m.mbl0, m.mbl1)

#DESCRIBE MODEL
summary(m.mbl1)

#INTERPRET COEFFICIENTS
cint <- confint(m.mbl1, level = 0.95)
print("ODDS RATIO")
(e <- cbind( exp(coef(m.mbl1)), exp(cint))) #exponentiated, adjusted

#VISUALIZE
plot_model(m.mbl1)
performance(m.mbl1)

#TABLE
tab_model(m.mbl1, transform = "exp", title = "Model Predicted Odds Ratio")
```

##### Inference

-   Being in the IMPASSE condition *increases* the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 4.7 (z = 3.92, p \< 0.001) . **Participants in the impasse condition were 4.7x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control**.

-   Being in the IMPASSE condition *increases* the odds of giving 'triangle-like' response rather than an orthogonal (or satisficing) response by a factor of 8.61 (z = 3.30, p \< 0.001 ). **Participants in the impasse condition were more than 8.6x as likely to give an a triangular response rather than an orthogonal response compared to participants in control**.

-   As with the (binary) logistic regression on accuracy \~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 *in the control condition.* (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)

##### Visualize

```{r}
#| label: VIS-TEST-INTERPRETATION-LAB

#:::::::: PLOT

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m.mbl1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.05, #can manually adjust to account for directional test
           ci.lvl = 0.95 ) +  #can manually adjusted for directional test   
  labs(title = "ODDS RATIO | Test Phase State ~ condition",
       subtitle = "(p for one two-tailed test)")

#SJPLOT | MODEL | TABLE
tab_model(m.mbl1)

```

##### Diagnostics

```{r}

print("MODEL PERFORMANCE")
performance(m.mbl1)

#General Goodness of Fit
#library(generalhoslem)
#logitgof(df$state, catm$fitted.values, g = 3)
#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).
#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables

# print("MODEL DIAGNOSTICS")
# check_model(m.mbl1) can't do overall diagnostics, have to do them on individual model equations

```



##### Fit Model [brms]

```{r}


#library(brms) #bayesian mixed regression models

#BAYESIAN MIXED VERSION
mixcat.1 <- brm( state ~ condition + (1|subject), data = df, family = "categorical",
                                          file = "analysis/models/sgc3a_brms_mixedcat_teststate_LAB.rds") # cache model (can be removed)))

#DESCRIBE MODEL
summary(mixcat.1)
# report(mixcat.1)

#VISUALIZE

plot(mixcat.1)
plot_model(mixcat.1)
check_posterior_predictions(mixcat.1, draws=1000)
library(bayesplot)
library(bayestestR)
plot(rope(mixcat.1, ci = 0.89))


#PERFORMANCE
performance(mixcat.1)

#TABLE
tab_model(mixcat.1) #, transform = "exp", title = "Model Predicted Odds Ratio")
```

##### Inference
-   Being in the IMPASSE condition *increases* the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 43.86. 

-   Being in the IMPASSE condition *increases* the odds of giving 'triangle-like' response rather than an orthogonal (or satisficing) response by a factor of 6.64. 

**TODO RECONCILE:: brms model provides _substantially_ higher estimate for blank/uncertain response (vs) the mblogit frequentist model


##### Visualize

```{r}

#:::::::: PLOT

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(mixcat.1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.05, #can manually adjust to account for directional test
           ci.lvl = 0.95 ) +  #can manually adjusted for directional test   
  labs(title = "ODDS RATIO | Test Phase State ~ condition",
       subtitle = "(p for one two test)")

```

##### Diagnostics



##### TODO

-   diagnostics on random effects 
-   reconcilliation of mblogit() vs brms versions of the model; seems like they should yield similar estimates

