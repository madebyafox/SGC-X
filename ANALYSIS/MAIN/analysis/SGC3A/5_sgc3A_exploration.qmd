---
subtitle: 'Study SGC3A | 5 Exploratory Analyses'
---

\newpage

# Exploratory Analyses {#sec-SGC3A-exploration}

**TODO**\
- response consistency - clarify core questions being asked\
- review models already created in ARCHIVE?\
- explore response consistency - fix references
- what predicts consistency?
- consider zero inflated / hurdles with ospan score as preditor

*The purpose of this notebook is exploratory analyses of data collected for study SGC3A.*


```{r}
#| label: SETUP
#| warning : false
#| message : false

#misc utilities
library(Hmisc) # %nin% operator
library(broom)
library(modelr)
library(distributional)
library(jtools)
library(pwr) #power analysis
library(mosaic) #favstats

#visualization
library(ggpubr) #arrange plots
library(cowplot) #arrange shift function plots
library(ggformula) #easy graphs
library(vcd) #mosaic plots
library(vcdExtra) #mosaic plots
library(kableExtra) #printing tables 
library(sjPlot) #visualize model coefficients
library(ggdist) #uncertainty viz 
library(ggstatsplot) #plots with stats

#models and performance
library(report) #easystats reporting
library(see) #easystats visualization
library(performance) #easystats model diagnostics
library(qqplotr) #confint on qq plot
library(gmodels) #contingency table and CHISQR
library(equatiomatic) #extract model equation
library(pscl) #zeroinfl / hurdle models 
library(lme4) #mixed effects models 
library(lmerTest) #for CIs in glmer 
library(ggeffects) #visualization log regr models
library(nnet) #multinomial logistic regression [not mixed]
library(mclogit) #frequentist mixed multinomial logistic regression [mblogit]
library(brms) #bayesian mixed multinomials [+ other bayesian reg models]

library(tidyverse) #ALL THE THINGS

#OUTPUT OPTIONS
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
options(ggplot2.summarise.inform = FALSE)
options(scipen=1, digits=3)

#GRAPH THEMEING
theme_set(theme_minimal()) 
```

```{r}
#| label: IMPORT-DATA
#| warning : false
#| message : false

#IMPORT DATA 
# df_items <- read_rds('analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.rds')
# df_subjects <- read_rds('analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds')
#weird... doesn't respect project level execute dir in yml unless included in index; otherwise breaks render

```

Exploratory Questions

Consistency \| How consistent are learners in their interpretation of the graph? Do they adopt an interpretation on the first question and hold constant? Or do they change interpretations from question to question? Are there any interpretations that serve as 'absorbing states' (i.e. once encountered, the learner does not exist this state).

Time Course of Exploration \| What is the relationship between response accuracy (and interpretation) and time spent on each item?

Can exploration strategies be derived from mouse cursor activity?

-   does response time predict interpretation vs. non interpretation?

TODO: - does impasse yield different exploration behavior? (characterize mouse) - does impasse yield more time on task? (characterize response time ? number of answers then de-selected?)

TODO: Think about characterizing how variable the interpretations are across a participant. Do they form an interpretation and hold it constant? Or do they change question to question.

#### Special Visualizations

**SPLIT SIDE BY SIDE**
```{r}
# quota_halves <- ggplot(vdem_2015, aes(x = quota, y = prop_fem)) +
# boxes <- ggplot(df_s, aes(x = pretty_condition, y = task_percent)) +
#   
#   
#   geom_half_point(aes(color = pretty_condition),
#                   transformation = position_quasirandom(width = 0.1),
#                   side = "l", size = 1.5, alpha = 0.5) +
#   geom_half_boxplot(aes(fill = pretty_condition), side = "r", width = 0.5) +
#   scale_fill_viridis_d(option = "plasma", end = 0.8) +
#   scale_color_viridis_d(option = "plasma", end = 0.8) +
#   guides(color = "none", fill = "none") +
#   labs(x = "Condition", y = "Proportion of correct responses (test phase)") +
#   theme_clean()
# 
# # quota_densities <- ggplot(vdem_2015, aes(x = prop_fem, fill = quota)) +
# densities <- ggplot(df_s, aes(x = task_percent, fill = pretty_condition)) +
#   geom_density(alpha = 0.6) +
#   scale_fill_viridis_d(option = "plasma", end = 0.8) +
#   labs(x = "proportion of correct responses (test phase)", y = "Density", fill = "Condition") +
#   theme_clean() +
#   theme(legend.position = "bottom")
# 
# boxes | densities


```


**CUSTOM ONLY**
```{r}

# ggplot(df_s, # data we are using
#        aes(x = task_percent,
#            y = pretty_condition)) +
#   stat_slab(aes(fill = pretty_condition), # slab geometry
#             geom = "slab",
#             position = position_nudge(y = .20), # nudged up by .20
#             scale = .5,
#             # color = "grey",
#             alpha = 1.0) + # transparency
#   geom_boxplot(# boxplot geometry; grey contrast w/ points
#                width = .20,
#                size = 0.5,
#                show.legend = FALSE,
#                alpha = .25) + # transparency
#   geom_point(aes(color = pretty_condition), # point geometry
#              position = position_jitter(width = .01, height = .05), # 
#              size = 1.0,
#              shape = 19, # circles
#              alpha = .5) + # transparency
#   labs(title = "Distribution of Test Phase Accuracy",
#        y = "Test Phase Proportion Correct", x = "Condition") +
#   theme_clean() + theme(legend.position = "blank") +
#   ggeasy::easy_rotate_y_labels(angle = 90)
```
**WIP CUSTOM + STATSEXPRESSIONS LABELS ONLY**
```{r}

#WORKING.. THE ONE
# ggplot(df_s, aes(x = task_percent, y = pretty_condition, fill=pretty_condition)) +
#   stat_slab(
#             geom = "slab",
#             position = position_nudge(y = .20), # nudged up by .20
#             scale = .5) + 
#   geom_boxplot(aes(fill = NULL),
#                width = .20,
#                size = 0.5,
#                show.legend = FALSE,
#                alpha = .25) + 
#   geom_point(aes(color = pretty_condition), 
#              position = position_jitter(width = .01, height = .05), 
#              size = 1.0,
#              shape = 19, # circles
#              alpha = .5) + 
#   labs(title = "Distribution of Total Accuracy",
#        x = "Total Proportion Correct", y = "Condition") +
#   theme(legend.position = "blank") 


#NOT WORKING, post on github issues
# centrality_description(df_s, y = task_percent, x = pretty_condition) |>
#   ggplot(aes(y = task_percent, x = pretty_condition)) +
#   geom_point() +
#   geom_label(aes(label = expression), parse = TRUE, nudge_y = -0.05) + 
#   coord_flip()+
#   stat_slab(
#             geom = "slab",
#             position = position_nudge(y = .20), # nudged up by .20
#             scale = .5) + 
#   geom_boxplot(aes(fill = NULL),
#                width = .20,
#                size = 0.5,
#                show.legend = FALSE,
#                alpha = .25) + 
#   geom_point(aes(color = pretty_condition), 
#              position = position_jitter(width = .01, height = .05), 
#              size = 1.0,
#              shape = 19, # circles
#              alpha = .5) + 
#   labs(title = "Distribution of Test Phase Accuracy",
#        x = "Test Phase Proportion Correct", y = "Condition") +
#   theme(legend.position = "blank") 
```

**CUSTOM + BETWEEN, WORKING** 
```{r}
  
p <-   ggbetweenstats(data = df_s, x = pretty_condition, y = task_percent,
               plot.type = "box", type = "nonparametric",
               centrality.type = "parametric",
               package = "RColorBrewer",
               palette = "PRGn",
               centrality.point.args = list(color="black", size = 3, shape = 1),
               point.args = list(alpha=0), #suppress points
               ggplot.component = ## modify further with `{ggplot2}` functions
                list(
                  aes(color = pretty_condition, fill = pretty_condition),
                  scale_colour_manual(values = paletteer::paletteer_c("viridis::viridis", 3)),
                  scale_fill_manual(values = paletteer::paletteer_c("viridis::viridis", 3)),
                  theme(axis.text.x = element_text(angle = 90)))
               ) +
  ggdist::stat_halfeye(
    alpha = 0.7, 
    point_colour = NA,
    adjust = .5, 
    width = .5, .width = 0, 
    justification = -.5) +
  geom_boxplot(
    alpha = 0.1,
    width = .2, 
    outlier.shape = NA
  ) +
  geom_point(
    size = 2,
    alpha = .5,
    position = position_jitter(
      seed = 1, width = .05, height = .02
    )
  )  +
coord_flip() + theme_clean() + theme(legend.position = "blank")
p$layers[[3]]=NULL #remove default boxplot
e <- statsExpressions::oneway_anova(data = df_s, x = pretty_condition, y = task_percent,
               type = "nonparametric") #gen stats test
#labels are layer 4
p + labs(title = "Distribution of Total Accuracy",
         y = "Proportion of correct responses in test phase", x = "",
         subtitle = "Impasse condition yields higher scores and greater variance",
         caption=e$expression[[1]])


```

```{r}

title = "Descriptive Statistics of Response Accuracy (Total % Correct)"
tbl1 <- mosaic::favstats(~task_percent, data = df_s) 
tbl1 %>% kbl (caption = title) %>% kable_classic()


title = "Descriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION"
tbl2 <- mosaic::favstats(task_percent ~ pretty_condition, data = df_s) 
tbl2 %>% kbl (caption = title) %>% kable_classic()

```

Across both conditions, overall accuracy on the task ranges from `r tbl1$min *100` to `r tbl1$max *100` with a mean of `r tbl1$mean * 100`. We see that the distribution of this outcome variable is clearly bimodal: with modes near the floor (0% correct) and ceiling (100% correct) of the scale. This bimodality is sensical considering the nature of the task, where each item in the task indexes a different information extraction operation over the same coordinate system.

A score of 100% indicates that the participant correctly interpreted the interval-coordinate system throughout the task, *starting at the first question*. A score of 0% indicates the individual *never* correctly interpreted the coordinate system. A score somewhere inbetween indicates that an individual deciphered the coordinate system *sometime over the course the task*.



### WIP ZERO INFLAED BETA REGRESSION

```{r}
#DATA SETUP
# 
# library(tidyfst) #conditional mutate
# 
# df <- df_subjects %>% 
#   mutate(
#     condition = pretty_condition,
#     scaled = (s_SCALED + 13)/26,  #transform scaled onto 0-1 scale
#     accuracy = DV_percent_NABS,
#     shifted_acc = accuracy, #a left shifted only to get rid of 1s
#     shifted_scaled = scaled #a left shifted only to get rid of 1s
#   ) %>% mutate_when(
#   (accuracy == 1) , shifted_acc = 0.999
# )  %>% mutate_when(
#   scaled ==1, shifted_scaled = 0.999
# )
# 
# 
# #ZERO INFLATED [allows zeros, but not ones] 
# zinb_scaled <- brm(
#   bf(shifted_scaled ~ condition, #mean of (0,1) values, mu
#      phi ~ condition, #precision of (0,1) values, phi
#      zi ~ condition), #zero inflated is extreme? alpha
#   data = df,
#   family = zero_inflated_beta(),
#   chains = 4, iter = 2000, warmup = 1000,
#   cores = 4, seed = 1234,
#   backend = "cmdstanr"
# )
# summary(zinb_scaled)
# plot_model(zinb_scaled, vline.color="red")


```




### WIP ZERO-ONE INFLATED BETA Regression

https://sometimesir.com/posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/#zoib-regression
https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#2-fractional-logistic-regression

**TODO** 
- explore priors
- explore random effects
- explore non-beta distribution; once the 1s and zeros are gone, do we need the beta? 

A zero-inflated beta regression allows us to model a separate data generating process for the 0s and 1s and the rest.

A ZOINB is a mixture of:

1.  A logistic regression model that predicts if an outcome is either 0 or 1 or not 0 or 1, defined by (or alternatively, a model that predicts if outcomes are extreme (0 or 1) or not (between 0 and 1); "IS EXTREME" model; defined by $\alpha$

2.  A logistic regression model that predicts if any of the 0 or 1 outcomes are actually 1s, defined by (or alternatively, a model that predicts if the extreme values are 1) "WHICH EXTREME" model defined by $\gamma$

3.  A beta regression model that predicts if an outcome is between 0 and 1 if it's not zero or not one, defined by and (or alternatively, a model that predicts the non-extreme (0 or 1) values) "THE REST" model $\mu$ and $\phi$

Can also do mixed! (but no reason do this here, as the item level data is not beta reg )

```{r}

#DATA SETUP
# sub <- df_subjects %>% dplyr::select(condition, DV_percent_NABS) 
# don't need to transform anymore
# library(tidyfst)
# df <- df_subjects %>% 
#   mutate(
#     condition = pretty_condition,
#     scaled = (s_SCALED + 13)/26,  #transform scaled onto 0-1 scale
#     accuracy = DV_percent_NABS,
#     shifted_acc = accuracy, #a left shifted only to get rid of 1s
#     shifted_scaled = scaled #a left shifted only to get rid of 1s
#   ) %>% mutate_when(
#   (accuracy == 1) , shifted_acc = 0.999
# )  %>% mutate_when(
#   scaled ==1, shifted_scaled = 0.999
# )
# 
# 
# #ZERO INFLATED [allows zeros, but not ones]
# zinb <- brm(
#   bf(shifted_acc ~ condition, #mean of (0,1) values, mu
#      phi ~ condition, #precision of (0,1) values, phi
#      zi ~ condition), #zero inflated is extreme? alpha
#   data = df,
#   family = zero_inflated_beta(),
#   chains = 4, iter = 2000, warmup = 1000,
#   cores = 4, seed = 1234,
#   backend = "cmdstanr"
# )
# summary(zinb)
# plot_model(zinb, vline.color="red")
# 
# #ZERO INFLATED [allows zeros, but not ones] THIS ONE HERE 
# zinb_scaled <- brm(
#   bf(shifted_scaled ~ condition, #mean of (0,1) values, mu
#      phi ~ condition, #precision of (0,1) values, phi
#      zi ~ condition), #zero inflated is extreme? alpha
#   data = df,
#   family = zero_inflated_beta(),
#   chains = 4, iter = 2000, warmup = 1000,
#   cores = 4, seed = 1234,
#   backend = "cmdstanr"
# )
# summary(zinb_scaled)
# plot_model(zinb_scaled, vline.color="red")
# 
# 
# #ZERO ONE INFLATED [allows zeros and 1s]
# zoinb <- brm(
#   bf(accuracy ~ condition, #mean of (0,1) values, mu
#      phi ~ condition, #precision of (0,1) values, phi
#      zoi ~ condition, #is extreme? alpha
#      coi ~ condition), #is 1? gamma
#   data = df,
#   family = zero_one_inflated_beta(),
#   chains = 4, iter = 2000, warmup = 1000,
#   cores = 4, seed = 1234,
#   backend = "cmdstanr"
# )
# summary(zoinb)
# plot_model(zoinb, vline.color="red")
# 
# #ZERO ONE INFLATED [allows zeros and 1s ]
# zoinb_scaled <- brm(
#   bf(scaled ~ condition, #mean of (0,1) values, mu
#      phi ~ condition, #precision of (0,1) values, phi
#      zoi ~ condition, #is extreme? alpha
#      coi ~ condition), #is 1? gamma
#   data = df,
#   family = zero_one_inflated_beta(),
#   chains = 4, iter = 2000, warmup = 1000,
#   cores = 4, seed = 1234,
#   backend = "cmdstanr"
# )
# summary(zoinb_scaled)
# plot_model(zoinb_scaled, vline.color="red")

```

```{r}
# library(extraDistr)
# 
# ggplot() +
#   geom_function(fun = dprop, args = list(mean = 0.999, size = 8),
#                 aes(color = "Beta(mean = 0.99, size = 8)"),
#                 size = 1) +
#   scale_color_viridis_d(option = "plasma", name = "") +
#   theme_clean() +
#   theme(legend.position = "bottom") + labs(
#     "ALL CORRECT"
#   )
# 
# ggplot() +
#   geom_function(fun = dprop, args = list(mean = 0.001, size = 8),
#                 aes(color = "Beta(mean = 0.001, size = 8)"),
#                 size = 1) +
#   scale_color_viridis_d(option = "plasma", name = "") +
#   theme_clean() +
#   theme(legend.position = "bottom") + labs(
#     "ALL WRONG"
#   )
# 
# #todo PLOT ACTUAL ESTIMATES
# ggplot() +
#   geom_function(fun = dprop, args = list(mean = 0.577, size = 0.565),
#                 aes(color = "IMPASSE"),
#                 size = 1) +
#   geom_function(fun = dprop, args = list(mean = 0.473, size = 0.411),
#                 aes(color = "INTERCEPT"),
#                 size = 1) +
#   scale_color_viridis_d(option = "plasma", name = "") +
#   theme_clean() +
#   theme(legend.position = "bottom") + labs(
#     "FIGURED IT OUT HALFWAY "
#   )
# 
# pp_check(zinb_scaled)
```
      


```{r}
# summary(zoinb)
# plot_model(zoinb, vline.color = "red")
# plot(model_parameters(zoinb))
# plot(rope(zoinb))
# 
# 
# #MAKES SENSE! 
# #CONDITION shifts mu to the right (increases); no effect on PHI, decreases extreme, but increases 1s
# summary(zoinb_scaled)
# plot_model(zoinb_scaled,  vline.color = "red")
# plot(model_parameters(zoinb_scaled))
# plot(rope(zoinb_scaled))

```

More plots 

```{r}
# 
# tidy(zoinb, effects = "fixed")
# 
# paste("INTERPRET ALPHA")
# zoi_intercept <- tidy(zoinb, effects = "fixed") %>% 
#   filter(component == "cond", term == "zoi_(Intercept)") %>% 
#   pull(estimate)
# 
# # Logit scale intercept
# zoi_intercept
# 
# # Transformed to a probability/proportion
# plogis(zoi_intercept)
# 
# nrow(df %>% filter(accuracy %in% c(0,1) & condition == "control"))/nrow(df %>% filter(condition == "control"))

```
The value of the zoi_intercept should be the same as the proportion of 0s and 1s (vs non 0s and 1s) in the control condition... and it does!

VISUALIZE ZERO INFLATED COMPONENT
```{r}
# beta_zoi_pred_int <- zoinb %>% 
#   predicted_draws(newdata = tibble(condition = c("control", "impasse"))) %>% 
#   mutate(is_extreme = .prediction %in% c(0,1),
#          is_one = .prediction ==1, 
#          .prediction = ifelse(is_extreme & !is_one, .prediction - 0.01, .prediction),
#          .prediction = ifelse(is_one, .prediction + 0.01, .prediction))
# 
# ggplot(beta_zoi_pred_int, aes(x = .prediction)) +
#   geom_histogram(aes(fill = is_extreme), binwidth = 0.05, 
#                  boundary = 0, color = "white") +
#   geom_vline(xintercept = 0) +
#   geom_vline(xintercept = 1) +
#   scale_fill_viridis_d(option = "plasma", end = 0.5,
#                        guide = guide_legend(reverse = TRUE)) +
#   labs(x = "Predicted proportion of Correct Responses", 
#        y = "Count", fill = "Is extreme?") +
#   facet_wrap(vars(condition), ncol = 2,
#              labeller = labeller(condition = c(`control` = "Control", 
#                                            `impasse` = "Impasse"))) + 
#   theme_clean() +
#   theme(legend.position = "bottom")
# ```
# ```{r}
# beta_zoi_pred_int <- zoinb_scaled %>% 
#   predicted_draws(newdata = tibble(condition = c("control", "impasse"))) %>% 
#   mutate(is_extreme = .prediction %in% c(0,1),
#          is_one = .prediction ==1, 
#          .prediction = ifelse(is_extreme & !is_one, .prediction - 0.01, .prediction),
#          .prediction = ifelse(is_one, .prediction + 0.01, .prediction))
# 
# ggplot(beta_zoi_pred_int, aes(x = .prediction)) +
#   geom_histogram(aes(fill = is_extreme), binwidth = 0.05, 
#                  boundary = 0, color = "white") +
#   geom_vline(xintercept = 0) +
#   geom_vline(xintercept = 1) +
#   scale_fill_viridis_d(option = "plasma", end = 0.5,
#                        guide = guide_legend(reverse = TRUE)) +
#   labs(x = "Predicted proportion of Correct Responses", 
#        y = "Count", fill = "Is extreme?") +
#   facet_wrap(vars(condition), ncol = 2,
#              labeller = labeller(condition = c(`control` = "Control", 
#                                            `impasse` = "Impasse"))) + 
#   theme_clean() +
#   theme(legend.position = "bottom")

```


```{r}
# pp_check(zoinb, draws = 8000 )

```












## WORKING MEMORY 

During the 2021-2022 online replication study, participants completed a working memory task —the OSPAN task- after completing the SGC3A study.  We added this task to explore possible sources of variance between individuals on the graph comprehension task.  

**Here we address the question: Does working memory (as measured by the OSPAN task) help explain performance on the interval graph comprehension task?**

_First we join the load data for the subset of SGC3A participants who completed the OSPAN task. Note that this is a slightly lower n than the online replication, as a couple of participants failed to finish the OSPAN task, and will be excluded from this analysis._

```{r}

#LOAD OSPAN PARTICIPANTS
df_ospan <- read_csv("analysis/SGC3A/data/1-study-level/sgc3a_ospan.csv") #not scored
sgc3a <- read_rds("analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds") 
sgc3a_items <- read_rds("analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.rds") 

##ADD OSPAN DATA TO SUBJECT LEVEL DATA

#select needed ospan columns
df_ospan <- df_ospan %>% rename( OSPAN.weighted = weighted, OSPAN.math_acc = math_acc,
                                 OSPAN.order_acc = order_acc) %>% dplyr::select(
                                 subject, OSPAN.weighted, OSPAN.math_acc, OSPAN.order_acc) %>% 
  mutate(
      z_ospan = zscore(OSPAN.weighted),
      ospan_split = as.factor(OSPAN.weighted > median(OSPAN.weighted)),
      ospan_split = recode_factor(ospan_split, "FALSE" = "low-memory", "TRUE" = "high-memory")
    
  )

#get only the participants for whom we have ospan data 
df_sgc3a <- sgc3a %>% filter(subject %in% df_ospan$subject)
df_no_ospan <- sgc3a %>% filter(subject %nin% df_ospan$subject)
online_no_ospan <- df_no_ospan %>% filter(mode == "asynch")

#add q1 state field [3 category summary of interepretation]
df_sgc3a <- df_sgc3a %>% mutate(
  item_q1_state =  recode_factor(item_q1_SCALED, 
                         "-1" = "orth-like",
                         "-0.5" = "unknown",
                         "0" = "unknown",
                         "0.5" = "tri-like",
                         "1" = "tri-like"),
)

#join the ospan columns to the scored sgc3a data 
df_sgc3a <- merge(df_sgc3a, df_ospan)


##ADD OSPAN DATA TO ITEM LEVEL DATA
#get only the participants for whom we have ospan data 
df_sgc3a_items <- sgc3a_items %>% filter(subject %in% df_ospan$subject) 
df_sgc3a_items <- merge(df_sgc3a_items, df_ospan)




#define dataset
df_s <- df_sgc3a
df_i <- df_sgc3a_items

#SAVE OSPAN JOINED DATA 
df_s %>% write_rds("analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants_ospan.rds")
df_s %>% write.csv("analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants_ospan.csv")
df_i %>% write_rds("analysis/SGC3A/data/2-scored-data/sgc3a_scored_items_ospan.rds")
df_i %>% write_csv("analysis/SGC3A/data/2-scored-data/sgc3a_scored_items_ospan.csv")
```

_What is the distribution of performance on the OSPAN task?_


```{r}
#| label: DESC-SUBJ-OSPAN
 
title = "Descriptive Statistics of OSPAN Task Accuracy"
ospan.stats <- rbind(
  "MATH" = df_s %>% dplyr::select(OSPAN.math_acc) %>% unlist() %>% favstats(),
  "ORDER" = df_s %>%  dplyr::select(OSPAN.order_acc) %>% unlist() %>% favstats(),
  "WEIGHTED" = df_s %>% dplyr::select(OSPAN.weighted) %>% unlist() %>% favstats()

)
ospan.stats %>% kbl (caption = title) %>% kable_classic() %>%
  footnote(general = "MATH = %correct of all math questions;
           ORDER = % correct of OSPAN ordering (out of 30); WEIGHTED = math*ospan number correct", general_title = "Note: ",footnote_as_chunk = T)

```


```{r}
#| label: VIS-SUBJ-OSPAN

# #GGFORMULA | DENSITY HISTOGRAM 
  gf_dhistogram(~z_ospan, data = df_s) +
  labs(x = "OSPAN (weighted) score",
       y = "% of subjects",
       title = "Distribution of OSPAN SCORE",
       subtitle = "")
 
##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE
p <- gghistogram(df_s, x = "z_ospan", binwidth = 0.5,
   add = "mean", rug = TRUE,
   fill = "pretty_condition", #, palette = c("#00AFBB", "#E7B800"),
   add_density = TRUE)
facet(p, facet.by=c("pretty_condition")) +
  labs( title = "Distribution of OSPAN Score",
        subtitle ="The shape of the distribution of OSPAN scores is similar across conditions",
        x = "OSPAN (weighted) score", y = "number of subjects") +
  theme_minimal() + theme(legend.position = "blank")


#GGSTATS
ggbetweenstats(y = z_ospan, x = pretty_condition, data = df_s,
               type = "parametric", var.equal = FALSE,
               title = "Independent Samples T-Test indicates minimal difference between means by Condition")


#PROPORTIONAL BAR CHART OF MED SPLIT BY CONDITION
ggbarstats( x = ospan_split, y = pretty_condition, data = df_s) + labs(
  title = "The Proportion of high (vs) low WM scores is comparable across conditions"
)



```
_As we would expect, there is no meaningful difference between the means (or shape of distribution) of the outcome weighted OSPAN by Graph Comprehension task condition, nor between the proportions of high (vs) low scores after taking a median split of the weighted OSPAN score.  This indicates that performance on the OSPAN task is not affected by the experimental condition of the GRAPH COMPREHENSION task.  This is as we would expect: (1) The OSPAN task is separate from the GC task, and is taken afterward, and (2) we do not expect that the GC task changes the WM capacity measured in the OSPAN task in any way. This confirms that if we split participants into low (vs) high WM groups based on OSPAN measure, that these individuals are effectively randomized between the two GC task experimental conditions._


### OSPAN ~ Q1 Accuracy

**Does the OSPAN score help explain variance in Q1 Accuracy?**

```{r}

#SETUP DATA
df_s = df_s #just for reference


##STATSPLOT
ggstatsplot::grouped_ggbarstats(data = df_s, 
                                    y = ospan_split, x = item_q1_NABS, 
                                    grouping.var = pretty_condition,
                                    type = "nonparametric", equal.var = FALSE,
                                    annotation.args = list(
    title = "Potential INTERACTION between condition and WM Score"))
```

_It appears as though there may be an interaction between WM score and CONDITION, such that in the control condition, participants perform equally well (or poorly, as the case may be), regardless of of WM score. In the impasse condition, readers with high WM scores have a higher proportion of correct responses than readers with low WM scores. One possible explanation of these observations is that perhaps higher WM capacity enables readers to take advantage of the implicit scaffolding the impasse provides._

#### Statistical Tests


To explore the relationship between ACCURACY, CONDITION and OSPAN score, we can run two separate CHI square tests evaluating independence of condition and accuracy, one for each level of working memory (the groups are roughly equal due to the median split). If WM interacts with condition, we should expect to see one chi square indicate independence (low working memory) and one not (high working memory).  Additionally, we can compute a Breslow-Day test for Homogeneity of Odds Ratios, to test the null hypothesis that the odds ratios for accuracy by condition do not differ across levels of working memory. 
```{r}

#IS ACCURACY INDEPENDENT OF CONDITION for low WM?
paste("Accuracy independent of CONDITION for LOW WM?")
x <- df_s %>% filter(ospan_split == "low-memory")
(t <- table(x$item_q1_NABS, x$condition))
# chisq.test(t)
(p <- chiSquare(item_q1_NABS ~ condition, data = x))
paste("Reject null hypothesis that CONDITION is independent of ACCURACY for low WM? p = ",p[4])


#IS ACCURACY INDEPENDENT OF CONDITION for low WM?
paste("Accuracy independent of CONDITION for HIGH WM?")
x <- df_s %>% filter(ospan_split == "high-memory")
(t <- table(x$item_q1_NABS, x$condition))
# chisq.test(t)
(p <- chiSquare(item_q1_NABS ~ condition, data = x))
paste("Reject null hypothesis that CONDITION is independent of ACCURACY for low WM? p = ",p[4])

library(DescTools)
#Calculates the Breslow-Day test of homogeneity for a 2 \times 2 \times k2×2×k table, in order to investigate if all #kk strata have the same OR. If OR is not given, the Mantel-Haenszel estimate is used.
t <- table(df_s$item_q1_NABS, df_s$ospan_split, df_s$condition)
BreslowDayTest(t, OR = 1)


```
_A Chi Square test examining independence of Q1 accuracy and experimental condition performed only on participants with low WM scores was non-significant, indicating that for participants with low working memory scores, experimental condition did not affect Q1 score accuracy.   However, the same Chi Square test performed on participants with high WM scores approached significance (p = 0.057), suggesting that condition may infact affect score for high WM participants._

_A Brewslow-Day Test of homogeneity of Odds Ratios allows us to test the null hypothesis that the odds for each response outcome (correct, incorrect) are the same for each condition across each level of working memory. Essentially, that the odds ratio of a positive response for each condition does not differ by working memory.  The significant p-value for this test allows us to reject this null hypothesis and conclude we have evidence that working memory score changes the odds ratio of a correct vs incorrect response across conditions._

#### Logistic Regression

**Logistic Regression** Q1 Accuracy ~ Condition + OSPAN

##### Fit Model

```{r}

#DATASETUP
df <- df_i %>% filter(q==1) %>% mutate(
  accuracy = as.factor(score_niceABS),
  l_condition = fct_relevel(condition, "121"),
  l_split = fct_relevel(ospan_split, "high-memory")
)

#:::::::: FIT MODELS

#fit empty model
m.0 <- glm(  accuracy ~ 1, data = df, family = "binomial")
# summary(m.0)

#fit condition as predictor 
m.c <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
summary(m.c)

#is condition better than empty model?
print("Is Condition model better than empty model?")
print("Expect YES")
test_lrt(m.0, m.c)



```


```{r}
#fit OSPAN as predictor 
m.o <- glm( accuracy ~ ospan_split, data = df, family = "binomial")
#summary(m.o)

#is ospan better than empty model?
print("Is OSPAN model better than empty model?")
print("Expect NO")
test_lrt(m.0, m.o)


# fit condition AND OSPAN MAIN EFFECTS
m.co <- glm( accuracy ~ pretty_condition + ospan_split, data = df, family = "binomial")
summary(m.co)
# car::Anova(m.cw)

#is CONDITION + OSPAN better than CONDITION model?
print("Is CONDITION + OSPAN model better than CONDITION model?")
print("Expect NO")
test_lrt(m.c, m.co)

# fit condition AND OSPAN with INTERACTION 
m.cio <- glm( accuracy ~  pretty_condition:ospan_split + pretty_condition + ospan_split , data = df, family = "binomial")
# summary(m.cio)
# car::Anova(m.ciw)

#is CONDITION + OSPAN IXN better than CONDITION model?
print("Is CONDITION + OSPAN IXN model better than CONDITION model?")
print("Expect YES")
test_lrt(m.cio, m.c)

# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: 

# one-sided (right tail) z test for B COEFFICIENT
#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients

#SANITY CHECK 2-tailed test should match the model output
# tt <- 2*pnorm(m.3$coefficients, lower.tail = F)
# paste("p value for two-tailed test, null B = 0 : ",round(tt,3))
# ot <- pnorm(m.3$coefficients, lower.tail = F)
# paste("BUT we want a one tailed directional, null: B <= 0: ",round(ot,3))
# paste("adjusted confint for directional hypothesis")
# (dcint <- confint(m1, level = 0.90)) # get 90% for right side))
# # https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte

compare_performance(m.0, m.c, m.o, m.co, m.cio)

test_lrt(m.c, m.co)

```

_The condition + ospan + interaction model has the lowest residual error, though is not a significantly better fit (judged by likelihood ratio test) than the condition only model. 

##### Describe

```{r}

summary(m.cio)
car::Anova(m.cio)
```
##### Inference

Significant main effect condition, non significant main effect of OSPAN, non significant interaction.

Although we can see (ref plots below) that high_working memory participants were more likely to correctly answer Q1, the difference in proportions for this sample size (~30 per cell in the 2x2 contingency table of condition X OSPAN score) failed to reach statistical significance at the 0.05 alpha level.

##### Visualize
```{r}
plot_model(m.c, type = "eff")
plot_model(m.cio, type = "int")
```

##### Diagnostics
```{r}

#PERFORMANCE
performance(m.cio)

#DIAGNOSTICS
check_model(m.cio)
```


### OSPAN ~ Q1 Interpretation

**Does the OSPAN score help explain variance in Q1 interpetation?**

```{r}
##STATSPLOT
ggstatsplot::grouped_ggbarstats(data = df_s, 
                                    y = ospan_split, x = item_q1_state, 
                                    grouping.var = pretty_condition,
                                    type = "nonparametric", equal.var = FALSE,
                                    annotation.args = list(
    title = "Potential INTERACTION between condition and WM Score"))
```
_Again we see a potential interaction between WM score and CONDITION looking at the interpretation indicated by responses on the first question. In the control condition, low and high working memory participants offered very similar proportions of each type of interpretation (with the high working memory subjects offering slightly more unknown and tri-like interpretations). Alternatively in the impasse condition, high working memory Ss have substantially fewer orthogonal interpretations, with far more triangular and unknown interpretations._

#### Statistical Tests


To explore the relationship between INTERPREATION STATE, CONDITION and OSPAN score, we can run two separate CHI square tests evaluating independence of condition and interpretation, one for each level of working memory (the groups are roughly equal due to the median split). If WM interacts with condition, we should expect to see one chi square indicate independence (low working memory) and one not (high working memory).  Additionally, we can compute a Breslow-Day test for Homogeneity of Odds Ratios, to test the null hypothesis that the odds ratios for accuracy by condition do not differ across levels of working memory. 
```{r}

#IS INTERPRETATION STATE INDEPENDENT OF CONDITION for low WM?
paste("Accuracy independent of CONDITION for LOW WM?")
x <- df_s %>% filter(ospan_split == "low-memory")
(t <- table(x$item_q1_state, x$condition))
# chisq.test(t)
chiSquare(item_q1_state ~ condition, data = x)
# NOT expected

#IS ACCURACY INDEPENDENT OF CONDITION for HIGH WM?
paste("Accuracy independent of CONDITION for HIGH WM?")
x <- df_s %>% filter(ospan_split == "high-memory")
(t <- table(x$item_q1_state, x$condition))
# chisq.test(t)
chiSquare(item_q1_state ~ condition, data = x)
# YES, IT IS (EXPECTED)

library(DescTools)
#Calculates the Breslow-Day test of homogeneity for a 2 \times 2 \times k2×2×k table, in order to investigate if all #kk strata have the same OR. If OR is not given, the Mantel-Haenszel estimate is used.
t <- table(df_s$item_q1_state, df_s$ospan_split, df_s$condition)
BreslowDayTest(t, OR = 1)


```

A Brewslow-Day Test of homogeneity of Odds Ratios allows us to test the null hypothesis that the odds for each response outcome (orth-like, unknown, tri-like) are the same for each condition across each level of working memory. Essentially, that the odds ratio of a positive response for each condition does not differ by working memory.  

_We must reject the null hypothesis that (for low-working memory participants) condition and interpretation are independent. They do appear to be related, such that even for low-working memory participants, the impasse condition yields many more unknown and triangle-like responses than the control condition.  Together with the non-significant p-value for that does not allow us to reject the null hypothesis of homogeneous odds ratios across levels of working memory score, we do not have evidence of an interaction between working memory score and INTERPRETATION on question 1. 



#### Logistic Regression

**Logistic Regression** Q1 Accuracy ~ Condition + OSPAN

##### Fit Model

```{r}

#DATASETUP
df <- df_i %>% filter(q==1) %>% mutate(
  accuracy = as.factor(score_niceABS),
  l_condition = fct_relevel(condition, "121"),
  l_split = fct_relevel(ospan_split, "high-memory")
)

#:::::::: FIT MODELS

#fit empty model
m.0 <- glm(  accuracy ~ 1, data = df, family = "binomial")
# summary(m.0)

#fit condition as predictor 
m.c <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
#summary(m.c)

#is condition better than empty model?
print("Is Condition model better than empty model?")
print("Expect YES")
test_lrt(m.0, m.c)

#fit OSPAN as predictor 
m.o <- glm( accuracy ~ ospan_split, data = df, family = "binomial")
#summary(m.o)

#is ospan better than empty model?
print("Is OSPAN model better than empty model?")
print("Expect NO")
test_lrt(m.0, m.o)


# fit condition AND OSPAN MAIN EFFECTS
m.co <- glm( accuracy ~ pretty_condition + ospan_split, data = df, family = "binomial")
summary(m.co)
# car::Anova(m.cw)

#is CONDITION + OSPAN better than CONDITION model?
print("Is CONDITION + OSPAN model better than CONDITION model?")
print("Expect NO")
test_lrt(m.c, m.co)

# fit condition AND OSPAN with INTERACTION 
m.cio <- glm( accuracy ~  pretty_condition:ospan_split + pretty_condition + ospan_split , data = df, family = "binomial")
# summary(m.cio)
# car::Anova(m.ciw)

#is CONDITION + OSPAN IXN better than CONDITION model?
print("Is CONDITION + OSPAN IXN model better than CONDITION model?")
print("Expect YES")
test_lrt(m.cio, m.c)

# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: 

# one-sided (right tail) z test for B COEFFICIENT
#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients

#SANITY CHECK 2-tailed test should match the model output
# tt <- 2*pnorm(m.3$coefficients, lower.tail = F)
# paste("p value for two-tailed test, null B = 0 : ",round(tt,3))
# ot <- pnorm(m.3$coefficients, lower.tail = F)
# paste("BUT we want a one tailed directional, null: B <= 0: ",round(ot,3))
# paste("adjusted confint for directional hypothesis")
# (dcint <- confint(m1, level = 0.90)) # get 90% for right side))
# # https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte

```

##### Describe

```{r}
summary(m.cio)
car::Anova(m.cio)
```
##### Inference

Main effect of condition, no main effect of OSPAN and non significant interaction. 

Although we can see (ref plots below) that high_working memory participants were more likely to correctly answer Q1, the difference in proportions for this sample size (~30 per cell in the 2x2 contingency table of condition X OSPAN score) failed to reach statistical significance at the 0.05 alpha level.

##### Visualize
```{r}
plot_model(m.c, type = "eff")
plot_model(m.cio, type = "int")
```

##### Diagnostics
```{r}

#PERFORMANCE
performance(m.cio)

#DIAGNOSTICS
check_model(m.cio)
```





### OSPAN ~ Task Accuracy 

**Does the OSPAN score help explain variance in test phase accuracy?**

```{r}

##SETUP DATA
df <- df_s

##STATSPLOT
ggstatsplot::grouped_ggbetweenstats(data = df, 
                                    y = item_test_NABS, x = pretty_condition, 
                                    grouping.var = ospan_split,
                                    p.adjust.method = "holm",
                                    type = "nonparametric", equal.var = FALSE,
                                    annotation.args = list(
    title = "Potential INTERACTION between condition and WM Score"))

#NOTE: GROUPED BETWEEN runs multiple mann-whitey tests (with specifed p corrections), one for 
##each elvel of the grouping factor
```
_There appears to be an interaction between CONDITION and OSPAN score such that only participants with high working memory scores had better performance in the impasse condition. This could potential be driving the effect of the impasse condition._


#### Statistical Tests

The outcome variables are not normally distributed, and do not have equal variance. As there is no non-parametric alternative to the two_way (factorial) anova, we can calculate multiple Wilcoxon Rank Sum (extended Mann Whitney) tests.  (This is what the grouped_ggbetweenstats plot, above, calculates)

```{r}

#control group
Paste("OSPAN affects score in CONTROL condition?")
x <- df_s %>% filter(pretty_condition=="control")
kruskal.test( item_test_NABS ~ ospan_split , data = x)
wilcox.test(x$item_test_NABS ~ x$ospan_split,
                 paired = FALSE, alternative = "two.sided") #less, greater


Paste("OSPAN affects score in IMPASSE condition?")
x <- df_s %>% filter(pretty_condition=="impasse")
kruskal.test( item_test_NABS ~ ospan_split , data = x)
wilcox.test(x$item_test_NABS ~ x$ospan_split,
                 paired = FALSE, alternative = "two.sided") #less, greater

```

#### Linear Regression


##### Fit Model (transformed)
```{r}

#PREP DATA
df_s <- df_s %>% mutate(
  p_acc = item_test_NABS/8,
  c_acc = scale(item_test_NABS), #mean centered accuracy score
  med_acc = scale(item_test_NABS, center = median(item_test_NABS), scale = F)
)

#:::::::: FIT MODELS

#empty model
m.0 <- lm( data = df_s, p_acc ~ 1)
# summ(m.0)

#condition predictor
m.c <- lm( data = df_s, p_acc ~ pretty_condition)
#summ(m.c)

paste("Condition predictor improves fit over empty?")
test_lrt(m.c, m.0)

#ospan predictor
m.o <- lm( data = df_s, p_acc ~ ospan_split)
# summ(m.o)

paste("Ospan (only) predictor improves fit over empty?")
test_lrt(m.0, m.o)


#add ospan predictor
m.co <- lm( data = df_s, p_acc ~ pretty_condition + ospan_split)
summary(m.co)

paste("Condition + OSPAN improves fit over Condition?")
test_lrt(m.c, m.co)

#add interaction term  
m.cio <- lm( data = df_s, p_acc ~ pretty_condition + ospan_split + pretty_condition:ospan_split)
summary(m.cio)

paste("Condition : OSPAN interaction improves fit over main effects?")
test_lrt(m.co, m.cio)

compare_performance(m.0, m.c, m.o, m.co, m.cio)

car::Anova(m.cio)
```
_The model predicting test phase performance from condition, ospan and an interaction term had the highest R2, lowest residual error and lowest AIC._



##### Fit Model
```{r}

#PREP DATA
df_s <- df_s 

#:::::::: FIT MODELS

#empty model
m.0 <- lm( data = df_s, item_test_NABS ~ 1)
# summ(m.0)

#condition predictor
m.c <- lm( data = df_s, item_test_NABS ~ pretty_condition)
# summ(m.c)

paste("Condition predictor improves fit over empty?")
test_lrt(m.c, m.0)

#ospan predictor
m.o <- lm( data = df_s, item_test_NABS ~ ospan_split)
# summ(m.o)

paste("Ospan (only) predictor improves fit over empty?")
test_lrt(m.0, m.o)


#add ospan predictor
m.co <- lm( data = df_s, item_test_NABS ~ pretty_condition + ospan_split)
summary(m.co)

paste("Condition + OSPAN improves fit over Condition?")
test_lrt(m.c, m.co)

#add interaction term  
m.cio <- lm( data = df_s, item_test_NABS ~ pretty_condition + ospan_split + pretty_condition:ospan_split)
summary(m.cio)

paste("Condition : OSPAN interaction improves fit over main effects?")
test_lrt(m.co, m.cio)

compare_performance(m.0, m.c, m.o, m.co, m.cio)
```
_The model predicting test phase performance from condition, ospan and an interaction term had the highest R2, lowest residual error and lowest AIC._

##### Describe 

```{r}
#Describe 
summary(m.cio)
anova(m.cio)
```

$\beta_0$ intercept is MEAN when both predictors are at reference: (control condition low memory) [M = 1.129]
$\beta_{1impasse}$ intercept is DIFFERENCE to (IMPASSE condition low memory) [M 1.72 - 1.129 = 0.591]
$\beta_{1high}$ intercept is DIFFERENCE to (CONTROL condition HIGH memory) [M 1.21 - 1.129 = 0.079]
$\beta_{1 impasse : high}$ is DIFFERENCE to ?? no idea can't figure this one out. its not to reference levels

BUT don't fret re: significance of predictors in the model, rather, to see if effects are significant, run anova on the model. 

##### Visualize

**By comparing the average performance by condition ...**
```{r}
#PLOT EFFECTS
plot_model(m.c, type = "pred")
#TODO GET + ylim(0,8) to work
```

**...to the average by condition broken out by working memory, we see that perhaps working memory score drives some of the effect of improved performance in the impasse condition.**

```{r}
plot_model(m.cio, type = "int") 
#+ ylim(0,8)
```
##### Diagnostics
```{r}

#PERFORMANCE
performance(m.cio)

#DIAGNOSTICS
check_model(m.cio)

```



#### Mixed Logistic Regression

```{r}
#PREP DATA
df_i <- df_sgc3a_items %>% filter(q %nin% c(6,9)) %>% mutate(
  accuracy = as.factor(score_niceABS),
  ospan = as.factor(ospan_split)
) #test phase discrim only

```
##### Fit Model

```{r}

## 1 | SETUP RANDOM INTERCEPT SUBJECT

#:: EMPTY MODEL (baseline, no random effect)
print("Empty fixed model")
m0 = glm(accuracy ~ 1, family = "binomial", data = df_i) 
# summary(m0)

#:: RANDOM INTERCEPT SUBJECT
print("Subject intercept random model")
mm.rS <- glmer(accuracy ~ (1|subject), data = df_i, family = "binomial",
               control=glmerControl(optimizer="bobyqa",
            optCtrl=list(maxfun=100000)))
# summary(mm.rS)

# :: TEST random effect
paste("AIC decreases w/ new model?", m0$aic > AIC(logLik(mm.rS)))
test_lrt(m0,mm.rS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,mm.rS))$p[2])


## 2 | ADD RANDOM INTERCEPT ITEM?

#:: RANDOM INTERCEPT SUBJECT + INTERCEPT Q
# print("Subject & Question random intercepts")
# mm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = "binomial")
# # summary(mm.rSQ)
# 
# # :: TEST random effect
# paste("AIC decreases w/ new model?", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)) )
# test_lrt(mm.rS,mm.rSQ) #same as anova(m0, m1, test = "Chi")
# paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rS,mm.rSQ))$p[2])

## 3 | ADD FIXED EFFECT CONDITION

# print("FIXED Condition + Subject & Question random intercepts")
# mm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q), 
#                 data = df_i, family = "binomial")
# summary(mm.CrSQ)
# 
# paste("AIC decreases w/ new model", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.CrSQ)) )
# test_lrt(mm.rSQ,mm.CrSQ) #same as anova(m0, m1, test = "Chi")
# paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rSQ,mm.CrSQ))$p[2])

print("FIXED Condition + Subject random intercepts")
mm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject) ,
                data = df_i, family = "binomial")
# summary(mm.CrS)

paste("AIC decreases w/ new model", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )
test_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rS,mm.CrS))$p[2])


## 4 | ADD FIXED EFFECT OSPAN only 
print("FIXED OSPAN + Subject random intercepts")
mm.OrS <- glmer(accuracy ~ ospan + (1|subject) ,
                data = df_i, family = "binomial")
# summary(mm.OrS)
paste("OSPAN decreases AIC over random?", AIC(logLik(mm.rS)) > AIC(logLik(mm.OrS)) )
test_lrt(mm.rS,mm.OrS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rS,mm.OrS))$p[2])

## 5 | MODEL OF INTEREST! OSPAN + CONDITION + suject

print("FIXED OSPAN + CONDITION + IXN + Subject random intercepts")
mm.COrS <- glmer(accuracy ~ pretty_condition + ospan + (1|subject) ,
                data = df_i, family = "binomial")
# summary(mm.COrS)
paste("OSPAN decreases AIC over CONDITION alone?", AIC(logLik(mm.COrS)) > AIC(logLik(mm.CrS)) )
test_lrt(mm.COrS,mm.CrS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.COrS,mm.CrS))$p[2])


print("FIXED OSPAN + CONDITION + IXN + Subject random intercepts")
mm.CIOrS <- glmer(accuracy ~ pretty_condition + ospan + pretty_condition:ospan + (1|subject) ,
                data = df_i, family = "binomial")

paste("IXN decreases AIC over main effects alone?", AIC(logLik(mm.CIOrS)) > AIC(logLik(mm.COrS)) )
test_lrt(mm.CIOrS,mm.COrS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.CIOrS,mm.COrS))$p[2])

```

##### Describe

```{r}

# best model
m1 <- mm.CIOrS

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m1)
car::Anova(m1)

#:::::::: MANUAL ONE-SIDED SIGTEST 

# one-sided (right tail) z test for B COEFFICIENT
#SANITY CHECK 2-tailed test should match the model output
# tt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
# paste("p value for two-tailed test, null B = 0 : ",round(tt,5))
# ot <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
# paste("BUT we want a one tailed directional, null: B <= 0: ",round(ot,5))

#:::::::: INTERPRET COEFFICIENTS

se <- sqrt(diag(stats::vcov(m1)))
# table of estimates with 95% CI
paste("LOG ODDS")
(tab <- cbind(Est = fixef(m1), LL = fixef(m1) - 1.96 * se, UL = fixef(m1) + 1.96 *
    se))
paste("ODDS RATIOS")
(e <- exp(tab))

```

##### Inference

TODO

##### Visualize

```{r}
#:::::::: PLOT

#GGSTATS | MODEL | LOG ODDS 
ggcoefstats(m1, output = "plot", 
              conf.level = 0.90) + 
  labs(x = "Log Odds Estimate", 
       subtitle = "p is for two tailed test")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(m1, type="int",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Predicted Probability of Accuracy",
           axis.title = c("Condition","Probability of Accurate Response"))

#SJPLOT | MODEL | TABLE
#tab_model(m1)

```

##### Narrative

**By comparing the average performance by condition ...**
```{r}
#PLOT EFFECTS
plot_model(mm.CrS, type = "pred")
#TODO GET + ylim(0,8) to work
```

**...to the average by condition broken out by working memory, we see that perhaps working memory score drives some of the effect of improved performance in the impasse condition.**

```{r}
plot_model(mm.CIOrS, type = "int") 
#+ ylim(0,8)
```

_It is compelling to see that only high_working memory participants achieved the high probabilites of correct response in impasse_.




##### Diagnostics

```{r}
#| warning: false
#| message: false

print("SANITY CHECK REPORTING")
report(m1)

print("MODEL PERFORMANCE")
performance(m1)

print("DIAGNOSTICS")
check_model(m1)
```




### TODO OSPAN ~ Test Phase Interpretation 

**Does the OSPAN score help explain variance in test phase interpretation?**

```{r}

##STATSPLOT
ggstatsplot::grouped_ggbarstats(data = df_i, 
                                    y = score_STATE, x = ospan_split, 
                                    grouping.var = pretty_condition,
                                    p.adjust.method = "holm",
                                    type = "nonparametric", equal.var = FALSE,
                                    annotation.args = list(
    title = "Potential INTERACTION between condition and WM Score"))


##STATSPLOT
ggstatsplot::grouped_ggbarstats(data = df_i, 
                                    y = pretty_condition, x = score_STATE, 
                                    grouping.var = ospan_split,
                                    proportion.test = FALSE, k = 1,
                                    type = "nonparametric", equal.var = FALSE,
                                    annotation.args = list(
    title = "High WM moves Ss in impasse from orth to unknown, and unknown to tri?"))
```

#### Statistical Tests

We can evaluate the affect of OSPAN score on interpretation state by running two separate CHI SQR tests of independence (one on each condition), to determine if STATE is independent of OSPAN. 

```{r}

#control group
paste("OSPAN affects score in CONTROL condition?")
x <- df_i %>% filter(pretty_condition=="control")
kruskal.test( score_STATE ~ ospan_split , data = x)
chisq.test( x = x$ospan_split, y = x$score_STATE)


paste("OSPAN affects score in IMPASSE condition?")
x <- df_i %>% filter(pretty_condition=="impasse")
kruskal.test( score_STATE ~ ospan_split , data = x)
chisq.test( x = x$ospan_split, y = x$score_STATE)


```

Results of the chi square tests (also shown in grouped bar plot) indicate that interpretation state is independent of ospan score in the control condition, but not the impasse condition. This suggests that WM Score influences interpretation in the IMPASSE condition, but not the CONTROL condition, consistent with the findings for overall score accuracy.

#### MIXED MULTINOMIAL REGRESSION

*Does OSPAN and condition affect the response state of of all items?*

*Fit a MIXED logistic regression predicting interpretation state (k=3) by condition(k = 2) and OSPAN (k=2).*

-   Can use mclogit mblogit() with random effect *or* bayesian brms package b/c nlme, lme4 don't support random effects on multinomial (ie no categorical family on glmer())

-   Alternative would be to manually run 2 X binomial mixed models \[should compare outcomes\]

-   2 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) \[essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing \[reference category\] vs \[this category\])

-   For *each* equation:

    -   $\beta_{0}$ *= Log Odds of \[this category type vs. reference category type) response in CONTROL condition*
    -   $e^{\beta_{0}}$ *= ODDS of \[this category type vs. reference category type\] response in CONTROL condition*
    -   $\beta_{1}$ *=* $\beta_{1impasse}$ *Log Odds (Log OR; change in odds for \[this category\] type response in impasse (vs) control \[log scale\])*
    -   $e^{\beta_{1}}$ *= ODDS RATIO of \[this. vs reference category type\] response in IMPASSE (vs) CONTROL*
    -   Two-tailed NHST *Null hypothesis:* $\beta_{impasse} = 0$ *the odds for \[this category of response vs. reference\] are not different for IMPASSE condition*
    -   *Alternative hypothesis:* $\beta_{impasse} \ne 0$ *the odds of \[this category of response vs. reference\] increases or decreases for IMPASSE condition*

##### Fit Model \[mblogit\]

```{r}

#setup
df <- d

#check reference level 
print("Categories (first is reference)")
levels(df$state)

#FIT EMPTY MODEL
# print("EMPTY MODEL")
m.mbl0 <- mblogit(state ~ pretty_condition ,  #no random intercepts; fixed only model 
                  data = df)
#summary(m.mbl0)

#FIT PREDICTOR MODEL
# print("PREDICTOR MODEL")
m.mbl1 <- mblogit(state ~ pretty_condition , 
                  random = ~ 1 | subject , 
                  data = df)
# summary(m.mbl1)

#COMPARE MODEL FIT
paste("AIC wth predictor is lower than empty model?", AIC(m.mbl0) > AIC(m.mbl1))
test_lrt(m.mbl0, m.mbl1)

#DESCRIBE MODEL
summary(m.mbl1)
# car::Anova(m.mbl1)

#INTERPRET COEFFICIENTS
cint <- confint(m.mbl1, level = 0.95)
print("ODDS RATIO")
(e <- cbind( exp(coef(m.mbl1)), exp(cint))) #exponentiated, adjusted

#PERFORMANCE
performance(m.mbl1)

#TABLE
tab_model(m.mbl1, transform = "exp", title = "Model Predicted Odds Ratio")
```

##### Inference

-   Being in the IMPASSE condition *increases* the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 4.7 (z = 3.92, p \< 0.001) . **Participants in the impasse condition were 4.7x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control**.

-   Being in the IMPASSE condition *increases* the odds of giving 'triangle-like' response rather than an orthogonal (or satisficing) response by a factor of 8.61 (z = 3.30, p \< 0.001 ). **Participants in the impasse condition were more than 8.6x as likely to give an a triangular response rather than an orthogonal response compared to participants in control**.

-   As with the (binary) logistic regression on accuracy \~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 *in the control condition.* (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)

-   TODO: these estimates seem low to me, given the estimates for the Q1 only model. Also different from brms estimates (below) Suspect NaNs error thrown with mblogit() may be relevant

##### Visualize

```{r}
#| label: VIS-TEST-INTERPRETATION-LAB

#:::::::: PLOT

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m.mbl1, vline.color = "red", 
           transform = "exp", #for some reason have to manually add for mixed?
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.05, #can manually adjust to account for directional test
           ci.lvl = 0.95) +  #can manually adjusted for directional test   
  labs(title = "ODDS RATIO | Test Phase State ~ condition",
       subtitle = "(p for  two-tailed test)")

#SJPLOT | MODEL | TABLE
tab_model(m.mbl1)

```

##### Diagnostics

```{r}

print("MODEL PERFORMANCE")
performance(m.mbl1)

#General Goodness of Fit
#library(generalhoslem)
#logitgof(df$state, catm$fitted.values, g = 3)
#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).
#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables

# print("MODEL DIAGNOSTICS")
# check_model(m.mbl1) can't do overall diagnostics, have to do them on individual model equations

```

##### Fit Model \[brms\]

```{r}


#library(brms) #bayesian mixed regression models

#BAYESIAN MIXED VERSION
mixcat.1 <- brm( state ~ condition + (1|subject), data = df, family = "categorical",
                                          file = "analysis/models/sgc3a_brms_mixedcat_teststate_LAB.rds") # cache model (can be removed)))

#DESCRIBE MODEL
summary(mixcat.1)
# report(mixcat.1)

#VISUALIZE

plot(mixcat.1)
plot_model(mixcat.1)
check_posterior_predictions(mixcat.1, draws=1000)
library(bayesplot)
library(bayestestR)
plot(rope(mixcat.1, ci = 0.89))


#PERFORMANCE
performance(mixcat.1)

#TABLE
tab_model(mixcat.1) #, transform = "exp", title = "Model Predicted Odds Ratio")
```

##### Inference

-   Being in the IMPASSE condition *increases* the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 43.86.

-   Being in the IMPASSE condition *increases* the odds of giving 'triangle-like' response rather than an orthogonal (or satisficing) response by a factor of 6.64.

-   TODO RECONCILE:: brms model provides *substantially* higher estimate for blank/uncertain response (vs) the mblogit frequentist model. I think these should be similar, as they were for the non-mixed veresions. Suspect NaNs error thrown with mblogit() may be relevant

##### Visualize

```{r}

#:::::::: PLOT

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(mixcat.1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.05, #can manually adjust to account for directional test
           ci.lvl = 0.95 ) +  #can manually adjusted for directional test   
  labs(title = "ODDS RATIO | Test Phase State ~ condition",
       subtitle = "(p for one two test)")


result <- estimate_density(mixcat.1)
plot(result, stack = FALSE, priors = TRUE)

result <- describe_posterior(mixcat.1)
plot(result, stack = FALSE, priors = TRUE)

result <- p_direction(mixcat.1)
plot(result, stack = FALSE)

prior_summary(mixcat.1)

hypothesis(mixcat.1, "muunknown_condition121 > 0 ")


```

##### Diagnostics

##### TODO

-   priors? used default flat priors... ok?
-   posterior predictive checks
-   diagnostics on random effects
-   reconcilliation of mblogit() vs brms versions of the model; seems like they should yield similar estimates






## TODO SEQUENCE
## TODO BEST FIT

## ARCHIVE
### Q1 ACCURACY 

**What explains Q1 Accuracy?**

##### Multiple Logistic Regression [Q1 Absolute]

```{r}
# #CREATE DATAFRAME OF Q1
# df <- df_items %>% filter(q == 1) %>% mutate( accuracy = as.factor(score_niceABS))
# 
# #MODEL
# m2 <- glm( accuracy ~ rt_s, data = df, family = "binomial")
# summary(m2)
# confint(m2)
# performance(m2)
# report(m2)
# 
# library(effects)
# plot(allEffects(m2))
# 
# 
# m3 <- glm( accuracy ~ condition + rt_s, data = df, family = "binomial")
# summary(m3)
# confint(m3)
# performance(m3)
# report(m3)
# 
# plot(allEffects(m3))
# 
# compare_performance(m,m2,m3)
```

```{r}
#evaluate model using kfold CV
# https://www.statology.org/k-fold-cross-validation-in-r/

# #specify the cross-validation method
# ctrl <- trainControl(method = "cv", number = 5)
# #fit a regression model and use k-fold CV to evaluate performance
# mk <- train( accuracy ~ condition, data = df, method = "glm", trControl = ctrl, family = "binomial")
# print(mk)
# 


```



#### Mass Movement

"movement of mass" from one mode to another

Considering only families of unimodal distributions, the most probably distribution (as predicted by package `performance`) is negative-binomial.

```{r}
#| label: MASS-111
# df <- df_subjects %>% filter(condition==111)
# multimode::modetest(df$s_NABS)
# n_modes = multimode::nmodes(data = df$s_NABS, bw=2)
# multimode::locmodes(df$s_NABS,mod0 =  n_modes, display = TRUE)

```

```{r}
#| label: MASS-121
# df <- df_subjects %>% filter(condition==121)
# multimode::modetest(df$s_NABS)
# n_modes = multimode::nmodes(data = df$s_NABS, bw=2)
# multimode::locmodes(df$s_NABS,mod0 =  n_modes, display = TRUE)

```


### RESPONSE LATENCY

-   [TODO: Investigate super high and super low response times.]{style="color: red;"}.
-   [TODO: Investigate appropriate models for response time data. (see: https://lindeloev.github.io/shiny-rt/)]{style="color: red;"}.
-   Especially see https://lindeloev.github.io/shiny-rt/ for ideas on modelling reaction time data

+-----------------------+----------------------------------------+
| Research Question     |                                        |
+=======================+========================================+
| **Hypothesis**        |                                        |
+-----------------------+----------------------------------------+
| **Analysis Strategy** |                                        |
+-----------------------+----------------------------------------+
| **Alternatives**      |                                        |
+-----------------------+----------------------------------------+
| **Inference**         |                                        |
+-----------------------+----------------------------------------+

### Q1 Response Latency

#### Linear Regression (Log Transform)

##### (In Person)

###### Visualization

```{r}
#| label: VIS-TEST-Q1TIME
# 
# df_lab <- df_subjects %>% filter(mode == "lab-synch")
#  
# #HISTOGRAM
# stats = df_lab %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_q1_rt))
# gf_dhistogram(~log(item_q1_rt), fill = ~pretty_condition, data = df_lab) %>% gf_facet_grid(~pretty_condition) +
#   # gf_vline(data = stats, xintercept = ~mean, color = "red") +
#   labs(title = "(LAB) First Question Response Time",,
#        # x = "Response Time (seconds)",
#        # y = "proportion of participants",
#        subtitle = "") + 
#   theme_minimal()

```

###### Model

```{r}
#| label: MODEL-Q1TIME-LAB
# 
# #SCORE predicted by CONDITION
# lab.q1t.lm1 <- lm(log(item_q1_rt) ~ pretty_condition, data = df_lab)
# paste("Model")
# summary(lab.q1t.lm1)
# paste("Partition Variance")
# anova(lab.q1t.lm1)
# paste("Confidence Interval on Parameter Estimates")
# confint(lab.q1t.lm1)
# report(lab.q1t.lm1) #sanity check
# #print model equation
# eq <- extract_eq(lab.q1t.lm1, use_coefs = TRUE)
```

```{r}
# #| label: VISMODEL-Q1-LATENCY-LAB
# 
# #MODEL ESTIMATES WITH UNCERTAINTY
# 
# #setup references 
# #lab.q1t.lm1 <- lm(log(item_q1_rt) ~ condition, data = df_lab)
# m <- lab.q1t.lm1
# df <- df_lab 
# call <- m$call %>% as.character()
# 
# # uncertainty model visualization
# df <- df  %>%
#   data_grid(pretty_condition) %>%
#   augment(m, newdata = ., se_fit = TRUE) 
# 
# #transform log
# df$.fitted <- exp(df$.fitted)
# df$.se.fit <- exp(df$.se.fit)
# 
# df %>% 
#   ggplot(aes(y = pretty_condition, color = pretty_condition)) +
#   stat_halfeye( scale = .5,
#       aes(
#         xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),
#         fill = stat(cut_cdf_qi(cdf, 
#                 .width = c(.90, .95),
#                 labels = scales::percent_format())))) +
#   scale_fill_brewer(direction = -1) + 
#   labs (title = "(LAB) Q1 Response Latency ~ Condition", 
#         x = "model predicted mean (seconds)", y = "Condition", fill = "Interval",
#         subtitle = paste("lm(",call[2],")"),
#         caption = "note: model log(predictions) have exponentiated to original scale") + theme(legend.position = "blank")

```

###### Diagnostics

```{r}
#| label: DIAG-Q1TIME-lab
# 
# #model diagnostics
# check_model(lab.q1t.lm1, panel = TRUE)
```



###### Inference

OLS Linear Regression on Q1 response time shows that condition explains a small but statistically significant amount of variance (impasse \> control). However, the model violates the assumption of normally distributed residuals.

##### (Online Replication)

###### Visualization

```{r}
#| label: VIS-TEST-Q1TIME-online
# 
# df_online <- df_subjects %>% filter(mode == "asynch")
#  
# #HISTOGRAM
# stats = df_online %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_q1_rt))
# gf_dhistogram(~log(item_q1_rt), fill = ~pretty_condition, data = df_lab) %>% gf_facet_grid(~pretty_condition) +
#   # gf_vline(data = stats, xintercept = ~mean, color = "red") +
#   labs(title = "(ONLINE) First Question Response Time",
#        # x = "Response Time (seconds)",
#        # y = "proportion of participants",
#        subtitle = "") + 
#   theme_minimal()

```

###### Model

```{r}
#| label: MODEL-Q1TIME-online

# #SCORE predicted by CONDITION
# rep.q1t.lm1 <- lm(log(item_q1_rt) ~ pretty_condition, data = df_online)
# paste("Model")
# summary(rep.q1t.lm1)
# paste("Partition Variance")
# anova(rep.q1t.lm1)
# paste("Confidence Interval on Parameter Estimates")
# confint(rep.q1t.lm1)
# report(rep.q1t.lm1) #sanity check
# #print model equation
# eq <- extract_eq(rep.q1t.lm1, use_coefs = TRUE)
```

```{r}
#| label: VISMODEL-Q1-LATENCY-ONLINE

#MODEL ESTIMATES WITH UNCERTAINTY
# 
# #setup references 
# # rep.q1t.lm1 <- lm(log(item_q1_rt) ~ condition, data = df_online)
# m <- rep.q1t.lm1
# df <- df_online 
# call <- m$call %>% as.character()
# 
# # uncertainty model visualization
# df <- df  %>%
#   data_grid(pretty_condition) %>%
#   augment(m, newdata = ., se_fit = TRUE) 
# 
# #transform log
# df$.fitted <- exp(df$.fitted)
# df$.se.fit <- exp(df$.se.fit)
# 
# df %>% 
#   ggplot(aes(y = pretty_condition, color = pretty_condition)) +
#   stat_halfeye( scale = .5,
#       aes(
#         xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),
#         fill = stat(cut_cdf_qi(cdf, 
#                 .width = c(.90, .95),
#                 labels = scales::percent_format())))) +
#   scale_fill_brewer(direction = -1) + 
#   labs (title = "(ONLINE) Q1 Response Latency ~ Condition", 
#         x = "model predicted mean (seconds)", y = "Condition", fill = "Interval",
#         subtitle = paste("lm(",call[2],")"),
#         caption = "note: model log(predictions) have exponentiated to original scale") + theme(legend.position = "blank")

```

###### Diagnostics

```{r}
#| label: DIAG-Q1TIME-online

#model diagnostics
# check_model(rep.q1t.lm1, panel = TRUE)
```



###### Inference

OLS Linear Regression on Q1 response time shows that condition explains a small but statistically significant amount of variance (impasse \> control). However, the model violates the assumption of normally distributed residuals.

### TODO RESPONSE CONSISTENCY


### H1B \| TEST PHASE ACCURACY

DECISION: There is no need to differentiate between test phase and scaffold phase; can just consider all items, BECAUSE even during the scaffold phase, incorrect responses are available in the control condition. The satisfice stragey yields a 0 absolute score, just like an orthogonal. Including all items (i=13) rather than just test phase(i=8) gives a little more statistical power in the mixed effects models. 

+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Do Ss in the IMPASSE condition score higher across the test phase than those in the CONTROL group?                                                                                                                                                                                 |
+=======================+====================================================================================================================================================================================================================================================================================+
| **Hypothesis**        | (H1B) Participants in the IMPASSE condition will have higher test phase performance than those in the CONTROL condition.                                                                                                                                                           |
+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | **data**: `df_items` where `q nin 1,2,3,4,5,6,9` (the 8 discriminating test phase Qs ), `df_subjects`                                                                                                                                                                              |
|                       |                                                                                                                                                                                                                                                                                    |
|                       | **outcome**:                                                                                                                                                                                                                                                                       |
|                       |                                                                                                                                                                                                                                                                                    |
|                       | -   \[at item level\] : *accuracy* ( factor(incorrect/correct) from `score_niceABS` \[absolute score\]                                                                                                                                                                             |
|                       | -   \[subject level\]: accuracy (number of test phase qs correct from total `s_NABS`)                                                                                                                                                                                              |
|                       |                                                                                                                                                                                                                                                                                    |
|                       | **predictor**: `condition` \[between-subjects factor\]                                                                                                                                                                                                                             |
+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  Wilcoxon-Rank Sum (Mann-Whitney) test on subject-level total accuracy of test phase (`item_test_NABS`)                                                                                                                                                                         |
|                       | 2.  Mixed Logistic Regression\                                                                                                                                                                                                                                                     |
|                       |     `accuracy` \~ `condition` + (1 \| `subject` )\                                                                                                                                                                                                                                 |
|                       |     model effect of condition on probability of correct response \[during test phase\] while accounting for subject (and item-level?) effects                                                                                                                                      |
+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Alternatives**      | -   Ordinal Mixed Logistic Regression\                                                                                                                                                                                                                                             |
|                       |     `interpretation` \~ `condition` + (1 \| `subject` )\                                                                                                                                                                                                                           |
|                       |     model effect of condition on \[ordered correctness of interpretation\] \[during test phase\] while accounting for subject (and item-level?) effects                                                                                                                            |
|                       | -   Shift in Modal Mass (descriptive)\                                                                                                                                                                                                                                             |
|                       |     describe & visualize shift in deciles between conditions for `scaled_score` (at subject level)                                                                                                                                                                                 |
|                       | -   OLS Linear Regression: bimodal distribution at tails makes the mean a poor predictor; LMs violate assumptions of normally distributed residuals; both absolute and scaled scores yield non-normal residuals; no transformation of the outcome variables yield normal residuals |
+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Notes**             | **Also exploring:**                                                                                                                                                                                                                                                                |
|                       |                                                                                                                                                                                                                                                                                    |
|                       | -   Hurdle model (mixture model w/ binomial + \[poisson or negbinom count; 0s from 1 DGP)                                                                                                                                                                                          |
|                       | -   Zero Inflated model (mixture model w/ binomial + poisson or negbinom count; 0s from 2 DGPs)                                                                                                                                                                                    |
|                       | -   Beta regression hurdle model? (mixture with location and scale parameters \[mean, variance\] and hurdles for floor and ceiling effects)                                                                                                                                        |
|                       | -   Other way to account for the severe bimodality?                                                                                                                                                                                                                                |
+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

```{r}
#| label: SETUP-TEST-ACC


#item level
df = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct"),
  q = as.factor(q)
)

#:::::::: STACKED PROPORTIONAL BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~pretty_mode) + 
   labs(title = "Test Phase Accuracy",
       x = "Condition",
       fill = "",
       subtitle="Impasse Condition yields a greater proportion of correct responses")


#:::::::: FACETED HISTOGRAM
stats = df_subjects %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))
gf_props(~item_test_NABS, 
         fill = ~pretty_condition, data = df_subjects) %>% 
  gf_facet_grid(pretty_condition ~ pretty_mode) %>% 
  gf_vline(data = stats, xintercept = ~mean, color = "red") +
  labs(x = "# Correct in Test Phase",
       y = "proportion of subjects",
       title = "Test Phase Absolute Score (# Correct)",
       subtitle = "") + theme(legend.position = "blank")

#:::::::: FACETED HISTOGRAM
stats = df_subjects %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS/8))
gf_props(~item_test_NABS/8, 
         fill = ~pretty_condition, data = df_subjects) %>% 
  gf_facet_grid(pretty_condition ~ pretty_mode) %>% 
  gf_vline(data = stats, xintercept = ~mean, color = "red") +
  labs(x = "% Correct in Test Phase",
       y = "proportion of subjects",
       title = "Test Phase Absolute Score (% Correct)",
       subtitle = "") + theme(legend.position = "blank")

```


```{r}

#:::::::: IN PERSON ONLY
df_i <- df_items %>% filter(mode == "lab-synch") %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

df_s <- df_subjects %>% filter(mode == "lab-synch") %>% mutate(
  test_score = item_test_NABS,
  test_percent = item_test_NABS/8
)


```

#### Describe

```{r}

title = "Descriptive Statistics of Response Accuracy (Test Phase % Correct)"
tbl1 <- mosaic::favstats(~test_percent, data = df_s) 
tbl1 %>% kbl (caption = title) %>% kable_classic()


title = "Descriptive Statistics of Response Accuracy (Test Phase % Correct) BY CONDITION"
tbl2 <- mosaic::favstats(test_percent ~ condition, data = df_s) 
tbl2 %>% kbl (caption = title) %>% kable_classic()

```

Across both conditions, overall accuracy on the test phase ranges from `r tbl1$min *100` to `r tbl1$max *100` with a mean of `r tbl1$mean * 100`. We see that the distribution of this outcome variable is clearly bimodal: with modes near the floor (0% correct) and ceiling (100% correct) of the scale. This bimodality is sensical considering the nature of the task, where each item in the task indexes a different information extraction operation over the same coordinate system.

A score of 100% indicates that the participant correctly interpreted the interval-coordinate system throughout the task, *starting at the first question*. A score of 0% indicates the individual *never* correctly interpreted the coordinate system. A score somewhere inbetween indicates that an individual deciphered the coordinate system *sometime over the course the task*.

#### WILCOXON RANK SUM (Mann-Whitney Test) --- SCORE

-   **Non parametric alternative** to t-test; compares median rather than mean by ranking data
-   Does not assume normality
-   Does not assume equal variance of samples (homogeneity of variance)

##### Test

```{r}


(w <- wilcox.test(df_s$test_score ~ df_s$condition,
                 paired = FALSE, alternative = "less")) #less, greater
report(w)

```

##### Visualize

```{r}


#[manual one-sided test]
(results <- statsExpressions::two_sample_test(y = test_score, x = condition, data = df_s,
                                type = "nonparametric", alternative = "less",
                                var.equal = FALSE))


#:::::::: STATSPLOT | VIOLIN
ggbetweenstats(y = test_score, x = condition, data = df_s,
               results.subtitle = FALSE, #override default [two tailed] test dsiplay
               subtitle = results$expression[[1]]
              )

```

##### Inference

A Mann-Whitney (Wilcoxon Rank Sum) test evaluating the difference in median accuracy score in the test phase of the graph comprehension task indicates that performance was better in the impasse (vs) control condition. \[report stats\]

#### WILCOXON RANK SUM (Mann-Whitney Test) --- PERCENTAGE

-   same as above, but on percentage rather than \# correct

##### Test

```{r}


(w <- wilcox.test(df_s$test_percent ~ df_s$condition,
                 paired = FALSE, alternative = "less")) #less, greater
report(w)

# df_s %>% wilcox_effsize(test_percent ~ condition)


```

##### Visualize

```{r}


#[manual one-sided test]
(results <- statsExpressions::two_sample_test(y = test_percent, x = condition, data = df_s,
                                type = "nonparametric", alternative = "less",
                                var.equal = FALSE))


#:::::::: STATSPLOT | VIOLIN
ggbetweenstats(y = test_percent, x = condition, data = df_s,
               results.subtitle = FALSE, #override default [two tailed] test dsiplay
               subtitle = results$expression[[1]]
              )

```

##### Inference

**Reported in dissertation**

Because the distribution of the outcome variable is not normally distributed, we evaluate the effect of CONDITION on ACCURACY via a non-parametric test. Consistent with our hypothesis, a Wilcoxon rank sum test (with continuity correction) on ACCURACY by CONDITION indicates that data in each condition likely come from different population distributions (W = 1438.00, p = 0.002; one-tailed), and that the distribution of the control condition is less (i.e. shifted to the left/ lower scores) than the impasse condition (\^{r} = -0.28, 95% CI \[-1.00, -0.11\]), a medium-sized effect.

#### MIXED LOGISTIC REGRESSION

*Fit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.*

##### Fit Model

```{r}
#| label: MODEL-MLOG-ABS-LAB

## 1 | SETUP RANDOM INTERCEPT SUBJECT

#:: EMPTY MODEL (baseline, no random effect)
print("Empty fixed model")
m0 = glm(accuracy ~ 1, family = "binomial", data = df_i) 
# summary(m0)

#:: RANDOM INTERCEPT SUBJECT
print("Subject intercept random model")
mm.rS <- glmer(accuracy ~ (1|subject), data = df_i, family = "binomial")
# summary(mm.rS)

# :: TEST random effect
paste("AIC decreases w/ new model?", m0$aic > AIC(logLik(mm.rS)))
test_lrt(m0,mm.rS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,mm.rS))$p[2])
```

*A likelihood ratio test indicates that a logistic regression model including SUBJECT as a random effect explains more variance in the data than an empty \[intercept only\] model.*

```{r}

## 2 | ADD FIXED EFFECT CONDITION

print("FIXED Condition + Subject random intercepts")
mm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject) ,
                data = df_i, family = "binomial")
summary(mm.CrS)


paste("AIC decreases w/ new model", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )
test_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rS,mm.CrS))$p[2])

```

```{r}
# print("FIXED Condition + Subject & Item random intercepts")
# mm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q) ,
#                 data = df_i, family = "binomial")
# summary(mm.CrSQ)
```

##### Describe

```{r}

# best model
m1 <- mm.CrS

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m1)
car::Anova(m1)

#:::::::: MANUAL ONE-SIDED SIGTEST 

# one-sided (right tail) z test for B COEFFICIENT
#SANITY CHECK 2-tailed test should match the model output
tt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("p value for two-tailed test, null B = 0 : ",round(tt,5))
ot <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("BUT we want a one tailed directional, null: B <= 0: ",round(ot,5))

#:::::::: INTERPRET COEFFICIENTS

se <- sqrt(diag(stats::vcov(m1)))
# table of estimates with 95% CI
paste("LOG ODDS")
(tab <- cbind(Est = fixef(m1), LL = fixef(m1) - 1.96 * se, UL = fixef(m1) + 1.96 *
    se))
paste("ODDS RATIOS")
(e <- exp(tab))

```


##### Visualize

```{r}
#:::::::: PLOT

#GGSTATS | MODEL | LOG ODDS 
ggcoefstats(m1, output = "plot", 
              conf.level = 0.90) + 
  labs(x = "Log Odds Estimate", 
       subtitle = "p is for two tailed test")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(m1, type="eff",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Predicted Probability of Accuracy",
           axis.title = c("Condition","Probability of Accurate Response"))

```

```{r}
#SJPLOT | MODEL | TABLE
# tab_model(m1)


# #MODEL SUMMARY | save latex table
# models <- list("odds ratios" = m1, "(log odds)" = m1)
# notes = list('N(subject) = 126 $\tau_{00}$(subject) = 22.22',
#              'N(question) = 13 $\tau_{00}$(question) = 0.31',
#                "*** indicates p < 0.001")
# # 
# modelsummary(models,
#              exponentiate = c(TRUE, FALSE),
#              shape = term ~ model + statistic,
#              fmt = 2, #two digits w/ trailing zero
#              estimate  = "{estimate} {stars}",
#              statistic = "conf.int",
#              coef_rename = c("pretty_conditionimpasse" = "Condition[impasse]"))

#              # coef_omit = "Intercept",
#              gof_omit = 'AIC|RMSE|ICC|BIC',
#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', notes = notes,
#              output = "analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex")

```

##### Diagnostics

```{r}
#| warning: false
#| message: false

print("SANITY CHECK REPORTING")
# report(m1)

print("MODEL PERFORMANCE")
performance(m1)

print("DIAGNOSTICS")
check_model(m1)
binned_residuals(m1)
```

##### TODO

-   sanity check interpretation
-   sanity check random effects structure :
    -   do I need to have ITEM as random intercept? What does it mean to have two random intercepts?
-   DIAGNOSTICS: What in the world is happening with the normality of random effects plot? Do the fixed effects residuals need to be normally distributed?
-   Are there other plots or recommended diagnostics for mixed log regression
-   consider multiple regression with rt, sequence cluster, confidence, etc.
-   What else needs to be interpreted with respect to the item and subject random effects?
-   Double check: can't have condition by subject or item slope bc subjects are nested in conditions, not crossed

### SHIFT IN MODAL MASS

The Effect of Condition on Total Scaled Score can be described as a 'shift' in mass between the low and high modes of each distribution.

*First, we use the Kolmogorov-Smirnov test as a Robust alternative to the t-test to test if the two distributions likely come from different populations.*

```{r}
#| label: COMPARE-DIST-NABS-comb

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
# mbp = "/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
# setwd(mbp)

#(requires shift function files loaded)
#LOAD MODAL SHIFT FUNCTION RESOURCES
source("analysis/utils/shift_function/Rallfun-v30.txt")
source("analysis/utils/shift_function/wilcox_modified.txt")
source("analysis/utils/shift_function/rgar_visualisation.txt")
source("analysis/utils/shift_function/rgar_utils.txt")
#NOTE: something in these breaks the stat_ecdf in ggplot2

#PREP DATA 
df <- df_subjects %>%
  dplyr::select(s_SCALED, pretty_condition) %>%
  mutate(
    data = as.numeric(s_SCALED),
    #flip order levels to correctly orient graph
    # gr = recode_factor(pretty_condition, "impasse" = "impasse", "control"="control")
    gr = as.character(pretty_condition)
  ) %>% dplyr::select(data,gr)


g1 <- df %>% filter(gr == "control") %>% dplyr::pull(data)
g2 <- df %>% filter(gr == "impasse") %>% dplyr::pull(data)


#COMPARE DISTRIBUTIONS WITH ROBUST TESTS

#What do common tests say about the difference?

# Kolmogorov-Smirnov test
#If y is numeric, a two-sample (Smirnov) test of the null hypothesis that x and y 
#were drawn from the same continuous distribution is performed. Alternatively, y ...

#null is X is drawn from CDF EQUAL TO Y
ks.test(g1,g2) 
print("SUGGESTS that impasse and control come from different population distributions")

# #null is X is NOT LESS THAN Y
ks.test(g1,g2, alternative = "greater") 
print("SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]")

#REGULAR T-TEST
t.test(g1,g2) # regular Welsh t-test

```

```{r}
#| label: SHIFT-FN-NABS-comb
#| warnings: false
#| messages: false

#IF THIS ERRORS, consider loadling plyr (older than dplyr)
# kernel density estimate + rug plot + superimposed deciles
kde <- plot.kde_rug_dec2(df)
# kde

# compute shift function
out <- shifthd( g1, g2, nboot=200)

# plot shift function
sf <- plot.sf(data=out) # function from rgar_visualisation.txt
# sf

# combine KDE + SF
cowplot::plot_grid(kde, sf, labels=c("A", "B"), ncol = 1, nrow = 2, rel_heights = c(1.5, 1),label_size = 18,hjust = -1,scale=.95)

```

### H1B \| TEST PHASE INTERPRETATION STATE

**Do Ss in the IMPASSE condition offer less-orthogonal interpretations across the test phase questions?**

While absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn't allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject's response, and determine if these interpretations differ by experimental condition. State is a 3-category variable that groups the following interpretations:

-   **"orthogonal-like"** \[reference category\] includes orthogonal and satisficing responses ==\> indicates a primarily *orthogonal* state of coordinate system understanding
-   **"unknown"** includes: blank, reference point, responses that can't be classified (including selecting all datapoints) =\> indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly *not* orthogonal nor triangular
-   **"triangle-like"** includes correct triangular and 'lines connecting' responses as well as responses that include both orthogonal *and* triangular answers =\> indicates some degree of angular/triangular coordinate understanding

+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Does Ss in the impasse condition produce less orthogonal responses across questions in the test phase?                                          |
+=======================+=================================================================================================================================================+
| **Hypothesis**        | H1A \| Ss in the IMPASSE condition will have a higher likelihood of producing unknown and triangle-like response states across test phase items |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | -   data: `df_items` where `q nin 1,2,3,4,5,6,9` (8 discriminant test phase items)                                                              |
|                       | -   outcome: `state` ( 3 level factor from high_interpretation )                                                                                |
|                       | -   predictor: `condition` \[between-subjects factor\]                                                                                          |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  MIXED Multinomial (Logistic) Regression on state predicted by condition                                                                     |
|                       |                                                                                                                                                 |
|                       | Alternative:                                                                                                                                    |
|                       |                                                                                                                                                 |
|                       | -   MIXED Ordinal regression on state (doesn't meet proportional odds assumption-I think)                                                       |
|                       | -   MIXED Multinomial or Ordinal regression on high_interpretation (some cells are 0, produces problems)                                        |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+

```{r}
#| label: SETUP-TEST-INTERPRETATION

#:::::::: PREP DATA
df = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(
  q = as.factor(q),
  subject = as.factor(subject)
)

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~pretty_mode) + 
   labs(title = "Test Phase Interpretation",
       x = "Condition",
       fill = "",
       subtitle="")

```

A proportional bar chart visualizing the proportion of each interpretations type by condition for each data collection modality (left/right facet) reveals that the pattern of responses are similar regardless of the data collection modality, by differ by condition. In the impasse condition, there are more triangular responses than in control. In the impasse condition, there are also more positive transition (i.e. triangle-like) and neutral (ie. blank or uncertain response types) than in the control condition.


```{r}

#:::::::: IN PERSON ONLY
df <- df_items %>% filter(mode == "lab-synch") %>% filter(q %nin% c(1,2,3,4,5,6,9)) 

#::::::::::::DESCRIPTIVES

table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row
      prop.table(margin=2) %>%  #return proportion (of column)
      addmargins(1) #sanity check sum of columns


(t <- table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row
      addmargins(1)) #sanity check sum of columns

```

#### MIXED MULTINOMIAL REGRESSION

**TODO** - add a 1 to the empty cell and re-run mblogit to show that estimates are more similar to the brms results - run individual mixed models to show results are similar to mblogit vs brms

*Does condition affect the response state of of items in the test phase?*

*Fit a MIXED logistic regression predicting interpretation state (k=3) by condition(k = 2).*

-   Can use mclogit mblogit() with random effect *or* bayesian brms package b/c nlme, lme4 don't support random effects on multinomial (ie no categorical family on glmer())

-   Alternative would be to manually run 2 X binomial mixed models \[should compare outcomes\]

-   2 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) \[essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing \[reference category\] vs \[this category\])

-   For *each* equation:

    -   $\beta_{0}$ *= Log Odds of \[this category type vs. reference category type) response in CONTROL condition*
    -   $e^{\beta_{0}}$ *= ODDS of \[this category type vs. reference category type\] response in CONTROL condition*
    -   $\beta_{1}$ *=* $\beta_{1impasse}$ *Log Odds (Log OR; change in odds for \[this category\] type response in impasse (vs) control \[log scale\])*
    -   $e^{\beta_{1}}$ *= ODDS RATIO of \[this. vs reference category type\] response in IMPASSE (vs) CONTROL*
    -   Two-tailed NHST *Null hypothesis:* $\beta_{impasse} = 0$ *the odds for \[this category of response vs. reference\] are not different for IMPASSE condition*
    -   *Alternative hypothesis:* $\beta_{impasse} \ne 0$ *the odds of \[this category of response vs. reference\] increases or decreases for IMPASSE condition*

##### Fit Model \[MANUAL INDIVIDUAL BINOMIALS\]

```{r}

#VERIFY RESULTS BELOW VIA MULTIPLE INDIVIDUAL MODELS

paste("ORTH vs. UNKNOWN")

d1 <- df %>% filter(state %nin% c("tri-like")) %>% droplevels()
table(d1$condition, d1$state)
plot(d1$condition, d1$state)

m.unknown <- glmer(state ~ pretty_condition + ( 1 | subject), family = "binomial", data = d1)
summary(m.unknown)
```

*Being in the IMPASSE condition increases* the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 7.7 ( logodds = 2.041, z = 0.559, p \< 0.001) . Participants in the impasse condition were 7.7x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control. (Conditional R2 = 0.59, marginal R2 = 0.13; SD(subject) = 1.93)

```{r}

paste("ORTH vs. TRIANGULAR")

d2 <- df %>% filter(state %nin% c("unknown")) %>% droplevels()
table(d2$condition, d2$state)
plot(d2$condition, d2$state)

m.tri <- glmer(state ~ pretty_condition + ( 1 | subject), family = "binomial", data = d2)
summary(m.tri)

```

*Being in the IMPASSE condition increases* the odds of giving an triangle-like response by a factor of 1604 ( logodds = 7.38, z = 1.86, p \< 0.001) . Participants in the impasse condition were 1604x as likely to give an triangle-like response rather than an orthogonal response compared to participants in control. (Conditional R2 = 0.96, marginal R2 = 0.0.16; SD(subject) = 8.29)

##### Fit Model \[mblogit\]

```{r}

#https://www.elff.eu/software/mclogit/manual/mblogit/
#"baseline category logit" model matches multinom()

#check reference level 
print("Categories (first is reference)")
levels(df$state)

#FIT EMPTY MODEL
# print("EMPTY MODEL")
m.mbl0 <- mblogit(state ~ pretty_condition ,  #no random intercepts; fixed only model 
                  data = df)
#summary(m.mbl0)

#FIT PREDICTOR MODEL
# print("PREDICTOR MODEL")
m.mbl1 <- mblogit(state ~ pretty_condition , 
                  random = ~ 1 | subject , 
                  data = df)
# summary(m.mbl1)

#COMPARE MODEL FIT
paste("AIC wth predictor is lower than empty model?", AIC(m.mbl0) > AIC(m.mbl1))
test_lrt(m.mbl0, m.mbl1)

#DESCRIBE MODEL
summary(m.mbl1)
# car::Anova(m.mbl1)

#INTERPRET COEFFICIENTS
cint <- confint(m.mbl1, level = 0.95)
print("ODDS RATIO")
(e <- cbind( exp(coef(m.mbl1)), exp(cint))) #exponentiated, adjusted

#PERFORMANCE
performance(m.mbl1)

#TABLE
tab_model(m.mbl1, transform = "exp", title = "Model Predicted Odds Ratio")
```

##### Inference

-   Being in the IMPASSE condition *increases* the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 4.7 (z = 3.92, p \< 0.001) . **Participants in the impasse condition were 4.7x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control**.

-   Being in the IMPASSE condition *increases* the odds of giving 'triangle-like' response rather than an orthogonal (or satisficing) response by a factor of 8.61 (z = 3.30, p \< 0.001 ). **Participants in the impasse condition were more than 8.6x as likely to give an a triangular response rather than an orthogonal response compared to participants in control**.

-   As with the (binary) logistic regression on accuracy \~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 *in the control condition.* (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)

-   TODO: these estimates seem low to me, given the estimates for the Q1 only model. Also different from brms estimates (below) Suspect NaNs error thrown with mblogit() may be relevant

##### Visualize

```{r}
#| label: VIS-TEST-INTERPRETATION-LAB

#:::::::: PLOT

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m.mbl1, vline.color = "red", 
           transform = "exp", #for some reason have to manually add for mixed?
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.05, #can manually adjust to account for directional test
           ci.lvl = 0.95) +  #can manually adjusted for directional test   
  labs(title = "ODDS RATIO | Test Phase State ~ condition",
       subtitle = "(p for  two-tailed test)")

#SJPLOT | MODEL | TABLE
tab_model(m.mbl1)

```

##### Diagnostics

```{r}

print("MODEL PERFORMANCE")
performance(m.mbl1)

#General Goodness of Fit
#library(generalhoslem)
#logitgof(df$state, catm$fitted.values, g = 3)
#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).
#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables

# print("MODEL DIAGNOSTICS")
# check_model(m.mbl1) can't do overall diagnostics, have to do them on individual model equations

```

##### Fit Model \[brms\]

```{r}


#library(brms) #bayesian mixed regression models

#BAYESIAN MIXED VERSION
mixcat.1 <- brm( state ~ condition + (1|subject), 
                 data = df, family = "categorical",
                 file = "analysis/SGC3A/models/sgc3a_brms_mixedcat_teststate_LAB.rds") # cache model (can be removed)))

#DESCRIBE MODEL
summary(mixcat.1)
# report(mixcat.1)

#VISUALIZE

plot(mixcat.1)
plot_model(mixcat.1)
check_posterior_predictions(mixcat.1, draws=1000)
library(bayesplot)
library(bayestestR)
plot(rope(mixcat.1, ci = 0.89))

#PERFORMANCE
performance(mixcat.1)

#TABLE
tab_model(mixcat.1,
          show.r2 = FALSE) #, transform = "exp", title = "Model Predicted Odds Ratio")

# modelsummary(mixcat.1, s)
#TODO OUTPUT TABLE 
#https://arelbundock.com/posts/modelsummary_multinomial_logit/
```

##### Inference

-   Being in the IMPASSE condition *increases* the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 43.86.

-   Being in the IMPASSE condition *increases* the odds of giving 'triangle-like' response rather than an orthogonal (or satisficing) response by a factor of 6.64.

-   TODO RECONCILE:: brms model provides *substantially* higher estimate for blank/uncertain response (vs) the mblogit frequentist model. I think these should be similar, as they were for the non-mixed veresions. Suspect NaNs error thrown with mblogit() may be relevant

##### Visualize

```{r}

# #:::::::: PLOT
# 
# #SJPLOT | MODEL | ODDS RATIO
# #library(sjPlot)
# plot_model(mixcat.1, vline.color = "red", 
#            show.intercept = TRUE, 
#            show.values = TRUE,
#            p.threshold = 0.05, #can manually adjust to account for directional test
#            ci.lvl = 0.95 ) +  #can manually adjusted for directional test   
#   labs(title = "ODDS RATIO | Test Phase State ~ condition",
#        subtitle = "(p for one two test)")
# 
# 
# result <- estimate_density(mixcat.1)
# plot(result, stack = FALSE, priors = TRUE)
# 
# result <- describe_posterior(mixcat.1)
# plot(result, stack = FALSE, priors = TRUE)
# 
# result <- p_direction(mixcat.1)
# plot(result, stack = FALSE)
# 
# prior_summary(mixcat.1)
# 
# # hypothesis(mixcat.1, "muunknown_condition121 > 0 ")


```

##### Diagnostics

##### TODO

-   priors? used default flat priors... ok?
-   posterior predictive checks
-   diagnostics on random effects
-   reconcilliation of mblogit() vs brms versions of the model; seems like they should yield similar estimates
