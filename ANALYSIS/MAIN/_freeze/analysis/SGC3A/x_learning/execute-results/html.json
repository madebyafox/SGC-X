{
  "hash": "42fc970552e5c95a700e1fd94f2c20ba",
  "result": {
    "markdown": "---\nsubtitle: 'SGCX | Modelling Reference'\n---\n\n\n\\newpage\n\n# Modelling Reference {#modelling .unnumbered}\n\n**In this notebook we use data from study SGC3A to explore different modelling techniques and assess their suitability for the bimodal accuracy distributions.**\n\n+---------------------+\n| Pre-Requisite       |\n+=====================+\n| 2_sgc3A_scoring.qmd |\n+---------------------+\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Hmisc) # %nin% operator\n\nlibrary(ggpubr) #arrange plots\nlibrary(cowplot) #arrange shift function plots\nlibrary(ggformula) #easy graphs\nlibrary(vcd) #mosaic plots\nlibrary(vcdExtra) #mosaic plots\nlibrary(kableExtra) #printing tables \nlibrary(sjPlot) #visualize model coefficients\n\n#plot model estimates with uncertainty\nlibrary(ggdist)\nlibrary(broom)\nlibrary(modelr)\nlibrary(distributional)\n\n#models and performance\nlibrary(lmerTest) #for CIs in glmer \nlibrary(ggstatsplot) #plots w/ embedded stats\nlibrary(report) #easystats reporting\nlibrary(see) #easystats visualization\nlibrary(performance) #easystats model diagnostics\nlibrary(qqplotr) #confint on qq plot\nlibrary(gmodels) #contingency table and CHISQR\nlibrary(equatiomatic) #extract model equation\nlibrary(pscl) #zeroinfl / hurdle models \nlibrary(lme4) #mixed effects models \nlibrary(ggeffects) #visualization log regr models\n\nlibrary(tidyverse) #ALL THE THINGS\n\n#OUTPUT OPTIONS\nlibrary(dplyr, warn.conflicts = FALSE)\noptions(dplyr.summarise.inform = FALSE)\noptions(ggplot2.summarise.inform = FALSE)\noptions(scipen=1, digits=3)\n\n#GRAPH THEMEING\ntheme_set(theme_minimal()) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(mbp)\n\n#IMPORT DATA \ndf_items <- read_rds('analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.rds')\ndf_subjects <- read_rds('analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds')\n\n#PREP DATA \ndf_lab <- df_subjects %>% filter(pretty_mode == \"laboratory\")\ndf_online <- df_subjects %>% filter(pretty_mode == \"online-replication\")\n```\n:::\n\n\n## SINGLE ITEM LEVEL\n\n**Q1 Absolute, Interpretation Scores**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#FILTER THE DATASET [use subjects, bc it has covariates on that record]\ndf_q1 <- df_subjects %>% mutate(\n  accuracy = recode_factor(item_q1_NABS, \"0\" =\"incorrect\",\"1\"=\"correct\"),\n  rt = item_q1_rt\n) %>% dplyr::select(\n  accuracy, rt, pretty_condition, pretty_mode\n)\n\n#GROUPED PROPORTIONAL BAR CHART\n# gf_props(~accuracy, fill = ~pretty_condition, \n#        position = position_dodge(), data = df_q1) %>% \n#   gf_facet_grid(~pretty_mode) +\n#    labs(x = \"Correct Response on Q 1\",\n#        title = \"Accuracy on First Question by Condition\",\n#        subtitle=\"Impasse Condition yields a greater proportion of correct responses\") #theme(legend.position = \"none\")\n\n#STACKED BAR CHART\ndf_q1 %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Accuracy on First Question by Condition\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/SETUP-Q1ACC-1.png){width=672}\n:::\n:::\n\n\n### CHI SQUARE\n\n#### (Combined)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#combined dataset \ndf <- df_q1 \n\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Accuracy on First Question by Condition\",\n            data = df, pretty_condition ~ accuracy, shade = T, color = 2)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/label - CHISQR-Q1TRI.by.COND-BOTH-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# CrossTable( x = df$condition, y = df$accuracy, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n <tr>\n <th style=\"border-top:double; text-align:center; font-style:italic; font-weight:normal; border-bottom:1px solid;\" rowspan=\"2\">accuracy</th>\n <th style=\"border-top:double; text-align:center; font-style:italic; font-weight:normal;\" colspan=\"2\">pretty_condition</th>\n <th style=\"border-top:double; text-align:center; font-style:italic; font-weight:normal; font-weight:bolder; font-style:italic; border-bottom:1px solid; \" rowspan=\"2\">Total</th>\n </tr>\n \n<tr>\n <td style=\"border-bottom:1px solid; text-align:center; padding:0.2cm;\">control</td>\n <td style=\"border-bottom:1px solid; text-align:center; padding:0.2cm;\">impasse</td>\n </tr>\n \n<tr> \n<td style=\"padding:0.2cm;  text-align:left; vertical-align:middle;\">incorrect</td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">136</span><br><span style=\"color:#339999;\">124</span><br><span style=\"color:#333399;\">52.5&nbsp;&#37;</span><br><span style=\"color:#339933;\">86.1&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">123</span><br><span style=\"color:#339999;\">135</span><br><span style=\"color:#333399;\">47.5&nbsp;&#37;</span><br><span style=\"color:#339933;\">71.5&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;  \"><span style=\"color:black;\">259</span><br><span style=\"color:#339999;\">259</span><br><span style=\"color:#333399;\">100&nbsp;&#37;</span><br><span style=\"color:#339933;\">78.5&nbsp;&#37;</span></td> \n</tr>\n \n<tr> \n<td style=\"padding:0.2cm;  text-align:left; vertical-align:middle;\">correct</td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">22</span><br><span style=\"color:#339999;\">34</span><br><span style=\"color:#333399;\">31&nbsp;&#37;</span><br><span style=\"color:#339933;\">13.9&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">49</span><br><span style=\"color:#339999;\">37</span><br><span style=\"color:#333399;\">69&nbsp;&#37;</span><br><span style=\"color:#339933;\">28.5&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;  \"><span style=\"color:black;\">71</span><br><span style=\"color:#339999;\">71</span><br><span style=\"color:#333399;\">100&nbsp;&#37;</span><br><span style=\"color:#339933;\">21.5&nbsp;&#37;</span></td> \n</tr>\n \n<tr> \n<td style=\"padding:0.2cm;  border-bottom:double; font-weight:bolder; font-style:italic; text-align:left; vertical-align:middle;\">Total</td>\n<td style=\"padding:0.2cm; text-align:center;   border-bottom:double;\"><span style=\"color:black;\">158</span><br><span style=\"color:#339999;\">158</span><br><span style=\"color:#333399;\">47.9&nbsp;&#37;</span><br><span style=\"color:#339933;\">100&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;   border-bottom:double;\"><span style=\"color:black;\">172</span><br><span style=\"color:#339999;\">172</span><br><span style=\"color:#333399;\">52.1&nbsp;&#37;</span><br><span style=\"color:#339933;\">100&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;   border-bottom:double;\"><span style=\"color:black;\">330</span><br><span style=\"color:#339999;\">330</span><br><span style=\"color:#333399;\">100&nbsp;&#37;</span><br><span style=\"color:#339933;\">100&nbsp;&#37;</span></td> \n</tr>\n<td style=\"text-align:right; font-size:0.9em; font-style:italic; padding:0.2cm;\" colspan=\"4\">&chi;<sup>2</sup>=9.500 &middot; df=1 &middot; Cramer's V=0.177 &middot; Fisher's p=0.001</td> \n</tr>\n \n</table> <p>\n <span style=\"color:black;\">observed values</span><br>\n <span style=\"color:#339999;\">expected values</span><br>\n <span style=\"color:#333399;\">&#37; within accuracy</span><br>\n <span style=\"color:#339933;\">&#37; within pretty_condition</span><br>\n </p>\n\n`````\n:::\n:::\n\n\n**Combining data across both sessions** (n=330), a Pearson's Chi-squared test suggests a statistically significant relationship between response accuracy on the first question and experimental condition, $\\chi_2$ (1) = 10.3, p = 0.001. The sample odds ratio (2.46, p = 0.001, 95% CI \\[1.37, 4.53\\]) indicates that the odds of providing a correct response to the first question are 2.46 higher for subjects in the impasse condition than those in the control condition.\n\n#### (In Person)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#lab only\ndf <- df_q1 %>% filter(pretty_mode == \"laboratory\")\n\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Accuracy on First Question by Condition\",\n            data = df, pretty_condition ~ accuracy, \n            shade = T)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/label - CHISQR-Q1TRI.by.COND-LAB-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# CrossTable( x = df$condition, y = df$score_niceABS, \n#             fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n <tr>\n <th style=\"border-top:double; text-align:center; font-style:italic; font-weight:normal; border-bottom:1px solid;\" rowspan=\"2\">accuracy</th>\n <th style=\"border-top:double; text-align:center; font-style:italic; font-weight:normal;\" colspan=\"2\">pretty_condition</th>\n <th style=\"border-top:double; text-align:center; font-style:italic; font-weight:normal; font-weight:bolder; font-style:italic; border-bottom:1px solid; \" rowspan=\"2\">Total</th>\n </tr>\n \n<tr>\n <td style=\"border-bottom:1px solid; text-align:center; padding:0.2cm;\">control</td>\n <td style=\"border-bottom:1px solid; text-align:center; padding:0.2cm;\">impasse</td>\n </tr>\n \n<tr> \n<td style=\"padding:0.2cm;  text-align:left; vertical-align:middle;\">incorrect</td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">52</span><br><span style=\"color:#339999;\">48</span><br><span style=\"color:#333399;\">53.6&nbsp;&#37;</span><br><span style=\"color:#339933;\">83.9&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">45</span><br><span style=\"color:#339999;\">49</span><br><span style=\"color:#333399;\">46.4&nbsp;&#37;</span><br><span style=\"color:#339933;\">70.3&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;  \"><span style=\"color:black;\">97</span><br><span style=\"color:#339999;\">97</span><br><span style=\"color:#333399;\">100&nbsp;&#37;</span><br><span style=\"color:#339933;\">77&nbsp;&#37;</span></td> \n</tr>\n \n<tr> \n<td style=\"padding:0.2cm;  text-align:left; vertical-align:middle;\">correct</td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">10</span><br><span style=\"color:#339999;\">14</span><br><span style=\"color:#333399;\">34.5&nbsp;&#37;</span><br><span style=\"color:#339933;\">16.1&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">19</span><br><span style=\"color:#339999;\">15</span><br><span style=\"color:#333399;\">65.5&nbsp;&#37;</span><br><span style=\"color:#339933;\">29.7&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;  \"><span style=\"color:black;\">29</span><br><span style=\"color:#339999;\">29</span><br><span style=\"color:#333399;\">100&nbsp;&#37;</span><br><span style=\"color:#339933;\">23&nbsp;&#37;</span></td> \n</tr>\n \n<tr> \n<td style=\"padding:0.2cm;  border-bottom:double; font-weight:bolder; font-style:italic; text-align:left; vertical-align:middle;\">Total</td>\n<td style=\"padding:0.2cm; text-align:center;   border-bottom:double;\"><span style=\"color:black;\">62</span><br><span style=\"color:#339999;\">62</span><br><span style=\"color:#333399;\">49.2&nbsp;&#37;</span><br><span style=\"color:#339933;\">100&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;   border-bottom:double;\"><span style=\"color:black;\">64</span><br><span style=\"color:#339999;\">64</span><br><span style=\"color:#333399;\">50.8&nbsp;&#37;</span><br><span style=\"color:#339933;\">100&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;   border-bottom:double;\"><span style=\"color:black;\">126</span><br><span style=\"color:#339999;\">126</span><br><span style=\"color:#333399;\">100&nbsp;&#37;</span><br><span style=\"color:#339933;\">100&nbsp;&#37;</span></td> \n</tr>\n<td style=\"text-align:right; font-size:0.9em; font-style:italic; padding:0.2cm;\" colspan=\"4\">&chi;<sup>2</sup>=2.547 &middot; df=1 &middot; Cramer's V=0.161 &middot; Fisher's p=0.091</td> \n</tr>\n \n</table> <p>\n <span style=\"color:black;\">observed values</span><br>\n <span style=\"color:#339999;\">expected values</span><br>\n <span style=\"color:#333399;\">&#37; within accuracy</span><br>\n <span style=\"color:#339933;\">&#37; within pretty_condition</span><br>\n </p>\n\n`````\n:::\n:::\n\n\n**For (In Person) data collection** (n=126) the Pearson's Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition that is not significant at the alpha level 0.05, $\\chi^2$ (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. In this particular data sample, the odds ratio (Odds Ratio = 2.18, p = 0.055, 95% CI \\[0.982, +Inf\\]) indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition .\n\n#### (Online Replication)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#online only\ndf <- df_q1 %>% filter(pretty_mode == \"online-replication\")\n\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Accuracy on First Question by Condition\",\n            data = df, pretty_condition ~ accuracy, shade = T)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/label - CHISQR-Q1TRI.by.COND-ONLINE-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# CrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, \n#             chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n <tr>\n <th style=\"border-top:double; text-align:center; font-style:italic; font-weight:normal; border-bottom:1px solid;\" rowspan=\"2\">accuracy</th>\n <th style=\"border-top:double; text-align:center; font-style:italic; font-weight:normal;\" colspan=\"2\">pretty_condition</th>\n <th style=\"border-top:double; text-align:center; font-style:italic; font-weight:normal; font-weight:bolder; font-style:italic; border-bottom:1px solid; \" rowspan=\"2\">Total</th>\n </tr>\n \n<tr>\n <td style=\"border-bottom:1px solid; text-align:center; padding:0.2cm;\">control</td>\n <td style=\"border-bottom:1px solid; text-align:center; padding:0.2cm;\">impasse</td>\n </tr>\n \n<tr> \n<td style=\"padding:0.2cm;  text-align:left; vertical-align:middle;\">incorrect</td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">84</span><br><span style=\"color:#339999;\">76</span><br><span style=\"color:#333399;\">51.9&nbsp;&#37;</span><br><span style=\"color:#339933;\">87.5&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">78</span><br><span style=\"color:#339999;\">86</span><br><span style=\"color:#333399;\">48.1&nbsp;&#37;</span><br><span style=\"color:#339933;\">72.2&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;  \"><span style=\"color:black;\">162</span><br><span style=\"color:#339999;\">162</span><br><span style=\"color:#333399;\">100&nbsp;&#37;</span><br><span style=\"color:#339933;\">79.4&nbsp;&#37;</span></td> \n</tr>\n \n<tr> \n<td style=\"padding:0.2cm;  text-align:left; vertical-align:middle;\">correct</td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">12</span><br><span style=\"color:#339999;\">20</span><br><span style=\"color:#333399;\">28.6&nbsp;&#37;</span><br><span style=\"color:#339933;\">12.5&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center; \"><span style=\"color:black;\">30</span><br><span style=\"color:#339999;\">22</span><br><span style=\"color:#333399;\">71.4&nbsp;&#37;</span><br><span style=\"color:#339933;\">27.8&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;  \"><span style=\"color:black;\">42</span><br><span style=\"color:#339999;\">42</span><br><span style=\"color:#333399;\">100&nbsp;&#37;</span><br><span style=\"color:#339933;\">20.6&nbsp;&#37;</span></td> \n</tr>\n \n<tr> \n<td style=\"padding:0.2cm;  border-bottom:double; font-weight:bolder; font-style:italic; text-align:left; vertical-align:middle;\">Total</td>\n<td style=\"padding:0.2cm; text-align:center;   border-bottom:double;\"><span style=\"color:black;\">96</span><br><span style=\"color:#339999;\">96</span><br><span style=\"color:#333399;\">47.1&nbsp;&#37;</span><br><span style=\"color:#339933;\">100&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;   border-bottom:double;\"><span style=\"color:black;\">108</span><br><span style=\"color:#339999;\">108</span><br><span style=\"color:#333399;\">52.9&nbsp;&#37;</span><br><span style=\"color:#339933;\">100&nbsp;&#37;</span></td>\n<td style=\"padding:0.2cm; text-align:center;   border-bottom:double;\"><span style=\"color:black;\">204</span><br><span style=\"color:#339999;\">204</span><br><span style=\"color:#333399;\">100&nbsp;&#37;</span><br><span style=\"color:#339933;\">100&nbsp;&#37;</span></td> \n</tr>\n<td style=\"text-align:right; font-size:0.9em; font-style:italic; padding:0.2cm;\" colspan=\"4\">&chi;<sup>2</sup>=6.351 &middot; df=1 &middot; Cramer's V=0.189 &middot; Fisher's p=0.009</td> \n</tr>\n \n</table> <p>\n <span style=\"color:black;\">observed values</span><br>\n <span style=\"color:#339999;\">expected values</span><br>\n <span style=\"color:#333399;\">&#37; within accuracy</span><br>\n <span style=\"color:#339933;\">&#37; within pretty_condition</span><br>\n </p>\n\n`````\n:::\n:::\n\n\n**For online data collection** (n=204), a Pearson's Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, $\\chi^2$ (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. The odds ratio (Odds Ratio = 2.68, p = 0.005, 95% CI \\[1.37, +Inf\\]) indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition .\n\n### LOGISTIC REGRESSION\n\n*Fit a logistic regression (at the subject level), predicting Q1 accuracy (absolute score) by condition.*\n\n#### Fit Model\n\n*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#combined\ndf <- df_items %>% filter(q==1) %>% mutate(\n  accuracy = as.factor(score_niceABS)\n)\n\n# FREQUENCY TABLE\n# my.table <- table(df$accuracy, df$pretty_condition)\n# addmargins(my.table) #counts\n# addmargins(prop.table(my.table)) #props\n\n# MODEL FITTING:::::::::::::::::::::::::::::::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\nprint(\"EMPTY MODEL\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"EMPTY MODEL\"\n```\n:::\n\n```{.r .cell-code}\nsummary(m0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = accuracy ~ 1, family = \"binomial\", data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.696  -0.696  -0.696  -0.696   1.753  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.294      0.134   -9.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 343.66  on 329  degrees of freedom\nAIC: 345.7\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n\n```{.r .cell-code}\n#: 2 CONDITION model\nm1 <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\nprint(\"PREDICTOR MODEL\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"PREDICTOR MODEL\"\n```\n:::\n\n```{.r .cell-code}\nsummary(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.819  -0.819  -0.548  -0.548   1.986  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.822      0.230   -7.93  2.2e-15 ***\npretty_conditionimpasse    0.901      0.285    3.16   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 333.07  on 328  degrees of freedom\nAIC: 337.1\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n\n```{.r .cell-code}\n#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > m1$aic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n```\n:::\n\n```{.r .cell-code}\ntest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName | Model | df | df_diff |  Chi2 |     p\n-------------------------------------------\nm0   |   glm |  1 |         |       |      \nm1   |   glm |  2 |       1 | 10.59 | 0.001\n```\n:::\n\n```{.r .cell-code}\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Likelihood Ratio test is significant? p =  0.00113745235691825\"\n```\n:::\n:::\n\n\n*The Condition predictor significantly improves model fit.*\n\n#### Learning Notes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"PREDICTOR MODEL\"\n```\n:::\n\n```{.r .cell-code}\nsummary(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.819  -0.819  -0.548  -0.548   1.986  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.822      0.230   -7.93  2.2e-15 ***\npretty_conditionimpasse    0.901      0.285    3.16   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 333.07  on 328  degrees of freedom\nAIC: 337.1\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n\n```{.r .cell-code}\n#: INTERPRET COEFFICIENTS\n\nprint(\"Coefficients —- LOG ODDS\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Coefficients —- LOG ODDS\"\n```\n:::\n\n```{.r .cell-code}\nconfint(m1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                         2.5 % 97.5 %\n(Intercept)             -2.299  -1.39\npretty_conditionimpasse  0.353   1.48\n```\n:::\n\n```{.r .cell-code}\nprint(\"Coefficients —- ODDS RATIOS\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Coefficients —- ODDS RATIOS\"\n```\n:::\n\n```{.r .cell-code}\ne <- cbind( exp(coef(m1)), exp(confint(m1))) #exponentiate\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n```{.r .cell-code}\ne\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                              2.5 % 97.5 %\n(Intercept)             0.162  0.10  0.248\npretty_conditionimpasse 2.463  1.42  4.374\n```\n:::\n:::\n\n\n**Understanding the logistic regression model**\n\n*The logistic regression intercept gives the log odds of the outcome for the reference level of the predictor variable*\n\n*The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.*\n\n**\\[the empty model**\n\n-   The intercept of an empty model (glm(accuracy \\~ 1) is equal to log(p/(1-p)), where p = the overall probability of a correct response (df\\$accuracy ==1 ).\n-   In SGC3A Q1 accuracy this = 71 correct / 330 = 0.215 -\\> log(0.215 / (1-0.215)) = -1.29.\n-   In other words, the intercept from the model with no predictor variables is the estimated log odds of a correct response for the whole sample.\n-   We can also transform the log of the odds back to a probability: p = ODDS/ (1+ODDS) = exp(-1.29)/(1+exp(-1.29)) = 0.215. This should matched the prediction of the empty model\n\n**\\[a dichotomous predictor\\]**\n\nnatural log (odds of +) = -1.822 + 0.901(x1) ; x1 = 0 for control, 1 for impasse\n\n-   INTERCEPT: log odds of (+ response) in control condition\n    -   log odds of (+) in control : -1.822 + 0.9(0) = -1.822\n    -   convert to odds by exponentiating the coefficients\\\n        log odds of (+) in control = exp(-1.822) = 0.162 odds\n    -   convert to probability by formula =\\>\\\n        p(+) = odds / (1+odds) = 0.162 / (1 + 0.162) = 0.139\\\n        probability of (+) in control = \\~14%\n-   B1 COEFFICIENT: DIFFERENCE in log odds of (+) in impasse vs. control\n    -   log odds of (+) in impasse: -1.822 + 0.901 = -0.921\n    -   convert to odds by exponentiating log odds\\\n        log odds (+) in impasse = exp(-0.921) = 0.398\n    -   convert to probability by formula =\\>\\\n        p(+) = odds / (1 + odds) = 0.398 / (1+0.398) = 0.285\\\n        probaility of (+) in impasse = \\~ 29%\n-   ODDS RATIO : exponentiated B1 COEFFICIENT\n    -   B1 = (slope of logit model = difference in log odds = log odds ratio\n\n    -   B1 = 0.901 is log odds ratio of (+) in impasse vs control\n\n    -   exp(b1) = exp(0.901) = 2.46\n\n    -   Ratio of odds in impasse are 2.46 times higher than in control. Bein in the impasse condition yields odds athat are 2.46 X higher than in control.\n\n+:----------------------------------------------------------------------+\n| MARGINAL\\                                                             |\n| total = 330 success : 71, failure : 259\\                              |\n| p(+) = 71 / 330 = 0.215 = 22%\\                                        |\n| odds(+) = 71 / 259 = 0.274                                            |\n+-----------------------------------------------------------------------+\n| CONTROL total = 158 success = 22; failure = 136\\                      |\n| p(+) = 22/158 = 0.139 = 14%\\                                          |\n| odds(+) = 22/136 = 0.162                                              |\n+-----------------------------------------------------------------------+\n| IMPASSE total = 172 success = 49; failure = 123\\                      |\n| p(+) = 49/172 = 0.285 = 29%\\                                          |\n| odds(+) = 49/123 = 0.398                                              |\n+-----------------------------------------------------------------------+\n\n#### Visualize\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(\"MODEL PERFORMANCE\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"MODEL PERFORMANCE\"\n```\n:::\n\n```{.r .cell-code}\nperformance(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Indices of model performance\n\nAIC     |     BIC | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n----------------------------------------------------------------------------------------------\n337.074 | 344.673 |     0.031 | 0.404 | 1.008 |    0.505 |   -16.847 |           0.021 | 0.673\n```\n:::\n\n```{.r .cell-code}\nprint(\"SANITY CHECK REPORTING\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"SANITY CHECK REPORTING\"\n```\n:::\n\n```{.r .cell-code}\nreport(m1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a logistic model (estimated using ML) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model's explanatory power is weak (Tjur's R2 = 0.03). The model's intercept, corresponding to pretty_condition = control, is at -1.82 (95% CI [-2.30, -1.39], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.90, 95% CI [0.35, 1.48], p = 0.002; Std. beta = 0.90, 95% CI [0.35, 1.48])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n```\n:::\n\n```{.r .cell-code}\nprint(\"MODEL PREDICTIONS\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"MODEL PREDICTIONS\"\n```\n:::\n\n```{.r .cell-code}\n# Retrieve predictions as probabilities \n# (for each level of the predictor)\np.control <- predict(m1,data.frame(pretty_condition=\"control\"),type=\"response\")\npaste(\"Probability of success in control,\", p.control)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Probability of success in control, 0.139240506329147\"\n```\n:::\n\n```{.r .cell-code}\np.impasse <- predict(m1,data.frame(pretty_condition=\"impasse\"),type=\"response\")\npaste(\"Probability of success in impasse,\", p.impasse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Probability of success in impasse, 0.284883720930631\"\n```\n:::\n\n```{.r .cell-code}\n#: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \n# library(ggstatsplot)\n# ggcoefstats(m1, output = \"plot\") + labs(x = \"Log Odds Estimate\")\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m1, type=\"std2\", vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#SJPLOT | MODEL | PROBABILITIES\n# plot_model(m1, type=\"pred\",\n#            show.intercept = TRUE, \n#            show.values = TRUE,\n#            title = \"Model Predicted Probability of Accuracy\",\n#            axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\nggeffect(model = m1) %>% plot()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$pretty_condition\n```\n:::\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#SANITY CHECK SJPLOT\n# library(effects)\n# plot(allEffects(m))\n```\n:::\n\n\n#### Diagnostics\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(m1)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbinned_residuals(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOk: About 100% of the residuals are inside the error bounds.\n```\n:::\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::\n\n\n#### Inference\n\nWe fit a logistic regression model to analyze the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 3.16, p = 0.0016). The model predicts that the odds of a correct response on the first question in the *impasse condition* increase by 146% ($e^{beta_1}$ = 2.46, 95% CI \\[1.42, 4.37\\]) over the *control condition*.\n\n*Equivalent statements:*\n\n-   being in impasse condition increases log odds of correct response by 0.901 (over control)\n-   being in impasse increases odds of correct response in impasse over control increases by factor of 2.46\n-   probability of correct response in control predicted as 28.5%, vs only 14% in control condition\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#PRETTY TABLE SJPLOT\ntab_model(m1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">accuracy</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Odds Ratios</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.16</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.10&nbsp;&ndash;&nbsp;0.25</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">pretty condition<br>[impasse]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">2.46</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.42&nbsp;&ndash;&nbsp;4.37</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.002</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">330</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> Tjur</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.031</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n### TODO ORDINAL REGRESSION\n\n*Fit an ordinal logistic regression (at the subject level), predicting Q1 interpretation by condition.*\n\n-   https://stats.oarc.ucla.edu/r/faq/ologit-coefficients/\n-   https://journals.sagepub.com/doi/full/10.1177/2515245918823199\n-   todo see ordinal regression video\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# #CREATE DATAFRAME OF Q1\n# df <- df_items %>% filter(q ==1) %>% mutate(scaled = as.factor(score_SCALED))\n# \n# #MODEL\n# m <- polr(scaled ~ condition , data = df, Hess=TRUE)\n# summary(m)\n# confint(m)\n# performance(m)\n# report(m)\n# \n# #exponentiate coefficients and CIs \n# ci <- confint(m)\n# ci\n# e <- coef(m)\n# e\n# # exp(cbind(e,ci))\n# \n# # Retrieve predictions as probabilities \n# # (for each level of the predictor)\n# # p.control <- predict(m,data.frame(condition=\"111\"),type=\"response\")\n# # paste(\"Probability of success in control,\", p.control)\n# # p.impasse <- predict(m,data.frame(condition=\"121\"),type=\"response\")\n# # paste(\"Probability of success in impasse,\", p.impasse)\n# \n# # Plot Predicted data and original data points\n# # ggplot(df, aes(x=condition, y=accuracy)) + \n# #   geom_point() +\n# #   stat_smooth(method=\"glm\", color=\"green\", se=FALSE,\n# #                 method.args = list(family=binomial))\n#   \n# #TO PLOT ALL EFFECTS\n# library(effects)\n# plot(allEffects(m))\n# \n# #SJPLOT\n# library(sjPlot)\n# plot_model(m, )\n# \n# \n# #CONVERT TO PROBABILITIES\n# newdat <- data.frame(condition=c(\"111\",\"121\"))\n# prob <- (phat <- predict(object = m, newdat, type=\"p\"))\n# prob\n# \n```\n:::\n\n\n## REPEATED ITEM LEVEL\n\n**Test Phase Accuracy (absolute score)**\n\n#### Mixed Logistic Regression\n\n*Fit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on test phase questions by condition; accounting for random effects of subject.*\n\n##### Fit Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#SETUP DATA \n#PREPARE DATA \nn_items = 8 #number of items in test\n\n#item level\ndf_test = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  accuracy = as.factor(score_niceABS),\n  q = as.factor(q)\n)\n\ndf <- df_test\n\nlibrary(lmerTest) #for CIs in glmer \n\n## 1 | SETUP RANDOM EFFECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \n\n#:: RANDOM INTERCEPT SUBJECT\nmm.rS <- glmer(accuracy ~ (1|subject), data = df,family = \"binomial\")\n\n# :: TEST random effect\npaste(\"AIC with random effect is lower than glm empty model?\", m0$aic > AIC(logLik(mm.rS)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"AIC with random effect is lower than glm empty model? TRUE\"\n```\n:::\n\n```{.r .cell-code}\ntest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName  |    Model | df | df_diff |    Chi2 |      p\n--------------------------------------------------\nm0    |      glm |  1 |         |         |       \nmm.rS | glmerMod |  2 |       1 | 1783.73 | < .001\n```\n:::\n\n```{.r .cell-code}\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Likelihood Ratio test is significant? p =  0\"\n```\n:::\n\n```{.r .cell-code}\n## 2 | ADD FIXED EFFECT\n\n# SUBJECT INTERCEPT | FIXED CONDITION \nmm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject), \n                data = df,family = \"binomial\")\n\n# :: TEST fixed factor \npaste(\"AIC with random effect is lower than glm empty model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"AIC with random effect is lower than glm empty model? TRUE\"\n```\n:::\n\n```{.r .cell-code}\ntest_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName   |    Model | df | df_diff | Chi2 |     p\n-----------------------------------------------\nmm.rS  | glmerMod |  2 |         |      |      \nmm.CrS | glmerMod |  3 |       1 | 4.98 | 0.026\n```\n:::\n\n```{.r .cell-code}\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS,mm.CrS))$p[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Likelihood Ratio test is significant? p =  0.0256331468201315\"\n```\n:::\n:::\n\n\n##### Visualize\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#: PRINT MODEL \nprint(\"PREDICTOR MODEL\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"PREDICTOR MODEL\"\n```\n:::\n\n```{.r .cell-code}\nsummary(mm.CrS)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition + (1 | subject)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    1385     1402     -689     1379     2637 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5307 -0.0193 -0.0097  0.1135  2.7426 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 117      10.8    \nNumber of obs: 2640, groups:  subject, 330\n\nFixed effects:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -9.182      0.677  -13.56   <2e-16 ***\npretty_conditionimpasse    1.632      0.753    2.17     0.03 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nprtty_cndtn -0.394\n```\n:::\n\n```{.r .cell-code}\n#: INTERPRET COEFFICIENTS\n\nprint(\"MODEL PERFORMANCE\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"MODEL PERFORMANCE\"\n```\n:::\n\n```{.r .cell-code}\nperformance(mm.CrS)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Indices of model performance\n\nAIC      |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n--------------------------------------------------------------------------------------------------------------\n1384.749 | 1402.385 |      0.973 |      0.005 | 0.973 | 0.203 | 1.000 |    0.130 |      -Inf |           0.015\n```\n:::\n\n```{.r .cell-code}\nprint(\"SANITY CHECK REPORTING\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"SANITY CHECK REPORTING\"\n```\n:::\n\n```{.r .cell-code}\nreport(mm.CrS)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model included subject as random effect (formula: ~1 | subject). The model's total explanatory power is substantial (conditional R2 = 0.97) and the part related to the fixed effects alone (marginal R2) is of 5.50e-03. The model's intercept, corresponding to pretty_condition = control, is at -9.18 (95% CI [-10.51, -7.85], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 1.63, 95% CI [0.16, 3.11], p = 0.030; Std. beta = 1.63, 95% CI [0.16, 3.11])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n```\n:::\n\n```{.r .cell-code}\n#: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(mm.CrS, type=\"std2\", vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(mm.CrS, type=\"pred\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$pretty_condition\n```\n:::\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\n# ggeffect(model = mm.CrS) %>% plot()\n\n#SANITY CHECK SJPLOT\n# library(effects)\n# plot(allEffects(mm.CrS))\n```\n:::\n\n\n##### Diagnostics\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(mm.CrS)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbinned_residuals(mm.CrS)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWarning: Probably bad model fit. Only about 75% of the residuals are inside the error bounds.\n```\n:::\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n:::\n\n\n##### Inference\n\nWe fit a mixed-effect binomial logistic regression model with random intercepts for subjects to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(3): 4.98, p \\< 0.05). Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 5 over the control condition $e^{\\beta_1}$ = 5.11, 95% CI \\[1.17,22,36\\], p \\< 0.05.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PRETTY TABLE SJPLOT\ntab_model(mm.CrS)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">accuracy</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Odds Ratios</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.00</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.00&nbsp;&ndash;&nbsp;0.00</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">pretty condition: impasse</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">5.11</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.17&nbsp;&ndash;&nbsp;22.36</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.030</strong></td>\n</tr>\n<tr>\n<td colspan=\"4\" style=\"font-weight:bold; text-align:left; padding-top:.8em;\">Random Effects</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&sigma;<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">3.29</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&tau;<sub>00</sub> <sub>subject</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">116.99</td>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">ICC</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.97</td>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">N <sub>subject</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">330</td>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">2640</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.005 / 0.973</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n#### TODO Mixed Ordinal Regression\n\n## SUBJECT-LEVEL\n\n**Test Phase Absolute Score (# questions)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#PREPARE DATA \nn_items = 8 #number of items in test\n\n#item level\ndf = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  accuracy = recode_factor(score_niceABS, \"0\" =\"incorrect\",\"1\"=\"correct\"),\n  q = as.factor(q)\n)\n\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"\",\n       title = \"Accuracy on Test Phase\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/SETUP-TEST-ACC-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#GROUPED PROPORTIONAL BAR CHART\n# gf_props(~accuracy, fill = ~pretty_condition, x =~pretty_condition,\n#        position = position_dodge(), data = df) %>% \n#   gf_facet_grid(~pretty_mode) +\n#    labs(x = \"Correct Responses in Test Phase\",\n#        title = \"Accuracy on Task by Condition\",\n#        subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n \n#FACETED HISTOGRAM\nstats = df_subjects %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))\ngf_props(~item_test_NABS, \n         fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_grid(pretty_condition ~ pretty_mode) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"# Correct\",\n       y = \"proportion of subjects\",\n       title = \"Test Phase Absolute Score (# Correct)\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/SETUP-TEST-ACC-2.png){width=672}\n:::\n:::\n\n\n#### Independent Samples T-Test\n\n*Compare mean ABS score for Ss in control vs. impasse condition.*\n\n-   The bimodal distribution of the subject-level score data do not meet the requirements for t-tests. However, a non-parametric alternative is available (Wilcoxon rank sum test / Man-Whitney test)\n-   Additional corrections are available for data with 'floor' and/or 'ceiling' effects via the 'DACR' package\n-   https://link.springer.com/article/10.3758/s13428-020-01407-2#Sec14 see also https://qmliu.shinyapps.io/DACFE/\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mosaic)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'mosaic'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    cross\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:lmerTest':\n\n    rand\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:lme4':\n\n    factorize\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:Matrix':\n\n    mean\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:modelr':\n\n    resample\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:vcd':\n\n    mplot\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:scales':\n\n    rescale\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:cowplot':\n\n    theme_map\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    stat\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n```\n:::\n\n```{.r .cell-code}\nfavstats(df_subjects$item_test_NABS ~ df_subjects$pretty_condition)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  df_subjects$pretty_condition min Q1 median Q3 max mean   sd   n missing\n1                      control   0  0      0  1   8 1.52 2.86 158       0\n2                      impasse   0  0      1  7   8 3.02 3.41 172       0\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggeasy) #easy adjustmenet to ggplots plots \ngf_boxplot(item_test_NABS ~ pretty_condition, data = df_subjects, width = 0.25) %>% \n  gf_jitter( width = 0.08, alpha = 0.5, color = ~pretty_condition) + \n  labs (title = \"Distribution of Task Accuracy by Condition\",\n        y = \"Task Accuracy (# correct)\", x = \"Condition\") +\n  easy_remove_legend()\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nFor comparison we run a standard Welch Two-sample T test, for which we do not meet the normal distribution requirement, followed by a Wilcoxon rank-sum (Mann-Whitney) test that is a nonparametric alternative for non-normally distributed data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#WELCH TWO SAMPLE T TEST\nwriteLines(\"\\n --------------DATA DON'T MEET NORMALITY REQS FOR WELCH---------\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n --------------DATA DON'T MEET NORMALITY REQS FOR WELCH---------\n```\n:::\n\n```{.r .cell-code}\nt <- t.test(df_subjects$item_test_NABS ~ df_subjects$pretty_condition)\nreport(t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\nThe Welch Two Sample t-test testing the difference of df_subjects$item_test_NABS by df_subjects$pretty_condition (mean in group control = 1.52, mean in group impasse = 3.02) suggests that the effect is negative, statistically significant, and small (difference = -1.50, 95% CI [-2.18, -0.82], t(325.38) = -4.34, p < .001; Cohen's d = -0.48, 95% CI [-0.69, -0.26])\n```\n:::\n\n```{.r .cell-code}\n#WILCOXON RANK BASED TEST\n# The Wilcoxon rank sum test is a non-parametric alternative to the independent two samples t-test for comparing two independent groups of samples, in the situation where the data are not normally distributed.\n#Synonymous: Mann-Whitney test, Mann-Whitney U test, Wilcoxon-Mann-Whitney test and two-sample Wilcoxon test.\n\nprint(\"\\n \\n --------------ROBUST ALTERNATIVE --------------\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\\n \\n --------------ROBUST ALTERNATIVE --------------\"\n```\n:::\n\n```{.r .cell-code}\nt <- wilcox.test(df_subjects$item_test_NABS ~ df_subjects$pretty_condition)\nreport(t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\nThe Wilcoxon rank sum test with continuity correction testing the difference in ranks between df_subjects$item_test_NABS and df_subjects$pretty_condition suggests that the effect is negative, statistically significant, and medium (W = 10180.00, p < .001; r (rank biserial) = -0.25, 95% CI [-0.36, -0.13])\n```\n:::\n:::\n\n\nNext, we calculate the t-test and ANOVA (F-test) based on a series of corrections provided for data with floor and/or ceiling effects.\n\n-   https://link.springer.com/article/10.3758/s13428-020-01407-2#Sec14\n\nUsing properties from truncated normal distributions, the authors propose an easy-to-use method for the *t*-test and ANOVA with ceiling/floor data. *The proposed method calculates the degrees of freedom based on the after-truncation sample sizes (where l = number of floor observations, and r = number of ceiling observations). The rationale was that the proposed method utilizes full information only from data points of n − r − l participants and partial information from data points of r + l participants of a group for the mean and variance estimation. Specifically, the corrected mean and variance estimates (Eqs. 14 and 15) are functions of mean and variance estimates using after-truncation data (n − r − l participants) and the standardized floor and ceiling threshold estimates. The thresholds are estimated using the ceiling and floor percentage estimates based on data points of n − r and n − l participants, respectively. This is a relatively conservative approach for calculating the degrees of freedom, which can help control the type I error rate. This feature can be beneficial, especially given the \"replication crisis\" in psychological and behavioral research.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#FLOOR-CEILING ADJUSTED T TESTS\nlibrary(DACF) #tests for data with floor and ceiling \n# https://www.rdocumentation.org/packages/DACF/versions/1.0.0\n\n\n#prepare data [vector of scores for each group]\nscore_111 <- df_subjects %>% filter(pretty_condition == \"control\") %>% dplyr::select(item_test_NABS) %>% pull()\nscore_121 <- df_subjects %>% filter(pretty_condition == \"impasse\") %>% dplyr::select(item_test_NABS) %>% pull()\n\n# recover the mean and variance for ceiling/floor data\na <- rec.mean.var(score_111) %>% unlist()\n# recover the mean and variance for ceiling/floor data\nb <- rec.mean.var(score_121) %>% unlist()\nr <- as.data.frame(rbind(\"control\"=a ,\"impasse\"=b))\nr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        ceiling.percentage floor.percentage est.mean est.var\ncontrol             0.0886            0.728    -7.15   153.3\nimpasse             0.1802            0.483     1.24    72.7\n```\n:::\n\n```{.r .cell-code}\n# method \"a\" uses original sample size\n# method \"b\" uses after-truncation sample size\n\n# perform adjusted t test\nlw.t.test(score_111,score_121,\"a\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in x2 == max(x2) | x1 == min(x2): longer object length is not a multiple\nof shorter object length\n\nWarning in x2 == max(x2) | x1 == min(x2): longer object length is not a multiple\nof shorter object length\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n$statistic\n[1] -2.95\n\n$p.value\n[1] 0.007\n\n$est.d\n[1] -0.781\n\n$conf.int\n[1] -14.25  -2.52\n```\n:::\n\n```{.r .cell-code}\n#FLOOR-CEILING ADJUSTED F* TEST ANOVA\nlw.f.star(df_subjects,item_test_NABS~pretty_condition,\"a\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$statistic\n[1] 50.6\n\n$p.value\n[1] 9.79e-12\n\n$est.f.squared\n[1] 0.153\n```\n:::\n\n```{.r .cell-code}\n# method \"a\" uses original sample size\n# method \"b\" uses after-truncation sample size\n```\n:::\n\n\nGroup 1 has ceiling and floor percentage of 10.5% and 30.2%, with corrected mean and variance respectively as 3.434 and 83.157. Group 2 has ceiling and floor percentage of 8.2% and 62.7%, with corrected mean and variance respectively as -6.504 and 210.664. The corrected t statistic is 4.778, p = 0. The estimated Cohen's d is 0.87 with a confidence interval as \\[5.742,14.135\\].\n\n\n#### Censored Linear Regression\n\n\n\n#### Linear Regression\n\n*LM on Test Phase absolute score **as number of questions**, rather than % correct.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#SCORE predicted by CONDITION\nlm.1 <- lm(item_test_NABS ~ pretty_condition, data = df_subjects)\npaste(\"Model\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Model\"\n```\n:::\n\n```{.r .cell-code}\nsummary(lm.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = item_test_NABS ~ pretty_condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3.02  -2.77  -1.52   2.98   6.48 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                1.519      0.251    6.04  4.1e-09 ***\npretty_conditionimpasse    1.498      0.348    4.30  2.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.16 on 328 degrees of freedom\nMultiple R-squared:  0.0535,\tAdjusted R-squared:  0.0506 \nF-statistic: 18.5 on 1 and 328 DF,  p-value: 0.0000222\n```\n:::\n\n```{.r .cell-code}\npaste(\"Partition Variance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Partition Variance\"\n```\n:::\n\n```{.r .cell-code}\nanova(lm.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: item_test_NABS\n                  Df Sum Sq Mean Sq F value   Pr(>F)    \npretty_condition   1    185     185    18.5 0.000022 ***\nResiduals        328   3274      10                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\npaste(\"Confidence Interval on Parameter Estimates\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Confidence Interval on Parameter Estimates\"\n```\n:::\n\n```{.r .cell-code}\nconfint(lm.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        2.5 % 97.5 %\n(Intercept)             1.025   2.01\npretty_conditionimpasse 0.814   2.18\n```\n:::\n\n```{.r .cell-code}\nreport(lm.1) #sanity check\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a linear model (estimated using OLS) to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.05, F(1, 328) = 18.52, p < .001, adj. R2 = 0.05). The model's intercept, corresponding to pretty_condition = control, is at 1.52 (95% CI [1.02, 2.01], t(328) = 6.04, p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 1.50, 95% CI [0.81, 2.18], t(328) = 4.30, p < .001; Std. beta = 0.46, 95% CI [0.25, 0.67])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n```\n:::\n\n```{.r .cell-code}\ncheck_model(lm.1)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#MODEL ESTIMATES WITH UNCERTAINTY\n\n#setup references\nm <- lm.1\ndf <- df_subjects\ncall <- m$call %>% as.character()\n\n# uncertainty model visualization\ndf  %>%\n  modelr::data_grid(pretty_condition) %>%\n  augment(lm.1, newdata = ., se_fit = TRUE) %>%\n  ggplot(aes(y = pretty_condition, color = pretty_condition)) +\n  stat_halfeye( scale = .5,\n      aes(\n        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n        fill = stat(cut_cdf_qi(cdf,\n                .width = c(.90, .95),\n                labels = scales::percent_format())))) +\n  scale_fill_brewer(direction = -1) +\n  labs (title = \"(LAB) Test Phase Accuracy ~ Condition\",\n        x = \"model predicted mean (% correct)\", y = \"Condition\", fill = \"Interval\",\n        subtitle = paste(\"lm(\",call[2],\")\")\n  ) + theme(legend.position = \"blank\")\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/VISMODEL-TEST-ABS-LAB-1.png){width=672}\n:::\n:::\n\n\n\n#### Censored (Tobit) Regression\n\nhttps://stats.oarc.ucla.edu/r/dae/tobit-models/\n\n**For censored data (i.e. truncated axis).** The tobit model, also called a censored regression model, is designed to estimate linear relationships between variables when there is either left- or right-censoring in the dependent variable (also known as censoring from below and above, respectively). Censoring from above takes place when cases with a value at or above some threshold, all take on the value of that threshold, so that the true value might be equal to the threshold, but it might also be higher. In the case of censoring from below, values those that fall at or below some threshold are censored.\n\n- censored vs truncated : There is sometimes confusion about the difference between truncated data and censored data. With censored variables, all of the observations are in the dataset, but we don’t know the “true” values of some of them. With truncation some of the observations are not included in the analysis because of the value of the variable. When a variable is censored, regression models for truncated data provide inconsistent estimates of the parameters. See Long (1997, chapter 7) for a more detailed discussion of problems of using regression models for truncated data to analyze censored data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#SETUP\ndf <- df_subjects %>% mutate(\n  accuracy = item_test_NABS\n)\n\nlo = 0\nhi = 8 \nrange(df$accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0 8\n```\n:::\n\n```{.r .cell-code}\nprint(\"Lo and Hi should equate to upper and lower bounds of the # Qs \")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Lo and Hi should equate to upper and lower bounds of the # Qs \"\n```\n:::\n\n```{.r .cell-code}\nlibrary(VGAM)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'VGAM' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: stats4\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: splines\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'VGAM'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:mosaic':\n\n    chisq, logit\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:distributional':\n\n    cdf\n```\n:::\n\n```{.r .cell-code}\n#FIT MODEL\nm1<- vglm(accuracy ~ condition, tobit(Lower = lo, Upper =hi ), data = df)\nsummary(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nvglm(formula = accuracy ~ condition, family = tobit(Lower = lo, \n    Upper = hi), data = df)\n\nCoefficients: \n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept):1  -5.4245     1.0366   -5.23  1.7e-07 ***\n(Intercept):2   2.2731     0.0797   28.52  < 2e-16 ***\ncondition121    5.7588     1.3435    4.29  1.8e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: mu, loglink(sd)\n\nLog-likelihood: -482 on 657 degrees of freedom\n\nNumber of Fisher scoring iterations: 12 \n\nNo Hauck-Donner effect found in any of the estimates\n```\n:::\n\n```{.r .cell-code}\n#CONFIDENCE INTERVALS\nb <- coef(m1)\nse <- sqrt(diag(vcov(m1)))\ncbind(LL = b - qnorm(0.975) * se, UL = b + qnorm(0.975) * se)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 LL    UL\n(Intercept):1 -7.46 -3.39\n(Intercept):2  2.12  2.43\ncondition121   3.13  8.39\n```\n:::\n\n```{.r .cell-code}\n#TEST FIT\n#We can test the significance of condition overall by fitting an empty model and using a likelihood ratio test.\nm0 <- vglm(accuracy ~ 1, tobit(Lower = lo, Upper = hi), data = df)\n(p <- pchisq(2 * (logLik(m1) - logLik(m0)), df = 2, lower.tail = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0000429\n```\n:::\n\n```{.r .cell-code}\npaste(\"P value of likelihood ratio test less than alpha = 0.05? \", p <0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"P value of likelihood ratio test less than alpha = 0.05?  TRUE\"\n```\n:::\n\n```{.r .cell-code}\ncompare_performance(m0,m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Comparison of Model Performance Indices\n\nName | Model |     AIC | AIC weights |     BIC | BIC weights |  RMSE | Sigma\n----------------------------------------------------------------------------\nm0   |  vglm | 988.879 |     < 0.001 | 996.477 |     < 0.001 | 7.989 | 8.002\nm1   |  vglm | 970.764 |       1.000 | 982.162 |       0.999 | 7.760 | 7.778\n```\n:::\n\n```{.r .cell-code}\n#DIAGNOSTICS\nplot(m1)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-23-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-23-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-23-4.png){width=672}\n:::\n\n```{.r .cell-code}\ndf$yhat <- fitted(m1)[,1]\ndf$rr <- resid(m1, type = \"response\")\ndf$rp <- resid(m1, type = \"pearson\")[,1]\n\npar(mfcol = c(2, 3))\n\nwith(df, {\n  plot(yhat, rr, main = \"Fitted vs Residuals\")\n  qqnorm(rr)\n  plot(yhat, rp, main = \"Fitted vs Pearson Residuals\")\n  qqnorm(rp)\n  plot(accuracy, rp, main = \"Actual vs Pearson Residuals\")\n  plot(accuracy, yhat, main = \"Actual vs Fitted\")\n})\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-23-5.png){width=672}\n:::\n\n```{.r .cell-code}\n#VARIANCE ACCOUNTED FOR\nprint(\"VARIANCE ACCOUNTED FOR\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"VARIANCE ACCOUNTED FOR\"\n```\n:::\n\n```{.r .cell-code}\n# correlation\n(r <- with(df, cor(yhat, accuracy)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.231\n```\n:::\n\n```{.r .cell-code}\n# variance accounted for\nr^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0535\n```\n:::\n\n```{.r .cell-code}\nperformance(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Indices of model performance\n\nAIC     |     BIC |  RMSE | Sigma\n---------------------------------\n970.764 | 982.162 | 7.760 | 7.778\n```\n:::\n\n```{.r .cell-code}\n#NOTE: censReg package also does Tobit regression [including mixed models]\n```\n:::\n\n-   The coefficient labeled “(Intercept):1” is the intercept or constant for the model.\n-   The coefficient labeled “(Intercept):2” is an ancillary statistic. If we exponentiate this value, we get a statistic that is analogous to the square root of the residual variance in OLS regression. logSigma is the variance of the model (logarithmized) (same as the second intercept in the VGAM output)\n-   The predicted value of test_phase_score is 5.75 points _higher_ for students in the impasse condition than for students in the control condition. (72% improvement in score!)\n\n\n\n\n\n\n**Using censReg package**\n-   https://cran.r-project.org/web/packages/censReg/vignettes/censReg.pdf\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(censReg) #censored regression\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'censReg' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: maxLik\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: miscTools\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nPlease cite the 'maxLik' package as:\nHenningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.\n\nIf you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:\nhttps://r-forge.r-project.org/projects/maxlik/\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nPlease cite the 'censReg' package as:\nHenningsen, Arne (2017). censReg: Censored Regression (Tobit) Models. R package version 0.5. http://CRAN.R-Project.org/package=censReg.\n\nIf you have questions, suggestions, or comments regarding the 'censReg' package, please use a forum or 'tracker' at the R-Forge site of the 'sampleSelection' project:\nhttps://r-forge.r-project.org/projects/sampleselection/\n```\n:::\n\n```{.r .cell-code}\n#SETUP\ndf <- df_subjects %>% mutate(\n  accuracy = item_test_NABS\n)\n\nlo=0\nhi=8\nrange(df$accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0 8\n```\n:::\n\n```{.r .cell-code}\n#FIT MODEL\nc1 <- censReg( accuracy ~ pretty_condition, left=lo, right=hi, data = df )\nsummary(c1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\ncensReg(formula = accuracy ~ pretty_condition, left = lo, right = hi, \n    data = df)\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n           330            198             87             45 \n\nCoefficients:\n                        Estimate Std. error t value  Pr(> t)    \n(Intercept)             -5.42513    1.21728  -4.457 8.32e-06 ***\npretty_conditionimpasse  5.75908    1.35811   4.241 2.23e-05 ***\nlogSigma                 2.27314    0.09373  24.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNewton-Raphson maximisation, 6 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-likelihood: -482 on 3 Df\n```\n:::\n\n```{.r .cell-code}\n#CONFIDENCE INTERVALS\nb <- coef(c1)\nse <- sqrt(diag(vcov(c1)))\ncbind(LL = b - qnorm(0.975) * se, UL = b + qnorm(0.975) * se)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           LL    UL\n(Intercept)             -7.81 -3.04\npretty_conditionimpasse  3.10  8.42\nlogSigma                 2.09  2.46\n```\n:::\n\n```{.r .cell-code}\n#TEST FIT\n#We can test the significance of condition overall by fitting an empty model and using a likelihood ratio test.\n\nc0 <- censReg( accuracy ~ 1, left=lo, right=hi, data = df )\n(p <- pchisq(2 * (logLik(c1) - logLik(c0)), df = 2, lower.tail = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'log Lik.' 0.0000429 (df=3)\n```\n:::\n\n```{.r .cell-code}\npaste(\"P value of likelihood ratio test less than alpha = 0.05? \", p <0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"P value of likelihood ratio test less than alpha = 0.05?  TRUE\"\n```\n:::\n\n```{.r .cell-code}\nperformance(c1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in get_residuals.default(model, verbose = verbose, type = \"response\", :\nCan't extract residuals from model.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Response residuals not available to calculate mean square error. (R)MSE\n  is probably not reliable.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# Indices of model performance\n\nAIC     |     BIC\n-----------------\n970.764 | 982.162\n```\n:::\n:::\n\n-   logSigma is the variance of the model (logarithmized) (same as the second intercept in the VGAM output)\n-   output should match that of VGAM\n\n\n\n\n\n\n#### Poisson Regression\n\nhttps://stats.oarc.ucla.edu/r/dae/poisson-regression/\n\nThe outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can consider it a type of *count*, (ie. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#POISSON\n\n#SCORE predicted by CONDITION --> POISSON DISTRIBUTION\np.1 <- glm(item_test_NABS ~ pretty_condition, data = df_subjects, family = \"poisson\")\npaste(\"Model\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Model\"\n```\n:::\n\n```{.r .cell-code}\nsummary(p.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = item_test_NABS ~ pretty_condition, family = \"poisson\", \n    data = df_subjects)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -2.46   -2.28   -1.74    1.51    3.69  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               0.4180     0.0645    6.48  9.4e-11 ***\npretty_conditionimpasse   0.6864     0.0781    8.79  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1579.3  on 329  degrees of freedom\nResidual deviance: 1496.7  on 328  degrees of freedom\nAIC: 1956\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n\n```{.r .cell-code}\npaste(\"Partition Variance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Partition Variance\"\n```\n:::\n\n```{.r .cell-code}\nanova(p.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: item_test_NABS\n\nTerms added sequentially (first to last)\n\n                 Df Deviance Resid. Df Resid. Dev\nNULL                               329       1579\npretty_condition  1     82.7       328       1497\n```\n:::\n\n```{.r .cell-code}\npaste(\"Confidence Interval on Parameter Estimates\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Confidence Interval on Parameter Estimates\"\n```\n:::\n\n```{.r .cell-code}\nconfint(p.1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                        2.5 % 97.5 %\n(Intercept)             0.289  0.542\npretty_conditionimpasse 0.535  0.841\n```\n:::\n\n```{.r .cell-code}\nreport(p.1) #sanity check\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a poisson model (estimated using ML) to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model's explanatory power is moderate (Nagelkerke's R2 = 0.22). The model's intercept, corresponding to pretty_condition = control, is at 0.42 (95% CI [0.29, 0.54], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.69, 95% CI [0.53, 0.84], p < .001; Std. beta = 0.69, 95% CI [0.53, 0.84])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n```\n:::\n\n```{.r .cell-code}\ncheck_model(p.1)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n#### Zero Inflated Poisson\n\nhttps://stats.oarc.ucla.edu/r/dae/zip/\\\nPoisson count process with excess zeros\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ZERO INFLATED POISSON\n\nzinfp.1 <- zeroinfl(item_test_NABS ~  item_q1_rt| pretty_condition , data = df_subjects)\nsummary(zinfp.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nzeroinfl(formula = item_test_NABS ~ item_q1_rt | pretty_condition, data = df_subjects)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.934 -0.821 -0.548  0.965  2.421 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 1.654243   0.059975   27.58   <2e-16 ***\nitem_q1_rt  0.001690   0.000849    1.99    0.047 *  \n\nZero-inflation model coefficients (binomial with logit link):\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                0.978      0.179    5.46  4.7e-08 ***\npretty_conditionimpasse   -1.055      0.236   -4.48  7.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -531 on 4 Df\n```\n:::\n\n```{.r .cell-code}\nreport(zinfp.1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a zero-inflated poisson model to predict item_test_NABS with item_q1_rt and pretty_condition (formula: item_test_NABS ~ item_q1_rt). The model's explanatory power is substantial (R2 = 0.35, adj. R2 = 0.35). The model's intercept, corresponding to item_q1_rt = 0, is at 1.65 (95% CI [1.54, 1.77], p < .001). Within this model:\n\n  - The effect of item q1 rt is statistically significant and positive (beta = 1.69e-03, 95% CI [2.52e-05, 3.35e-03], p = 0.047; Std. beta = 0.06, 95% CI [7.11e-04, 0.12])\n  - The effect of pretty condition [impasse] is statistically significant and negative (beta = -1.06, 95% CI [-1.52, -0.59], p < .001; Std. beta = -1.06, 95% CI [-1.52, -0.59])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n```\n:::\n\n```{.r .cell-code}\nperformance(zinfp.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------\n1070.173 | 1085.370 | 0.354 |     0.350 | 3.131 | 3.150 |    -1.609 |           0.044\n```\n:::\n\n```{.r .cell-code}\n# check_model(zinfp.1)\n```\n:::\n\n\n#### Negative Binomial Regression\n\nhttps://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/ - overdispersed count data (variance much greater than mean)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#NEGATIVE BIONOMIAL REGRESSION\n# - https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/\n# - Overdispersed Count variables\n\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'MASS' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'MASS'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n:::\n\n```{.r .cell-code}\nnb.1 <- glm.nb(item_test_NABS ~ pretty_condition, data = df_subjects)\nsummary(nb.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm.nb(formula = item_test_NABS ~ pretty_condition, data = df_subjects, \n    init.theta = 0.253501538, link = log)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.139  -1.102  -0.993   0.378   1.091  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)   \n(Intercept)                0.418      0.171    2.45   0.0143 * \npretty_conditionimpasse    0.686      0.232    2.95   0.0031 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.254) family taken to be 1)\n\n    Null deviance: 279.52  on 329  degrees of freedom\nResidual deviance: 270.97  on 328  degrees of freedom\nAIC: 1194\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2535 \n          Std. Err.:  0.0315 \n\n 2 x log-likelihood:  -1188.1290 \n```\n:::\n\n```{.r .cell-code}\nreport(nb.1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a negative-binomial model (estimated using ML) to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model's explanatory power is weak (Nagelkerke's R2 = 0.04). The model's intercept, corresponding to pretty_condition = control, is at 0.42 (95% CI [0.10, 0.77], p = 0.014). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.69, 95% CI [0.23, 1.14], p = 0.003; Std. beta = 0.69, 95% CI [0.23, 1.14])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n```\n:::\n\n```{.r .cell-code}\ncheck_model(nb.1)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#check model assumption\n#assumes conditional means are not equal to conditional variances\n#conduct likelihood ration test to compare and test [need poisson]\nm3 <- glm(item_test_NABS ~ pretty_condition, family = \"poisson\", data = df_subjects)\npchisq(2 * (logLik(nb.1) - logLik(m3)), df = 1, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'log Lik.' 4.3e-168 (df=3)\n```\n:::\n\n```{.r .cell-code}\n#A large (+) log likelihood suggests that the negative binomial is more appropriate than the Poisson model\n\n\n#EXPONENTIATE PARAMETER ESTIMATES\nest <- cbind(Estimate = coef(nb.1), confint(nb.1))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n```{.r .cell-code}\n#exponentiate parameter estimates\nprint(\"Exponentiated Estimates\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Exponentiated Estimates\"\n```\n:::\n\n```{.r .cell-code}\nexp(est)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Estimate 2.5 % 97.5 %\n(Intercept)                 1.52  1.10   2.15\npretty_conditionimpasse     1.99  1.26   3.13\n```\n:::\n:::\n\n\nThe variable condition has a coefficient of 0.67, (p \\< 0.005). This means that for the impasse condition, the expected log count \\# of questions increases by 0.67. By exponentiating the estimate we see that \\# question correct rate for the impasse condition is nearly 2x that of the control condition.\n\n**Diagnostics** ??\n\n#### Zero Inflated Negative Binomial Regression\n\nhttps://stats.oarc.ucla.edu/r/dae/zinb/ count data that are overdispersed and have excess zeros\n\nZero-inflated negative binomial regression is for modelling count variables with excessive zeros, and especially when the count data are overdispersed (mean is much larger than variance). It can help account for situations where theory suggests that excess zeros are generated by 2 separate processes, one that includes the other count values, and the other that is just the zeros, and thus that the *excess* zeros can be modelled independently.\n\nTotal Absolute Score (# items correct) may fit this situation, as the data are overdispersed (variance much greater than the mean) and there are are very large number of zeros. It is theoretically plausible that these excess zeros (no answers correct) are the result of a different 'process' ... (i.e) little understanding and/or resistance to restructuring understanding of the coordinate system. However, I am not certain if it is plausible to suggest that the zeros themselves are the result of two different processes: (ie. perhaps trying to understand, and not trying to understand?) \\<- this could maybe be disentangled by first question latency?\n\nThe model includes: - A logistic model to model which of the two processes the zero outcome is associated with - A negative binomial model to model the count process\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pscl) #  for zeroinfl negbinomial\n\n#ZERO INFLATED NEGATIVE BINOMIAL\nzinb.1 <- zeroinfl(item_test_NABS ~ pretty_condition | pretty_condition , data = df_subjects, dist = \"negbin\")\n#before the | is the count part, after the | is the logit model\npaste(\"Model\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Model\"\n```\n:::\n\n```{.r .cell-code}\nsummary(zinb.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nzeroinfl(formula = item_test_NABS ~ pretty_condition | pretty_condition, \n    data = df_subjects, dist = \"negbin\")\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.866 -0.794 -0.538  0.856  2.294 \n\nCount model coefficients (negbin with log link):\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               1.7126     0.0728   23.54  < 2e-16 ***\npretty_conditionimpasse   0.0451     0.0880    0.51  0.60810    \nLog(theta)                3.1851     0.8732    3.65  0.00026 ***\n\nZero-inflation model coefficients (binomial with logit link):\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                0.974      0.179    5.43  5.5e-08 ***\npretty_conditionimpasse   -1.056      0.236   -4.47  7.7e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 24.169 \nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -532 on 5 Df\n```\n:::\n\n```{.r .cell-code}\nreport(zinb.1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a zero-inflated negative-binomial model to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model's explanatory power is substantial (R2 = 0.36, adj. R2 = 0.36). The model's intercept, corresponding to pretty_condition = control, is at 1.71 (95% CI [1.57, 1.86], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically non-significant and positive (beta = 0.05, 95% CI [-0.13, 0.22], p = 0.608; Std. beta = 0.05, 95% CI [-0.13, 0.22])\n  - The effect of pretty condition [impasse] is statistically significant and negative (beta = -1.06, 95% CI [-1.52, -0.59], p < .001; Std. beta = -1.06, 95% CI [-1.52, -0.59])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n```\n:::\n\n```{.r .cell-code}\nperformance(zinb.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------\n1073.880 | 1092.876 | 0.363 |     0.359 | 3.150 | 3.174 |    -1.649 |           0.043\n```\n:::\n\n```{.r .cell-code}\n#   rootogram(zinb.1)\n\n\n\n# #EXPONENTIATE PARAMETER ESTIMATES\n# est <- cbind(Estimate = coef(zinb.1), confint(zinb.1))\n# #exponentiate parameter estimates\n# print(\"Exponentiated Estimates\")\n# exp(est)\n```\n:::\n\n\nIn the count model, the coefficient for the condition is very small, and not significant (suggesting it does not contribute to the count yielding process?).\n\nIn the zero-inflation model, the coefficient for the condition variable is -1.056 and statistically significant. This suggests that the log odds of being an excessive zero decrease by 1.06 if you are in the impasse condition (exponentiate it?)\n\n**TODO come back to this and discuss further**\\\n\n\n\n\n#### Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_performance(lm.1, p.1, nb.1, zinb.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Comparison of Model Performance Indices\n\nName   |    Model |      AIC | AIC weights |      BIC | BIC weights |  RMSE | Sigma | Score_log | Score_spherical |    R2 | R2 (adj.) | Nagelkerke's R2\n-------------------------------------------------------------------------------------------------------------------------------------------------------\nlm.1   |       lm | 1699.782 |     < 0.001 | 1711.179 |     < 0.001 | 3.150 | 3.160 |           |                 | 0.053 |     0.051 |                \np.1    |      glm | 1955.788 |     < 0.001 | 1963.386 |     < 0.001 | 3.150 | 2.136 |    -2.957 |           0.042 |       |           |           0.223\nnb.1   |   negbin | 1194.129 |     < 0.001 | 1205.526 |     < 0.001 | 3.150 | 0.909 |    -2.137 |           0.046 |       |           |           0.045\nzinb.1 | zeroinfl | 1073.880 |        1.00 | 1092.876 |        1.00 | 3.150 | 3.174 |    -1.649 |           0.043 | 0.363 |     0.359 |                \n```\n:::\n:::\n\n\nFor modelling test phase absolute score (# items correct) it seems that the zero inflated negative binomial model is the best fit according to R2 and AIC, however, I am not clear on the implications of the interpretation (non significant in count process, significant on logit process), and also not clear if \\# items correct is truly a count process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#uncertainty model visualization\n# df %>%\n  # data_grid(pretty_condition) %>%\n  # augment(m, newdata = ., se_fit = TRUE) %>%\n  # ggplot(aes(y = pretty_condition)) +\n  # stat_halfeye(\n  #   aes(xdist = dist_student_t(df = df.residual(m), \n  #       mu = .fitted, sigma = .se.fit)), scale = .5) +\n  # # add raw data in too (scale = .5 above adjusts the halfeye height so\n  # # that the data fit in as well)\n  # geom_jitter(aes(x = x), data = df, pch = \"|\", size = 2, \n  #             position =   position_nudge(y = -.15), alpha = 0.5) +  \n  # labs (title = \"Model Estimates with Uncertainty\", x = \"model coefficient\") + \n  # theme_minimal()\n```\n:::\n\n\n#### HURLDE BETA Regression\n\nhttps://github.com/markhwhiteii/beta_hurdle/blob/master/manuscript/beta_hurdle.pdf\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gamlss)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'gamlss' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: gamlss.data\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'gamlss.data'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:datasets':\n\n    sleep\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: gamlss.dist\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'gamlss.dist' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: nlme\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'nlme' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'nlme'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    collapse\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:lme4':\n\n    lmList\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: parallel\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n **********   GAMLSS Version 5.4-3  ********** \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nFor more on GAMLSS look at https://www.gamlss.com/\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType gamlssNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'gamlss'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:lme4':\n\n    refit\n```\n:::\n\n```{.r .cell-code}\n#CREATE SAMPLE DATA \nn <- 5000 \nmu <- 0.40 \nsigma <- 0.60 \np0 <- 0.13 \np1 <- 0.17 \np2 <- 1- p0- p1\na <- mu * (1- sigma ^ 2) / (sigma ^ 2) \nb <- a * (1- mu) / mu\n\n#CREATE DIST\nset.seed(1839) \ny <- rbeta(n, a, b) \ncat <- sample(1:3, n, prob = c(p0, p2, p1), replace = TRUE) \ny[cat == 1] <- 0 \ny[cat == 3] <- 1\n\n#VISUALIZE DISTRIBUTION\nx <- as.data.frame(y)\ngf_histogram(~x$y)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#this looks not unlike my distribution! \n\n#CREATE AN EMPTY MODEL\nfit <- gamlss( formula = y ~ 1, # formula for mu \n               formula.sigma = ~ 1, # formula for sigma \n               formula.nu = ~ 1, # formula for nu \n               formula.tau = ~ 1, # formula for tau \n               family = BEINF() )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGAMLSS-RS iteration 1: Global Deviance = 7799 \nGAMLSS-RS iteration 2: Global Deviance = 7778 \nGAMLSS-RS iteration 3: Global Deviance = 7778 \nGAMLSS-RS iteration 4: Global Deviance = 7778 \n```\n:::\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = y ~ 1, family = BEINF(), formula.sigma = ~1,  \n    formula.nu = ~1, formula.tau = ~1) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.3796     0.0196   -19.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.3951     0.0162    24.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.632      0.042   -38.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.4014     0.0382   -36.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  5000 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  4996 \n                      at cycle:  4 \n \nGlobal Deviance:     7778 \n            AIC:     7786 \n            SBC:     7812 \n******************************************************************\n```\n:::\n\n```{.r .cell-code}\nplot(fit)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-31-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\n\t Summary of the Randomised Quantile Residuals\n                           mean   =  0.000571 \n                       variance   =  1 \n               coef. of skewness  =  0.0294 \n               coef. of kurtosis  =  2.95 \nFilliben correlation coefficient  =  1 \n******************************************************************\n```\n:::\n\n```{.r .cell-code}\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nfit_mu <- inv_logit(fit$mu.coefficients) \npaste(\"MU: \",fit_mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"MU:  0.406229902102452\"\n```\n:::\n\n```{.r .cell-code}\nfit_sigma <- inv_logit(fit$sigma.coefficients) \npaste(\"SIGMA: \",fit_sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"SIGMA:  0.597499259410111\"\n```\n:::\n\n```{.r .cell-code}\nfit_nu <- exp(fit$nu.coefficients) \nfit_tau <- exp(fit$tau.coefficients) \nfit_p0 <- fit_nu / (1 + fit_nu + fit_tau) \npaste(\"P0: \",fit_p0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"P0:  0.135600165493784\"\n```\n:::\n\n```{.r .cell-code}\nfit_p1 <- fit_tau / (1 + fit_nu + fit_tau)\npaste(\"P1: \",fit_p1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"P1:  0.170800000002391\"\n```\n:::\n:::\n\n\n**BETA HURDLE INTERPRETATION** - beta component\\\n- MU \"location\" (mean)\\\n- SIGMA \"scale\" (positively related to variance; variance = sigma.squared *mean* (1-mean)\\\n- Rigby, Stasinopoulos, Heller, and De Bastiani (2017) \"reparameterized\" the beta distribution so that the two parameters determining the shape of the distribution would be more useful in a regression framework (see Ferrari & Cribari-Neto, 2004 for a different parameterization)\n\n**ZERO-ONE HURDLE COMPONENT**\\\n- The two additional parameters, ν NU and τTAU , are related to p0 and p1, respectively.\\\n- p0 is the probability that a case equals 0,\\\n- p1 is the probability that a case equals 1,\\\n- p2 (i.e., 1 −p0 −p1) is the probability that the case comes from the beta distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#SETUP DATA \n\nmin = 0 #min possible value of scale\nmax = 8 #max possible value of scale\n\nlibrary(mosaic) #for shuffling\n#1. Rescale accuracy using \n# recommended adjustment \n#rescaled = value-min/(max-min)\ndf <- df_subjects %>% mutate(\n  accuracy = item_test_NABS,\n  R_acc = (accuracy-min)/(max-min), #as %\n  T_acc = (accuracy * (nrow(df)-1) + 0.5)/nrow(df)/8, #transform for no 0 and 1\n  perm = shuffle(condition),\n  scaffold_rt = item_scaffold_rt\n) %>% dplyr::select(accuracy,R_acc, T_acc, condition, perm,scaffold_rt)\n\n#VISUALIZE DISTRIBUTION\ngf_histogram(~R_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = \"Histogram of accuracy\")\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#VISUALIZE DISTRIBUTION\ngf_histogram(~T_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = \"Histogram of [rescaled] accuracy\")\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-32-2.png){width=672}\n:::\n\n```{.r .cell-code}\ngf_histogram(~R_acc, fill = ~perm, data = df) %>% gf_facet_wrap(~perm) + labs(title = \"Histogram of shuffled accuracy\")\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-32-3.png){width=672}\n:::\n\n```{.r .cell-code}\n#SUMMARIZE SAMPLE\npaste(\"Grand mean\", mean(df$R_acc))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Grand mean 0.2875\"\n```\n:::\n\n```{.r .cell-code}\nlibrary(mosaic)\nstats = favstats(df$R_acc ~ df$condition)\nstats$mean <- mean(df$R_acc ~ df$condition)\nstats$var <- var(df$R_acc ~ df$condition)\nprint(\"Grand stats\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Grand stats\"\n```\n:::\n\n```{.r .cell-code}\nstats \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  df$condition min Q1 median    Q3 max  mean    sd   n missing   var\n1          111   0  0  0.000 0.125   1 0.190 0.358 158       0 0.128\n2          121   0  0  0.125 0.875   1 0.377 0.426 172       0 0.182\n```\n:::\n\n```{.r .cell-code}\nprint(\"P0\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"P0\"\n```\n:::\n\n```{.r .cell-code}\nnrow(df %>% filter(R_acc ==0))/nrow(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6\n```\n:::\n\n```{.r .cell-code}\nprint(\"P1\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"P1\"\n```\n:::\n\n```{.r .cell-code}\nnrow(df %>% filter(R_acc ==1))/nrow(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.136\n```\n:::\n\n```{.r .cell-code}\n#CREATE MODEL\n\n#CREATE AN EMPTY MODEL\nm0 <- gamlss( formula = R_acc ~ 1, # formula for mu \n              formula.sigma =  ~ 1, # formula for sigma \n              formula.nu =  ~ 1, # formula for nu \n              formula.tau =  ~ 1, # formula for tau \n              family = BEINF(), data = df )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGAMLSS-RS iteration 1: Global Deviance = 610 \nGAMLSS-RS iteration 2: Global Deviance = 609 \nGAMLSS-RS iteration 3: Global Deviance = 609 \nGAMLSS-RS iteration 4: Global Deviance = 609 \nGAMLSS-RS iteration 5: Global Deviance = 609 \n```\n:::\n\n```{.r .cell-code}\nm0 <- gamlss(R_acc ~ 1, ~ 1, ~ 1, ~ 1, \n            data = df, family = BEINF())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGAMLSS-RS iteration 1: Global Deviance = 610 \nGAMLSS-RS iteration 2: Global Deviance = 609 \nGAMLSS-RS iteration 3: Global Deviance = 609 \nGAMLSS-RS iteration 4: Global Deviance = 609 \nGAMLSS-RS iteration 5: Global Deviance = 609 \n```\n:::\n\n```{.r .cell-code}\nsummary(m0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ 1, sigma.formula = ~1, nu.formula = ~1,  \n    tau.formula = ~1, family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    0.225      0.113    1.98    0.048 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    0.177      0.100    1.76    0.079 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    0.822      0.129    6.39  5.7e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -0.660      0.184   -3.59  0.00038 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  326 \n                      at cycle:  5 \n \nGlobal Deviance:     609 \n            AIC:     617 \n            SBC:     632 \n******************************************************************\n```\n:::\n\n```{.r .cell-code}\nplot(m0)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-32-4.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\n\t Summary of the Randomised Quantile Residuals\n                           mean   =  0.0109 \n                       variance   =  0.996 \n               coef. of skewness  =  -0.0563 \n               coef. of kurtosis  =  2.79 \nFilliben correlation coefficient  =  0.998 \n******************************************************************\n```\n:::\n\n```{.r .cell-code}\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nm0_mu <- inv_logit(m0$mu.coefficients) \npaste(\"MU: \",m0_mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"MU:  0.555931874775555\"\n```\n:::\n\n```{.r .cell-code}\nm0_sigma <- inv_logit(m0$sigma.coefficients) \npaste(\"SIGMA: \",m0_sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"SIGMA:  0.544134514840075\"\n```\n:::\n\n```{.r .cell-code}\nm0_nu <- exp(m0$nu.coefficients) \npaste(\"NU: \",m0_nu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"NU:  2.27484150905293\"\n```\n:::\n\n```{.r .cell-code}\nm0_tau <- exp(m0$tau.coefficients) \npaste(\"TAU: \",m0_tau)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"TAU:  0.517080427912532\"\n```\n:::\n\n```{.r .cell-code}\nm0_p0 <- fit_nu / (1 + fit_nu + fit_tau) \npaste(\"P0: \",m0_p0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"P0:  0.135600165493784\"\n```\n:::\n\n```{.r .cell-code}\nm0_p1 <- fit_tau / (1 + fit_nu + fit_tau)\npaste(\"P1: \",m0_p1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"P1:  0.170800000002391\"\n```\n:::\n\n```{.r .cell-code}\n#CREATE PREDICTOR MODEL\nm1 <- gamlss(R_acc ~ condition, ~ condition, ~ condition, ~ condition, \n            data = df, family = BEINF())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGAMLSS-RS iteration 1: Global Deviance = 588 \nGAMLSS-RS iteration 2: Global Deviance = 587 \nGAMLSS-RS iteration 3: Global Deviance = 587 \nGAMLSS-RS iteration 4: Global Deviance = 587 \nGAMLSS-RS iteration 5: Global Deviance = 587 \n```\n:::\n\n```{.r .cell-code}\nsummary(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.144      0.203    0.71     0.48\ncondition121    0.124      0.244    0.51     0.61\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.280      0.174    1.61     0.11\ncondition121   -0.164      0.213   -0.77     0.44\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.377      0.208    6.63  1.4e-10 ***\ncondition121   -1.019      0.269   -3.79  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    -0.729      0.325   -2.24    0.026 *\ncondition121    0.102      0.394    0.26    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     587 \n            AIC:     603 \n            SBC:     633 \n******************************************************************\n```\n:::\n\n```{.r .cell-code}\n#LOOKING PREDICTOR MODEL\nm <- gamlss(R_acc ~ condition , \n            ~ condition , \n            ~ condition , \n            ~ condition , \n            data = df, family = BEINF())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGAMLSS-RS iteration 1: Global Deviance = 588 \nGAMLSS-RS iteration 2: Global Deviance = 587 \nGAMLSS-RS iteration 3: Global Deviance = 587 \nGAMLSS-RS iteration 4: Global Deviance = 587 \nGAMLSS-RS iteration 5: Global Deviance = 587 \n```\n:::\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.144      0.203    0.71     0.48\ncondition121    0.124      0.244    0.51     0.61\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.280      0.174    1.61     0.11\ncondition121   -0.164      0.213   -0.77     0.44\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.377      0.208    6.63  1.4e-10 ***\ncondition121   -1.019      0.269   -3.79  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    -0.729      0.325   -2.24    0.026 *\ncondition121    0.102      0.394    0.26    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     587 \n            AIC:     603 \n            SBC:     633 \n******************************************************************\n```\n:::\n\n```{.r .cell-code}\n#CREATE PREDICTOR MODEL ON SHUFFLED [PERMUTATION TEST]\nmperm <- gamlss(R_acc ~ perm, ~ perm, ~ perm, ~ perm, \n            data = df, family = BEINF())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGAMLSS-RS iteration 1: Global Deviance = 609 \nGAMLSS-RS iteration 2: Global Deviance = 608 \nGAMLSS-RS iteration 3: Global Deviance = 608 \nGAMLSS-RS iteration 4: Global Deviance = 608 \nGAMLSS-RS iteration 5: Global Deviance = 608 \n```\n:::\n\n```{.r .cell-code}\nsummary(mperm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ perm, sigma.formula = ~perm,  \n    nu.formula = ~perm, tau.formula = ~perm, family = BEINF(),      data = df) \n\n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.1914     0.1706    1.12     0.26\nperm121       0.0635     0.2278    0.28     0.78\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    0.237      0.148    1.60     0.11\nperm121       -0.117      0.202   -0.58     0.56\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.8535     0.1887    4.52  8.6e-06 ***\nperm121      -0.0595     0.2579   -0.23     0.82    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   -0.511      0.258   -1.98    0.048 *\nperm121       -0.294      0.368   -0.80    0.425  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     608 \n            AIC:     624 \n            SBC:     654 \n******************************************************************\n```\n:::\n\n```{.r .cell-code}\n#sanity check with scaled outcome, no zeros ones\nm3 <- gamlss(T_acc ~ condition, ~ condition, ~ condition, ~ condition, \n            data = df, family = BEINF())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGAMLSS-RS iteration 1: Global Deviance = -1812 \nGAMLSS-RS iteration 2: Global Deviance = -2024 \nGAMLSS-RS iteration 3: Global Deviance = -2038 \nGAMLSS-RS iteration 4: Global Deviance = -2040 \nGAMLSS-RS iteration 5: Global Deviance = -2040 \nGAMLSS-RS iteration 6: Global Deviance = -2040 \nGAMLSS-RS iteration 7: Global Deviance = -2040 \nGAMLSS-RS iteration 8: Global Deviance = -2040 \nGAMLSS-RS iteration 9: Global Deviance = -2040 \n```\n:::\n\n```{.r .cell-code}\nsummary(m3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in summary.gamlss(m3): summary: vcov has failed, option qr is used instead\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = T_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition, family = BEINF(),  \n    data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.1492     0.0981  -11.71  < 2e-16 ***\ncondition121   0.5677     0.1411    4.02 0.000071 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    1.4399     0.0745   19.33   <2e-16 ***\ncondition121   0.1694     0.1038    1.63      0.1    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -2.25e+01   3.78e+03   -0.01        1\ncondition121 -6.72e-15   5.24e+03    0.00        1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -2.26e+01   3.96e+03   -0.01        1\ncondition121  9.20e-15   5.48e+03    0.00        1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  9 \n \nGlobal Deviance:     -2040 \n            AIC:     -2024 \n            SBC:     -1994 \n******************************************************************\n```\n:::\n\n```{.r .cell-code}\n#m3 shouldn't show condition as significant for nu and tau, because T_acc was scaled to not include any 0s and 1s\n\n#investigate beta negative binomial distribution\n#https://en.wikipedia.org/wiki/Beta_negative_binomial_distribution\n\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nm1_mu <- inv_logit(m1$mu.coefficients) \npaste(\"MU: \",m1_mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"MU:  0.536038311159578\" \"MU:  0.531024352873784\"\n```\n:::\n\n```{.r .cell-code}\nm1_sigma <- inv_logit(m0$sigma.coefficients) \npaste(\"SIGMA: \",m1_sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"SIGMA:  0.544134514840075\"\n```\n:::\n\n```{.r .cell-code}\nm1_nu <- exp(m1$nu.coefficients) \npaste(\"NU: \",m1_nu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"NU:  3.96329406964311\"  \"NU:  0.360974858677686\"\n```\n:::\n\n```{.r .cell-code}\nm1_tau <- exp(m1$tau.coefficients) \npaste(\"TAU: \",m1_tau)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"TAU:  0.482542801248665\" \"TAU:  1.10746276561553\" \n```\n:::\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.144      0.203    0.71     0.48\ncondition121    0.124      0.244    0.51     0.61\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.280      0.174    1.61     0.11\ncondition121   -0.164      0.213   -0.77     0.44\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.377      0.208    6.63  1.4e-10 ***\ncondition121   -1.019      0.269   -3.79  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    -0.729      0.325   -2.24    0.026 *\ncondition121    0.102      0.394    0.26    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     587 \n            AIC:     603 \n            SBC:     633 \n******************************************************************\n```\n:::\n\n```{.r .cell-code}\nplot(m)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-32-5.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\n\t Summary of the Randomised Quantile Residuals\n                           mean   =  -0.00363 \n                       variance   =  1.04 \n               coef. of skewness  =  -0.0633 \n               coef. of kurtosis  =  3.03 \nFilliben correlation coefficient  =  0.999 \n******************************************************************\n```\n:::\n:::\n\n\n-   MU tells if mean is different by condition\\\n-   SIGMA tells if variance is different by condition\\\n-   NU coefficient tells if condition yields different probability at floor\n-   TAU coefficient tells if condition yields different probability at ceiling\n\n#### Beta Regression (% Correct)\n\nBeta regression on % correct (with standard transformation for including \\[0,1\\]) https://stats.stackexchange.com/questions/63350/how-to-interpret-the-coefficients-from-a-beta-regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# \nlibrary(betareg)\n\n#RESCLAE VARIABLE\n#beta reg can't handle 0s and 1s \nsub <- df_subjects %>% dplyr::select(condition, DV_percent_NABS)\nn = nrow(sub) %>% unlist()\nsub$dv_transformed = (sub$DV_percent_NABS * (n-1) + 0.5)/n\n \n#VISUALIZE VARIABLES\nhistogram(sub$dv_transformed)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngf_histogram(~dv_transformed, fill = ~condition, data = sub) %>% gf_facet_wrap(~condition)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-33-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#FIT MODEL\nmb <- betareg(dv_transformed ~ condition, data = sub)\nsummary(mb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nbetareg(formula = dv_transformed ~ condition, data = sub)\n\nStandardized weighted residuals 2:\n   Min     1Q Median     3Q    Max \n-1.057 -0.453 -0.216  0.541  1.690 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.969      0.108   -8.97   <2e-16 ***\ncondition121    0.556      0.143    3.89   0.0001 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(>|z|)    \n(phi)   0.6604     0.0425    15.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  506 on 3 Df\nPseudo R-squared: 0.0725\nNumber of iterations: 12 (BFGS) + 1 (Fisher scoring) \n```\n:::\n\n```{.r .cell-code}\nplot(mb)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-33-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-33-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-33-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-33-6.png){width=672}\n:::\n:::\n\n\n#### WIP \\| HURDLE MODEL\n\n-   https://data.library.virginia.edu/getting-started-with-hurdle-models/\\\n-   https://en.wikipedia.org/wiki/Hurdle_model#:\\~:text=A%20hurdle%20model%20is%20a,of%20the%20non%2Dzero%20values.\n\nclass of models for count data with both overdispersion and excess zeros;\\\ndifferent from zero-inflated models where the excess zeros are theorized to arise from two different processes; in the hurdle model, there is a model for P(x=0) and a separate model for P(x!=0)\n\nThe model includes: - A binary logit model to model whether the observation takes a positive count or not. - a truncated Poisson or Negative binomial model that only fits positive counts\n\nThis allows us to model: (1) Does the student get *any* questions right? (2) How many questions does the student get right?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pscl) #zero-inf and hurdle models \nlibrary(countreg) #rootogram\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 methods overwritten by 'countreg':\n  method                 from\n  print.zeroinfl         pscl\n  print.summary.zeroinfl pscl\n  summary.zeroinfl       pscl\n  coef.zeroinfl          pscl\n  vcov.zeroinfl          pscl\n  logLik.zeroinfl        pscl\n  predict.zeroinfl       pscl\n  residuals.zeroinfl     pscl\n  fitted.zeroinfl        pscl\n  terms.zeroinfl         pscl\n  model.matrix.zeroinfl  pscl\n  extractAIC.zeroinfl    pscl\n  print.hurdle           pscl\n  print.summary.hurdle   pscl\n  summary.hurdle         pscl\n  coef.hurdle            pscl\n  vcov.hurdle            pscl\n  logLik.hurdle          pscl\n  predict.hurdle         pscl\n  residuals.hurdle       pscl\n  fitted.hurdle          pscl\n  terms.hurdle           pscl\n  model.matrix.hurdle    pscl\n  extractAIC.hurdle      pscl\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'countreg'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:VGAM':\n\n    dzipois, pzipois, qzipois, rzipois\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:pscl':\n\n    hurdle, hurdle.control, hurdletest, zeroinfl, zeroinfl.control\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:vcd':\n\n    rootogram\n```\n:::\n\n```{.r .cell-code}\n#install.packages(\"countreg\", repos=\"http://R-Forge.R-project.org\")\n\n#SYNTAX OUTCOME ~ count model predictor | hurdle predictor\n\nh.1 <- pscl::hurdle(item_test_NABS ~ condition | condition , data = df_subjects,\n              zero.dist = \"binomial\", dist = \"poisson\", size = 8)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in optim(fn = zeroDist, gr = zeroGrad, par = c(start$zero, if (zero.dist\n== : unknown names in control: size\n```\n:::\n\n```{.r .cell-code}\nh.2 <- pscl::hurdle(item_test_NABS ~ condition | condition , data = df_subjects,\n              zero.dist = \"binomial\", dist = \"negbin\", size = 8)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n```\n:::\n\n```{.r .cell-code}\nsummary(h.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\npscl::hurdle(formula = item_test_NABS ~ condition | condition, data = df_subjects, \n    dist = \"poisson\", zero.dist = \"binomial\", size = 8)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.892 -0.818 -0.549  0.881  2.342 \n\nCount model coefficients (truncated poisson with log link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    1.7156     0.0653   26.29   <2e-16 ***\ncondition121   0.0447     0.0789    0.57     0.57    \nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.984      0.179   -5.50  3.7e-08 ***\ncondition121    1.054      0.235    4.48  7.4e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -533 on 4 Df\n```\n:::\n\n```{.r .cell-code}\nsummary(h.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\npscl::hurdle(formula = item_test_NABS ~ condition | condition, data = df_subjects, \n    dist = \"negbin\", zero.dist = \"binomial\", size = 8)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.866 -0.794 -0.538  0.856  2.294 \n\nCount model coefficients (truncated negbin with log link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    1.7126     0.0728   23.54  < 2e-16 ***\ncondition121   0.0451     0.0880    0.51  0.60810    \nLog(theta)     3.1851     0.8732    3.65  0.00026 ***\nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.984      0.179   -5.50  3.7e-08 ***\ncondition121    1.054      0.235    4.48  7.4e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 24.169\nNumber of iterations in BFGS optimization: 20 \nLog-likelihood: -532 on 5 Df\n```\n:::\n\n```{.r .cell-code}\nrootogram(h.1)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n\n```{.r .cell-code}\nrootogram(h.2)\n```\n\n::: {.cell-output-display}\n![](x_learning_files/figure-html/unnamed-chunk-34-2.png){width=672}\n:::\n\n```{.r .cell-code}\ncompare_performance(h.1,h.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Comparison of Model Performance Indices\n\nName |  Model |      AIC | AIC weights |      BIC | BIC weights |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n---------------------------------------------------------------------------------------------------------------------------------\nh.1  | hurdle | 1073.583 |       0.537 | 1088.780 |       0.886 | 0.351 |     0.347 | 3.150 | 3.169 |    -2.479 |           0.043\nh.2  | hurdle | 1073.880 |       0.463 | 1092.876 |       0.114 | 0.363 |     0.359 | 3.150 | 3.174 |    -2.132 |           0.043\n```\n:::\n:::\n\n\n## WIP UNKNOWN\n\n### Cummulative Ordinal (Bayesian)\n\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(brms)\n\n\n# #DEFINE DATA \n# df <- df_items %>% mutate(\n#   scaled = factor(score_SCALED, ordered = TRUE,  #ordered factor\n#                     levels = c(\"-1\", \"-0.5\", \"0\", \"0.5\",\"1\"))\n# )\n# \n# ord_cum <- brm( formula = scaled ~ condition,\n#                data = df,\n#                family = cumulative(\"probit\"),\n#                file = \"analysis/SGC3A/models/m_items_ord.cum.rds\" # cache model (can be removed)  \n# \n# )\n# \n# summary(ord_cum)\n# conditional_effects(ord_cum, \"condition\", categorical = TRUE)\n# \n# #SJPLOT\n# library(sjPlot)\n# plot_model(ord_cum)\n# \n# # m %>%\n# #   spread_draws(b_Intercept, r_condition[condition,]) %>%\n# #   mutate(condition_mean = b_Intercept + r_condition) %>%\n# #   ggplot(aes(y = condition, x = condition_mean)) +\n# #   stat_halfeye()\n# \n# # performance(ord_cum)\n# # plot(ord_cum)\n```\n:::\n\n\n### Adjacent-Category Ordinal (Bayesian)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# \n# #DEFINE DATA \n# df <- df_items %>% mutate(\n#   scaled = factor(score_SCALED, ordered = TRUE,  #ordered factor\n#                     levels = c(\"-1\", \"-0.5\", \"0\", \"0.5\",\"1\"))\n# )\n# \n# \n# # To specify an adjacent-category model, we use family = acat() instead of family = cumulative() as an argument to the brm() function. Then, to model condition with possible category-specific effects, we wrap this variable in cs() in the model’s formula:\n# \n# ord_acat <- brm( formula = scaled ~ cs(condition),\n#                data = df,\n#                family = acat(\"probit\"),\n#                file = \"analysis/SGC3A/models/m_items_ord.acat.rds\" # cache model (can be removed)  \n# )\n# \n# summary(ord_acat)\n# conditional_effects(ord_cum, \"condition\", categorical = TRUE)\n# conditional_effects(ord_acat, \"condition\", categorical = TRUE)\n# \n# #TIDYBAYES VISUALIZATION\n# library(tidybayes)\n# ord_acat %>%\n#   spread_draws(b_Intercept, r_condition[condition,]) %>%\n#   mutate(condition_mean = b_Intercept + r_condition) %>%\n#   ggplot(aes(y = condition, x = condition_mean)) +\n#   stat_halfeye()\n# \n```\n:::\n",
    "supporting": [
      "x_learning_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}