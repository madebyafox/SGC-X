---
title: 'SGC_3A: The Insight Hypothesis'
author: 'Amy Rae Fox'
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
      smooth_scroll: no
  pdf_document:
    toc: yes
    toc_depth: '3'
---
WIP PUBLIC WEBSITE VERSION OF ANALYSIS
```{r SETUP, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE) 

#IMPORT LIBRARIES
library(ggpubr) #arrange plots
# library(rmdformats)

# library(tables) # pretty tables
# library(pastecs) #stat.desc
library(mosaic) #simple descriptives [favstats]
library(ggplot2) #graphs
# library(car) #ANOVA, qqplot
library(rstatix) #nice effsizes https://rpkgs.datanovia.com/rstatix/index.html
# library(effectsize) #effect size
library(fitdistrplus) #fitting theoretical distributions
library(dplyr) #tidyverse data handling
library(see) #pubready plots, esp. violin

# library(pwr) #power analysis
```

# INTRODUCTION
<br>
In Study 3A we explore a hypothesis that emerged from analysis of Study 2, namely that <b>presenting a learning with a situation that induces a state of impasse will increase the probability they have a moment of insight</b>. In the context of Study 2, an impasse state was (unintentionally) induced when the combination of question + data set yielded no available answer in the incorrect (cartesian) interpretation of the graph.  In Study 3A, we test this hypothesis by comparing performance between a (treatment) group receiving impasse-inducing questions followed by normal questions, and a non-impasse control. 
<br>

![control](static/nonimpasse.png)
![impasse](static/impasse.png)

### Hypotheses 
H1. Learners posed with impasse-inducing questions will be more likely to correct interpret the graph.

```{r}
#FOR PUBLIC WEB VERSION
# --- 
# **To try the study yourself: **
# visit TODO INSERT LINK     
# *Enter "github" as your session code, and number of the condition you wish to test*    
# session code= GITHUB
# condition code for CONTROL = 111
# condition code for IMPASSE = 121
# <br> <br>
```  

# METHODS
### Design
We employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor). 

Independent Variables: B-S (Scaffold: control,impasse) W-S (Item x 15)

Dependent Variables 
1. Response Accuracy : Is the response triangular-correct? 0 (false), 1 (true)
2. Response Latency : Time from stimulus onset to clicking 'Submit' button: time in (s)

### Sample
Data was collected by convenience sample of a university subject pool. Initial data (Fall 2017, Spring 2018) were collected in-person, with large groups of students simultaneously completing the study (independently) in a computer lab. In Fall 2021 we collected additional data to replicate results in a remote format (students completing the study asynchronously on their own computers). 

### Materials
Stimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. In both conditions, the questions were identical. Participants in the IMPASSE condition saw a graph with a slightly different dataset, such that the some of the questions posed an IMPASSE structure, where no datapoint intersects the orthogonal projection from the x-axis required if reading the graph in accordance with the cartesian coordinate system. The complete list of questions can be found
<!--html_preserve--><a href="static/sgcx_questions.csv" target="_blank">here</a><!--/html_preserve-->. Examples of graphs+datasets stimuli for each condition are depicted below. The green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. 
<br>

![](static/3A_conditions.png) 

### Procedure

Participants completed the study via a web-browser. Upon starting, they submitted informed consent, before reading task instructions. Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees would be presented in a graph, and they would be answering question about the schedule. Then participants completed a test block of 15 items. In the IMPASSE condition, the first five questions included an IMPASSE problem state. The remaining 10 questions were not structured as impasse.  Following the test block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief. 

# DESCRIPTIVES
```{r IMPORT-DATA}
#IMPORT PARTICIPANT DATA from fall and spring files
fall_participants <- "data/fall17_sgc3a_participants.csv"
spring_partcipants <- "data/spring18_sgc3a_participants.csv"
online_participants <- "data/fall21_sgc3a_participants.csv"

df_fall <- read.csv(fall_participants)
df_spring <- read.csv(spring_partcipants)
df_online <- read.csv(online_participants)

#indicate study modality
df_fall$mode <- "lab"
df_spring$mode <- "lab"
df_online$mode <- "online"

#Create combined data frame
df_subjects <- rbind(df_fall, df_spring, df_online) #, df_replication)
df_subjects$tri_min <- df_subjects$triangular_time / 1000 / 60
df_subjects$test_min <- df_subjects$tt_t / 1000 / 60
df_subjects$learn_min <- df_subjects$ts_t / 1000 / 60

#Create factors
df_subjects <- df_subjects %>% mutate(
  subject = as.factor(subject),
  session = as.factor(session),
  term = as.factor(term),
  condition = as.factor(condition),
  explicit = as.factor(explicit),
  impasse = as.factor(impasse),
  axis = as.factor(axis)
)

df_online <- df_subjects %>% filter(term =="fall21")
df_lab <- df_subjects %>% filter(term != "fall21")

#Remove extraneous dfs
rm(df_fall, df_spring)
```

## Participants
```{r PARTICIPANTS}
#Describe participants
subject.stats <- rbind(
  "lab"= df_lab %>% dplyr::select(age) %>% unlist() %>% favstats(),
  "online" = df_online %>% dplyr::select(age) %>% unlist() %>% favstats()
)
subject.stats$female <- c(
  (df_lab %>% filter(sex=="Female") %>% count())$n,
  (df_online %>% filter(sex=="Female") %>% count())$n
)

#participants per condition
mode.stats <- df_subjects %>% group_by(mode,condition) %>%
  summarize(n=n())
```

For **in-person** collection, `r subject.stats["lab",]$n` participants (`r round(subject.stats["lab",]$female/subject.stats["lab",]$n,1) * 100` % female ) undergraduate STEM majors at a public American University participated _in person_ in exchange for course credit (age: `r min(df_lab$age)` - `r max(df_lab$age)` years).  Participants were randomly assigned to one of two experimental groups, with `r mode.stats[1,3]` in the control condition, and `r mode.stats[2,3]` in the experimental IMPASSE condition. 

For **online replication** `r subject.stats["online",]$n` participants (`r round(subject.stats["online",]$female/subject.stats["online",]$n,1) * 100` % female ) undergraduate STEM majors at a public American University participated _online, asynchronously_ in exchange for course credit (age: `r min(df_online$age)` - `r max(df_online$age)` years).  Participants were randomly assigned to one of two experimental groups, with `r mode.stats[3,3]` in the control condition, and `r mode.stats[4,3]` in the experimental IMPASSE condition. 

## Response Accuracy
Response accuracy refers to how many questions the subject answers with a correct (triangular) interpretation.

```{r DESCRIBE-SCORES}
#DESCRIBE distribution of triangular-correct scores
score.stats <- rbind(
  "lab"= favstats(df_lab$triangular_score),
  "online"= favstats(df_online$triangular_score)
)
score.stats
```

<br>
For *in person* collection, accuracy scores (n = `r score.stats["lab",]$n`) range from `r round(score.stats["lab",]$min,2)` to `r round(score.stats["lab",]$max,2)` with a mean score of (M = `r round(score.stats["lab",]$mean,2)`, SD = `r round(score.stats["lab",]$sd,2)`).


For *online replication*, (online) accuracy scores (n = `r score.stats["online",]$n`) range from `r round(score.stats["online",]$min,2)` to `r round(score.stats["online",]$max,2)` with a mean score of (M = `r round(score.stats["online",]$mean,2)`, SD = `r round(score.stats["online",]$sd,2)`).


```{r VIS-SCORES}
#VISUALIZE distribution of response accuracy
plab <- gf_histogram(~triangular_score, data = df_lab) %>%
  gf_vline(xintercept = score.stats["lab",]$mean, color = "blue") +
  labs(title="In-Person")

ponline <- gf_histogram(~triangular_score, data = df_online) %>%
  gf_vline(xintercept = score.stats["online",]$mean, color = "blue") +
  labs(title="Online")

plot <-ggarrange(plab, ponline, common.legend = TRUE, nrow = 1, ncol =2)
annotate_figure(plot, top = text_grob("Score Accuracy by Study",
   color = "black", face = "bold", size = 14))

```

```{r}

ggplot(df_lab, aes(x = condition, y = triangular_score, fill = condition)) +
  geom_violindot(fill_dots = "black") +
  theme_modern() +
  scale_fill_material_d()

```

<br>
However, inspection of the quantile-quantile plots reveal that response accuracy does not approximate a normal distribution. After exploring several transformations and comparing against alternative distributions (log-normal, poisson, exp, nbinom), we conclude that we will need to use <b>robust tests</b> to analyze response accuracy. 
```{r SCORE-NORMALITY}
plot(fitdist(df_subjects$triangular_score, "norm"))

#EXPLORE ALTERNATIVES
# fit_n  <- fitdist(df_fall$triangular_score, "norm")
# fit_p  <- fitdist(df_fall$triangular_score, "pois")
# fit_b  <- fitdist(df_fall$triangular_score, "nbinom")

# par(mfrow=c(2,2))
# plot.legend <- c("normal", "poisson","nbinomial")
# denscomp(list(fit_n, fit_p, fit_b), legendtext = plot.legend)
# cdfcomp (list(fit_n, fit_p, fit_b), legendtext = plot.legend)
# qqcomp  (list(fit_n, fit_p, fit_b), legendtext = plot.legend)
# ppcomp  (list(fit_n, fit_p, fit_b), legendtext = plot.legend)

```

## Response Latency

```{r DESCRIBE-TIMES}
#DESCRIBE distribution of response time
time.stats <- rbind(
  "lab"= favstats(df_lab$tri_min),
  "online"= favstats(df_online$tri_min)
)
time.stats <- time.stats %>% dplyr::select(-missing) #don't need missing column
time.stats
```
<br>
For *in person* response latency (for test block) (n = `r time.stats["lab",]$n`) range from `r round(time.stats["lab",]$min,2)` to `r round(time.stats["lab",]$max,2)` minutes, with a mean duration of (M = `r round(time.stats["lab",]$mean,2)`, SD = `r round(time.stats["lab",]$sd,2)`) minutes.

For *online replication* (online) response latency (for test block) (n = `r time.stats["online",]$n`) range from `r round(time.stats["online",]$min,2)` to `r round(time.stats["online",]$max,2)` minutes, with a mean duration of (M = `r round(time.stats["online",]$mean,2)`, SD = `r round(time.stats["online",]$sd,2)`).

```{r VIS-TIMES}
#VISUALIZE distribution of response time
plab <- gf_dhistogram(~tri_min, data = df_lab) %>%
  gf_vline(xintercept = time.stats["lab",]$mean, color = "black") %>%
  gf_fitdistr(color="red")+
  labs(title="In Person")

ponline <- gf_dhistogram(~tri_min, data = df_subjects) %>%
  gf_vline(xintercept = time.stats["online",]$mean, color = "black") %>%
  gf_fitdistr(color="red")+
  labs(title="Online")

plot <-ggarrange(plab, ponline, common.legend = TRUE, nrow = 1, ncol =2)

annotate_figure(plot, top = text_grob("Total Time by Study",
   color = "black", face = "bold", size = 14))
```

The data may need to be log-transformed. But we will address this when modelling with the variable. 



### TODO ADD ITEM LEVEL

# HYPOTHESIS TESTING

## Response Accuracy by Condition
The experimental hypothesis (H1) is that structuring the data to pose an impasse (condition 121) will produce significantly better performance than non-impasse (condition 111). The null hypothesis (H0) is that there will be no difference in performance between conditions.  

### [EXPLORE]

```{r DESCRIBE-ACCURACY-BY-CONDITION}
#DESCRIBE scores by condition
score.cond.stats <- rbind(
  "lab" = favstats(triangular_score ~ condition, data = df_lab),
  "online" = favstats(triangular_score ~ condition, data = df_online)
)
score.cond.stats
```
For **in person** study, participants in the impasse group had (on average) higher scores (M = `r round(score.cond.stats[2,]$mean,2)` SD = `r round(score.cond.stats[2,]$sd,2)`) than those in the non-impasse control group (M = `r round(score.cond.stats[1,]$mean,2)`, SD = `r round(score.cond.stats[1,]$sd,2)`).

For **online replication** study, participants in the impasse group had (on average) higher scores (M = `r round(score.cond.stats[4,]$mean,2)` SD = `r round(score.cond.stats[4,]$sd,2)`) than those in the non-impasse control group (M = `r round(score.cond.stats[3,]$mean,2)`, SD = `r round(score.cond.stats[3,]$sd,2)`).

```{r VIZ-SCORES-BY-CONDITION}
#VISUALIZE scores by condition
condlables <- c("111"="control", "121"="impasse")
plab <- gf_dhistogram( ~triangular_score, fill= ~condition, data = df_lab) %>% 
  gf_facet_grid(condition~., labeller=labeller(condition=condlables)) %>% 
  gf_vline(xintercept = ~mean, data = score.cond.stats[c(1:2),], color = "blue")+
  labs(title="In Person")

ponline <- gf_dhistogram( ~triangular_score, fill= ~condition, data = df_online) %>% 
  gf_facet_grid(condition~., labeller=labeller(condition=condlables)) %>% 
  gf_vline(xintercept = ~mean, data = score.cond.stats[c(3:4),], color = "blue")+
  labs(title="Online")

plot <-ggarrange(plab, ponline, legend = FALSE, nrow = 1, ncol =2)
annotate_figure(plot, top = text_grob("Score Accuracy by Condition",
   color = "black", face = "bold", size = 14))
```

```{r BOX-SCORES-BY-CONDITION}
#VISUALIZE scores by condition
plab <- gf_boxplot(triangular_score ~ condition, data=df_lab) %>% 
  gf_jitter(color=~condition, alpha=0.5) +
  labs (title = "In Person")

ponline <- gf_boxplot(triangular_score ~ condition, data = df_online) %>% 
  gf_jitter(color=~condition, alpha=0.5)+
  labs(title ="Online")

plot <-ggarrange(plab, ponline, legend = FALSE, nrow = 1, ncol =2)
annotate_figure(plot, top = text_grob("Score Accuracy by Condition",
   color = "black", face = "bold", size = 14))

```


### [MODEL]
Because the response accuracy data are not-normal, we will test the veracity of our hypothesis using Wilcoxon rank-sum test (Wilcoxon, 1945).

```{r accuracy.wilcoxon.lab}
#Wilcoxon Rank Sum Test
m1 <- wilcox.test(triangular_score ~ condition, data = df_lab, exact=FALSE, 
                  alternative="less")
m1

#Calculate Effect size
m1eff <- wilcox_effsize(triangular_score ~ condition, data = df_lab)
m1eff
```
**In Person**
A one-tailed Wilcoxon rank-sum test reveals that the cummulative score (number of triangular-consistent responses)(Mdn = `r score.cond.stats$median[2]` points ) in the IMPASSE group were significantly higher than than scores in the non-impasse control condition (Mdn = `r score.cond.stats$median[1]`), W = `r m1$statistic[[1]]` , p < 0.001, _r_ = `r m1eff$effsize[[1]]`, a moderate-sized effect. 

```{r accuracy.wilcoxon.online}
#Wilcoxon Rank Sum Test
m1 <- wilcox.test(triangular_score ~ condition, data = df_online, exact=FALSE, 
                  alternative="less")
m1

#Calculate Effect size
m1eff <- wilcox_effsize(triangular_score ~ condition, data = df_online)
m1eff
```

**Online**
A one-tailed Wilcoxon rank-sum test reveals that the cummulative score (number of triangular-consistent responses)(Mdn = `r score.cond.stats$median[4]` points ) in the IMPASSE group were significantly higher than than scores in the non-impasse control condition (Mdn = `r score.cond.stats$median[3]`), W = `r m1$statistic[[1]]` , p < 0.05, _r_ = `r m1eff$effsize[[1]]`, a small effect. 

**TODO:: Investigate outliers in online study; may need to have stricter response time + strategy-consistent response criteria** 

### [REPLICATION]

Because the effect of impasse scaffold was smaller in the remote-online sample than in-person sample, we'll test whether scores in in-person sample were significantly smaller than those online using another Wilcoxon rank-sum test. 
```{r accuracy.wilcoxon.modes}
temp <- df_subjects %>% filter(condition==121)

#Wilcoxon Rank Sum Test
m1 <- wilcox.test(triangular_score ~ mode, data = temp, exact=FALSE, 
                  alternative="greater")
m1

#Calculate Effect size
m1eff <- wilcox_effsize(triangular_score ~ mode, data = temp)
m1eff
```
A one-tailed Wilcoxon rank-sum test reveals that the cummulative score (number of triangular-consistent responses)(Mdn = `r score.cond.stats$median[2]` points ) in the IN-PERSON (impasse condition) sample _were not_ significantly higher than than scores in the REMOTE-ONLINE (impasse condition) sample (Mdn = `r score.cond.stats$median[4]`), W = `r m1$statistic[[1]]` , p = `r round(m1$p.value,2)` _r_ = `r m1eff$effsize[[1]]`. 

**Inference**
Our replication study had comparable results to the in-person study, however the effect size was smaller, therefore we should consider increasing the sample size of future studies conducted online so as to ensure we have sufficient statistical power detect a smaller effect. 



# DILLIGENCE

## Assumptions of Wilcoxon Rank-Sum
The Wilcoxon rank-sum test is the non-parametric alternative to a independent samples t-test and requires the following assumptions:  
1. The two samples are independent of one another <-- MET by random sampling + assignment
2. The two populations have equal variance or spread <--TEST
```{r Wilcoxon rank-sum test assumptions}
df_subjects %>% group_by(condition,mode) %>% summarize(var=var(triangular_score))
```
Variances between conditions in both Lab and Online samples are comparable

# DATA DICTIONARY