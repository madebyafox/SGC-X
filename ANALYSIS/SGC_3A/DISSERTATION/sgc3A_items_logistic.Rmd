---
title: "SGC3_A Items Logistic Regression"
author: "Amy Rae Fox"
output: 
  # rmdformats::robobook:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r SETUP, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#IMPORT LIBRARIES
library(rmdformats)
library(dplyr) #tidyverse data handling
library(tidyr) #pivot
library(knitr) #printing tables
library(forcats)#for factor re-ordering
library(ggpubr) #joining plots (alt to gridExtra)
# library(tables) # pretty tables
# library(pastecs) #stat.desc
# library(mosaic) #simple descriptives [favstats]
library(rstatix) #nice effsizes https://rpkgs.datanovia.com/rstatix/index.html
library(ggplot2) #graphs
library(sjPlot) #fancy contingency tables
# library(car) #ANOVA, qqplot
# library(effectsize) #effect size
# library(pwr) #power analysis

# library(lme4) #linear mixed effects model


#Set custom colors 
lgrey = "#cacec8"
lgreen = "#c6edbb"
lyellow = "#FBEEBB"
lblue = "#BCD9EE"

```
  
# INTRODUCTION

To gain additional insight into participants' pattern of behavior, we will model data at the individual item level. 

```{r IMPORT-DATA, message= FALSE}  

#IMPORT DATA from fall and spring files
fall_items <- "data/fall17_sgc3a_blocks.csv"
spring_items <- "data/spring18_sgc3a_blocks.csv"
online_items <- "data/fall21_sgc3a_blocks.csv"
df_fall <- read.csv(fall_items)
df_spring <- read.csv(spring_items)
df_online <- read.csv(online_items)

#indicate study modality
df_fall$mode <- "lab"
df_spring$mode <- "lab"
df_online$mode <- "online"

#Create combined data frame
df_items <- rbind(df_fall, df_spring,df_online)

#REMOVE Question 6 (attention check)
df_items <- df_items %>% filter(q != 6)

#Create extra fields 
df_items$time_sec <- df_items$rt / 1000 #item time in seconds

#Create answer-consistency column (desired values in column : TRI, ORTH, BOTH, NONE)
df_items$consistency = 0 #set initial dummy values
df_items <- df_items %>% mutate(consistency = replace(consistency, correct==1 & orth_correct==1, "Both"), #both
                                consistency = replace(consistency, correct==0 & orth_correct==1, "Ortho"), #orthogonal
                                consistency = replace(consistency, correct==1 & orth_correct==0, "Tri"), #triangular
                                consistency = replace(consistency, correct==0 & orth_correct==0, "Neither"), #neither
                                consistency = replace(consistency, answer=="", "BLANK")) #neither and BLANK
                   
#Create TOTAL column for future sorting
df_totals <- df_items %>% filter(!q ==16) %>% group_by(subject)  %>% summarise(TOTAL_CORRECT = sum(correct),
          ON_STRATEGY   = sum(correct,orth_correct))

df_items <- left_join(df_items, df_totals)

# #Code incorrect responses (not triangular or otho correct)
# df_items$response <- df_items$orth_correct + df_items$correct #items not correct in tri or orth interpretation
# df_items$response <- dplyr::recode( df_items$response, `1`= "right", `2`="both", `0`="wrong" )
# df_items$incorrect <- dplyr::recode( df_items$response, "right"=0, "both"=0, "wrong"=1 )

#Create factors 
df_items <- df_items %>% mutate(
  subject = as.factor(subject),
  session = as.factor(session),
  term = as.factor(term),
  condition = as.factor(condition),
  consistency = as.factor(consistency),
  explicit = as.factor(explicit),
  impasse = as.factor(impasse),
  correct = as.factor(correct),
  orth_correct = as.factor(orth_correct),
  axis = as.factor(axis),
  q = as.factor(q),
  question = as.factor(question),
  error = 0 #temporary holder for error codes
)

#Change values of column names for later reshaping
# df_items <- rename(df_items, rs_incorrect = incorrect)
df_items <- rename(df_items, rs_tri = correct)
df_items <- rename(df_items, rs_ortho = orth_correct)

#Separate free response from (main) multiple choice blocks
df_freeResponse <- df_items %>% filter(q==16)
df_items <- df_items %>% filter (q!=16)

#Separate into analysis dataframes
df_lab <- df_items %>% filter(mode=="lab")
df_online <- df_items %>% filter(mode=="online")

#Remove unecessary objects
rm(df_fall, df_spring, df_totals)
```

# HYPOTHESIS TESTING

## Response Accuracy
Our motivating hypothesis is that inducing a state of impasse will increase the probability that a learner will experience a moment of insight, and thus restructure their interpretation of the graph and develop a correct-triangular interpretation.

At the **participant-level**, this translates to the hypothesis that Cummulative Score will be significantly higher for participants in the IMPASSE group than control.  At the **item-level**, this translates to the hypothesis that items answered by participants in the IMPASSE group will have a significantly higher probability of being correct (vs incorrect). 

### [EXPLORE]

#### In Person
```{r}
#create a contingency table of # of correct responses in each condition
lab.ctable <- table(df_lab$condition, df_lab$rs_tri)
#Print table with proportion of incorrect(0) and correct(1) responses for each condition
lab.cprop <- prop.table(lab.ctable,1)
lab.cprop

mosaicplot(lab.cprop, shade=TRUE, main="Accuracy by Condition", xlab = "Condition", ylab="Accuracy")
```
<br> For the in-person sample, participants in the non-impasse control condition answered `r round(lab.cprop[1,1],3)*100`% of the items incorrectly, and only `r round(lab.cprop[1,2],3)*100`% correctly. In the IMPASSE condition, participants answered `r round(lab.cprop[2,1],3)*100`% of items incorrectly, and `r round(lab.cprop[2,2],3)*100`% correctly. 

_The lack of alignment between rows and columns suggests that these variables (accuracy and experimental condition) are NOT independent, which is good, if we are expecting a systematic relationship (such as condition causing changes in accuracy)._

**TODO: What do the standaridzed residuals mean?**
The color shading in the mosaic visualizes the sign and absolute size of each residual rij: Cells corresponding to small residuals (|rij| < 2) are shaded white. Cells with medium sized residuals (2 ≤ |rij| < 4) are shaded light blue and light red for positive and negative residuals, respectively. Cells with large residuals (|rij| ≥ 4) are shaded with a fully saturated blue and red, respectively.

```{r}
facetlabels <- c("111"="control", "121"="impasse")
recovery_plot <- ggplot2::ggplot(df_lab, aes(q, fill = rs_tri, colour = rs_tri)) 
recovery_plot +
  geom_histogram(binwidth = 1, stat = "count", alpha = 0.6) +
  scale_colour_manual(values = c("#DF4738", "#009E73")) +
  facet_wrap(~ condition, labeller = labeller(condition=facetlabels)) +
  labs(x = "Question", y = "Count", fill = "Accuracy", colour = "Accuracy", title="IN-PERSON Accuracy by Item") +
  theme_bw()
```
We can see this effect in the plot of accuracy by question and condition. 

#### Remote Replication

```{r}
#create a contingency table of # of correct responses in each condition
net.ctable <- table(df_online$condition, df_online$rs_tri)
#Print table with proportion of incorrect(0) and correct(1) responses for each condition
net.cprop <- prop.table(net.ctable,1)
net.cprop

mosaicplot(net.cprop, shade=TRUE, main="Accuracy by Condition", xlab = "Condition", ylab="Accuracy")
```
<br> 
For the remote-online sample, participants in the non-impasse control condition answered `r round(net.cprop[1,1],3)*100`% of the items incorrectly, and only `r round(net.cprop[1,2],3)*100`% correctly. In the IMPASSE condition, participants answered `r round(net.cprop[2,1],3)*100`% of items incorrectly, and `r round(net.cprop[2,2],3)*100`% correctly. 

```{r}
facetlabels <- c("111"="control", "121"="impasse")
recovery_plot <- ggplot2::ggplot(df_online, aes(q, fill = rs_tri, colour = rs_tri)) 
recovery_plot +
  geom_histogram(binwidth = 1, stat = "count", alpha = 0.6) +
  scale_colour_manual(values = c("#DF4738", "#009E73")) +
  facet_wrap(~ condition, labeller = labeller(condition=facetlabels)) +
  labs(x = "Question", y = "Count", fill = "Accuracy", colour = "Accuracy", title="ONLINE Accuracy by Item") +
  theme_bw()
```

Plotting data from the online replication study, however, are consistent with the impression gleaned from the mosaic plot of accuracy X condition. Namely, that condition (control v. impasse) may not have the same effect that it did in person. Recall that the Wilcoxon rank-sum test performed on the participant level data (number of correct items) revealed that condition did have a statistically significant impact on cummulative score, but with a smaller sized effect than the in-person study, suggesting we may need to increase sample size for online collection to account for greater noise in the data. 

### [MODEL]


# Learning Notes

### Logistic Regression and Odds Ratios
Start with a Logistic Regression for the fixed effect only 
(technically invalid because we violate assumption of independence of the items)

```{r}
inperson.m1 <-  glm(rs_tri ~ condition, data = df_lab, family = binomial)
summary(inperson.m1)
```
The output tells us that the main effect of condition was significant (b = 0.83, SE = 0.10, p < 0.001). 
- The raw b values is difficult to interpret because it tells us about the change in the log odds of the outcome associated with a unit change in the predictor (condition). To understand this, we'll need to transform the log odds to odds. 
- The deviance statistics (the -2LL) can be used to tell us whether adding predictors to the model improves the fit. In this case, the residual deviance (2202.7 on df=1763) is less than the null deviance (2269.8 on 1763), which tells us that the fit is better with the model predictor. 

To test whether the model with condition predicting accuracy is (stat significantly) better than the null model, we evaluate a pchisq test.
```{r}
model.deviance <- inperson.m1 %>% 
  anova() %>% 
  tibble::as_tibble() %>% 
  dplyr::mutate(
    chi_p = pchisq(Deviance, Df, lower.tail = FALSE)
  )
model.deviance
```
The chi-square test is significant, suggesting that the logistic regression model with condition as a fixed effect is better than the null model. 

- The pchisq test returns the value x from the probability density function of the chisquare distribution, with degrees of freedom equal to df. (Enter into function a chisquare value and degrees of freedom, and it returns the probability of obtaining a value as large as the value entered or smaller. Since the p-value is the probability of getting a value as big as the one observed or _larger_, in order to convert the output to a p-value we need the lower.tail=false parameter to return the upper tail of the distribution.)
- The value we pass into the pchisq function is the Deviance value of the model (difference between null deviance and model deviance), as well as the df.

Next, to convert the model parameters to (understandable?) odds rations, we can use the exp() function, which returns the exponent $e^{x}$ (the reverse of the natural log). **The resulting value tells us about the change in the odds (rather than log odds) of the outcome associated with a unit change in the predictor. (i.e. The log odds ratio for the predictor)

```{r}
#Convert model ouptut from log odds to odds

#get the b coefficient and confidence interval for the model
b_ci <- cbind(coef(inperson.m1), confint(inperson.m1))
#return the exponent of each  value, giving us the odds ratio and confidence interval
exp(b_ci)


##In a tidyverse way...
b_values <- coef(inperson.m1) %>%
  as_tibble()

b_ci <- confint(inperson.m1) %>%
  as_tibble()

OR <- b_values %>%
  bind_cols(b_ci) %>%
  exp() %>%
  rename("odds ratio" = value)
OR
```

**If the value of the odds ratio is greater than 1, then it indicates that as the predictor increases, the odds of the outcome occuring increases.** 
- If the value is less than 1, this means that the odds of the outcome occuring decrease. 
- An odds ratio of 1 indicates no effect at all
- If the 95% confidence interval for the OR includes 1, the results are not statistically significant.
- The odds ratio is [odds outcome1 / odds outcome2]
[RECALL that odds are the probability of an event happening, divided by the probability that it won't. For drawing a heart from a deck of cards, the probability is 0.25, probability of not heart is 0.75, so odds is 0.25/0.75, or 1:3)]


**The odds ratio for the main effect of treatment is 2.30 CI=[1.88, 2.82], which means that the odds of a participant in the impasse group answering a given item correctly are 2.3 times higher (or 70% more likely) those of a participant in the control group. 


What is this in terms of probability? To answer this question we need an informative baserate for the probability of a correct response under the control condition. We can derive this from the distribution of correct:incorrect responses in the control group. 

```{r}
#create a contingency table of # of correct responses in each condition
lab.ctable <- table(df_lab$condition, df_lab$rs_tri)
#Print table with proportion of incorrect(0) and correct(1) responses for each condition
lab.cprop <- prop.table(lab.ctable,1)
lab.cprop
```
From our contingency table, we see that the probability of a correct response is only 25%. 

$p_{treatment} = \frac{OR X p_{control}}{1+ OR X p_{control} - p_{control}}$

```{r}
br = 0.25
or = OR[2,1]$`odds ratio`
p_t = (or*br)/(1+(or*br)-br)
p_t
round(p_t *100,2)
```
_Thus, an odds ratio of 2.3 translates to a `r round(p_t *100,2)` probability of correct answer in the IMPASSE group, relative to an assumed 25% probability of correct answer in the control group._ 

[So, what does an OR mean?  Here it is in plain language.                       
- An OR of 1.2 means there is a 20% increase in the odds of an outcome with a given exposure.
- An OR of 2 means there is a 100% increase in the odds of an outcome with a given exposure.  Or this could be stated that there is a doubling of the odds of the outcome.  Note, this is not the same as saying a doubling of the risk.
- An OR of 0.2 means there is an 80% decrease in the odds of an outcome with a given exposure.]


### Contingency Table 
```{r}

#Print XTABS table with expeced values 
# https://strengejacke.github.io/sjPlot/
# library(sjPlot)
df_lab %>%  select(condition, rs_tri) %>% 
  sjtab(fun = "xtab", var.labels=c("conditon", "accuracy"),
        statistics="phi",
        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T)

```
From https://strengejacke.github.io/sjstats/reference/crosstable_statistics.html... the sjtab() function prints (very pretty) contingency tables, including test statistics, and observed and expected values. I (think) that the chi-square test in this table, however, is not applicable to these data, because the test assumes independence, whereas these data are derived from repeated measures. The expected values are those calculated under the null hypotehsis of the Chi Square test.  The Chisquare provides a method for testing the association between the row and column variables in a two-way table. The null hypothesis H0 assumes that there is no association between the variables (in other words, one variable does not vary according to the other variable), while the alternative hypothesis Ha claims that some association does exist. The alternative hypothesis does not specify the type of association, so close attention to the data is required to interpret the information provided by the test.
The chi-square test is based on a test statistic that measures the divergence of the observed data from the values that would be expected under the null hypothesis of no association. This requires calculation of the expected values based on the data. The expected value for each cell in a two-way table is equal to (row total*column total)/n, where n is the total number of observations included in the table.