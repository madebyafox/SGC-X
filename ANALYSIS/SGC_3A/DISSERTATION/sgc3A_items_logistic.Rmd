---
title: "SGC3_A Mixed Effects Logistic Regression"
author: "Amy Rae Fox"
output: 
  # rmdformats::robobook:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
---
TODO :: verify subject totals match participant level analysis. 
```{r SETUP, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#IMPORT LIBRARIES
library(rmdformats)
library(dplyr) #tidyverse data handling
library(tidyr) #pivot
library(knitr) #printing tables
library(forcats)#for factor re-ordering
library(ggpubr) #joining plots (alt to gridExtra)
# library(tables) # pretty tables
# library(pastecs) #stat.desc
# library(mosaic) #simple descriptives [favstats]
library(rstatix) #nice effsizes https://rpkgs.datanovia.com/rstatix/index.html
library(ggplot2) #graphs
library(sjPlot) #fancy contingency tables
library(lme4) #glmer
# library(car) #ANOVA, qqplot
# library(effectsize) #effect size
# library(pwr) #power analysis

# library(lme4) #linear mixed effects model


#Set custom colors 
lgrey = "#cacec8"
lgreen = "#c6edbb"
lyellow = "#FBEEBB"
lblue = "#BCD9EE"

```
  
# INTRODUCTION

To gain additional insight into participants' pattern of behavior, we will model data at the individual item level. 

```{r IMPORT-DATA, message= FALSE}  

#IMPORT DATA from fall and spring files
fall_items <- "data/fall17_sgc3a_blocks.csv"
spring_items <- "data/spring18_sgc3a_blocks.csv"
online_items <- "data/fall21_sgc3a_blocks.csv"
df_fall <- read.csv(fall_items)
df_spring <- read.csv(spring_items)
df_online <- read.csv(online_items)

#indicate study modality
df_fall$mode <- "lab"
df_spring$mode <- "lab"
df_online$mode <- "online"

#Create combined data frame
df_items <- rbind(df_fall, df_spring,df_online)

#Create extra fields 
df_items$time_sec <- df_items$rt / 1000 #item time in seconds

#Create answer-consistency column (desired values in column : TRI, ORTH, BOTH, NONE)
df_items$consistency = 0 #set initial dummy values
df_items <- df_items %>% mutate(consistency = replace(consistency, correct==1 & orth_correct==1, "Both"), #both
                                consistency = replace(consistency, correct==0 & orth_correct==1, "Ortho"), #orthogonal
                                consistency = replace(consistency, correct==1 & orth_correct==0, "Tri"), #triangular
                                consistency = replace(consistency, correct==0 & orth_correct==0, "Neither"), #neither
                                consistency = replace(consistency, answer=="", "BLANK")) #neither and BLANK
                   
#Create TOTAL column for future sorting
df_totals <- df_items %>% filter(!q ==16) %>% group_by(subject)  %>% summarise(TOTAL_CORRECT = sum(correct),
          ON_STRATEGY   = sum(correct,orth_correct))

df_items <- left_join(df_items, df_totals)

# #Code incorrect responses (not triangular or otho correct)
# df_items$response <- df_items$orth_correct + df_items$correct #items not correct in tri or orth interpretation
# df_items$response <- dplyr::recode( df_items$response, `1`= "right", `2`="both", `0`="wrong" )
# df_items$incorrect <- dplyr::recode( df_items$response, "right"=0, "both"=0, "wrong"=1 )

#Create factors 
df_items <- df_items %>% mutate(
  subject = as.factor(subject),
  session = as.factor(session),
  term = as.factor(term),
  condition = as.factor(condition),
  consistency = as.factor(consistency),
  explicit = as.factor(explicit),
  impasse = as.factor(impasse),
  correct = as.factor(correct),
  orth_correct = as.factor(orth_correct),
  axis = as.factor(axis),
  q = as.factor(q),
  question = as.factor(question),
  error = 0 #temporary holder for error codes
)

#Change values of column names for later reshaping
# df_items <- rename(df_items, rs_incorrect = incorrect)
df_items <- rename(df_items, rs_tri = correct)
df_items <- rename(df_items, rs_ortho = orth_correct)

#TODOTHIS IS JUST TEMP FOR TESTING
#Remove items 6 (attn check) and 9 (non discriminant)
df_others <- df_items %>% filter(q %in% c(6,9))
df_items <- df_items %>% filter (!q %in% c(6,9) )

#Separate free response from (main) multiple choice blocks
df_freeResponse <- df_items %>% filter(q==16)
df_items <- df_items %>% filter (q!=16)

#Separate into analysis dataframes
df_lab <- df_items %>% filter(mode=="lab")
df_online <- df_items %>% filter(mode=="online")

#Remove unecessary objects
rm(df_fall, df_spring, df_totals)
```

# HYPOTHESIS TESTING

Our motivating hypothesis is that inducing a state of impasse will increase the probability that a learner will experience a moment of insight, and thus restructure their interpretation of the graph and develop a correct-triangular interpretation.

At the **participant-level**, this translates to the hypothesis that Cummulative Score will be significantly higher for participants in the IMPASSE group than control.  At the **item-level**, this translates to the hypothesis that items answered by participants in the IMPASSE group will have a significantly higher probability of being correct (vs incorrect). 

## [EXPLORE]

### In Person

A contingency table of item accuracy (0=incorrect, 1=correct) for each condition, with row-level proportions.
```{r}
#create a contingency table of # of correct responses in each condition
lab.ctable <- table(df_lab$condition, df_lab$rs_tri)
#Print table with proportion of incorrect(0) and correct(1) responses for each condition
lab.cprop <- prop.table(lab.ctable,1)
lab.cprop
```

```{r}
mosaicplot(lab.cprop, shade=TRUE, main="Accuracy by Condition", xlab = "Condition", ylab="Accuracy")
```
<br> For the in-person sample, participants in the non-impasse control condition answered `r round(lab.cprop[1,1],3)*100`% of the items incorrectly, and only `r round(lab.cprop[1,2],3)*100`% correctly. In the IMPASSE condition, participants answered `r round(lab.cprop[2,1],3)*100`% of items incorrectly, and `r round(lab.cprop[2,2],3)*100`% correctly. 

_The lack of alignment between rows and columns suggests that these variables (accuracy and experimental condition) are NOT independent, which is good, if we are expecting a systematic relationship (such as condition causing changes in accuracy)._

**TODO: What do the standaridzed residuals mean?**
[see https://www.zeileis.org/papers/Zeileis+Meyer+Hornik-2007.pdf]
Color shading of standardized residuals does not mean the effect is not significant... 
The color shading in the mosaic visualizes the sign and absolute size of each residual rij: Cells corresponding to small residuals (|rij| < 2) are shaded white. Cells with medium sized residuals (2 ≤ |rij| < 4) are shaded light blue and light red for positive and negative residuals, respectively. Cells with large residuals (|rij| ≥ 4) are shaded with a fully saturated blue and red, respectively.
SEE: 

```{r}
facetlabels <- c("111"="control", "121"="impasse")
recovery_plot <- ggplot2::ggplot(df_lab, aes(q, fill = rs_tri, colour = rs_tri)) 
recovery_plot +
  geom_histogram(binwidth = 1, stat = "count", alpha = 0.6) +
  scale_colour_manual(values = c("#DF4738", "#009E73")) +
  facet_wrap(~ condition, labeller = labeller(condition=facetlabels)) +
  labs(x = "Question", y = "Count", fill = "Accuracy", colour = "Accuracy", title="IN-PERSON Accuracy by Item") +
  theme_bw()
```
We can see this effect in the plot of accuracy by question and condition. 

### Remote Replication

A contingency table of item accuracy (0=incorrect, 1=correct) for each condition, with row-level proportions.
```{r}
#create a contingency table of # of correct responses in each condition
net.ctable <- table(df_online$condition, df_online$rs_tri)
#Print table with proportion of incorrect(0) and correct(1) responses for each condition
net.cprop <- prop.table(net.ctable,1)
net.cprop
```

Visualizing the proportions of accurate v innacurate responses across conditions.

```{r}
mosaicplot(net.cprop, shade=TRUE, main="Accuracy by Condition", xlab = "Condition", ylab="Accuracy")
```
<br> 
For the remote-online sample, participants in the non-impasse control condition answered `r round(net.cprop[1,1],3)*100`% of the items incorrectly, and only `r round(net.cprop[1,2],3)*100`% correctly. In the IMPASSE condition, participants answered `r round(net.cprop[2,1],3)*100`% of items incorrectly, and `r round(net.cprop[2,2],3)*100`% correctly. 

```{r}
facetlabels <- c("111"="control", "121"="impasse")
recovery_plot <- ggplot2::ggplot(df_online, aes(q, fill = rs_tri, colour = rs_tri)) 
recovery_plot +
  geom_histogram(binwidth = 1, stat = "count", alpha = 0.6) +
  scale_colour_manual(values = c("#DF4738", "#009E73")) +
  facet_wrap(~ condition, labeller = labeller(condition=facetlabels)) +
  labs(x = "Question", y = "Count", fill = "Accuracy", colour = "Accuracy", title="ONLINE Accuracy by Item") +
  theme_bw()
```

Plotting data from the online replication study, however, are consistent with the impression gleaned from the mosaic plot of accuracy X condition. Namely, that condition (control v. impasse) may not have the same effect that it did in person. Recall that the Wilcoxon rank-sum test performed on the participant level data (number of correct items) revealed that condition did have a statistically significant impact on cummulative score, but with a smaller sized effect than the in-person study, suggesting we may need to increase sample size for online collection to account for greater noise in the data. 

## [MODEL APPROACH]

Approach: Mixed Effects Logistic Regression 

In this model, the log odds of the outcomes aer modelled as the linear combination of the predictors when data are clustered over the fixed and random effects. 

Rationale: 
1. Mixed effects model allows us to account for differential effects of the treatment (between-subjects condition) on individual subjects and items via random intercepts and slopes. It is appropriate for our case of repeated measures where each of teh 15 items are measured for each subject.
2. Logistic regression (part of generalized linear model family with logistic distribution) is appropriate for modelling an outcome variable that is binary [correct, incorrect]. 

TUTORIAL --> https://slcladal.github.io/regression.html#Mixed-Effects_Binomial_Logistic_Regression

[see also]
http://euclid.psych.yorku.ca/www/psy6136/
https://stats.oarc.ucla.edu/r/dae/mixed-effects-logistic-regression/
https://stats.oarc.ucla.edu/other/mult-pkg/introduction-to-generalized-linear-mixed-models/
https://online.stat.psu.edu/stat504/
https://online.stat.psu.edu/stat504/lesson/6
https://online.stat.psu.edu/statprogram/

(Hypothesized Model Specification)
Mixed Effects Logistic Regression Model
outcome: rs_tri [0,1] representing answer correctnes
Fixed Effects: condition[111,121]
Random Effects: intercept(subject), slope(question)

**TODO**
- Think about random slopes vs intersects for items and subjects. 
- Probably need to separate test vs scaffold questions
- Probably need to remove attn check #6 and non-discriminant #9.


Assumptions for logistic regression:
- The response variable  is a binomial random variable with a single trial and success probability where P(success) = 1-P(failure). Thus,  corresponds to "success" and occurs with probability , and  corresponds to "failure" and occurs with probability .
- The set of predictor or explanatory variables  are fixed (not random) and can be discrete, continuous, or a combination of both. 
- Together, the data is collected for the ith individual in the vector...These are assumed independent by the sampling mechanism. This also allows us to combine or group the data, by summing over trials for which i is constant. In this section of the notes, we focus on a single explanatory variable .

## [MODEL IN PERSON]


### Fit Random Effects

Thinking about random effects
It is reasonable to expect that performance on the task could be affected by individual differences in subjects, and individual differences in items, completely separate from hypothesized of fixed effects of condition. A mixed model allows us to account for these random effects. But should they be modelled as random _intercepts_ or random _slopes_? 

**For a variable to be modelled as a random _slope_, it must vary within the designated fixed effect.** The only fixed effect we will be modelling is (between-subjects) condition. [NOTE to consider response time. can of worms]

- SUBJECTS are only assigned to 1 condition, therefore we cannot assign _by-participant varying condition slopes_. That is, the effect of condition cannot be different for each participant across conditions, because each participant is only IN one condition. Our only option is to model SUBJECT as a random intercept. 

- QUESTIONS are completed across conditions. Q1 in the control condition is identical to Q1 in the impasse condition. **[note is this true for Q1-Q5]**, therefore we _can_ assign a _by-question varying condition slopes_. That is, the effect of condition can be different for each question across conditions, which would account for a differential influence of the condition treatment on individual items. (e.g., impasse aids Q7 but not Q10). **For this study, this is not theorized, but perhaps should be evaluated**. [how is this different than an interaction?]

#### SUBJECT as random effect
The first random effect that is theoretically relevant to this study design is SUBJECT. Each subject has 15 individual observations (one for each question). 

**Modelling SUBJECT as a random effect (intercept) allows us to account for individual differences in accuracy _by subject_.** This means that each level of the random variable (subject) will have a different intercept. Because some subjects will be better (more experienced, or put forth more effort, any source of individual differece) than others,we want to account for this effect separately than our hypothesized main effect of CONDITION. 

1a. Generate a minimal base-line model WITHOUT fixed effects

_for consistency, I will use a model notation as
m[num random effects][num fixed effects]_[sample]
```{r}
# baseline model glm, prediting outcome from the mean
m0.glm_lab = glm(rs_tri ~ 1, family = binomial, data = df_lab) 
summary(m0.glm_lab)
``` 

**This baseline fixed effects model predicts accuracy from mean, revealing the total variance to be accounted for 
- Null deviance == residual deviance = 2269.8  on 1763  degrees of freedom.**

1b. Generatea base-line MIXED model using the “glmer” function, with a _random intercept for SUBJECT._

```{r}
# base-line mixed-model, predicting outcome from mean + random intercept for subject
m0_lab = glmer(rs_tri ~ (1|subject), family = binomial, data = df_lab)
summary(m0_lab)
```
**The model summary indicates that the random intercept is a signifcant predictor. **

_But is it justified, in terms of variance explained?_

1c. Is inclusion of a random effect justified?

To check if including the random effect is justified, we  by compare the AIC from the baseline fixed model (glm) to AIC from the baseline glmer model (glmer). 
_If the AIC of the glmer object is smaller than the AIC of the glm object, then this indicates that the mixed model explains more variance, and is thus justified._

```{r}
aic.glmer <- AIC(logLik(m0_lab))
aic.glm <- AIC(logLik(m0.glm_lab))
aic.glm; aic.glmer
```
**The AIC of the MIXED (glmer) model is smaller, which suggests that including the random intercepts is justified.** 

1d. Does inclusion of the random effect explain +significantly more_ variance than baseline? 

_To confirm whether the AIC reduction is sufficient for justifying the inclusion of a random-effect structure, we also test whether the mixed-effects minimal base-line model explains significantly more variance by applying a Model Likelihood Ratio Test to the fixed- and the mixed effects minimal base-line models using pchisq._

```{r} 
# test random effects
null.id = -2 * logLik(m0.glm_lab) + 2 * logLik(m0_lab)
pchisq(as.numeric(null.id), df=1, lower.tail=F) 

```
**The p-value of the Model Likelihood Ratio Test is lower than .05 which shows that the inclusion of the random-effects structure is warranted. We can now continue with the model fitting process.**

#### QUESTION as random effect

- modelling QUESTION as a random _intercept_ allows us to account for variability in the difficulty of _individual questions_. This allows us to account for the fact that some items (questions) may be inherently harder than others, completely independent of any effect of CONDITION. **For this study, this is pragmatically justified insofar as the questions test different interval relations, which differ in complexity.**

- modelling QUESTION as a _by-condition random slope_ means that (fixed) factor condition can have a different effect on each item. (eg. impasse aids Q7 but not Q10). **Are there any compelling reasons to expect by-item varying condition effects? Is this theoretically motivated?** For this study, this is not hypothesized, but is would not be unprecedented, and should be evaluated**. [how is this different than an interaction?]

1e. Construct model with random intercept for subject and random intercept for question. 
```{r}
#add random intercept for question
#.1 is personal shorthand for 1st iteration of random effects structure
m0.1_lab <- glmer(rs_tri ~ 1 + (1 | subject) + (1 | q), family = "binomial", data = df_lab)
summary(m0.1_lab)
```
**TODO: CAMERON QUETION: In this specification, all the effects are still random, no? Why is there a fixed effect in the output, and what does it correspond to? **

_The output tells us the family (binomial for binary outcomes) and the link (logit), followed fit indices and the variance of the random effects. In this case the variability in the intercept (on the log odds scale) between *subjects* and between *questions*. The standard deviation is also displayed (simply the square root of the variance, not the standard error of the estimate of the variance). We also get the number of unique units at each level. Last are the fixed effects, as before._

1f. Test if the addition of _question as random intercept_ explains significantly more variance than subject as intercept alone.
```{r} 
# test random effects
anova(m0_lab, m0.1_lab)
```
The ANOVA output shows us that although adding question as a random intercept add complexity (more parameters), it also explains (significantly) more variance, as evidenced by the reduced AIC value, and significant p value on the ChiSq. 

Is a random intercept for question the most appropriate structure, or should question be a random slope nested on the condition as a grouping variable?

1g. Construct model with random intercept for subject and random slope for question by condition. 
```{r}
#add random intercept for question
#.1 is personal shorthand for 1st iteration of random effects structure
m0.2_lab <- glmer(rs_tri ~  (1 | subject) + (1 + condition | q), family = "binomial", data = df_lab)
summary(m0.2_lab)
```
The model summary gives us estimated for the random effects component of the 
model, as well as a estimated correlation between intercepts and slopes. In this case, correlation is -1, indicating ... **THIS LOOKS BAD, WHAT DOES IT MEAN?** [Winter, 2019 pg.239]

1h. Is the model with random slope for question by condition better than the model with question as a random intercept? **IS THIS THE RIGHT QUESTION TO BE ASKING?**

```{r}
anova(m0.1_lab, m0.2_lab)
```
The anova output reveals that the question-as-slope model does reduce AIC and therefore explains more variance, infact significantly more variance according to the ChiSq (p < 0.001)

**ON A PERSONAL NOTE: I think I am disappointed by this result. But need to think more carefully about implications and in particular if this effect may be driven by question #9 which needs to be removed.**

### Fit Fixed Effects

_The next step is to fit the model, which means that we aim to find the “best” model, (i.e. the minimal adequate model). In this case, we will use a **manual step-wise step-up, forward elimination procedure**._

LOGICAL STRUCTURE
1. check if model adding factor violates incomplete information assumption
2. add factor to model
3. check if adding factor explains more variance
4. check if adding factor is a significant predictor
5. [for multiple fixed] check if factor violates multicolinearity
6. continue with each factor... 
7. choose minimally complex but maximally predictive model

_Before we begin with the model fitting process we need to add ´control = glmerControl(optimizer = “bobyqa”)´ to avoid unnecessary failures to converge._

```{r}
#add control statement to mimimal model to avoid unecessary failures to converge
# m0.glmer <- glmer(rs_tri ~ 1+ (1|subject), family = binomial, data = df_lab, control=glmerControl(optimizer="bobyqa"))
```

### + FIXED [condition]

2a. Does [+CONDITION] violate assumption of incomplete information or result in complete separation? 

_- To avoid incomplete information (a combination of variables does not occur in the data), we tabulate the variables we intend to include and make sure that all possible combinations are present in the data. _
_- Including variables although not all combinations are present in the data would lead to unreliable models that report (vastly) inaccurate results._
_- A special case of incomplete information is complete separation which occurs if one predictor perfectly explains an outcome (in that case the incomplete information would be caused by a level of the dependent variable)._

```{r}
# add Priming
ifelse(min(ftable(df_lab$rs_tri, df_lab$condition)) == 0, "incomplete information", "okay")
```
**Adding CONDITION would NOT violate incomplete information assumptions.**

```{r}
#create a new model that adds fixed effect for CONDITION to the prior random effects structure
m1.2_lab <- update(m0.2_lab, .~.+condition)
summary(m1.2_lab)
```
MODEL FIT: 
--> INVESTIGATE THIS --> boundary (singular) fit: see ?isSingular
--> Hmmm... what does the correlation mean in the random section?


```{r}
#test for significant difference between baseline and CONDITION model
anova(m1.2_lab, m0.2_lab, test = "Chi") 
```
**The ANOVA ouptut on the baseline [intercept subject slope condition] and fixed effects CONDITION model show that:**

- Fixed main effect of CONDITION improves model fit (the addition of CONDITION explains more variance (lower AIC and BIC) than random effect only model)
- CONDITION correlates significantly with outcome(accuracy) (significant p value on chisq) <--is this what this means?

PICKUP HERE WITH BODO WINTER TEXTBOOK + ADDRESS EARLIER QUESTIONS



```{r}
# tab_model(m0.2_lab, m1.2_lab, show.aic = TRUE)
tab_model(m0.1_rem, m1.1_rem, show.aic = TRUE,
          dv.labels = c("accuracy ~ (1+subject)+(1+q)", "accuracy ~ condition + (1 + condition|q)+(1+subject)")
)
```


#TEMPORARY
## [MODEL ONLINE]


### Fit Random Effects

Thinking about random effects
It is reasonable to expect that performance on the task could be affected by individual differences in subjects, and individual differences in items, completely separate from hypothesized of fixed effects of condition. A mixed model allows us to account for these random effects. But should they be modelled as random _intercepts_ or random _slopes_? 

**For a variable to be modelled as a random _slope_, it must vary within the designated fixed effect.** The only fixed effect we will be modelling is (between-subjects) condition. [NOTE to consider response time. can of worms]

- SUBJECTS are only assigned to 1 condition, therefore we cannot assign _by-participant varying condition slopes_. That is, the effect of condition cannot be different for each participant across conditions, because each participant is only IN one condition. Our only option is to model SUBJECT as a random intercept. 

- QUESTIONS are completed across conditions. Q1 in the control condition is identical to Q1 in the impasse condition. **[note is this true for Q1-Q5]**, therefore we _can_ assign a _by-question varying condition slopes_. That is, the effect of condition can be different for each question across conditions, which would account for a differential influence of the condition treatment on individual items. (e.g., impasse aids Q7 but not Q10). **For this study, this is not theorized, but perhaps should be evaluated**. [how is this different than an interaction?]

#### SUBJECT as random effect
The first random effect that is theoretically relevant to this study design is SUBJECT. Each subject has 15 individual observations (one for each question). 

**Modelling SUBJECT as a random effect (intercept) allows us to account for individual differences in accuracy _by subject_.** This means that each level of the random variable (subject) will have a different intercept. Because some subjects will be better (more experienced, or put forth more effort, any source of individual differece) than others,we want to account for this effect separately than our hypothesized main effect of CONDITION. 

1a. Generate a minimal base-line model WITHOUT fixed effects

_for consistency, I will use a model notation as
m[num random effects][num fixed effects]_[sample]
```{r}
# baseline model glm, prediting outcome from the mean
m0.glm_rem = glm(rs_tri ~ 1, family = binomial, data = df_online) 
summary(m0.glm_rem)
``` 
**This baseline fixed effects model predicts accuracy from mean, revealing the total variance to be accounted for 
- Null deviance == residual deviance = Null deviance: 2612  on 1959  degrees of freedom**

1b. Generatea base-line MIXED model using the “glmer” function, with a _random intercept for SUBJECT._

```{r}
# base-line mixed-model, predicting outcome from mean + random intercept for subject
m0_rem = glmer(rs_tri ~ (1|subject), family = binomial, data = df_online)
summary(m0_rem)
```
**The model summary indicates that the random intercept is a signifcant predictor. **

_But is it justified, in terms of variance explained?_

1c. Is inclusion of a random effect justified?

To check if including the random effect is justified, we  by compare the AIC from the baseline fixed model (glm) to AIC from the baseline glmer model (glmer). 
_If the AIC of the glmer object is smaller than the AIC of the glm object, then this indicates that the mixed model explains more variance, and is thus justified._

```{r}
aic.glmer <- AIC(logLik(m0_rem))
aic.glm <- AIC(logLik(m0.glm_rem))
aic.glm; aic.glmer
```
**The AIC of the MIXED (glmer) model is smaller, which suggests that including the random intercepts is justified.** 

1d. Does inclusion of the random effect explain +significantly more_ variance than baseline? 

_To confirm whether the AIC reduction is sufficient for justifying the inclusion of a random-effect structure, we also test whether the mixed-effects minimal base-line model explains significantly more variance by applying a Model Likelihood Ratio Test to the fixed- and the mixed effects minimal base-line models using pchisq._

```{r} 
# test random effects
null.id = -2 * logLik(m0.glm_rem) + 2 * logLik(m0_rem)
pchisq(as.numeric(null.id), df=1, lower.tail=F) 

```
**The p-value of the Model Likelihood Ratio Test is lower than .05 which shows that the inclusion of the random-effects structure is warranted. We can now continue with the model fitting process.**

#### QUESTION as random effect

1e. Construct model with random intercept for subject and random intercept for question. 
```{r}
#add random intercept for question
#.1 is personal shorthand for 1st iteration of random effects structure
m0.1_rem <- glmer(rs_tri ~ 1 + (1 | subject) + (1 | q), family = "binomial", data = df_online)
summary(m0.1_rem)
```
**TODO: CAMERON QUETION: In this specification, all the effects are still random, no? Why is there a fixed effect in the output, and what does it correspond to? **

_The output tells us the family (binomial for binary outcomes) and the link (logit), followed fit indices and the variance of the random effects. In this case the variability in the intercept (on the log odds scale) between *subjects* and between *questions*. The standard deviation is also displayed (simply the square root of the variance, not the standard error of the estimate of the variance). We also get the number of unique units at each level. Last are the fixed effects, as before._

1f. Test if the addition of _question as random intercept_ explains significantly more variance than subject as intercept alone.
```{r} 
# test random effects
anova(m0_rem, m0.1_rem)
```
The ANOVA output shows us that although adding question as a random intercept add complexity (more parameters), it also explains (significantly) more variance, as evidenced by the reduced AIC value, and significant p value on the ChiSq. 

Is a random intercept for question the most appropriate structure, or should question be a random slope nested on the condition as a grouping variable?

1g. Construct model with random intercept for subject and random slope for question by condition. 
```{r}
#add random intercept for question
#.1 is personal shorthand for 1st iteration of random effects structure
m0.2_rem <- glmer(rs_tri ~  (1 | subject) + (1 + condition | q), family = "binomial", data = df_online)
summary(m0.2_rem)
```
The model summary gives us estimated for the random effects component of the 
model, as well as a estimated correlation between intercepts and slopes. In this case, correlation is 0.29, indicating ... **THIS IS REALLY DIFFERENT from the corr in the in person data. What does this mean?** [Winter, 2019 pg.239]

1h. Is the model with random slope for question by condition better than the model with question as a random intercept? **IS THIS THE RIGHT QUESTION TO BE ASKING?**

```{r}
anova(m0.1_rem, m0.2_rem)
```
The anova output reveals that the question-as-slope model does NOT reduce AIC and, and the ChiSqr is not significant. 

**??SO FOR ONLINE, the condition did not affect individual item difficulty, but in person it did? Is this consistent with our inutions about the online replication?**

### Fit Fixed Effects

_The next step is to fit the model, which means that we aim to find the “best” model, (i.e. the minimal adequate model). In this case, we will use a **manual step-wise step-up, forward elimination procedure**._

LOGICAL STRUCTURE
1. check if model adding factor violates incomplete information assumption
2. add factor to model
3. check if adding factor explains more variance
4. check if adding factor is a significant predictor
5. [for multiple fixed] check if factor violates multicolinearity
6. continue with each factor... 
7. choose minimally complex but maximally predictive model

_Before we begin with the model fitting process we need to add ´control = glmerControl(optimizer = “bobyqa”)´ to avoid unnecessary failures to converge._

```{r}
#add control statement to mimimal model to avoid unecessary failures to converge
# m0.glmer <- glmer(rs_tri ~ 1+ (1|subject), family = binomial, data = df_lab, control=glmerControl(optimizer="bobyqa"))
```

### + FIXED [condition]

2a. Does [+CONDITION] violate assumption of incomplete information or result in complete separation? 

_- To avoid incomplete information (a combination of variables does not occur in the data), we tabulate the variables we intend to include and make sure that all possible combinations are present in the data. _
_- Including variables although not all combinations are present in the data would lead to unreliable models that report (vastly) inaccurate results._
_- A special case of incomplete information is complete separation which occurs if one predictor perfectly explains an outcome (in that case the incomplete information would be caused by a level of the dependent variable)._

```{r}
# add Priming
ifelse(min(ftable(df_online$rs_tri, df_online$condition)) == 0, "incomplete information", "okay")
```
**Adding CONDITION would NOT violate incomplete information assumptions.**

```{r}
#create a new model that adds fixed effect for CONDITION to the prior random effects structure
m1.1_rem <- update(m0.1_rem, .~.+condition)
summary(m1.1_rem)
```
MODEL FIT: 
--> Hmm... no  "boundary (singular) fit: see ?isSingular" warning this time
--> CONDITION fixed effect is not significant. 
--> what does the corelation of fixed effects mean?
--> what does it mean to have a signficant intercept for the fixed effect?



```{r}
#test for significant difference between baseline and CONDITION model
anova(m1.1_rem, m0.1_rem, test = "Chi") 
```
**The ANOVA ouptut on the baseline [intercept subject intercept condition] and fixed effects CONDITION model show that:**

- Fixed main effect of CONDITION does NOT improves model fit (the addition of CONDITION does NOT more variance (higher AIC and BIC) than random effect only model)
- CONDITION does NOT correlate significantly with outcome(accuracy) (significant p value on chisq) <--is this what this means?

PICKUP HERE WITH BODO WINTER TEXTBOOK + ADDRESS EARLIER QUESTIONS




# OTHER 

- Only once we have confirmed that the incomplete information, complete separation, and multicollinearity are not a major concern, we generate the more saturated model and test whether the inclusion of a predictor leads to a significant reduction in residual deviance. 
- If the predictor explains a significant amount of variance, it is retained in the model while being disregarded in case it does not explain a sufficient quantity of variance.

Does [adding factor] violate multi-colinearity?
- verify VIFs do not exceed a maximum of 3 for main effects (Zuur, Ieno, and Elphick 2010)
- Booth GD (1994) suggest that VIFs should ideally be lower than 3 for as higher values would indicate multicollinearity and thus that the model is unstable. 
- The value of 3 should be taken with a pinch of salt because there is no clear consensus about what the maximum VIF for interactions should be or if it should be considered at all. The reason is that we would, of course, expect the VIFs to increase when we are dealing with interactions as the main effects that are part of the interaction are very likely to correlate with the interaction itself. 
- However, if the VIFs are too high, then this will still cause the issues with the attribution of variance. The value of 3 was chosen based on recommendations in the standard literature on multicollinearity (Zuur et al. 2009; Neter, Wasserman, and Kutner 1990). 

```{r}

# inperson.m0 <- glmer(rs_tri ~ condition + (1|subject*q) , data = df_lab,
#                      family = binomial, 
#                      control = glmerControl(optimizer ="bobyqa"),
#                      nAGQ = 10)
# 
# # print the mod results without correlations among fixed effects
# print(inperson.m0, corr = FALSE)
```


```{r}
#build a model with subject as random intercept
inperson.m1 <- glmer(rs_tri ~ condition + (1 | subject), data = df_lab,
                     family = binomial, 
                     control = glmerControl(optimizer ="bobyqa"),
                     nAGQ = 10)

# print the mod results without correlations among fixed effects
print(inperson.m1, corr = FALSE)
```
- the middle section gives basic info for model comparison, followed by random effects estimates. This represents the _estimated variability in the intercept on the logit scale._ 
- the bottom section gives a table of fixed effects estimates. This represents _the regression coefficients, unstandardized on the logit scale, followed by their standard errors. 

TODO :: model comparison / evaluation w/ bootstrapping? (see ucla)
TODO:: random intercepts vs random slopes?
TODO :: comparing subject vs q randoms  vs both

- _random slopes_ allow each [group line] to have a different slope and that means that the random slope model allows the predictor to have a different effect for each subject 
- _random intercepts_ allow each group line to have a different y intercept

```{r}
#build a model with subject as random intercept, q random slope
inperson.m2 <- glmer(rs_tri ~ condition + (1 | subject), data = df_lab,
                     family = binomial, 
                     control = glmerControl(optimizer ="bobyqa"),
                     nAGQ = 10)

# print the mod results without correlations among fixed effects
print(inperson.m2, corr = FALSE)
```












# Learning Notes

### Logistic Regression and Odds Ratios
Start with a Logistic Regression for the fixed effect only 
(technically invalid because we violate assumption of independence of the items)

```{r}
inperson.m1 <-  glm(rs_tri ~ condition, data = df_lab, family = binomial)
summary(inperson.m1)
```
The output tells us that the main effect of condition was significant (b = 0.83, SE = 0.10, p < 0.001). 
- The raw b values is difficult to interpret because it tells us about the change in the log odds of the outcome associated with a unit change in the predictor (condition). To understand this, we'll need to transform the log odds to odds. 
- The deviance statistics (the -2LL) can be used to tell us whether adding predictors to the model improves the fit. In this case, the residual deviance (2202.7 on df=1763) is less than the null deviance (2269.8 on 1763), which tells us that the fit is better with the model predictor. 

To test whether the model with condition predicting accuracy is (stat significantly) better than the null model, we evaluate a pchisq test.
```{r}
model.deviance <- inperson.m1 %>% 
  anova() %>% 
  tibble::as_tibble() %>% 
  dplyr::mutate(
    chi_p = pchisq(Deviance, Df, lower.tail = FALSE)
  )
model.deviance
```
The chi-square test is significant, suggesting that the logistic regression model with condition as a fixed effect is better than the null model. 

- The pchisq test returns the value x from the probability density function of the chisquare distribution, with degrees of freedom equal to df. (Enter into function a chisquare value and degrees of freedom, and it returns the probability of obtaining a value as large as the value entered or smaller. Since the p-value is the probability of getting a value as big as the one observed or _larger_, in order to convert the output to a p-value we need the lower.tail=false parameter to return the upper tail of the distribution.)
- The value we pass into the pchisq function is the Deviance value of the model (difference between null deviance and model deviance), as well as the df.

Next, to convert the model parameters to (understandable?) odds rations, we can use the exp() function, which returns the exponent $e^{x}$ (the reverse of the natural log). **The resulting value tells us about the change in the odds (rather than log odds) of the outcome associated with a unit change in the predictor. (i.e. The log odds ratio for the predictor)

```{r}
#Convert model ouptut from log odds to odds

#get the b coefficient and confidence interval for the model
b_ci <- cbind(coef(inperson.m1), confint(inperson.m1))
#return the exponent of each  value, giving us the odds ratio and confidence interval
exp(b_ci)


##In a tidyverse way...
b_values <- coef(inperson.m1) %>%
  as_tibble()

b_ci <- confint(inperson.m1) %>%
  as_tibble()

OR <- b_values %>%
  bind_cols(b_ci) %>%
  exp() %>%
  rename("odds ratio" = value)
OR
```

**If the value of the odds ratio is greater than 1, then it indicates that as the predictor increases, the odds of the outcome occuring increases.** 
- If the value is less than 1, this means that the odds of the outcome occuring decrease. 
- An odds ratio of 1 indicates no effect at all
- If the 95% confidence interval for the OR includes 1, the results are not statistically significant.
- The odds ratio is [odds outcome1 / odds outcome2]
[RECALL that odds are the probability of an event happening, divided by the probability that it won't. For drawing a heart from a deck of cards, the probability is 0.25, probability of not heart is 0.75, so odds is 0.25/0.75, or 1:3)]


**The odds ratio for the main effect of treatment is 2.30 CI=[1.88, 2.82], which means that the odds of a participant in the impasse group answering a given item correctly are 2.3 times higher (or 70% more likely) those of a participant in the control group. 


What is this in terms of probability? To answer this question we need an informative baserate for the probability of a correct response under the control condition. We can derive this from the distribution of correct:incorrect responses in the control group. 

```{r}
#create a contingency table of # of correct responses in each condition
lab.ctable <- table(df_lab$condition, df_lab$rs_tri)
#Print table with proportion of incorrect(0) and correct(1) responses for each condition
lab.cprop <- prop.table(lab.ctable,1)
lab.cprop
```
From our contingency table, we see that the probability of a correct response is only 25%. 

$p_{treatment} = \frac{OR X p_{control}}{1+ OR X p_{control} - p_{control}}$

```{r}
br = 0.25
or = OR[2,1]$`odds ratio`
p_t = (or*br)/(1+(or*br)-br)
p_t
round(p_t *100,2)
```
_Thus, an odds ratio of 2.3 translates to a `r round(p_t *100,2)` probability of correct answer in the IMPASSE group, relative to an assumed 25% probability of correct answer in the control group._ 

[So, what does an OR mean?  Here it is in plain language.                       
- An OR of 1.2 means there is a 20% increase in the odds of an outcome with a given exposure.
- An OR of 2 means there is a 100% increase in the odds of an outcome with a given exposure.  Or this could be stated that there is a doubling of the odds of the outcome.  Note, this is not the same as saying a doubling of the risk.
- An OR of 0.2 means there is an 80% decrease in the odds of an outcome with a given exposure.]


### Contingency Table 
```{r}

#Print XTABS table with expeced values 
# https://strengejacke.github.io/sjPlot/
# library(sjPlot)
df_lab %>%  select(condition, rs_tri) %>% 
  sjtab(fun = "xtab", var.labels=c("conditon", "accuracy"),
        statistics="phi",
        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T)

```
From https://strengejacke.github.io/sjstats/reference/crosstable_statistics.html... the sjtab() function prints (very pretty) contingency tables, including test statistics, and observed and expected values. I (think) that the chi-square test in this table, however, is not applicable to these data, because the test assumes independence, whereas these data are derived from repeated measures. The expected values are those calculated under the null hypotehsis of the Chi Square test.  The Chisquare provides a method for testing the association between the row and column variables in a two-way table. The null hypothesis H0 assumes that there is no association between the variables (in other words, one variable does not vary according to the other variable), while the alternative hypothesis Ha claims that some association does exist. The alternative hypothesis does not specify the type of association, so close attention to the data is required to interpret the information provided by the test.
The chi-square test is based on a test statistic that measures the divergence of the observed data from the values that would be expected under the null hypothesis of no association. This requires calculation of the expected values based on the data. The expected value for each cell in a two-way table is equal to (row total*column total)/n, where n is the total number of observations included in the table.