[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SGC-X",
    "section": "",
    "text": "Study SGC2 | Description\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC2 | Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC3A | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC3A | 3 Description\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC3A | Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC3A | Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC4A | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4A | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4A | 3 Description\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC4B | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4B | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC4C | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4C | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC5A | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC5A | 2 Response Scoring\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "analysis/utils/scoring.html",
    "href": "analysis/utils/scoring.html",
    "title": "Scoring Strategy",
    "section": "",
    "text": "The purpose of this notebook is to describe the strategy for assigning a score ( a measure of accuracy) to response data for the SGC studies. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.)"
  },
  {
    "objectID": "analysis/utils/scoring.html#multiple-response-scoring",
    "href": "analysis/utils/scoring.html#multiple-response-scoring",
    "title": "Scoring Strategy",
    "section": "MULTIPLE RESPONSE SCORING",
    "text": "MULTIPLE RESPONSE SCORING\nThe graph comprehension task of the SGC studies presents readers with a graph, a question, and a series of checkboxes. Participants are instructed to use the graph to answer the question, and respond by selecting all the checkboxes that apply, where each checkbox corresponds to a datapoint in the graph.\n\n\n\nFigure 1. Sample Graph Comprehension (Question # 6)\n\n\nIn the psychology and education literatures on Tests & Measures, the format of this type of question is referred to as Multiple Response (MR), (also: Multiple Choice Multiple Answer (MCMA) and Multiple Answer Multiple Choice (MAMC)). It has a number of properties that make it different from traditional Single Answer Multiple Choice (SAMC) questions, where the respondent marks a single response from a number of options. In particular, there are a number of very different ways that MAMC questions can be scored.\nIn tranditional SAMC format questions, one point is given for selecting the option designated as correct, and zero points given for marking any of the alternative (i.e. distractor) options. Individual response options on MAMC questions, however might be partially correct (\\(i\\)), while responses on other answer options within the same item might be incorrect (\\(n – i\\)). In MR, it is not obvious how to allocate points when the respondent marks a true-correct option (i.e. options that should be selected, denoted \\(p\\)), as well as one or more false-correct options (i.e. options that should not be selected, denoted \\(q\\)). Should partial credit be awarded? If so, are options that respondents false-selected and false-unselected items equally penalized?\nSchmidt et al. (2021) performed a systematic literature review of publications proposing MAMC (or equivalent) scoring schemes, ultimately synthesizing over 80 sources into 27 distinct scoring approaches. Upon reviewing the benefits of trade-offs of each approach, for this study we choose utilize two of the schemes: dichotomous scoring ( Schmidt et al. (2021) scheme #1), and partial scoring \\([-1/q,0, +1/p]\\) ( Schmidt et al. (2021) scheme #26), as well as a scaled discriminant score that leverages partial scoring to discriminate between strategy-specific patterns of response.\n\nResponse Encoding\nFirst, we note that the question type evaluated by Schmidt et al. (2021) is referred to as Multiple True-False (MTF), a variant of MAMC where respondents are presented with a question (stem) and series of response options with True/False (e.g. radio buttons) for each. Depending on the implementation of the underlying instrument, it may or may not be possible for respondents to not respond to a particular option (i.e. leave the item ‘blank’). Although MTF questions have a different underlying implementation (and potentially different psychometric properties) they are identical in their mathematical properties; that is, responses to a MAMC question of ‘select all that apply’ can be coded as a series of T/F responses to each response option\n\n\n\nFigure 3.1: Figure 2. SAMC (vs) MAMC (vs) MTF\n\n\nIn this example (Figure 3.1), we see an example of a question with four response options (\\(n=4\\)) in each question type. In the SAMC approach (at left), there are four possible responses, given explicitly by the response options (respondent can select only one) \\((\\text{number of possible responses} = n)\\). With only four possible responses, we cannot entirely discriminate between all combinations of the underlying response variants we might be interested in, and must always choose an ‘ideal subset’ of possible distractors to present as response options. In the MAMC (middle) and MTF (at right), the same number of response options (\\(n=4\\)) yield a much greater number \\((\\text{number of possible responses} = 2^{n})\\). We can also see the equivalence between a MAMC and MTF format questions with the same response options. Options the respondent selects in MAMC are can be coded as T, and options they leave unselected can be coded as F. Thus, for response options (ABCD), a response of [AB] can also be encoded as [TTFF].\n\n\nScoring Schemes\nIn the sections that follow, we use the terminology:\nProperties of the Stimulus-Question\n\\[\\begin{align}\nn &= \\text{number of response options} \\\\  \n  &= p + q \\\\\n  p &= \\text{number of true-select options (i.e. should be selected)} \\\\\n  q &= \\text{number of true-unselect options (i.e. should not be selected)}\n\\end{align}\\]\nProperties of the Subject’s Response\n\\[\\begin{align}\ni &= \\text{number of options in correct state}, (0 ≤ i ≤ n) \\\\\nf &= \\text{resulting score}\n\\end{align}\\]\n\nDichotomous Scoring\nDichotomous Scoring is the strictest scoring scheme, where a response only receives points if it is exactly correct, meaning the respondent includes only correct-select options, and does select any additional (i.e. incorrect-select) options that should not be selected. This is also known as all or nothing scoring, and importantly, it ignores any partial knowledge that a participant may be expressing through their choice of options. They may select some but not all of the correct-select options, and one or more but not all of the correct-unselect items, but receive the same score as a respondent selects none of the correct-select options, or all of the correct-unselect options. In this sense, dichotomous scoring tells us only about perfect knowledge, and ignores any indication of partial knowledge the respondent may be indicating through their selection of response options.\nIn Dichotomous Scoring\n\nscore for the question is either 0 or 1\nfull credit is only given if all responses are correct; otherwise no credit\ndoes not account for partial knowledge. - with increasing number of response options, scoring becomes stricter as each statement must be marked correctly.\n\nThe algorithm for dichotomous scoring is given by:\n\\[\\begin{gather*}\nf =\n\\begin{cases}\n  1, \\text{if } i = n \\\\    \n  0, \\text{otherwise}    \n\\end{cases}\n\\end{gather*}\\] 0 i n\n\n\nCODE\nf_dichom <- function(i, n) {\n \n  # print(paste(\"i is :\",i,\" n is:\",n)) \n  \n  #if (n == 0 ) return error \n  ifelse( (n == 0), print(\"ERROR n can't be 0\"), \"\")\n  \n  #if (i > n ) return error \n  ifelse( (i > n), print(\"i n can't > n\"), \"\")\n  \n  #if (i==n) return 1, else 0\n  return (ifelse( (i==n), 1 , 0))\n \n}\n\n\n\n\nPartial Scoring [-1/n, +1/n]\nPartial Scoring refers to a class or scoring schemes that award the respondent partial credit depending on pattern of options they select. Schmidt et al. (2021) identify twenty-six different partial credit scoring schemes in the literature, varying in the range of possible scores, and the relative weighting of incorrectly selected (vs) incorrectly unselected options.\nA particularly elegant approach to partial scoring is referred to as the \\([-1/n, +1/n]\\) approach ( Schmidt et al. (2021) #17). This approach is appealing in the context of SGC3A, because it: (1) takes into account all information provided by the respondent: the pattern of what the select, and choose not to select.\nIn Partial Scoring \\([-1/n, +1/n]\\):\n\nScores range from [-1, +1]\nOne point is awarded if all options are correct\nOne point point is subtracted if all options are incorrect.\nIntermediate results are credited as fractions accordingly (\\(+1/n\\) for each correct, \\(-1/n\\) for each incorrect)\nThis results in at chance performance (i.e. half of the given options marked correctly), being awarded 0 points are awarded\n\nThis scoring is more consistent with the motivating theory that Triangular Graph readers start out with an incorrect (i.e. orthogonal, cartesian) interpretation of the coordinate system, and transition to a correct (i.e. triangular) interpretation. But the first step in making this transition is realizing the cartesian interpretation is incorrect, which may yield blank responses where the respondent is essentially saying, ‘there is no correct answer to this question’.\nSchmidt et al. (2021) describe the Partial \\({[-1/n, +1/n]}\\) scoring scheme as the only scoring method (of the 27 described) where respondents’ scoring results can be interpreted as a percentage of their true knowledge. One important drawback of this method is that a respondent may receive credit (a great deal of credit, depending on the number of answer options n) even if she did not select any options. In the case (such as ours) where there are many more response options \\(n\\) than there are options meant to be selected \\(p\\), this partial scoring algorithm poses a challenge because the respondent can achieve an almost completely perfect score by selecting a small number of options that should not be selected.\nThe algorithm for partial scoring\\([-1/n, +1/n]\\) is given by:\n\\[\\begin{align}\nf &= (1/n * i) - (1/n * (n-i)) \\\\\n&= (2i - n)/{n}\n\\end{align}\\]\n\n\nCODE\nf_partialN <- function(i, n) {\n\n# print(paste(\"i is :\",i,\" n is:\",n))\n\n#if(n==0) return error\nifelse((n==0),print(\"ERROR: n should not be 0\"),\"\")\n\n#if(i >n ) return error\nifelse((i > n),print(\"ERROR: i CANNOT BE GREATER THAN n\"),\"\")\n\nreturn ((2*i - n) / n) \n}\n\n\n\n\nPartial Scoring [-1/q, +1/p]\nOne drawback of the Partial Scoring \\([-1/n, +1/n]\\) approach is that treats the choice to select, and choice to not select options as equally indicative of the respondent’s understanding. That is to say, incorrectly selecting one particular option is no more or less informative than incorrectly not-selecting a different item. This represents an important difference between MAMC (i.e. “select all correct options”) vs MTF (i.e. “Mark each option as true or false”) questions.\nIn our study, the selection of any particular option (remember options represent data points on the stimulus graph) is indicative of a particular interpretation of the stimulus. Incorrectly selecting an option indicates an interpretation of the graph with respect to that particular option. Alternatively, failing to select a correct option might mean the individual has a different interpretation, or that they failed to find all the data points consistent with the interpretation.\nFor this reason, we consider another alternative Partial Scoring scheme that takes into consideration only the selected statements, without penalizing statements incorrectly not selected. (See Schmidt et al. (2021) method #26; also referred to as the Morgan-Method) This partial scoring scheme takes into consideration that the most effort-free (or ‘default’) response for any given item is the null, or blank response. Blank responses indicate no understanding, perhaps confusion, or refusal to answer. These lack of responses are awarded zero credit. Whereas taking the action to select an incorrect option is effortful, and is indicative of incorrect understanding.\nPartial Scoring \\([-1/q, +1/p]\\):\n\nawards +1/p for each correctly selected option (\\(p_s\\)), and subtracts \\(1/(n-p) = 1/q\\) for each incorrectly selected option (\\(q_s\\))\nonly considers selected options; does not penalize nor reward unselected options\n\nProperties of Item\n\\[\\begin{align}\np &= \\text{number of true-select options (i.e. should be selected)} \\\\\nq &= \\text{number of true-unselect options (i.e. should not be selected)} \\\\\nn &= \\text{number of options} \\: ( n = p + q)\n\\end{align}\\]\nProperties of Response\n\\[\\begin{align}\np_s &= \\text{number of true-select options selected (i.e. number of correctly checked options)}\\\\\nq_s &= \\text{number of true-unselect options selected (i.e. number of incorrectly checked options }\n\\end{align}\\]\nThe algorithm for partial scoring \\([-1/q, +1/p]\\) is given by:\n\\[\\begin{align}\nf &= (p_s / p) - ({q_s}/{q}) \\\\\n\\end{align}\\]\n\n\nCODE\nf_partialP <- function(t,p,f,q) {\n\n  #t = number of correct-selected options\n  #p = number of true options\n  #f = number of incorrect-selected options\n  #q = number of false options\n  #n = number of options + p + q\n  \n  ifelse( (p == 0), return(NA), \"\") #handle empty response set gracefully by returning nothing rather than 0\n  ifelse( (p != 0), return( (t / p) - (f/q)), \"\")\n}\n\n\n\n\n\nComparison of Schemes\nWhich scoring scheme is most appropriate for the goals of the graph comprehension task?\nConsider the following example:\nFor a question with \\(n = 5\\) response options (data points A, B, C, D and E) with a correct response of A, the schemes under consideration yield the following scores:\n\n\nCODE\ntitle <- \"Comparison of Scoring Schemes for n = 5 options [ A,B,C,D,E ]\"\n\ncorrect <- c( \"A____\",  \n              \"A____\",      \n              \"A____\",        \n              \"A____\",        \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\" ) \n\nresponse <- c(\"A____\",  \n              \"AB___\",      \n              \"A___E\",      \n              \"AB__E\",        \n              \"____E\",\n              \"___DE\",\n              \"_BCDE\",      \n              \"ABCDE\",      \n              \"_____\" )\n\ni <- c(        5,       \n               4,              \n               4,              \n               3,               \n               \n               3,\n               2,\n               0,\n               1,\n               4)\n\nabs <- c(f_dichom(5,5), \n         f_dichom(4,5), \n         f_dichom(4,5), \n         f_dichom(3,5), \n         \n         f_dichom(3,5), \n         f_dichom(2,5),\n         f_dichom(0,5),\n         f_dichom(1,5),\n         f_dichom(4,5))\n\npartial1 <- c(f_partialN(5,5), \n              f_partialN(4,5), \n              f_partialN(4,5), \n              f_partialN(3,5), \n              \n              f_partialN(3,5), \n              f_partialN(2,5),\n              f_partialN(0,5),\n              f_partialN(1,5),\n              f_partialN(4,5))\n\npartial2 <- c(f_partialP(1,1,0,4), \n              f_partialP(1,1,1,4), \n              f_partialP(1,1,1,4), \n              f_partialP(1,1,2,4), \n              \n              f_partialP(0,1,1,4),\n              f_partialP(0,1,2,4),\n              f_partialP(0,1,4,4),\n              f_partialP(1,1,4,4), \n              f_partialP(0,1,0,4))\n\nnames = c(    \"Correct Answer\",\n              \"Response\",\n              \"i \",\n              \"Dichotomous\",\n              \"Partial [-1/n, +1/n]\",\n              \"Partial[-1/q, +1/p]\")\n\ndt <- data.frame(correct, response, i, abs, partial1 , partial2)\n\nkbl(dt, col.names = names, caption = title, digits=3) %>%\n  kable_classic() %>%\n    add_header_above(c(\"Response Scenario \" = 3, \"Scores\" = 3)) %>% \n    pack_rows(\"Perfect Response\", 1, 1) %>%\n    pack_rows(\"Correct + Extra Incorrect Selections\", 2, 4) %>%\n    pack_rows(\"Only Incorrect Selections\", 5, 6) %>%\n    pack_rows(\"Completely Inverse Response \", 7, 7) %>%\n    pack_rows(\"Selected ALL or NONE\", 8, 9) %>%\n    footnote(general = paste(\"i = number of options in correct state; _ indicates option not selected\"),\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nComparison of Scoring Schemes for n = 5 options [ A,B,C,D,E ]\n \n\nResponse Scenario \nScores\n\n  \n    Correct Answer \n    Response \n    i  \n    Dichotomous \n    Partial [-1/n, +1/n] \n    Partial[-1/q, +1/p] \n  \n \n\n  Perfect Response\n\n    A____ \n    A____ \n    5 \n    1 \n    1.0 \n    1.00 \n  \n  Correct + Extra Incorrect Selections\n\n    A____ \n    AB___ \n    4 \n    0 \n    0.6 \n    0.75 \n  \n  \n    A____ \n    A___E \n    4 \n    0 \n    0.6 \n    0.75 \n  \n  \n    A____ \n    AB__E \n    3 \n    0 \n    0.2 \n    0.50 \n  \n  Only Incorrect Selections\n\n    A____ \n    ____E \n    3 \n    0 \n    0.2 \n    -0.25 \n  \n  \n    A____ \n    ___DE \n    2 \n    0 \n    -0.2 \n    -0.50 \n  \n  Completely Inverse Response \n\n    A____ \n    _BCDE \n    0 \n    0 \n    -1.0 \n    -1.00 \n  \n  Selected ALL or NONE\n\n    A____ \n    ABCDE \n    1 \n    0 \n    -0.6 \n    0.00 \n  \n  \n    A____ \n    _____ \n    4 \n    0 \n    0.6 \n    0.00 \n  \n\n\nNote:   i = number of options in correct state; _ indicates option not selected\n\n\n\n\nCODE\n#cleanup\nrm(dt, abs, correct,i,names,partial1,partial2,response,title)\n\n\n\nWe see that in the Dichotomous scheme, only the precisely correct response (row 1) yields a score other than zero. This scheme does now allow us to differentiate between different response patters.\nThe Partial \\([-1/n, +1/n]\\) scheme yields a range from \\([-1,1]\\), differentiating between responses. However, a blank response (bottom row) receives the same score (0.6) as the selection of the correct option + 1 incorrect option (row 2), which is problematic with for the goals of this study, where we need to differentiate between states of confusion or uncertainty yielding blank responses and the intentional selection of incorrect items.\nThe Partial \\([-1/q, +1/p]\\) scheme also yields a range of scores from \\([-1,1]\\). A blank response (bottom row) yields the same score (\\(0\\)) as the selection of all answer options (row 7); both are patterns of behavior we would expect to see if a respondent is confused or uncertain that there is a correct answer to the question.\n\nNext we consider an example from our study, with \\(n = 15\\) options and \\(p = 1\\) correct option to be selected.\n\n\nCODE\ntitle <- \"Comparison of Scoring Schemes for SGC3 with n=15 and p=1 options [A,B...N,O]  \"\n\ncorrect <- c( \"A____\",  \n              \"A____\",      \n              \"A____\",      \n              \"A____\",        \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\" ) \n\nresponse <- c(\"A__...__\",  \n              \"AB_...__\",      \n              \"A__..._O\",      \n              \"AB_..._O\",        \n              \"___..._O\",      \n              \"___...NO\",      \n              \"_BC...NO\",\n              \"ABC...NO\",      \n              \"___...__\" )\n\ni <- c(        15,       \n               14,              \n               14,              \n               13,\n               13,               \n               12,          \n               0,\n               1,\n               14)\n\nabs <- c(f_dichom(15,15), \n         f_dichom(14,15), \n         f_dichom(14,15), \n         f_dichom(13,15), \n         f_dichom(13,15),\n         f_dichom(12,15),\n         f_dichom(0,15),\n         f_dichom(1,15),\n         f_dichom(14,15))\n\npartial1 <- c(f_partialN(15,15), \n              f_partialN(14,15), \n              f_partialN(14,15), \n              f_partialN(13,15), \n              f_partialN(13,15),\n              f_partialN(12,15),\n              f_partialN(0,15),\n              f_partialN(1,15),\n              f_partialN(14,15))\n\npartial2 <- c(f_partialP(1,1,0,14), \n              f_partialP(1,1,1,14), \n              f_partialP(1,1,1,14), \n              f_partialP(1,1,2,14), \n              f_partialP(0,1,1,14),\n              f_partialP(0,1,2,14),\n              f_partialP(0,1,14,14),\n              f_partialP(1,1,14,14), \n              f_partialP(0,1,0,14))\n\nnames = c(    \"Correct Answer\",\n              \"Response\",\n              \"$i$ \",\n              \"Dichotomous\",\n              \"Partial [-1/n, +1/n]\",\n              \"Partial [-1/q, +1/p]\")\n\ndt <- data.frame(correct, response, i, abs, partial1 , partial2)\n\nkbl(dt, col.names = names, caption = title, digits=3) %>%\n  kable_classic() %>%\n    add_header_above(c(\"Response Scenario \" = 3, \"Scores\" = 3)) %>% \n    pack_rows(\"Perfect Response\", 1, 1) %>%\n    pack_rows(\"Correct + Extra Incorrect Selections\", 2, 4) %>%\n    pack_rows(\"Only Incorrect Selections\", 5, 6) %>%\n    pack_rows(\"Completely Inverse Response \", 7, 7) %>%\n    pack_rows(\"Selected ALL or NONE\", 8, 9) %>%\n    footnote(general = paste(\"i = number of options in correct state; _ indicates option not selected\"),\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nComparison of Scoring Schemes for SGC3 with n=15 and p=1 options [A,B...N,O]  \n \n\nResponse Scenario \nScores\n\n  \n    Correct Answer \n    Response \n    $i$  \n    Dichotomous \n    Partial [-1/n, +1/n] \n    Partial [-1/q, +1/p] \n  \n \n\n  Perfect Response\n\n    A____ \n    A__...__ \n    15 \n    1 \n    1.000 \n    1.000 \n  \n  Correct + Extra Incorrect Selections\n\n    A____ \n    AB_...__ \n    14 \n    0 \n    0.867 \n    0.929 \n  \n  \n    A____ \n    A__..._O \n    14 \n    0 \n    0.867 \n    0.929 \n  \n  \n    A____ \n    AB_..._O \n    13 \n    0 \n    0.733 \n    0.857 \n  \n  Only Incorrect Selections\n\n    A____ \n    ___..._O \n    13 \n    0 \n    0.733 \n    -0.071 \n  \n  \n    A____ \n    ___...NO \n    12 \n    0 \n    0.600 \n    -0.143 \n  \n  Completely Inverse Response \n\n    A____ \n    _BC...NO \n    0 \n    0 \n    -1.000 \n    -1.000 \n  \n  Selected ALL or NONE\n\n    A____ \n    ABC...NO \n    1 \n    0 \n    -0.867 \n    0.000 \n  \n  \n    A____ \n    ___...__ \n    14 \n    0 \n    0.867 \n    0.000 \n  \n\n\nNote:   i = number of options in correct state; _ indicates option not selected\n\n\n\n\nCODE\n#cleanup\nrm(dt, abs, correct,i,names,partial1,partial2,response,title)\n\n\nHere again we see that the Partial \\([-1/q, +1/p]\\) scheme allows us differentiate between patterns of responses, in a way that is more sensible for the goals of the SGC3 graph comprehension task.\n\n\n\n\n\n\nDecision\n\n\n\nThe Partial \\([-1/q, +1/p]\\) scheme is more appropriate for scoring the graph comprehension task than the Partial \\([-1/n, +1/n]\\) scheme because it allows us to differentially penalize incorrectly selected and incorrectly not selected answer options."
  },
  {
    "objectID": "analysis/utils/scoring.html#sec-scoringOverview",
    "href": "analysis/utils/scoring.html#sec-scoringOverview",
    "title": "Scoring Strategy",
    "section": "SCORING SGC DATA",
    "text": "SCORING SGC DATA\nIn SGC_3A we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key.\n5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\nFor each study in the SGC project, MR data will be scored by following these steps:\n(1) Preparing answer keys: For each dataset+question set combination, an answer key is that defines the ‘correct’ answer set under each interpretation of the graph (i.e. a triangular answer, an orthogonal answer, etc).\n(2) Calculate strategy scores: Using the strategy specific answer keys, an interpretation subscore is calculated for each response for each interpretation.\n(3) Interpretation classification: The interpretation subscores are compared in order to classify each response as a particular interpretation. If no classification can be made, the response is classified as ‘?’.\n(4) Calculate Absolute and Scaled Scores: Two final scores are calculated for each response; an Absolute score that indicates if the response was precisely correct according to the triangular interpretation, and a Scaled score that assigns a numeric value to the interpretation given by the response (ranging from -1 to +1)\n\n1. Prepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#LOAD INDIVIDUAL KEY FILES \nkey_111_raw <- read_csv('analysis/utils/keys/SGCX_scaffold_111_key.csv') %>% mutate(condition = \"DEFAULT\", phase = \"scaffold\")\nkey_121_raw <- read_csv('analysis/utils/keys/SGCX_scaffold_121_key.csv')%>% mutate(condition = 121, phase = \"scaffold\")\ncs = rep('c', 23) %>% str_c(collapse=\"\") #create column spec \nkey_test_raw <- read_csv('analysis/utils/keys/SGCX_test_key.csv', col_types = cs)%>% mutate(condition = \"DEFAULT\", phase = \"test\") \n\n#JOIN THEM\nkeys_raw <- rbind(key_111_raw, key_121_raw, key_test_raw )\n\n#CLEANUP\nrm(key_111_raw, key_121_raw, key_test_raw)\n\n\nIn order to calculate scores using the \\([-1/q, +1/p]\\) algorithm, we need to define the subset of all response options (set N) that should be selected (set P) and should not be selected (set Q). In order to calculate subscores for each graph interpretation (i.e. triangular, orthogonal, tversky) we must define these sets independently for each interpretation. For each question, the keys_raw dataframe gives us set N (all response options), and a set P (options that should be selected) for each interpretation. From these we must derive set Q for each interpretation.\n\nSET \\(N\\), all response options (superset) . This set is the same across all interpretations (a property of the question) and is given in the answer key.\nSET \\(P\\), \\(P \\subset N\\) , the subset of options that should be selected (rewarded as +1/p) . This set differs by interpretation, and is given in the answer key.\nSET \\(A, A \\subset N, A \\sqcup P\\) , the subset of options that should not be selected, but if they are, aren’t penalized (i.e. these options are ignored. Not rewarded, nor penalized). These include any options referenced in the question (i.e. select shifts that start at the same time as X; don’t penalize if they also select ‘X’), as well as options within 0.5hr offset from the data point to accommodate reasonable visual errors. This set differs by interpretation, and is given in the answer key (columns REF_POINT and _also).\nSET \\(Q\\), the subset of options that should not be selected and are penalized (as -1/q). This set differs by interpretation and is not given in the answer key. We can derive set Q for each interpretation by \\(Q = N - (P \\cup A)\\)\n\nThe next step in scoring is preparing interpretation-specific answer keys that specify sets N, P, A and Q for each question.\n\nTriangular Key\nFirst we construct a key set based on the ‘Triangular’ interpretation (i.e. the actually correct answers).\n\n\nCODE\nverify_tri = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TRIANGULAR KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tri <- keys_raw %>% \n  select(Q, condition, OPTIONS, TRIANGULAR, TRI_allow, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    TRI_allow = str_replace_na(TRI_allow,\"\"),\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TRIANGULAR,\n    set_p = str_replace_na(set_p,\"\"),#replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TRI_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"),#replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DEFINE SETS N, P, A\nfor (x in 1:nrow(keys_tri)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tri[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tri[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tri[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tri[x,'set_q'] = Q\n  keys_tri[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tri[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tri <- keys_tri %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tri <- keys_tri %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup \nrm(N,A,P,Q,n_q,s,x,tempunion)\n\n\nThis leaves us a dataframe keys_tri that define the sets of response options consistent with the triangular graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each option in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nOrthogonal Key\nNext we construct a key set based on the ‘Orthogonal’ interpretation.\n\n\nCODE\nverify_orth = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT ORTHOGONAL KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_orth <- keys_raw %>% \n  select(Q, condition, OPTIONS, ORTHOGONAL, ORTH_allow, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    ORTH_allow = str_replace_na(ORTH_allow,\"\"),\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = ORTHOGONAL,\n    set_p = str_replace_na(set_p,\"\"),#replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(ORTH_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answer options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_orth)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_orth[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_orth[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_orth[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_orth[x,'set_q'] = Q\n  keys_orth[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  # print(tempunion)\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n\n  verify_orth[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_orth <- keys_orth %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_orth <- keys_orth %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x, cs)\n\n\nThis leaves us a dataframe keys_orth that define the sets of response options consistent with the orthogonal graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nTversky Keys\nNext we construct the key set based on a partial-understanding strategy we refer to as ‘Tversky’. We use the label Tversky as shorthand for a partial interpretation of the coordinate system where subjects select a set of responses that lay along a connecting line from the referenced data point or referenced time for that item. The term is named for Barbara Tversky based on her work on graphical primitives (e.g. “lines connect, arrows direct, boxes contain”).\n\n\nCODE\nverify_max = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-MAX\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_max <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_max, TV_max_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_max = str_replace_na(TV_max,\"\"),\n    TV_max_allow = str_replace_na(TV_max_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_max,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_max_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_max)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_max[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_max[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_max[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_max[x,'set_q'] = Q\n  keys_tversky_max[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_max[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_max <- keys_tversky_max %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_max <- keys_tversky_max %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\nverify_tversky_start = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-START\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_start <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_start, TV_start_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_start = str_replace_na(TV_start,\"\"),\n    TV_start_allow = str_replace_na(TV_start_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_start,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_start_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_start)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_start[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_start[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_start[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_start[x,'set_q'] = Q\n  keys_tversky_start[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_start[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_start <- keys_tversky_start %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_start <- keys_tversky_start %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\nverify_tversky_end = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-END\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_end <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_end, TV_end_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_end = str_replace_na(TV_end,\"\"),\n    TV_end_allow = str_replace_na(TV_end_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_end,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_end_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_end)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_end[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_end[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_end[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_end[x,'set_q'] = Q\n  keys_tversky_end[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_end[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_end <- keys_tversky_end %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_end <- keys_tversky_end %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\nverify_tversky_duration = c()\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-DURATION\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_duration <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_dur, TV_dur_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_dur = str_replace_na(TV_dur,\"\"),\n    TV_dur_allow = str_replace_na(TV_dur_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_dur,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_dur_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_duration)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_duration[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_duration[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_duration[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_duration[x,'set_q'] = Q\n  keys_tversky_duration[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_duration[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_duration <- keys_tversky_duration %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_duration <- keys_tversky_duration %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\nThis leaves us four dataframes, each corresponding to a different variant of a ‘lines connecting to reference point’ strategy.\n- keys_tversky_max : the superset of lines connecting options - keys_tversky_start : lines connecting to the rightward diagonal (start time) of the reference point - keys_tversky_end: lines connecting to the leftward diagonal (end time) of the reference point - keys_tversky_duration: lines connecting to the horizontal y-intercept (duration) of the reference point\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nSatisficing Key\nNext we construct two keys based on the ‘Satisficing’ strategy. Satisficing involves selecting any data points within 0.5hr visual offset of the orthogonal interpretation of the graph (because no orthogonal response option is available). One key represents selecting a point slightly to the left of the orthogonal, and the other key represents selecting a point slightly to the right of the orthogonal. The “Satisficing” strategy involves the reader selecting data points nearest to the orthogonal projection from the reference point in the question. We observe this strategy in some readers when there is no orthogonal response available (i.e. in the impasse condition), so they select the points nearest to the projection (i.e. “close enough”).\n\n\nCODE\nverify_satisfice_right = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT SATISFICE RIGHT KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_satisfice_right <- keys_raw %>% \n  select(Q, condition, OPTIONS, SATISFICE_right, REF_POINT) %>% \n  mutate(\n    #replace NAs\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n\n    #P options that SHOULD be selected (rewarded)\n    set_p = SATISFICE_right,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n\n    #A options that are ignored if selected\n    set_a = str_c(REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n\n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_satisfice_right)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_satisfice_right[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_satisfice_right[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_satisfice_right[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to data frame\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_satisfice_right[x,'set_q'] = Q\n  keys_satisfice_right[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_satisfice_right[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_satisfice_right <- keys_satisfice_right %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q)%>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_satisfice_right <- keys_satisfice_right %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\n\nverify_satisfice_left = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT SATISFICE left KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_satisfice_left <- keys_raw %>% \n  select(Q, condition, OPTIONS, SATISFICE_left, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = SATISFICE_left,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_satisfice_left)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_satisfice_left[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_satisfice_left[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_satisfice_left[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to data frame\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_satisfice_left[x,'set_q'] = Q\n  keys_satisfice_left[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_satisfice_left[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_satisfice_left <- keys_satisfice_left %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q)%>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_satisfice_left <- keys_satisfice_left %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\nThis leaves us a dataframe keys_satisfice that define the sets of response options consistent with the orthogonal graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nCODE\n#cleanup\nrm(verify_tri, verify_orth, verify_max, verify_tversky_duration, verify_tversky_end, verify_tversky_start, verify_satisfice_right, verify_satisfice_left)\n\n\nFinally, we need to clean up and generalize our answer keys to accommodate the experimental conditions for Study SGC4-SGC5. In both of these studies the answer set (and underlying graphed data set) are identical, the conditions differ only based on the structure of the gridlines or marks used to represent the data, or interactive mode of the answer format.\n\n\nCODE\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nwrite.csv(keys_raw,\"analysis/utils/keys/parsed_keys/keys_raw\", row.names = FALSE)\nwrite.csv(keys_orth,\"analysis/utils/keys/parsed_keys/keys_orth\", row.names = FALSE)\nwrite.csv(keys_tri,\"analysis/utils/keys/parsed_keys/keys_tri\", row.names = FALSE)\nwrite.csv(keys_satisfice_left,\"analysis/utils/keys/parsed_keys/keys_satisfice_left\", row.names = FALSE)\nwrite.csv(keys_satisfice_right,\"analysis/utils/keys/parsed_keys/keys_satisfice_right\", row.names = FALSE)\nwrite.csv(keys_tversky_duration,\"analysis/utils/keys/parsed_keys/keys_tversky_duration\", row.names = FALSE)\nwrite.csv(keys_tversky_end,\"analysis/utils/keys/parsed_keys/keys_tversky_end\", row.names = FALSE)\nwrite.csv(keys_tversky_max,\"analysis/utils/keys/parsed_keys/keys_tversky_max\", row.names = FALSE)\nwrite.csv(keys_tversky_start,\"analysis/utils/keys/parsed_keys/keys_tversky_start\", row.names = FALSE)\n\n\n\n\n\n2. Calculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangular interpretation?\nscore_ORTH How consistent is the response with the orthogonal interpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation?\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\nTo facilitate scoring, we import the following helper functions in each scoring script.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\nprint(calc_subscore)\n\n\nfunction (question, cond, response, keyframe) \n{\n    if (cond == 121 & question < 6) {\n        p = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% \n            unlist()\n        q = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% \n            unlist()\n        pn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(n_p)\n        qn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(n_q)\n    }\n    else {\n        p = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(set_p) %>% pull(set_p) %>% \n            str_split(\"\") %>% unlist()\n        q = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(set_q) %>% pull(set_q) %>% \n            str_split(\"\") %>% unlist()\n        pn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(n_p)\n        qn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(n_q)\n    }\n    if (response != \"\") {\n        response = response %>% str_split(\"\") %>% unlist()\n    }\n    ps = length(intersect(response, p))\n    qs = length(intersect(response, q))\n    x = f_partialP(ps, pn, qs, qn) %>% unlist() %>% as.numeric()\n    rm(p, q, pn, qn, ps, qs)\n    return(x)\n}\n\n\n\n\n3. Derive Interpretation\nNext, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\nprint(derive_interpretation)\n\n\nfunction (df) \n{\n    threshold_range = 0.5\n    threshold_frenzy = 4\n    for (x in 1:nrow(df)) {\n        t = df[x, ] %>% dplyr::select(score_TV_max, score_TV_start, \n            score_TV_end, score_TV_duration)\n        t.long = gather(t, score, value, 1:4)\n        t.long[t.long == \"\"] = NA\n        if (length(unique(t.long$value)) == 1) {\n            if (is.na(unique(t.long$value))) {\n                df[x, \"score_TVERSKY\"] = NA\n                df[x, \"tv_type\"] = NA\n            }\n        }\n        else {\n            df[x, \"score_TVERSKY\"] = as.numeric(max(t.long$value, \n                na.rm = TRUE))\n            df[x, \"tv_type\"] = t.long[which.max(t.long$value), \n                \"score\"]\n        }\n        t = df[x, ] %>% dplyr::select(score_SAT_left, score_SAT_right)\n        t.long = gather(t, score, value, 1:2)\n        t.long[t.long == \"\"] = NA\n        if (length(unique(t.long$value)) == 1) {\n            if (is.na(unique(t.long$value))) {\n                df[x, \"score_SATISFICE\"] = NA\n                df[x, \"sat_type\"] = NA\n            }\n        }\n        else {\n            df[x, \"score_SATISFICE\"] = as.numeric(max(t.long$value, \n                na.rm = TRUE))\n            df[x, \"sat_type\"] = t.long[which.max(t.long$value), \n                \"score\"]\n        }\n        t = df[x, ] %>% dplyr::select(score_TRI, score_TVERSKY, \n            score_SATISFICE, score_ORTH)\n        t.long = gather(t, score, value, 1:4)\n        t.long[t.long == \"\"] = NA\n        df[x, \"top_score\"] = as.numeric(max(t.long$value, na.rm = TRUE))\n        df[x, \"top_type\"] = t.long[which.max(t.long$value), \"score\"]\n        r = as.numeric(range(t.long$value, na.rm = TRUE))\n        r = diff(r)\n        df[x, \"range\"] = r\n        if (r < threshold_range) {\n            df[x, \"best\"] = \"?\"\n        }\n        else {\n            p = df[x, \"top_type\"]\n            if (p == \"score_TRI\") {\n                df[x, \"best\"] = \"Triangular\"\n            }\n            else if (p == \"score_ORTH\") {\n                df[x, \"best\"] = \"Orthogonal\"\n            }\n            else if (p == \"score_TVERSKY\") {\n                df[x, \"best\"] = \"Tversky\"\n            }\n            else if (p == \"score_SATISFICE\") {\n                df[x, \"best\"] = \"Satisfice\"\n            }\n        }\n        if (!is.na(df[x, \"score_BOTH\"])) {\n            if (df[x, \"score_BOTH\"] == 1) {\n                df[x, \"best\"] = \"both tri + orth\"\n            }\n        }\n        if (df[x, \"num_o\"] == 0) {\n            df[x, \"best\"] = \"blank\"\n        }\n        if (df[x, \"num_o\"] > threshold_frenzy) {\n            df[x, \"best\"] = \"frenzy\"\n        }\n        if (!is.na(df[x, \"score_REF\"])) {\n            if (df[x, \"score_REF\"] == 1) {\n                df[x, \"best\"] = \"reference\"\n            }\n        }\n    }\n    rm(t, t.long, x, r, p)\n    rm(threshold_frenzy, threshold_range)\n    df$int2 <- factor(df$best, levels = c(\"Triangular\", \"Tversky\", \n        \"Satisfice\", \"Orthogonal\", \"reference\", \"both tri + orth\", \n        \"blank\", \"frenzy\", \"?\"))\n    df$interpretation <- factor(df$best, levels = c(\"Orthogonal\", \n        \"Satisfice\", \"frenzy\", \"?\", \"reference\", \"blank\", \"both tri + orth\", \n        \"Tversky\", \"Triangular\"))\n    df$high_interpretation <- fct_collapse(df$interpretation, \n        orthogonal = c(\"Satisfice\", \"Orthogonal\"), neg.trans = c(\"frenzy\", \n            \"?\"), neutral = c(\"reference\", \"blank\"), pos.trans = c(\"Tversky\", \n            \"both tri + orth\"), triangular = \"Triangular\")\n    df$tv_type = as.factor(df$tv_type)\n    df$top_type = as.factor(df$top_type)\n    df$high_interpretation = factor(df$high_interpretation, levels = c(\"orthogonal\", \n        \"neg.trans\", \"neutral\", \"pos.trans\", \"triangular\"))\n    df <- df %>% dplyr::select(-best)\n    return(df)\n}\n\n\n\n\n4. Derive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\nprint(calc_scaled)\n\n\nfunction (v) \n{\n    v <- recode(v, Orthogonal = -1, Satisfice = -1, frenzy = -0.5, \n        `?` = -0.5, reference = 0, blank = 0, `both tri + orth` = 0.5, \n        Tversky = 0.5, Triangular = 1)\n    return(v)\n}\n\n\n\n\n5. Summarize by Subject\nThe final step in the scoring procedure is to summarise the item-level scores by subject, and save certain summaries to the subject-level record. We also construct two long-format dataframes containing cummulative progress scores (the point-in-time [absolute, scaled] scores for each subject on each question).\n\n\nCODE\nprint(summarise_bySubject)\n\n\nfunction (subjects, items) \n{\n    subjects_summary <- items %>% filter(q %nin% c(6, 9)) %>% \n        group_by(subject) %>% dplyr::summarise(subject = as.character(subject), \n        s_TRI = sum(score_TRI, na.rm = TRUE), s_ORTH = sum(score_ORTH, \n            na.rm = TRUE), s_TVERSKY = sum(score_TVERSKY, na.rm = TRUE), \n        s_SATISFICE = sum(score_SATISFICE, na.rm = TRUE), s_REF = sum(score_REF, \n            na.rm = TRUE), s_ABS = sum(score_ABS, na.rm = TRUE), \n        s_NABS = sum(score_niceABS, na.rm = TRUE), s_SCALED = sum(score_SCALED, \n            na.rm = TRUE), DV_percent_NABS = s_NABS/13, rt_m = sum(rt_s)/60, \n        item_avg_rt = mean(rt_s), item_min_rt = min(rt_s), item_max_rt = max(rt_s), \n        item_n_TRI = sum(interpretation == \"Triangular\"), item_n_ORTH = sum(interpretation == \n            \"Orthogonal\"), item_n_TV = sum(interpretation == \n            \"Tversky\"), item_n_SAT = sum(interpretation == \"Satisfice\"), \n        item_n_OTHER = sum(interpretation %nin% c(\"Triangular\", \n            \"Orthogonal\", \"Tversky\", \"Satisfice\")), item_n_POS = sum(high_interpretation == \n            \"pos.trans\"), item_n_NEG = sum(high_interpretation == \n            \"neg.trans\"), item_n_NEUTRAL = sum(high_interpretation == \n            \"neutral\")) %>% arrange(subject) %>% slice(1L)\n    subjects_q1 <- items %>% filter(q == 1) %>% mutate(item_q1_NABS = score_niceABS, \n        item_q1_SCALED = score_SCALED, item_q1_interpretation = interpretation, \n        item_q1_rt = rt_s, ) %>% dplyr::select(subject, item_q1_NABS, \n        item_q1_SCALED, item_q1_interpretation, item_q1_rt) %>% \n        arrange(subject)\n    subjects_q5 <- items %>% filter(q == 5) %>% mutate(item_q5_NABS = score_niceABS, \n        item_q5_SCALED = score_SCALED, item_q5_interpretation = interpretation, \n        item_q5_rt = rt_s, ) %>% dplyr::select(subject, item_q5_NABS, \n        item_q5_SCALED, item_q5_interpretation, item_q5_rt) %>% \n        arrange(subject)\n    subjects_q7 <- items %>% filter(q == 7) %>% mutate(item_q7_NABS = score_niceABS, \n        item_q7_interpretation = interpretation, item_q7_rt = rt_s, \n        ) %>% dplyr::select(subject, item_q7_NABS, item_q7_interpretation, \n        item_q7_rt) %>% arrange(subject)\n    subjects_q15 <- items %>% filter(q == 15) %>% mutate(item_q15_NABS = score_niceABS, \n        item_q15_interpretation = interpretation, item_q15_rt = rt_s, \n        ) %>% dplyr::select(subject, item_q15_NABS, item_q15_interpretation, \n        item_q15_rt) %>% arrange(subject)\n    subjects_scaffold <- items %>% filter(q < 6) %>% group_by(subject) %>% \n        dplyr::summarise(item_scaffold_NABS = sum(score_niceABS), \n            item_scaffold_SCALED = sum(score_SCALED), item_scaffold_rt = sum(rt_s)/60) %>% \n        dplyr::select(subject, item_scaffold_NABS, item_scaffold_SCALED, \n            item_scaffold_rt) %>% arrange(subject)\n    subjects_test <- items %>% filter(q %nin% c(1, 2, 3, 4, 5, \n        6, 9)) %>% group_by(subject) %>% dplyr::summarise(item_test_NABS = sum(score_niceABS), \n        item_test_SCALED = sum(score_SCALED), item_test_rt = sum(rt_s)/60) %>% \n        dplyr::select(subject, item_test_NABS, item_test_SCALED, \n            item_test_rt) %>% arrange(subject)\n    print(unique(subjects_summary$subject == subjects$subject))\n    print(unique(subjects_summary$subject == subjects_q1$subject))\n    print(unique(subjects_summary$subject == subjects_q5$subject))\n    print(unique(subjects_summary$subject == subjects_q7$subject))\n    print(unique(subjects_summary$subject == subjects_q15$subject))\n    print(unique(subjects_summary$subject == subjects_scaffold$subject))\n    print(unique(subjects_summary$subject == subjects_test$subject))\n    x = merge(subjects, subjects_summary, by.x = \"subject\", by.y = \"subject\")\n    x = merge(x, subjects_q1)\n    x = merge(x, subjects_q5)\n    x = merge(x, subjects_q7)\n    x = merge(x, subjects_q15)\n    x = merge(x, subjects_scaffold)\n    x = merge(x, subjects_test)\n    subjects <- x\n    rm(subjects_q1, subjects_q5, subjects_q7, subjects_q15, subjects_scaffold, \n        subjects_test, subjects_summary, x)\n    return(subjects)\n}\n\n\nCODE\nprint(progress_Absolute)\n\n\nfunction (items) \n{\n    x <- items %>% filter(q %nin% c(6, 9)) %>% dplyr::select(subject, \n        mode, pretty_condition, q, score_niceABS)\n    wide <- x %>% pivot_wider(names_from = q, names_glue = \"q_{q}\", \n        values_from = score_niceABS)\n    wide$c1 = wide$q_1\n    wide$c2 = wide$c1 + wide$q_2\n    wide$c3 = wide$c2 + wide$q_3\n    wide$c4 = wide$c3 + wide$q_4\n    wide$c5 = wide$c4 + wide$q_5\n    wide$c6 = wide$c5 + wide$q_7\n    wide$c7 = wide$c6 + wide$q_8\n    wide$c8 = wide$c7 + wide$q_10\n    wide$c9 = wide$c8 + wide$q_11\n    wide$c10 = wide$c9 + wide$q_12\n    wide$c11 = wide$c10 + wide$q_13\n    wide$c12 = wide$c11 + wide$q_14\n    wide$c13 = wide$c12 + wide$q_15\n    wide <- wide %>% dplyr::select(subject, mode, pretty_condition, \n        c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13)\n    df_absolute_progress <- wide %>% pivot_longer(cols = c1:c13, \n        names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n    df_absolute_progress$question <- as.integer(df_absolute_progress$question)\n    rm(x, wide)\n    return(df_absolute_progress)\n}\n\n\nCODE\nprint(progress_Scaled)\n\n\nfunction (items) \n{\n    x <- items %>% filter(q %nin% c(6, 9)) %>% select(subject, \n        mode, pretty_condition, q, score_SCALED)\n    wide <- x %>% pivot_wider(names_from = q, names_glue = \"q_{q}\", \n        values_from = score_SCALED)\n    wide$c1 = wide$q_1\n    wide$c2 = wide$c1 + wide$q_2\n    wide$c3 = wide$c2 + wide$q_3\n    wide$c4 = wide$c3 + wide$q_4\n    wide$c5 = wide$c4 + wide$q_5\n    wide$c6 = wide$c5 + wide$q_7\n    wide$c7 = wide$c6 + wide$q_8\n    wide$c8 = wide$c7 + wide$q_10\n    wide$c9 = wide$c8 + wide$q_11\n    wide$c10 = wide$c9 + wide$q_12\n    wide$c11 = wide$c10 + wide$q_13\n    wide$c12 = wide$c11 + wide$q_14\n    wide$c13 = wide$c12 + wide$q_15\n    wide <- wide %>% select(subject, mode, pretty_condition, \n        c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13)\n    df_scaled_progress <- wide %>% pivot_longer(cols = c1:c13, \n        names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n    df_scaled_progress$question <- as.integer(df_scaled_progress$question)\n    rm(x, wide)\n    return(df_scaled_progress)\n}\n\n\n\n\n\n\n\n\nSchmidt, Dennis, Tobias Raupach, Annette Wiegand, Manfred Herrmann, and Philipp Kanzow. 2021. “Relation Between Examinees’ True Knowledge and Examination Scores: Systematic Review and Exemplary Calculations on Multiple-True-False Items.” Educational Research Review 34 (November): 100409. https://doi.org/10.1016/j.edurev.2021.100409."
  },
  {
    "objectID": "analysis/utils/modelling_ref.html",
    "href": "analysis/utils/modelling_ref.html",
    "title": "Modelling Reference",
    "section": "",
    "text": "In this notebook we use data from study SGC3A to explore different modelling techniques and assess their suitability for the bimodal accuracy distributions in the SGC project data."
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#continuous-outcome",
    "href": "analysis/utils/modelling_ref.html#continuous-outcome",
    "title": "Modelling Reference",
    "section": "CONTINUOUS OUTCOME",
    "text": "CONTINUOUS OUTCOME\nDoes CONDITION have an effect on TEST PHASE ABSOLUTE SCORE?\n(# questions correct on test phase of task, [in lab] participants)\n(Can also be transformed to proportion or percentage).\n\n\nCODE\n#::::::::::::SETUP DATA\ndf = df_subjects %>% filter(mode == \"lab-synch\")\n\n#::::::::::::DESCRIPTIVES\nmosaic::favstats(test_score ~ condition, data = df)\n\n\n  condition min Q1 median Q3 max mean   sd  n missing\n1   control   0  0      0  1   8 1.71 3.05 62       0\n2   impasse   0  0      2  7   8 3.33 3.40 64       0\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\n\n# #GGFORMULA | FACETED HISTOGRAM\n# stats = df %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))\n# gf_props(~item_test_NABS, \n#          fill = ~pretty_condition, data = df) %>% \n#   gf_facet_grid(~pretty_condition) %>% \n#   gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n#   labs(x = \"# Correct\",\n#        y = \"proportion of subjects\",\n#        title = \"Test Phase Absolute Score (# Correct)\",\n#        subtitle = \"\") + theme(legend.position = \"blank\")\n\n##GGPUBR | HIST+DENSITY SCORE \np <- gghistogram(df, x = \"test_score\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"condition\")) +\n  labs( title = \"Distribution of TEST Absolute Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"Total Absolute Score (Test Phase)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = condition, y = test_score,\n                        fill = condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=condition, y = test_score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=condition, y = test_score, color = condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\nIndependent Samples T-Test (Student’s T)\n\nTests null hypothesis that true difference in population mean is == 0\nAssumes normally distributed variables\nAssumes equal variance of samples (homogeneity of variance)\nSGC accuracy data violate both homogeneity and normality*\n\n\n\nCODE\n(t <- t.test( test_score ~ condition,data = df,\n              paired = FALSE, var.equal = TRUE, alternative = c(\"two.sided\"))) # less, greater for one sided tests\n\n\n\n    Two Sample t-test\n\ndata:  test_score by condition\nt = -3, df = 124, p-value = 0.006\nalternative hypothesis: true difference in means between group control and group impasse is not equal to 0\n95 percent confidence interval:\n -2.759 -0.478\nsample estimates:\nmean in group control mean in group impasse \n                 1.71                  3.33 \n\n\nCODE\nreport(t)\n\n\nWarning in .effectsize_t.test(model, type = type, verbose = verbose, ...):\nUnable to retrieve data from htest object. Using t_to_d() approximation.\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Two Sample t-test testing the difference of test_score by condition (mean in group control = 1.71, mean in group impasse = 3.33) suggests that the effect is negative, statistically significant, and medium (difference = -1.62, 95% CI [-2.76, -0.48], t(124) = -2.81, p = 0.006; Cohen's d = -0.50, 95% CI [-0.86, -0.15])\n\n\n\n\nCODE\n#STATSPLOT | VIOLIN\n\n#one tailed tests must be done manually by extracting results expression and adding as subtitle\n#default is two tailed test\n# results <- two_sample_test( data = df, x = pretty_condition, \n#                             y = item_test_NABS,\n#                             alternative = \"g\")\n\nggbetweenstats(y = test_score, x = condition, data = df,\n               type = \"parametric\", var.equal = TRUE,\n               alternative  = \"g\",\n               pairwide.display = \"significant\", ) \n\n\n\n\n\nCODE\n# + labs(subtitle = results$expression[[1]])\n\n\n\n\nIndependent Samples T-Test (Welch’s T)\n\nTests null hypothesis that true difference in population mean is == 0\nAssumes normally distributed variables\nDoes not assumes equal variance of samples (homogeneity of variance)\nSGC accuracy data violates normality assumption\n\n\n\nCODE\n(t <- t.test( test_score ~ condition,data = df,\n              paired = FALSE, var.equal = FALSE))\n\n\n\n    Welch Two Sample t-test\n\ndata:  test_score by condition\nt = -3, df = 123, p-value = 0.006\nalternative hypothesis: true difference in means between group control and group impasse is not equal to 0\n95 percent confidence interval:\n -2.76 -0.48\nsample estimates:\nmean in group control mean in group impasse \n                 1.71                  3.33 \n\n\nCODE\nreport(t)\n\n\nWarning in .effectsize_t.test(model, type = type, verbose = verbose, ...):\nUnable to retrieve data from htest object. Using t_to_d() approximation.\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of test_score by condition (mean in group control = 1.71, mean in group impasse = 3.33) suggests that the effect is negative, statistically significant, and medium (difference = -1.62, 95% CI [-2.76, -0.48], t(123.25) = -2.81, p = 0.006; Cohen's d = -0.51, 95% CI [-0.86, -0.15])\n\n\n\n\nCODE\n#STATSPLOT | VIOLIN\nggbetweenstats(y = test_score, x = condition, data = df,\n               type = \"parametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\n\n\nYuen’s T-Test (Trimmed Means)\n\nRobust alternative to to t-test is Yuen’s t-test which uses trimmed means\nTrimmed means are not desireable for this research scenario because they trim data from the extremes, and in this study these are true, interesting values\nhttps://garstats.wordpress.com/2017/11/28/trimmed-means/\n\n\n\nCODE\n(y <- yuenbt( formula = test_score ~ condition, data = df,\n              side = TRUE, EQVAR = FALSE))\n\n\nCall:\nyuenbt(formula = test_score ~ condition, data = df, side = TRUE, \n    EQVAR = FALSE)\n\nTest statistic: -2.99 (df = NA), p-value = 0.00835\n\nTrimmed mean difference:  -2.53 \n95 percent confidence interval:\n-4.15     -0.907 \n\n\n\n\nCODE\n#STATSPLOT | VIOLIN\nggbetweenstats(y = test_score, x = condition, data = df,\n               type = \"robust\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\n\n\nWilcoxon Rank Sum (Mann-Whitney Test)\n\nNon parametric alternative to t-test; compares median rather than mean by ranking data\nDoes not assume normality\nDoes not assume equal variance of samples (homogeneity of variance)\nAppropriate for SGC accuracy data\n\n\n\nCODE\n(w <- wilcox.test(df$test_score ~ df$condition,\n                 paired = FALSE, alternative = \"two.sided\")) #less, greater\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df$test_score by df$condition\nW = 1438, p-value = 0.003\nalternative hypothesis: true location shift is not equal to 0\n\n\nCODE\nreport(w)\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum test with continuity correction testing the difference in ranks between df$test_score and df$condition suggests that the effect is negative, statistically significant, and medium (W = 1438.00, p = 0.003; r (rank biserial) = -0.28, 95% CI [-0.45, -0.08])\n\n\n\n\nCODE\n#STATSPLOT | VIOLIN\nggbetweenstats(y = test_score, x = condition, data = df,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\n\n\nFloor/Ceiling Corrections + Wilcoxon Rank Sum\n\nThe bimodal distribution of the subject-level score data do not meet the requirements for t-tests.\nHowever, a non-parametric alternative is available (Wilcoxon rank sum test / Man-Whitney test)\nAdditional corrections are available for data with ‘floor’ and/or ‘ceiling’ effects via the ‘DACR’ package\nhttps://link.springer.com/article/10.3758/s13428-020-01407-2#Sec14 see also https://qmliu.shinyapps.io/DACFE/\n\nFor comparison we run a standard followed by a Wilcoxon rank-sum (Mann-Whitney) test that is a nonparametric alternative for non-normally distributed data.\n\n\nCODE\n(t <- wilcox.test(df$test_score ~ df$condition))\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df$test_score by df$condition\nW = 1438, p-value = 0.003\nalternative hypothesis: true location shift is not equal to 0\n\n\nNext, we calculate the t-test and ANOVA (F-test) based on a series of corrections provided for data with floor and/or ceiling effects.\n\nhttps://link.springer.com/article/10.3758/s13428-020-01407-2#Sec14\n\nUsing properties from truncated normal distributions, the authors propose an easy-to-use method for the t-test and ANOVA with ceiling/floor data.\nThe proposed method calculates the degrees of freedom based on the after-truncation sample sizes (where l = number of floor observations, and r = number of ceiling observations). The rationale was that the proposed method utilizes full information only from data points of n − r − l participants and partial information from data points of r + l participants of a group for the mean and variance estimation. Specifically, the corrected mean and variance estimates (Eqs. 14 and 15) are functions of mean and variance estimates using after-truncation data (n − r − l participants) and the standardized floor and ceiling threshold estimates. The thresholds are estimated using the ceiling and floor percentage estimates based on data points of n − r and n − l participants, respectively. This is a relatively conservative approach for calculating the degrees of freedom, which can help control the type I error rate. This feature can be beneficial, especially given the “replication crisis” in psychological and behavioral research.\n\n\nCODE\n#FLOOR-CEILING ADJUSTED T TESTS\nlibrary(DACF) #tests for data with floor and ceiling \n# https://www.rdocumentation.org/packages/DACF/versions/1.0.0\n\n#prepare data [vector of scores for each group]\nscore_111 <- df %>% filter(condition == \"control\") %>% dplyr::select(test_score) %>% pull()\nscore_121 <- df %>% filter(condition == \"impasse\") %>% dplyr::select(test_score) %>% pull()\n\n# recover the mean and variance for ceiling/floor data\na <- rec.mean.var(score_111) %>% unlist()\n# recover the mean and variance for ceiling/floor data\nb <- rec.mean.var(score_121) %>% unlist()\nr <- as.data.frame(rbind(\"control\"=a ,\"impasse\"=b))\nr\n\n\n        ceiling.percentage floor.percentage est.mean est.var\ncontrol              0.113            0.710    -7.70   207.4\nimpasse              0.172            0.422     2.39    51.8\n\n\nCODE\n# method \"a\" uses original sample size\n# method \"b\" uses after-truncation sample size\n\n# perform adjusted t test\nlw.t.test(score_111,score_121,\"b\")\n\n\n$statistic\n[1] -4.94\n\n$p.value\n[1] 0.000012\n\n$est.d\n[1] -0.887\n\n$conf.int\n[1] -14.22  -5.97\n\n\nCODE\n#FLOOR-CEILING ADJUSTED F* TEST ANOVA\nlw.f.star(df,test_score~condition,\"b\")\n\n\n$statistic\n[1] 5.85\n\n$p.value\n[1] 0.0321\n\n$est.f.squared\n[1] 0.158\n\n\nCODE\n# method \"a\" uses original sample size\n# method \"b\" uses after-truncation sample size\n\n\nThe control condition has 11% of data at ceiling and 71% at floor, with corrected mean of -7 and variance of 207. The impasse condition has 17% at ceiling, and only 42% at floor, with with corrected mean of 2.39 and variance respectively as of 58.\nA corrected t-test t statistic is -4.94, p = 0.05. The estimated Cohen’s d is -0.89 with a confidence interval 0f [-14.22, -5.97].\nA correct F-test (ANOVA) has a corrected F-statstic of 5.85, p < 0.05, and Fsquared effect size of 0.195\n\n\nLinear Regression\n\nAssumes homogeneity of variance\nAssumes normally distributed residuals\nwhen default dummy coding is used;\n\nintercept = predicted mean of first group,\npredictor coefficient = difference to mean of second group\n\n\n\n\nCODE\n#SCORE predicted by CONDITION\nlm.1 <- lm(test_score ~ condition, data = df)\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(lm.1)\n\n\n\nCall:\nlm(formula = test_score ~ condition, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3.33  -1.71  -1.71   3.67   6.29 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         1.710      0.411    4.16 0.000058 ***\nconditionimpasse    1.618      0.576    2.81   0.0058 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.23 on 124 degrees of freedom\nMultiple R-squared:  0.0598,    Adjusted R-squared:  0.0522 \nF-statistic: 7.89 on 1 and 124 DF,  p-value: 0.00579\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(lm.1)\n\n\nAnalysis of Variance Table\n\nResponse: test_score\n           Df Sum Sq Mean Sq F value Pr(>F)   \ncondition   1     82    82.5    7.89 0.0058 **\nResiduals 124   1297    10.5                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(lm.1)\n\n\n                 2.5 % 97.5 %\n(Intercept)      0.897   2.52\nconditionimpasse 0.478   2.76\n\n\nCODE\nreport(lm.1) #sanity check\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict test_score with condition (formula: test_score ~ condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.06, F(1, 124) = 7.89, p = 0.006, adj. R2 = 0.05). The model's intercept, corresponding to condition = control, is at 1.71 (95% CI [0.90, 2.52], t(124) = 4.16, p < .001). Within this model:\n\n  - The effect of condition [impasse] is statistically significant and positive (beta = 1.62, 95% CI [0.48, 2.76], t(124) = 2.81, p = 0.006; Std. beta = 0.49, 95% CI [0.14, 0.83])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\ncheck_model(lm.1)\n\n\n\n\n\n\n\nCODE\n#MODEL ESTIMATES WITH UNCERTAINTY\n\n#setup references\nm <- lm.1\ndf <- df\ncall <- m$call %>% as.character()\n\n# uncertainty model visualization\ndf  %>%\n  modelr::data_grid(condition) %>%\n  augment(lm.1, newdata = ., se_fit = TRUE) %>%\n  ggplot(aes(y = condition, color = condition)) +\n  stat_halfeye( scale = .5,\n      aes(\n        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n        fill = stat(cut_cdf_qi(cdf,\n                .width = c(.90, .95),\n                labels = scales::percent_format())))) +\n  scale_fill_brewer(direction = -1) +\n  labs (title = \"(LAB) Test Phase Accuracy ~ Condition\",\n        x = \"model predicted mean (% correct)\", y = \"Condition\", fill = \"Interval\",\n        subtitle = paste(\"lm(\",call[2],\")\")\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nCODE\n#sjPlot\np1 <- plot_model(lm.1,  type = \"eff\", \n           show.data = TRUE, jitter = TRUE,\n           show.p = TRUE) \n\n#BS TO MANUALLY ADD REGRESSION FORMULA AS SUBTITLE \n# library(equatiomatic)\n# library(latex2exp)\n# (x <- extract_eq(lm.1, use_coefs = TRUE, ital_vars=TRUE, coef_digits = 1, raw_tex = FALSE))\n# b = TeX(x)\n# p1[[\"condition\"]][[\"labels\"]][[\"subtitle\"]]= expression( paste(widehat(accuracy), \" = \", 1.7, \" + \", \"1.6\", \"*\", condition[impasse]) )\np1\n\n\n$condition\n\n\n\n\n\n\n\nCensored (Tobit) Regression\nhttps://stats.oarc.ucla.edu/r/dae/tobit-models/\nFor censored data (i.e. truncated axis). The tobit model, also called a censored regression model, is designed to estimate linear relationships between variables when there is either left- or right-censoring in the dependent variable (also known as censoring from below and above, respectively). Censoring from above takes place when cases with a value at or above some threshold, all take on the value of that threshold, so that the true value might be equal to the threshold, but it might also be higher. In the case of censoring from below, values those that fall at or below some threshold are censored.\n\ncensored vs truncated : There is sometimes confusion about the difference between truncated data and censored data.\nWith censored variables, all of the observations are in the dataset, but we don’t know the “true” values of some of them.\nWith truncation some of the observations are not included in the analysis because of the value of the variable.\nWhen a variable is censored, regression models for truncated data provide inconsistent estimates of the parameters. See Long (1997, chapter 7) for a more detailed discussion of problems of using regression models for truncated data to analyze censored data.\n\n\n\nCODE\n#set censoring values \nlo = 0\nhi = 8 \nrange(df$test_score)\n\n\n[1] 0 8\n\n\nCODE\nprint(\"Lo and Hi should equate to upper and lower bounds of the # Qs \")\n\n\n[1] \"Lo and Hi should equate to upper and lower bounds of the # Qs \"\n\n\nCODE\nlibrary(VGAM)\n\n#FIT MODEL\nm1<- vglm(test_score ~ condition, tobit(Lower = lo, Upper =hi ), data = df)\nsummary(m1)\n\n\n\nCall:\nvglm(formula = test_score ~ condition, family = tobit(Lower = lo, \n    Upper = hi), data = df)\n\nCoefficients: \n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept):1      -4.185      1.483   -2.82   0.0048 ** \n(Intercept):2       2.203      0.127   17.38   <2e-16 ***\nconditionimpasse    5.577      1.982    2.81   0.0049 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: mu, loglink(sd)\n\nLog-likelihood: -196 on 249 degrees of freedom\n\nNumber of Fisher scoring iterations: 11 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nCODE\n#CONFIDENCE INTERVALS\nb <- coef(m1)\nse <- sqrt(diag(vcov(m1)))\ncbind(LL = b - qnorm(0.975) * se, UL = b + qnorm(0.975) * se)\n\n\n                    LL    UL\n(Intercept):1    -7.09 -1.28\n(Intercept):2     1.95  2.45\nconditionimpasse  1.69  9.46\n\n\nCODE\n#TEST FIT\n#We can test the significance of condition overall by fitting an empty model and using a likelihood ratio test.\nm0 <- vglm(test_score ~ 1, tobit(Lower = lo, Upper = hi), data = df)\n(p <- pchisq(2 * (logLik(m1) - logLik(m0)), df = 2, lower.tail = FALSE))\n\n\n[1] 0.0134\n\n\nCODE\npaste(\"P value of likelihood ratio test less than alpha = 0.05? \", p <0.05)\n\n\n[1] \"P value of likelihood ratio test less than alpha = 0.05?  TRUE\"\n\n\nCODE\ncompare_performance(m0,m1)\n\n\n# Comparison of Model Performance Indices\n\nName | Model |     AIC | AIC weights |     BIC | BIC weights |  RMSE | Sigma\n----------------------------------------------------------------------------\nm0   |  vglm | 404.682 |       0.035 | 410.355 |       0.131 | 7.529 | 7.560\nm1   |  vglm | 398.060 |       0.965 | 406.569 |       0.869 | 7.101 | 7.144\n\n\nCODE\n#DIAGNOSTICS\nplot(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCODE\ndf$yhat <- fitted(m1)[,1]\ndf$rr <- resid(m1, type = \"response\")\ndf$rp <- resid(m1, type = \"pearson\")[,1]\n\npar(mfcol = c(2, 3))\n\nwith(df, {\n  plot(yhat, rr, main = \"Fitted vs Residuals\")\n  qqnorm(rr)\n  plot(yhat, rp, main = \"Fitted vs Pearson Residuals\")\n  qqnorm(rp)\n  plot(test_score, rp, main = \"Actual vs Pearson Residuals\")\n  plot(test_score, yhat, main = \"Actual vs Fitted\")\n})\n\n\n\n\n\nCODE\n#VARIANCE ACCOUNTED FOR\nprint(\"VARIANCE ACCOUNTED FOR\")\n\n\n[1] \"VARIANCE ACCOUNTED FOR\"\n\n\nCODE\n# correlation\n(r <- with(df, cor(yhat, test_score)))\n\n\n[1] 0.245\n\n\nCODE\n# variance accounted for\nr^2\n\n\n[1] 0.0598\n\n\nCODE\nperformance(m1)\n\n\n# Indices of model performance\n\nAIC     |     BIC |  RMSE | Sigma\n---------------------------------\n398.060 | 406.569 | 7.101 | 7.144\n\n\nCODE\n#NOTE: censReg package also does Tobit regression [including mixed models]\n\n\n\nThe coefficient labeled “(Intercept):1” is the intercept or constant for the model.\nThe coefficient labeled “(Intercept):2” is an ancillary statistic. If we exponentiate this value, we get a statistic that is analogous to the square root of the residual variance in OLS regression. logSigma is the variance of the model (logarithmized) (same as the second intercept in the VGAM output)\nThe predicted value of test_phase_score is 5.75 points higher for students in the impasse condition than for students in the control condition. (72% improvement in score!)\n\nUsing censReg package - https://cran.r-project.org/web/packages/censReg/vignettes/censReg.pdf\n\n\nCODE\nlibrary(censReg) #censored regression\n\n\nLoading required package: maxLik\n\n\nLoading required package: miscTools\n\n\n\nPlease cite the 'maxLik' package as:\nHenningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.\n\nIf you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:\nhttps://r-forge.r-project.org/projects/maxlik/\n\n\n\nPlease cite the 'censReg' package as:\nHenningsen, Arne (2017). censReg: Censored Regression (Tobit) Models. R package version 0.5. http://CRAN.R-Project.org/package=censReg.\n\nIf you have questions, suggestions, or comments regarding the 'censReg' package, please use a forum or 'tracker' at the R-Forge site of the 'sampleSelection' project:\nhttps://r-forge.r-project.org/projects/sampleselection/\n\n\nCODE\n#FIT MODEL\nc1 <- censReg( test_score ~ condition, left=lo, right=hi, data = df )\nsummary(c1)\n\n\n\nCall:\ncensReg(formula = test_score ~ condition, left = lo, right = hi, \n    data = df)\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n           126             71             37             18 \n\nCoefficients:\n                 Estimate Std. error t value Pr(> t)    \n(Intercept)       -4.1854     1.7149  -2.441 0.01466 *  \nconditionimpasse   5.5775     1.9995   2.789 0.00528 ** \nlogSigma           2.2033     0.1424  15.471 < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNewton-Raphson maximisation, 5 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-likelihood: -196 on 3 Df\n\n\nCODE\n#CONFIDENCE INTERVALS\nb <- coef(c1)\nse <- sqrt(diag(vcov(c1)))\ncbind(LL = b - qnorm(0.975) * se, UL = b + qnorm(0.975) * se)\n\n\n                    LL     UL\n(Intercept)      -7.55 -0.824\nconditionimpasse  1.66  9.496\nlogSigma          1.92  2.482\n\n\nCODE\n#TEST FIT\n#We can test the significance of condition overall by fitting an empty model and using a likelihood ratio test.\n\nc0 <- censReg( test_score ~ 1, left=lo, right=hi, data = df )\n(p <- pchisq(2 * (logLik(c1) - logLik(c0)), df = 2, lower.tail = FALSE))\n\n\n'log Lik.' 0.0134 (df=3)\n\n\nCODE\npaste(\"P value of likelihood ratio test less than alpha = 0.05? \", p <0.05)\n\n\n[1] \"P value of likelihood ratio test less than alpha = 0.05?  TRUE\"\n\n\nCODE\nperformance(c1)\n\n\nWarning in get_residuals.default(model, verbose = verbose, type = \"response\", :\nCan't extract residuals from model.\n\n\nWarning: Response residuals not available to calculate mean square error. (R)MSE\n  is probably not reliable.\n\n\nWarning: Models of class 'censReg' are not yet supported.\n\n\nNULL\n\n\n\nlogSigma is the variance of the model (logarithmized) (same as the second intercept in the VGAM output)\noutput should match that of VGAM"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#count-outcome",
    "href": "analysis/utils/modelling_ref.html#count-outcome",
    "title": "Modelling Reference",
    "section": "COUNT OUTCOME",
    "text": "COUNT OUTCOME\nDoes CONDITION have an effect on TEST PHASE ABSOLUTE SCORE?\n(# questions correct on test phase of task, [in lab] participants)\nThe outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can try considering it a type of count, (i.e. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function). Note that the process of answering questions on a test do not seem to strictly match the assumptions of a Poisson process.\n\n\nCODE\n#::::::::::::SETUP DATA\ndf = df_subjects %>% filter(mode == \"lab-synch\")\n\n#::::::::::::DESCRIPTIVES\nmosaic::favstats(test_score ~ condition, data = df)\n\n\n  condition min Q1 median Q3 max mean   sd  n missing\n1   control   0  0      0  1   8 1.71 3.05 62       0\n2   impasse   0  0      2  7   8 3.33 3.40 64       0\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\n\n# #GGFORMULA | FACETED HISTOGRAM\n# stats = df %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))\n# gf_props(~item_test_NABS, \n#          fill = ~pretty_condition, data = df) %>% \n#   gf_facet_grid(~pretty_condition) %>% \n#   gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n#   labs(x = \"# Correct\",\n#        y = \"proportion of subjects\",\n#        title = \"Test Phase Absolute Score (# Correct)\",\n#        subtitle = \"\") + theme(legend.position = \"blank\")\n\n##GGPUBR | HIST+DENSITY SCORE \np <- gghistogram(df, x = \"test_score\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"condition\")) +\n  labs( title = \"Distribution of TEST Absolute Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"Total Absolute Score (Test Phase)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = condition, y = test_score,\n                        fill = condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=condition, y = test_score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=condition, y = test_score, color = condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\nPoisson Regression\nhttps://stats.oarc.ucla.edu/r/dae/poisson-regression/\nGeneral Linear model using the Poisson distribution\n\n\nCODE\n#SCORE predicted by CONDITION --> POISSON DISTRIBUTION\np.1 <- glm(test_score ~ condition, data = df, family = \"poisson\")\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsumm(p.1)\n\n\n\n\n  \n    Observations \n    126 \n  \n  \n    Dependent variable \n    test_score \n  \n  \n    Type \n    Generalized linear model \n  \n  \n    Family \n    poisson \n  \n  \n    Link \n    log \n  \n\n \n\n  \n    𝛘²(1) \n    33.28 \n  \n  \n    Pseudo-R² (Cragg-Uhler) \n    0.23 \n  \n  \n    Pseudo-R² (McFadden) \n    0.04 \n  \n  \n    AIC \n    764.95 \n  \n  \n    BIC \n    770.62 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    z val. \n    p \n  \n \n\n  \n    (Intercept) \n    0.54 \n    0.10 \n    5.52 \n    0.00 \n  \n  \n    conditionimpasse \n    0.67 \n    0.12 \n    5.60 \n    0.00 \n  \n\n\n Standard errors: MLE\n\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(p.1)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: test_score\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev\nNULL                        125        604\ncondition  1     33.3       124        570\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(p.1)\n\n\nWaiting for profiling to be done...\n\n\n                 2.5 % 97.5 %\n(Intercept)      0.340  0.721\nconditionimpasse 0.436  0.902\n\n\nCODE\nreport(p.1) #sanity check\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a poisson model (estimated using ML) to predict test_score with condition (formula: test_score ~ condition). The model's explanatory power is moderate (Nagelkerke's R2 = 0.23). The model's intercept, corresponding to condition = control, is at 0.54 (95% CI [0.34, 0.72], p < .001). Within this model:\n\n  - The effect of condition [impasse] is statistically significant and positive (beta = 0.67, 95% CI [0.44, 0.90], p < .001; Std. beta = 0.67, 95% CI [0.44, 0.90])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nCODE\nplot_model(p.1)\n\n\n\n\n\nCODE\ncheck_model(p.1)\n\n\n\n\n\nThe variable condition has a coefficient of 0.67, (p < 0.005). This means that for the impasse condition, the expected log count # of questions increases by 0.67. By exponentiating the estimate we see that # question correct rate for the impasse condition is 1.95x that of the control condition. However, model diagnostics suggest the residuals are not normally distributed.\n\n\nNegative Binomial Regression\nhttps://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/ -overdispersed count data (variance much greater than mean)\n\nsimilar to Poisson regression, but using the negative binomial distribution, which can better account for ‘overdispersed’ data where variance is much greater than the mean\n\n\n\nCODE\n#NEGATIVE BIONOMIAL REGRESSION\n\nlibrary(MASS)\n\n#fit model \nnb.1 <- glm.nb(test_score ~ condition, data = df)\n\n#check overdispersion need \n#assumes conditional means are not equal to conditional variances\n#conduct likelihood ration test to compare and test [need poisson]\nm.t <- glm(test_score ~ condition, family = \"poisson\", data = df)\n#pchisq(2 * (logLik(nb.1) - logLik(m.t)), df = 1, lower.tail = FALSE)\n#A large (+) log likelihood suggests that the negative binomial is more appropriate than the Poisson model\ntest_lrt(m.t, nb.1)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName |  Model | df | df_diff |   Chi2 |      p\n----------------------------------------------\nm.t  |    glm |  2 |         |        |       \nnb.1 | negbin |  3 |       1 | 277.71 | < .001\n\n\nCODE\n#EXPONENTIATE PARAMETER ESTIMATES\nest <- cbind(Estimate = coef(nb.1), confint(nb.1))\n\n\nWaiting for profiling to be done...\n\n\nCODE\n#exponentiate parameter estimates\nprint(\"Exponentiated Estimates\")\n\n\n[1] \"Exponentiated Estimates\"\n\n\nCODE\nexp(est)\n\n\n                 Estimate 2.5 % 97.5 %\n(Intercept)          1.71  1.07   2.89\nconditionimpasse     1.95  0.98   3.86\n\n\nCODE\nsummary(nb.1)\n\n\n\nCall:\nglm.nb(formula = test_score ~ condition, data = df, init.theta = 0.2977734703, \n    link = log)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.220  -1.066  -1.066   0.447   1.067  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)  \n(Intercept)         0.536      0.252    2.13    0.033 *\nconditionimpasse    0.666      0.348    1.92    0.055 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.298) family taken to be 1)\n\n    Null deviance: 114.32  on 125  degrees of freedom\nResidual deviance: 110.70  on 124  degrees of freedom\nAIC: 489.2\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2978 \n          Std. Err.:  0.0586 \n\n 2 x log-likelihood:  -483.2380 \n\n\nCODE\nreport(nb.1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a negative-binomial model (estimated using ML) to predict test_score with condition (formula: test_score ~ condition). The model's explanatory power is weak (Nagelkerke's R2 = 0.05). The model's intercept, corresponding to condition = control, is at 0.54 (95% CI [0.07, 1.06], p = 0.033). Within this model:\n\n  - The effect of condition [impasse] is statistically non-significant and positive (beta = 0.67, 95% CI [-0.02, 1.35], p = 0.055; Std. beta = 0.67, 95% CI [-0.02, 1.35])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nCODE\nplot_model(nb.1)\n\n\n\n\n\nCODE\ncheck_model(nb.1)\n\n\n\n\n\nThe variable condition has a coefficient of 0.67, (p < 0.005). This means that for the impasse condition, the expected log count # of questions increases by 0.67. By exponentiating the estimate we see that # question correct rate for the impasse condition is nearly 1.95x that of the control condition. However, model diagnostics suggest the residuals are not normally distributed.\n\n\nCODE\n#COMPARE POISSON AND NEGATIVE BINOMIAL\ncompare_performance(p.1, nb.1)\n\n\n# Comparison of Model Performance Indices\n\nName |  Model |     AIC | AIC weights |     BIC | BIC weights | Nagelkerke's R2 |  RMSE | Sigma | Score_log | Score_spherical\n-----------------------------------------------------------------------------------------------------------------------------\np.1  |    glm | 764.945 |     < 0.001 | 770.618 |     < 0.001 |           0.234 | 3.208 | 2.145 |    -3.020 |           0.069\nnb.1 | negbin | 489.238 |        1.00 | 497.747 |        1.00 |           0.048 | 3.208 | 0.945 |    -2.213 |           0.074\n\n\nCODE\ntest_lrt(p.1, nb.1)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName |  Model | df | df_diff |   Chi2 |      p\n----------------------------------------------\np.1  |    glm |  2 |         |        |       \nnb.1 | negbin |  3 |       1 | 277.71 | < .001\n\n\nAIC, Pseudo-R2 and a likelihood ratio test indicate that the negative binomial regression model are a better fit for the distribution of test-phase scores."
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#mixture-models",
    "href": "analysis/utils/modelling_ref.html#mixture-models",
    "title": "Modelling Reference",
    "section": "MIXTURE MODELS",
    "text": "MIXTURE MODELS\n\nZero Inflated Poisson\nhttps://stats.oarc.ucla.edu/r/dae/zip/\nA Poisson count process with excess zeros\n\nThe Zero-Inflated model allows specification of two models (a mixture) where some of the zeros are included in the DGP for the Poisson model, while only the ‘excess’ zeros are included in the DGP for the Binomial model [the zero-inflated part]\nThe model includes:\n\nA logistic model to model which of the two processes the zero outcome is associated with\nA poisson model to model the count process\n\nCan specify different predictors for each part of the model\npredictors after the | are for the binomial zero-inflated part of the model, while those infront are for the poisson process\n\n\n\nCODE\n#ZERO INFLATED POISSON\n\nzinfp.1 <- zeroinfl(test_score ~  condition| condition , data = df)\nsummary(zinfp.1)\n\n\n\nCall:\nzeroinfl(formula = test_score ~ condition | condition, data = df)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.988 -0.575 -0.575  1.090  2.117 \n\nCount model coefficients (poisson with log link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        1.7702     0.0979   18.07   <2e-16 ***\nconditionimpasse  -0.0231     0.1199   -0.19     0.85    \n\nZero-inflation model coefficients (binomial with logit link):\n                 Estimate Std. Error z value Pr(>|z|)   \n(Intercept)         0.890      0.280    3.18   0.0015 **\nconditionimpasse   -1.213      0.378   -3.21   0.0013 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -213 on 4 Df\n\n\nCODE\nreport(zinfp.1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a zero-inflated poisson model to predict test_score with condition (formula: test_score ~ condition). The model's explanatory power is substantial (R2 = 0.39, adj. R2 = 0.38). The model's intercept, corresponding to condition = control, is at 1.77 (95% CI [1.58, 1.96], p < .001). Within this model:\n\n  - The effect of condition [impasse] is statistically non-significant and negative (beta = -0.02, 95% CI [-0.26, 0.21], p = 0.847; Std. beta = -0.02, 95% CI [-0.26, 0.21])\n  - The effect of condition [impasse] is statistically significant and negative (beta = -1.21, 95% CI [-1.95, -0.47], p = 0.001; Std. beta = -1.21, 95% CI [-1.95, -0.47])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\n\nCODE\nperformance(zinfp.1)\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-----------------------------------------------------------------------------------\n434.837 | 446.183 | 0.391 |     0.381 | 3.208 | 3.260 |    -1.694 |           0.069\n\n\nCODE\n#check_model(zinfp.1)\n\n\nIn the count model, the coefficient for the condition is not significant.\nIn the zero-inflation model, the coefficient for the condition variable is -1.23 and statistically significant. This suggests that the log odds of being an excessive zero decrease by 1.23 if you are in the impasse condition\n\n\nZero Inflated Negative Binomial Regression\nhttps://stats.oarc.ucla.edu/r/dae/zinb/\ncount data that are overdispersed and have excess zeros\nZero-inflated negative binomial regression is for modelling count variables with excessive zeros, and especially when the count data are overdispersed (mean is much larger than variance). It can help account for situations where theory suggests that excess zeros are generated by 2 separate processes, one that includes the other count values, and the other that is just the zeros, and thus that the excess zeros can be modelled independently.\nTotal Absolute Score (# items correct) may fit this situation, as the data are overdispersed (variance much greater than the mean) and there are are very large number of zeros. It is theoretically plausible that these excess zeros (no answers correct) are the result of a different ‘process’ … (i.e) little understanding and/or resistance to restructuring understanding of the coordinate system. However, I am not certain if it is plausible to suggest that the zeros themselves are the result of two different processes: (ie. perhaps trying to understand, and not trying to understand?) <- this could maybe be disentangled by first question latency?\nThe model includes:\n\nA logistic model to model which of the two processes the zero outcome is associated with\nA negative binomial model to model the count process\n\n\n\nCODE\nlibrary(pscl) #  for zeroinfl negbinomial\n\n#ZERO INFLATED NEGATIVE BINOMIAL\nzinb.1 <- zeroinfl(test_score ~ condition | condition , \n                   data = df, dist = \"negbin\")\n#before the | is the count part, after the | is the logit model\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(zinb.1)\n\n\n\nCall:\nzeroinfl(formula = test_score ~ condition | condition, data = df, dist = \"negbin\")\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.970 -0.568 -0.568  1.070  2.091 \n\nCount model coefficients (negbin with log link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        1.7689     0.1048   16.88   <2e-16 ***\nconditionimpasse  -0.0232     0.1282   -0.18    0.856    \nLog(theta)         3.7504     2.1488    1.75    0.081 .  \n\nZero-inflation model coefficients (binomial with logit link):\n                 Estimate Std. Error z value Pr(>|z|)   \n(Intercept)         0.888      0.280    3.17   0.0015 **\nconditionimpasse   -1.214      0.379   -3.21   0.0013 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 42.54 \nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -213 on 5 Df\n\n\nCODE\nreport(zinb.1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a zero-inflated negative-binomial model to predict test_score with condition (formula: test_score ~ condition). The model's explanatory power is substantial (R2 = 0.40, adj. R2 = 0.39). The model's intercept, corresponding to condition = control, is at 1.77 (95% CI [1.56, 1.97], p < .001). Within this model:\n\n  - The effect of condition [impasse] is statistically non-significant and negative (beta = -0.02, 95% CI [-0.27, 0.23], p = 0.856; Std. beta = -0.02, 95% CI [-0.27, 0.23])\n  - The effect of condition [impasse] is statistically significant and negative (beta = -1.21, 95% CI [-1.96, -0.47], p = 0.001; Std. beta = -1.21, 95% CI [-1.96, -0.47])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\n\nCODE\nperformance(zinb.1)\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-----------------------------------------------------------------------------------\n436.585 | 450.766 | 0.398 |     0.388 | 3.208 | 3.274 |    -1.740 |           0.068\n\n\nCODE\n#   rootogram(zinb.1)\n\n\n\n# #EXPONENTIATE PARAMETER ESTIMATES\n# est <- cbind(Estimate = coef(zinb.1), confint(zinb.1))\n# #exponentiate parameter estimates\n# print(\"Exponentiated Estimates\")\n# exp(est)\n\n\nIn the count model, the coefficient for the condition is very small, and not significant (suggesting it does not contribute to the count yielding process?).\nIn the zero-inflation model, the coefficient for the condition variable is -1.056 and statistically significant. This suggests that the log odds of being an excessive zero decrease by 1.06 if you are in the impasse condition\n\n\nCODE\ncompare_performance(zinfp.1, zinb.1)\n\n\nSome of the nested models seem to be identical\n\n\n# Comparison of Model Performance Indices\n\nName    |    Model |     AIC | AIC weights |     BIC | BIC weights |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n------------------------------------------------------------------------------------------------------------------------------------\nzinfp.1 | zeroinfl | 434.837 |       0.705 | 446.183 |       0.908 | 0.391 |     0.381 | 3.208 | 3.260 |    -1.694 |           0.069\nzinb.1  | zeroinfl | 436.585 |       0.295 | 450.766 |       0.092 | 0.398 |     0.388 | 3.208 | 3.274 |    -1.740 |           0.068\n\n\nCODE\ntest_likelihoodratio(zinfp.1, zinb.1)\n\n\nSome of the nested models seem to be identical\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName    |    Model | df | df_diff | Chi2 |     p\n------------------------------------------------\nzinfp.1 | zeroinfl |  4 |         |      |      \nzinb.1  | zeroinfl |  5 |       1 | 0.25 | 0.615\n\n\nTODO come back to this and discuss further\n\n\n\nHurdle Model\n\nhttps://data.library.virginia.edu/getting-started-with-hurdle-models/\nhttps://en.wikipedia.org/wiki/Hurdle_model#:~:text=A%20hurdle%20model%20is%20a,of%20the%20non%2Dzero%20values.\n\nclass of models for count data with both overdispersion and excess zeros;\ndifferent from zero-inflated models where the excess zeros are theorized to arise from two different processes; in the hurdle model, there is a separate model for P(x=0) and a separate model for P(x!=0)\nThe model includes:\n\nA binary logit model to model whether the observation takes a positive count or not. (1) Does the student get any questions right?\na truncated Poisson or Negative binomial model that only fits positive counts (2) How many questions does the student get right?\n\n\n\nCODE\nlibrary(pscl) #zero-inf and hurdle models \nlibrary(countreg) #rootogram\n\n\nRegistered S3 methods overwritten by 'countreg':\n  method                 from\n  print.zeroinfl         pscl\n  print.summary.zeroinfl pscl\n  summary.zeroinfl       pscl\n  coef.zeroinfl          pscl\n  vcov.zeroinfl          pscl\n  logLik.zeroinfl        pscl\n  predict.zeroinfl       pscl\n  residuals.zeroinfl     pscl\n  fitted.zeroinfl        pscl\n  terms.zeroinfl         pscl\n  model.matrix.zeroinfl  pscl\n  extractAIC.zeroinfl    pscl\n  print.hurdle           pscl\n  print.summary.hurdle   pscl\n  summary.hurdle         pscl\n  coef.hurdle            pscl\n  vcov.hurdle            pscl\n  logLik.hurdle          pscl\n  predict.hurdle         pscl\n  residuals.hurdle       pscl\n  fitted.hurdle          pscl\n  terms.hurdle           pscl\n  model.matrix.hurdle    pscl\n  extractAIC.hurdle      pscl\n\n\n\nAttaching package: 'countreg'\n\n\nThe following objects are masked from 'package:VGAM':\n\n    dzipois, pzipois, qzipois, rzipois\n\n\nThe following objects are masked from 'package:pscl':\n\n    hurdle, hurdle.control, hurdletest, zeroinfl, zeroinfl.control\n\n\nThe following object is masked from 'package:vcd':\n\n    rootogram\n\n\nCODE\n#install.packages(\"countreg\", repos=\"http://R-Forge.R-project.org\")\n\n#SYNTAX OUTCOME ~ count model predictor | hurdle predictor\n\nh.1 <- pscl::hurdle(test_score ~ condition | condition , data = df,\n              zero.dist = \"binomial\", dist = \"poisson\", size = 8)\n\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\n\nWarning in optim(fn = zeroDist, gr = zeroGrad, par = c(start$zero, if (zero.dist\n== : unknown names in control: size\n\n\nCODE\nh.2 <- pscl::hurdle(test_score ~ condition | condition , data = df,\n              zero.dist = \"binomial\", dist = \"negbin\", size = 8)\n\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\n\nCODE\nsummary(h.1)\n\n\n\nCall:\npscl::hurdle(formula = test_score ~ condition | condition, data = df, \n    dist = \"poisson\", zero.dist = \"binomial\", size = 8)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.988 -0.575 -0.575  1.090  2.117 \n\nCount model coefficients (truncated poisson with log link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        1.7702     0.0979   18.07   <2e-16 ***\nconditionimpasse  -0.0231     0.1199   -0.19     0.85    \nZero hurdle model coefficients (binomial with logit link):\n                 Estimate Std. Error z value Pr(>|z|)   \n(Intercept)        -0.894      0.280   -3.19   0.0014 **\nconditionimpasse    1.209      0.377    3.20   0.0014 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -213 on 4 Df\n\n\nCODE\nsummary(h.2)\n\n\n\nCall:\npscl::hurdle(formula = test_score ~ condition | condition, data = df, \n    dist = \"negbin\", zero.dist = \"binomial\", size = 8)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.970 -0.568 -0.568  1.070  2.091 \n\nCount model coefficients (truncated negbin with log link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        1.7689     0.1048   16.88   <2e-16 ***\nconditionimpasse  -0.0232     0.1282   -0.18    0.856    \nLog(theta)         3.7506     2.1492    1.75    0.081 .  \nZero hurdle model coefficients (binomial with logit link):\n                 Estimate Std. Error z value Pr(>|z|)   \n(Intercept)        -0.894      0.280   -3.19   0.0014 **\nconditionimpasse    1.209      0.377    3.20   0.0014 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 42.546\nNumber of iterations in BFGS optimization: 19 \nLog-likelihood: -213 on 5 Df\n\n\nCODE\nrootogram(h.1)\n\n\n\n\n\nCODE\nrootogram(h.2)\n\n\n\n\n\nCODE\nplot(compare_performance(h.1,h.2))\n\n\nSome of the nested models seem to be identical\n\n\n\n\n\nCODE\ntest_lrt(h.1, h.2)\n\n\nSome of the nested models seem to be identical\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName |  Model | df | df_diff | Chi2 |     p\n-------------------------------------------\nh.1  | hurdle |  4 |         |      |      \nh.2  | hurdle |  5 |       1 | 0.25 | 0.615"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#beta-regression",
    "href": "analysis/utils/modelling_ref.html#beta-regression",
    "title": "Modelling Reference",
    "section": "BETA REGRESSION",
    "text": "BETA REGRESSION\n\nBETA Regression\nBeta regression on % correct (with standard transformation for including [0,1])\nhttps://stats.stackexchange.com/questions/63350/how-to-interpret-the-coefficients-from-a-beta-regression\n\n\nCODE\n# \nlibrary(betareg)\n\n#RESCLAE VARIABLE\n#beta reg can't handle 0s and 1s \nsub <- df_subjects %>% dplyr::select(condition, DV_percent_NABS)\nn = nrow(sub) %>% unlist()\nsub$dv_transformed = (sub$DV_percent_NABS * (n-1) + 0.5)/n\n \n#VISUALIZE VARIABLES\nhistogram(sub$dv_transformed)\n\n\n\n\n\nCODE\ngf_histogram(~dv_transformed, fill = ~condition, data = sub) %>% gf_facet_wrap(~condition)\n\n\n\n\n\nCODE\n#FIT MODEL\nmb <- betareg(dv_transformed ~ condition, data = sub)\nsummary(mb)\n\n\n\nCall:\nbetareg(formula = dv_transformed ~ condition, data = sub)\n\nStandardized weighted residuals 2:\n   Min     1Q Median     3Q    Max \n-1.057 -0.453 -0.216  0.541  1.690 \n\nCoefficients (mean model with logit link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        -0.969      0.108   -8.97   <2e-16 ***\nconditionimpasse    0.556      0.143    3.89   0.0001 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(>|z|)    \n(phi)   0.6604     0.0425    15.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  506 on 3 Df\nPseudo R-squared: 0.0725\nNumber of iterations: 12 (BFGS) + 1 (Fisher scoring) \n\n\nCODE\nplot(mb)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBETA HURDLE Regression\nhttps://github.com/markhwhiteii/beta_hurdle/blob/master/manuscript/beta_hurdle.pdf\n\nMU tells if mean is different by condition\nSIGMA tells if variance is different by condition\nNU coefficient tells if condition yields different probability at floor\nTAU coefficient tells if condition yields different probability at ceiling\n\n\n\nCODE\n#BETA HURDLE REGRESSION EXAMPLE\nlibrary(gamlss)\n\n\nLoading required package: gamlss.data\n\n\n\nAttaching package: 'gamlss.data'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nLoading required package: gamlss.dist\n\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThe following objects are masked from 'package:ordinal':\n\n    ranef, VarCorr\n\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\n\nLoading required package: parallel\n\n\n **********   GAMLSS Version 5.4-3  ********** \n\n\nFor more on GAMLSS look at https://www.gamlss.com/\n\n\nType gamlssNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'gamlss'\n\n\nThe following object is masked from 'package:lme4':\n\n    refit\n\n\nCODE\n#CREATE SAMPLE DATA \nn <- 5000 \nmu <- 0.40 \nsigma <- 0.60 \np0 <- 0.13 \np1 <- 0.17 \np2 <- 1- p0- p1\na <- mu * (1- sigma ^ 2) / (sigma ^ 2) \nb <- a * (1- mu) / mu\n\n#CREATE DIST\nset.seed(1839) \ny <- rbeta(n, a, b) \ncat <- sample(1:3, n, prob = c(p0, p2, p1), replace = TRUE) \ny[cat == 1] <- 0 \ny[cat == 3] <- 1\n\n#VISUALIZE DISTRIBUTION\nx <- as.data.frame(y)\ngf_histogram(~x$y)\n\n\n\n\n\nCODE\n#this looks not unlike my distribution! \n\n#CREATE AN EMPTY MODEL\nfit <- gamlss( formula = y ~ 1, # formula for mu \n               formula.sigma = ~ 1, # formula for sigma \n               formula.nu = ~ 1, # formula for nu \n               formula.tau = ~ 1, # formula for tau \n               family = BEINF() )\n\n\nGAMLSS-RS iteration 1: Global Deviance = 7799 \nGAMLSS-RS iteration 2: Global Deviance = 7778 \nGAMLSS-RS iteration 3: Global Deviance = 7778 \nGAMLSS-RS iteration 4: Global Deviance = 7778 \n\n\nCODE\nsummary(fit)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = y ~ 1, family = BEINF(), formula.sigma = ~1,  \n    formula.nu = ~1, formula.tau = ~1) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.3796     0.0196   -19.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.3951     0.0162    24.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.632      0.042   -38.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.4014     0.0382   -36.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  5000 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  4996 \n                      at cycle:  4 \n \nGlobal Deviance:     7778 \n            AIC:     7786 \n            SBC:     7812 \n******************************************************************\n\n\nCODE\nplot(fit)\n\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  0.000571 \n                       variance   =  1 \n               coef. of skewness  =  0.0294 \n               coef. of kurtosis  =  2.95 \nFilliben correlation coefficient  =  1 \n******************************************************************\n\n\nCODE\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nfit_mu <- inv_logit(fit$mu.coefficients) \npaste(\"MU: \",fit_mu)\n\n\n[1] \"MU:  0.406229902102452\"\n\n\nCODE\nfit_sigma <- inv_logit(fit$sigma.coefficients) \npaste(\"SIGMA: \",fit_sigma)\n\n\n[1] \"SIGMA:  0.597499259410111\"\n\n\nCODE\nfit_nu <- exp(fit$nu.coefficients) \nfit_tau <- exp(fit$tau.coefficients) \nfit_p0 <- fit_nu / (1 + fit_nu + fit_tau) \npaste(\"P0: \",fit_p0)\n\n\n[1] \"P0:  0.135600165493784\"\n\n\nCODE\nfit_p1 <- fit_tau / (1 + fit_nu + fit_tau)\npaste(\"P1: \",fit_p1)\n\n\n[1] \"P1:  0.170800000002391\"\n\n\nBETA HURDLE INTERPRETATION - beta component\n- MU “location” (mean)\n- SIGMA “scale” (positively related to variance; variance = sigma.squared mean (1-mean)\n- Rigby, Stasinopoulos, Heller, and De Bastiani (2017) “reparameterized” the beta distribution so that the two parameters determining the shape of the distribution would be more useful in a regression framework (see Ferrari & Cribari-Neto, 2004 for a different parameterization)\nZERO-ONE HURDLE COMPONENT\n- The two additional parameters, ν NU and τTAU , are related to p0 and p1, respectively.\n- p0 is the probability that a case equals 0,\n- p1 is the probability that a case equals 1,\n- p2 (i.e., 1 −p0 −p1) is the probability that the case comes from the beta distribution\n\n\nCODE\n#MY DATA\n#SETUP DATA \n\nmin = 0 #min possible value of scale\nmax = 13 #max possible value of scale\n\nlibrary(mosaic) #for shuffling\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following objects are masked from 'package:VGAM':\n\n    chisq, logit\n\n\nThe following object is masked from 'package:lmerTest':\n\n    rand\n\n\nThe following object is masked from 'package:lme4':\n\n    factorize\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:modelr':\n\n    resample\n\n\nThe following object is masked from 'package:vcd':\n\n    mplot\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nThe following object is masked from 'package:cowplot':\n\n    theme_map\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\n\nCODE\n#1. Rescale accuracy using \n# recommended adjustment \n#rescaled = value-min/(max-min)\ndf <- df_subjects %>% mutate(\n  accuracy = s_NABS,\n  R_acc = (accuracy-min)/(max-min), #as %\n  T_acc = (accuracy * (nrow(df)-1) + 0.5)/nrow(df)/max, #transform for no 0 and 1\n  perm = shuffle(condition),\n  scaffold_rt = item_scaffold_rt\n) %>% dplyr::select(accuracy,R_acc, T_acc, condition, perm,scaffold_rt)\n\n#VISUALIZE DISTRIBUTION\ngf_histogram(~R_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = \"Histogram of accuracy\")\n\n\n\n\n\nCODE\n#VISUALIZE DISTRIBUTION\ngf_histogram(~T_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = \"Histogram of [rescaled] accuracy\")\n\n\n\n\n\nCODE\ngf_histogram(~R_acc, fill = ~perm, data = df) %>% gf_facet_wrap(~perm) + labs(title = \"Histogram of shuffled accuracy\")\n\n\n\n\n\nCODE\n#SUMMARIZE SAMPLE\npaste(\"Grand mean\", mean(df$R_acc))\n\n\n[1] \"Grand mean 0.288344988344988\"\n\n\nCODE\nlibrary(mosaic)\nstats = favstats(df$R_acc ~ df$condition)\nstats$mean <- mean(df$R_acc ~ df$condition)\nstats$var <- var(df$R_acc ~ df$condition)\nprint(\"Grand stats\")\n\n\n[1] \"Grand stats\"\n\n\nCODE\nstats \n\n\n  df$condition min Q1 median    Q3 max  mean    sd   n missing   var\n1      control   0  0  0.000 0.154   1 0.190 0.343 158       0 0.118\n2      impasse   0  0  0.154 0.788   1 0.379 0.395 172       0 0.156\n\n\nCODE\nprint(\"P0\")\n\n\n[1] \"P0\"\n\n\nCODE\nnrow(df %>% filter(R_acc ==0))/nrow(df)\n\n\n[1] 0.458\n\n\nCODE\nprint(\"P1\")\n\n\n[1] \"P1\"\n\n\nCODE\nnrow(df %>% filter(R_acc ==1))/nrow(df)\n\n\n[1] 0.0939\n\n\nCODE\n#CREATE MODEL\n\n#CREATE AN EMPTY MODEL\nm0 <- gamlss( formula = R_acc ~ 1, # formula for mu \n              formula.sigma =  ~ 1, # formula for sigma \n              formula.nu =  ~ 1, # formula for nu \n              formula.tau =  ~ 1, # formula for tau \n              family = BEINF(), data = df )\n\n\nGAMLSS-RS iteration 1: Global Deviance = 611 \nGAMLSS-RS iteration 2: Global Deviance = 610 \nGAMLSS-RS iteration 3: Global Deviance = 610 \nGAMLSS-RS iteration 4: Global Deviance = 610 \n\n\nCODE\nm0 <- gamlss(R_acc ~ 1, ~ 1, ~ 1, ~ 1, \n            data = df, family = BEINF())\n\n\nGAMLSS-RS iteration 1: Global Deviance = 611 \nGAMLSS-RS iteration 2: Global Deviance = 610 \nGAMLSS-RS iteration 3: Global Deviance = 610 \nGAMLSS-RS iteration 4: Global Deviance = 610 \n\n\nCODE\npaste(\"THE EMPTY MODEL\")\n\n\n[1] \"THE EMPTY MODEL\"\n\n\nCODE\nsummary(m0)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ 1, sigma.formula = ~1, nu.formula = ~1,  \n    tau.formula = ~1, family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  -0.2304     0.0947   -2.43    0.015 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.4262     0.0775     5.5  7.7e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.0201     0.1157    0.17     0.86\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.563      0.198   -7.91  3.9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  326 \n                      at cycle:  4 \n \nGlobal Deviance:     610 \n            AIC:     618 \n            SBC:     634 \n******************************************************************\n\n\nCODE\nplot(m0)\n\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  0.0111 \n                       variance   =  0.973 \n               coef. of skewness  =  0.0732 \n               coef. of kurtosis  =  2.69 \nFilliben correlation coefficient  =  0.996 \n******************************************************************\n\n\nCODE\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nm0_mu <- inv_logit(m0$mu.coefficients) \npaste(\"MU: \",m0_mu)\n\n\n[1] \"MU:  0.442655135079248\"\n\n\nCODE\nm0_sigma <- inv_logit(m0$sigma.coefficients) \npaste(\"SIGMA: \",m0_sigma)\n\n\n[1] \"SIGMA:  0.604970105601303\"\n\n\nCODE\nm0_nu <- exp(m0$nu.coefficients) \npaste(\"NU: \",m0_nu)\n\n\n[1] \"NU:  1.02032203257766\"\n\n\nCODE\nm0_tau <- exp(m0$tau.coefficients) \npaste(\"TAU: \",m0_tau)\n\n\n[1] \"TAU:  0.209464832192797\"\n\n\nCODE\nm0_p0 <- fit_nu / (1 + fit_nu + fit_tau) \npaste(\"P0: \",m0_p0)\n\n\n[1] \"P0:  0.135600165493784\"\n\n\nCODE\nm0_p1 <- fit_tau / (1 + fit_nu + fit_tau)\npaste(\"P1: \",m0_p1)\n\n\n[1] \"P1:  0.170800000002391\"\n\n\nCODE\n#CREATE PREDICTOR MODEL\nm1 <- gamlss(R_acc ~ condition, ~ condition, ~ condition, ~ condition, \n            data = df, family = BEINF())\n\n\nGAMLSS-RS iteration 1: Global Deviance = 573 \nGAMLSS-RS iteration 2: Global Deviance = 572 \nGAMLSS-RS iteration 3: Global Deviance = 572 \nGAMLSS-RS iteration 4: Global Deviance = 572 \n\n\nCODE\npaste(\"THE PREDICTOR MODEL\")\n\n\n[1] \"THE PREDICTOR MODEL\"\n\n\nCODE\nsummary(m1)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)  \n(Intercept)        -0.421      0.171   -2.46    0.014 *\nconditionimpasse    0.274      0.205    1.33    0.183  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)   \n(Intercept)        0.3949     0.1407    2.81   0.0053 **\nconditionimpasse   0.0309     0.1687    0.18   0.8549   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.767      0.178    4.30  2.3e-05 ***\nconditionimpasse   -1.440      0.247   -5.84  1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -1.264      0.314   -4.02 0.000072 ***\nconditionimpasse   -0.471      0.405   -1.16     0.25    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  4 \n \nGlobal Deviance:     572 \n            AIC:     588 \n            SBC:     618 \n******************************************************************\n\n\nCODE\n#LOOKING PREDICTOR MODEL\nm <- gamlss(R_acc ~ condition , \n            ~ condition , \n            ~ condition , \n            ~ condition , \n            data = df, family = BEINF())\n\n\nGAMLSS-RS iteration 1: Global Deviance = 573 \nGAMLSS-RS iteration 2: Global Deviance = 572 \nGAMLSS-RS iteration 3: Global Deviance = 572 \nGAMLSS-RS iteration 4: Global Deviance = 572 \n\n\nCODE\npaste(\"THE PREDICTOR MODEL\")\n\n\n[1] \"THE PREDICTOR MODEL\"\n\n\nCODE\nsummary(m)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)  \n(Intercept)        -0.421      0.171   -2.46    0.014 *\nconditionimpasse    0.274      0.205    1.33    0.183  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)   \n(Intercept)        0.3949     0.1407    2.81   0.0053 **\nconditionimpasse   0.0309     0.1687    0.18   0.8549   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.767      0.178    4.30  2.3e-05 ***\nconditionimpasse   -1.440      0.247   -5.84  1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -1.264      0.314   -4.02 0.000072 ***\nconditionimpasse   -0.471      0.405   -1.16     0.25    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  4 \n \nGlobal Deviance:     572 \n            AIC:     588 \n            SBC:     618 \n******************************************************************\n\n\nCODE\n#CREATE PREDICTOR MODEL ON SHUFFLED [PERMUTATION TEST]\nmperm <- gamlss(R_acc ~ perm, ~ perm, ~ perm, ~ perm, \n            data = df, family = BEINF())\n\n\nGAMLSS-RS iteration 1: Global Deviance = 611 \nGAMLSS-RS iteration 2: Global Deviance = 610 \nGAMLSS-RS iteration 3: Global Deviance = 610 \nGAMLSS-RS iteration 4: Global Deviance = 610 \n\n\nCODE\nsummary(mperm)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ perm, sigma.formula = ~perm,  \n    nu.formula = ~perm, tau.formula = ~perm, family = BEINF(),      data = df) \n\n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -0.2029     0.1397   -1.45     0.15\npermimpasse  -0.0524     0.1899   -0.28     0.78\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.4570     0.1134    4.03    7e-05 ***\npermimpasse  -0.0594     0.1553   -0.38      0.7    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.0564     0.1679    0.34     0.74\npermimpasse  -0.0691     0.2317   -0.30     0.77\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.461      0.277   -5.27  2.5e-07 ***\npermimpasse   -0.200      0.395   -0.51     0.61    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  4 \n \nGlobal Deviance:     610 \n            AIC:     626 \n            SBC:     656 \n******************************************************************\n\n\nCODE\n#sanity check with scaled outcome, no zeros ones\nm3 <- gamlss(T_acc ~ condition, ~ condition, ~ condition, ~ condition, \n            data = df, family = BEINF())\n\n\nGAMLSS-RS iteration 1: Global Deviance = -1207 \nGAMLSS-RS iteration 2: Global Deviance = -1335 \nGAMLSS-RS iteration 3: Global Deviance = -1345 \nGAMLSS-RS iteration 4: Global Deviance = -1346 \nGAMLSS-RS iteration 5: Global Deviance = -1346 \nGAMLSS-RS iteration 6: Global Deviance = -1346 \nGAMLSS-RS iteration 7: Global Deviance = -1346 \nGAMLSS-RS iteration 8: Global Deviance = -1346 \nGAMLSS-RS iteration 9: Global Deviance = -1346 \n\n\nCODE\nsummary(m3)\n\n\nWarning in summary.gamlss(m3): summary: vcov has failed, option qr is used instead\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = T_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition, family = BEINF(),  \n    data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -1.1773     0.0971  -12.13  < 2e-16 ***\nconditionimpasse   0.5857     0.1387    4.22 0.000031 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        1.2553     0.0740   16.97   <2e-16 ***\nconditionimpasse  -0.0122     0.1030   -0.12     0.91    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n                  Estimate Std. Error t value Pr(>|t|)\n(Intercept)      -2.25e+01   3.78e+03   -0.01        1\nconditionimpasse -6.72e-15   5.24e+03    0.00        1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n                  Estimate Std. Error t value Pr(>|t|)\n(Intercept)      -2.26e+01   3.96e+03   -0.01        1\nconditionimpasse  9.20e-15   5.48e+03    0.00        1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  9 \n \nGlobal Deviance:     -1346 \n            AIC:     -1330 \n            SBC:     -1300 \n******************************************************************\n\n\nCODE\n#m3 shouldn't show condition as significant for nu and tau, because T_acc was scaled to not include any 0s and 1s\n\n#investigate beta negative binomial distribution\n#https://en.wikipedia.org/wiki/Beta_negative_binomial_distribution\n\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nm1_mu <- inv_logit(m1$mu.coefficients) \npaste(\"MU: \",m1_mu)\n\n\n[1] \"MU:  0.396369901111619\" \"MU:  0.568036956954873\"\n\n\nCODE\nm1_sigma <- inv_logit(m0$sigma.coefficients) \npaste(\"SIGMA: \",m1_sigma)\n\n\n[1] \"SIGMA:  0.604970105601303\"\n\n\nCODE\nm1_nu <- exp(m1$nu.coefficients) \npaste(\"NU: \",m1_nu)\n\n\n[1] \"NU:  2.15227350452983\"  \"NU:  0.236870904856183\"\n\n\nCODE\nm1_tau <- exp(m1$tau.coefficients) \npaste(\"TAU: \",m1_tau)\n\n\n[1] \"TAU:  0.282617628302748\" \"TAU:  0.624417570712948\"\n\n\nCODE\nsummary(m)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)  \n(Intercept)        -0.421      0.171   -2.46    0.014 *\nconditionimpasse    0.274      0.205    1.33    0.183  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)   \n(Intercept)        0.3949     0.1407    2.81   0.0053 **\nconditionimpasse   0.0309     0.1687    0.18   0.8549   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.767      0.178    4.30  2.3e-05 ***\nconditionimpasse   -1.440      0.247   -5.84  1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -1.264      0.314   -4.02 0.000072 ***\nconditionimpasse   -0.471      0.405   -1.16     0.25    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  4 \n \nGlobal Deviance:     572 \n            AIC:     588 \n            SBC:     618 \n******************************************************************\n\n\nCODE\nplot(m)\n\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  -0.0141 \n                       variance   =  1.03 \n               coef. of skewness  =  0.0142 \n               coef. of kurtosis  =  3.05 \nFilliben correlation coefficient  =  0.998 \n******************************************************************\n\n\n\nMU tells if mean is different by condition\nSIGMA tells if variance is different by condition\nNU coefficient tells if condition yields different probability at floor\nTAU coefficient tells if condition yields different probability at ceiling"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#binomial-outcome",
    "href": "analysis/utils/modelling_ref.html#binomial-outcome",
    "title": "Modelling Reference",
    "section": "BINOMIAL OUTCOME",
    "text": "BINOMIAL OUTCOME\n\n\nCODE\n#PREPARE DATA \ndf <- df_items %>% filter(q ==1) %>% filter(mode == \"lab-synch\")\n# %>% mutate(\n#   accuracy = recode_factor(score_niceABS, \"0\" =\"incorrect\",\"1\"=\"correct\"),\n#   scaled = as.ordered(score_SCALED),\n#   q = as.factor(q),\n#   high_interpretation = as.factor(high_interpretation)\n# )\n\n#GROUPED PROPORTIONAL BAR CHART\ngf_props(~accuracy, fill = ~pretty_condition, x =~pretty_condition,\n       position = position_dodge(), data = df) %>%\n  gf_facet_grid(~pretty_mode) +\n   labs(x = \"Question 1 Accuracy\",\n       title = \"Accuracy on Q1\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n   labs(#y = \"\",\n       title = \"Accuracy on Test Phase\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\n\nCHI SQUARE\n\n\nCODE\n#::::::::::::CROSSTABLE\n# CrossTable( x = df$condition, y = df$accuracy, \n#              fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n#::::::::::::MOSAIC PLOT\n# note: blue indicates cell count higher than expected, \n# red indicates cell count less than expected; under null hypothesis\n# mosaicplot(main=\"Accuracy on First Question by Condition\",\n#             data = df, pretty_condition ~ accuracy, \n#             shade = T)\n\n#::::::::::::TABLE\ndf %>% sjtab( fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=F, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = c(\"auto\"))\n\n\n\n \n accuracy\n pretty_condition\n Total\n \n \n\n control\n impasse\n \n \n \n0\n524883.9 %\n454970.3 %\n979777 % \n\n \n \n1\n101416.1 %\n191529.7 %\n292923 % \n\n \n \nTotal\n6262100 %\n6464100 %\n126126100 % \n\nχ2=2.547 · df=1 · φ=0.161 · p=0.111 \n\n \n \n observed values\n expected values\n % within pretty_condition\n \n\n\n\nCODE\n#::::::::::::BAR PLOT\nggbarstats(data = df, x = accuracy, y = condition,\n           type = \"nonparametric\")\n\n\n\n\n\nCODE\n#::::::::::::CHISQR TEST\n(x <- stats::chisq.test(x = df$accuracy, y = df$condition, simulate.p.value = T))\n\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  df$accuracy and df$condition\nX-squared = 3, df = NA, p-value = 0.09\n\n\nCODE\n#::::::::::::POWER ANALYSIS\n(po <- pwr.chisq.test( w = 0.1, df=(2-1), N = nrow(df), sig.level = 0.05))\n\n\n\n     Chi squared power calculation \n\n              w = 0.1\n              N = 126\n             df = 1\n      sig.level = 0.05\n          power = 0.202\n\nNOTE: N is the number of observations\n\n\nA Chi-Square test of independence of Q1 accuracy [correct,incorrect] by condition indicates the question accuracy is not dependent on condition. However, this test may be underpowered, as with the given sample size it has only 20% power to detect a small effect (w = 0.1)\n\n\nLOGISTIC REGRESSION\nFit a logistic regression (at the subject-item level), predicting Q1 accuracy (absolute score) by condition.\nnote: this example uses the combined dataset rather than lab-only, as learning notes were done with the combined and I don’t want to recalcualte all the marginal probabilities by hand for learning purposes.\n\nFit a logistic regression predicting accuracy (absolute score) (n = r nrow(df)) by condition. (k = 2).\nParameter estimate: \\(\\beta_{0}\\) = Log Odds of (correct) responses in CONTROL condition\n\n\\(e^{\\beta_{0}}\\) = ODDS of correct response in CONTROL condition\n\nParameter estimate: \\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for correct response in impasse (vs) control [log scale])\n\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of correct response in IMPASSE (vs) CONTROL\n\nNull hypothesis:\\(\\beta_{impasse} \\le 0\\) the odds for a correct response does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\gt 0\\) the odds of a correct response increases\n\n\nFit Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\n\nCODE\n#combined dataset, not lab only\ndf <- df_items %>% filter(q==1) %>% mutate(\n  accuracy = as.factor(score_niceABS)\n)\n\n# FREQUENCY TABLE\n# my.table <- table(df$accuracy, df$pretty_condition)\n# addmargins(my.table) #counts\n# addmargins(prop.table(my.table)) #props\n\n# MODEL FITTING:::::::::::::::::::::::::::::::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\nprint(\"EMPTY MODEL\")\n\n\n[1] \"EMPTY MODEL\"\n\n\nCODE\nsummary(m0)\n\n\n\nCall:\nglm(formula = accuracy ~ 1, family = \"binomial\", data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.696  -0.696  -0.696  -0.696   1.753  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.294      0.134   -9.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 343.66  on 329  degrees of freedom\nAIC: 345.7\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\n#: 2 CONDITION model\nm1 <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.819  -0.819  -0.548  -0.548   1.986  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.822      0.230   -7.93  2.2e-15 ***\npretty_conditionimpasse    0.901      0.285    3.16   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 333.07  on 328  degrees of freedom\nAIC: 337.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\n#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > m1$aic)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff |  Chi2 |     p\n-------------------------------------------\nm0   |   glm |  1 |         |       |      \nm1   |   glm |  2 |       1 | 10.59 | 0.001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.00113745235691825\"\n\n\nThe Condition predictor significantly improves model fit.\n\n\nLearning Notes\n\n\nCODE\n# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.819  -0.819  -0.548  -0.548   1.986  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.822      0.230   -7.93  2.2e-15 ***\npretty_conditionimpasse    0.901      0.285    3.16   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 333.07  on 328  degrees of freedom\nAIC: 337.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\n#: INTERPRET COEFFICIENTS\n\nprint(\"Coefficients —- LOG ODDS\")\n\n\n[1] \"Coefficients —- LOG ODDS\"\n\n\nCODE\nconfint(m1)\n\n\nWaiting for profiling to be done...\n\n\n                         2.5 % 97.5 %\n(Intercept)             -2.299  -1.39\npretty_conditionimpasse  0.353   1.48\n\n\nCODE\nprint(\"Coefficients —- ODDS RATIOS\")\n\n\n[1] \"Coefficients —- ODDS RATIOS\"\n\n\nCODE\ne <- cbind( exp(coef(m1)), exp(confint(m1))) #exponentiate\n\n\nWaiting for profiling to be done...\n\n\nCODE\ne\n\n\n                              2.5 % 97.5 %\n(Intercept)             0.162  0.10  0.248\npretty_conditionimpasse 2.463  1.42  4.374\n\n\nCODE\n# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: \n\n# one-sided (right tail) z test for B COEFFICIENT\n#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients\n\n#SANITY CHECK 2-tailed test should match the model output\ntt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"p value for two-tailed test, null B = 0 : \",round(tt,3))\n\n\n[1] \"p value for two-tailed test, null B = 0 :  0.002\"\n\n\nCODE\not <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"BUT we want a one tailed directional, null: B <= 0: \",round(ot,3))\n\n\n[1] \"BUT we want a one tailed directional, null: B <= 0:  0.001\"\n\n\nCODE\npaste(\"adjusted confint for directional hypothesis\")\n\n\n[1] \"adjusted confint for directional hypothesis\"\n\n\nCODE\n(dcint <- confint(m1, level = 0.90)) # get 90% for right side))\n\n\nWaiting for profiling to be done...\n\n\n                          5 %  95 %\n(Intercept)             -2.22 -1.46\npretty_conditionimpasse  0.44  1.38\n\n\nCODE\n# https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte\n\n\n#:::::::: INTERPRET COEFFICIENTS [directional]\n\n# print(\"Confidence Interval —- LOG ODDS\")\n# confint(m1) #not adjusted for 1-tailed\n# print(\"Coefficients —- ODDS RATIOS\")\n# (e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiated, not adjusted\n# (e <- cbind( exp(coef(m1)), exp(dcint))) #exponentiated, adjusted\n\n\nUnderstanding the logistic regression model\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\ntable(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n     \n      control impasse   Sum\n  0     0.861   0.715 0.785\n  1     0.139   0.285 0.215\n  Sum   1.000   1.000 1.000\n\n\nCODE\ntable(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1) #sanity check sum of columns\n\n\n     \n      control impasse Sum\n  0       136     123 259\n  1        22      49  71\n  Sum     158     172 330\n\n\nThe logistic regression intercept gives the log odds of the outcome for the reference level of the predictor variable\nThe logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.\n[the empty model\n\nThe intercept of an empty model (glm(accuracy ~ 1) is equal to log(p/(1-p)), where p = the overall probability of a correct response (df$accuracy ==1 ).\nIn SGC3A Q1 accuracy this = 71 correct / 330 = 0.215 -> log(0.215 / (1-0.215)) = -1.29.\nIn other words, the intercept from the model with no predictor variables is the estimated log odds of a correct response for the whole sample.\nWe can also transform the log of the odds back to a probability: p = ODDS/ (1+ODDS) = exp(-1.29)/(1+exp(-1.29)) = 0.215. This should matched the prediction of the empty model\n\n[a dichotomous predictor]\nnatural log (odds of +) = -1.822 + 0.901(x1) ; x1 = 0 for control, 1 for impasse - b0 intercept is ODDS OF CORRECT RESPONSE IN REFERENCE (control) - b1 intercept is ODDS RATIO (difference in odds) FOR CORRECT RESPONSE IN IMPASSE\n\nINTERCEPT: log odds of (+ response) in control condition\n\nlog odds of (+) in control : -1.822 + 0.9(0) = -1.822\nconvert to odds by exponentiating the coefficients\nlog odds of (+) in control = exp(-1.822) = 0.162 odds\nconvert to probability by formula =>\np(+) = odds / (1+odds) = 0.162 / (1 + 0.162) = 0.139\nprobability of (+) in control = ~14%\n\nB1 COEFFICIENT: DIFFERENCE in log odds of (+) in impasse vs. control\n\nlog odds of (+) in impasse: -1.822 + 0.901 = -0.921\nconvert to odds by exponentiating log odds\nlog odds (+) in impasse = exp(-0.921) = 0.398\nconvert to probability by formula =>\np(+) = odds / (1 + odds) = 0.398 / (1+0.398) = 0.285\nprobaility of (+) in impasse = ~ 29%\n\nODDS RATIO : exponentiated B1 COEFFICIENT\n\nB1 = (slope of logit model = difference in log odds = log odds ratio\nB1 = 0.901 is log odds ratio of (+) in impasse vs control\nexp(b1) = exp(0.901) = 2.46\nRatio of odds in impasse are 2.46 times higher than in control. Bein in the impasse condition yields odds athat are 2.46 X higher than in control.\n\n\n\n\n\n\n\n\nMARGINAL\ntotal = 330 success : 71, failure : 259\np(+) = 71 / 330 = 0.215 = 22%\nodds(+) = 71 / 259 = 0.274\n\n\nCONTROL total = 158 success = 22; failure = 136\np(+) = 22/158 = 0.139 = 14%\nodds(+) = 22/136 = 0.162\n\n\nIMPASSE total = 172 success = 49; failure = 123\np(+) = 49/172 = 0.285 = 29%\nodds(+) = 49/123 = 0.398\n\n\n\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \nggcoefstats(m1, output = \"plot\", \n              conf.level = 0.90) + \n  labs(x = \"Log Odds Estimate\", \n       subtitle = \"p is for two tailed test\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m1, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) + #manually adjusted for directional test   \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m1, type=\"eff\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\n# tab_model(m1)\n\n\n\n\nDiagnostics\n\n\nCODE\nprint(\"SANITY CHECK REPORTING\")\n\n\n[1] \"SANITY CHECK REPORTING\"\n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic model (estimated using ML) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model's explanatory power is weak (Tjur's R2 = 0.03). The model's intercept, corresponding to pretty_condition = control, is at -1.82 (95% CI [-2.30, -1.39], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.90, 95% CI [0.35, 1.48], p = 0.002; Std. beta = 0.90, 95% CI [0.35, 1.48])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nCODE\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(m1)\n\n\n# Indices of model performance\n\nAIC     |     BIC | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n----------------------------------------------------------------------------------------------\n337.074 | 344.673 |     0.031 | 0.404 | 1.008 |    0.505 |   -16.847 |           0.021 | 0.673\n\n\nCODE\nprint(\"MODEL DIAGNOSTICS\")\n\n\n[1] \"MODEL DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m1)\n\n\n\n\n\nCODE\nbinned_residuals(m1)\n\n\nOk: About 100% of the residuals are inside the error bounds.\n\n\nCODE\n# logitgof(df$accuracy, m1$fitted.values, ord=FALSE)\n# hoslem.test(x = df$accuracy, y = fitted(m1), g =  2)\n# p should be non significant\n\n\n\n\nInference\nWe fit a logistic regression model to analyze the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 3.16, p = 0.0016). The model predicts that the odds of a correct response on the first question in the impasse condition increase by 146% (\\(e^{beta_1}\\) = 2.46, 95% CI [1.42, 4.37]) over the control condition.\nEquivalent statements:\n\nbeing in impasse condition increases log odds of correct response by 0.901 (over control)\nbeing in impasse increases odds of correct response in impasse over control increases by factor of 2.46\nprobability of correct response in control predicted as 28.5%, vs only 14% in control condition\n\n\n\nCODE\n#PRETTY TABLE SJPLOT\ntab_model(m1)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.16\n0.10 – 0.25\n<0.001\n\n\npretty condition[impasse]\n2.46\n1.42 – 4.37\n0.002\n\n\nObservations\n330\n\n\nR2 Tjur\n0.031"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#ordinal-outcome",
    "href": "analysis/utils/modelling_ref.html#ordinal-outcome",
    "title": "Modelling Reference",
    "section": "ORDINAL OUTCOME",
    "text": "ORDINAL OUTCOME\nDoes CONDITION affect the Q1 [ordered] type of response given?\n\n\nCODE\n#SETUP DATA\ndf <- df_items %>% filter(q==1)  %>% filter(mode == \"lab-synch\") \n#scaled has already been set as an ordered factor of score_SCALED\n\n#::::::::::::DESCRIPTIVES\nprop.table(table(df$scaled, df$condition))\n\n\n           \n            control impasse\n  orth      0.39683 0.15873\n  unknown   0.00794 0.00000\n  uncertain 0.00000 0.14286\n  lines     0.00794 0.05556\n  tri       0.07937 0.15079\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\ngf_props(~scaled, fill= ~condition, data = df) %>% \n  gf_facet_grid(condition ~ .) + easy_remove_legend()\n\n\n\n\n\nCODE\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = condition,\n                       fill = scaled)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"\",\n       title = \"Q1 Interpretation\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nDoes CONDITION affect the Q1 [ordered] state of understanding?\n\n\nCODE\n#SETUP DATA\ndf <- df_items %>% filter(q==1)  %>% filter(mode == \"lab-synch\") \n#scaled has already been set as an ordered factor of score_SCALED\n\n#::::::::::::DESCRIPTIVES\nprop.table(table(df$state, df$condition))\n\n\n           \n            control impasse\n  orth-like 0.39683 0.15873\n  unknown   0.00794 0.14286\n  tri-like  0.08730 0.20635\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\ngf_props(~state, fill= ~condition, data = df) %>% \n  gf_facet_grid(condition ~ .) + easy_remove_legend()\n\n\n\n\n\nCODE\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"\",\n       title = \"Q1 State\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\n\nORDINAL REGRESSION — Cumulative Link; Proportional Odds\nFit an ordinal logistic regression (at the subject level), predicting Q1 interpretation by condition.\n\nhttps://stats.oarc.ucla.edu/r/faq/ologit-coefficients/\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\ntodo see ordinal regression video: https://www.youtube.com/watch?v=rPcMcW25PPA&ab_channel=NCRMUK\nhttps://peopleanalytics-regression-book.org/ord-reg.html\nhttps://medium.com/evangelinelee/brant-test-for-proportional-odds-in-r-b0b373a93aa2\nhttps://github.com/runehaubo/ordinal/blob/master/old_vignettes/clm_tutorial.pdf\n\nLearning Notes\n\nproportional odds regression models effectively act as a series of stratified binomial models under the assumption that the ‘slope’ of the logistic function of each stratified model is the same.\nthus need to verify proportional odds assumption - ordinal regression requires an proportional odds assumption (the same slope holds for each equation)\nthis is required because the model simultaneously estimates k-1 equations, but each equation has the same slope, with different intercepts.\nconversely, a multinomial (categorical) model will have different slopes as well as intercepts - the intercepts are always ordered in size alpha 1 < alpha 2 < alpha k-1…\nTODO - see difference between the three types of ordinal models (adjacent category (vs) cumulative proportions; check Agresti book\n\n\nFit Model\n\n\nCODE\n#::::::::::::ORDINAL REGRESSION MODELS\n\n#EMPTY MODEL\npaste(\"EMPTY Ordinal regression of q1 SCALED score (ordered interpretation)\")\n\n\n[1] \"EMPTY Ordinal regression of q1 SCALED score (ordered interpretation)\"\n\n\nCODE\nom.0 <- clm(state ~ 1 , data = df)\n# summary(om.0)\n\n#PREDICTOR MODEL\npaste(\"Ordinal regression of q1 SCALED score (ordered interpretation)\")\n\n\n[1] \"Ordinal regression of q1 SCALED score (ordered interpretation)\"\n\n\nCODE\nom <- clm(state ~ condition, data = df)\n# summary(om)\n\n#COMPARE EMPTY AND PREDICTOR\ntest_lrt(om.0, om)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff |  Chi2 |      p\n--------------------------------------------\nom.0 |   clm |  2 |         |       |       \nom   |   clm |  3 |       1 | 25.77 | < .001\n\n\nCODE\n#::::::::: EQUIVALENT APPROACH USING POLYR \n\n# #MODEL\nm <- polr(state ~ condition , data = df, Hess=TRUE)\n# summary(m)\n\n#exponentiate coefficients and CIs\n# (ctable <- coef(summary(m)))\n# (p <- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2)\n# (ctable <- cbind(ctable, \"p value\" = p))\n# (ci <- confint(m))\n# (e <- coef(m))\n\n\nLikelihood ratio test suggests the predictor model is a better fit than the empty (intercept only) model.\n\n\nInference\n\n\nCODE\npaste(\"SUMMARY\")\n\n\n[1] \"SUMMARY\"\n\n\nCODE\nsummary(om)\n\n\nformula: state ~ condition\ndata:    df\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  126  -109.55 225.09 6(0)  2.18e-10 2.5e+01\n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \nconditionimpasse      1.9        0.4    4.74  2.1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n                  Estimate Std. Error z value\north-like|unknown    1.326      0.323    4.10\nunknown|tri-like     2.109      0.355    5.94\n\n\nCODE\n#LOG ODDS\npaste(\"IN LOG ODDS\")\n\n\n[1] \"IN LOG ODDS\"\n\n\nCODE\n(ctable <- coef(summary(om)))\n\n\n                  Estimate Std. Error z value Pr(>|z|)\north-like|unknown     1.33      0.323    4.10 4.07e-05\nunknown|tri-like      2.11      0.355    5.94 2.80e-09\nconditionimpasse      1.90      0.400    4.74 2.10e-06\n\n\nCODE\n(ci <- confint(om)) \n\n\n                 2.5 % 97.5 %\nconditionimpasse  1.14   2.72\n\n\nCODE\npaste(\"IN ODDS RATIOS\")\n\n\n[1] \"IN ODDS RATIOS\"\n\n\nCODE\n#ODDS RATIOS\nexp(coef(om))\n\n\north-like|unknown  unknown|tri-like  conditionimpasse \n             3.77              8.24              6.68 \n\n\nCODE\nexp(ci)\n\n\n                 2.5 % 97.5 %\nconditionimpasse  3.12   15.1\n\n\nOverall, participants in the impasse condition had higher odds (6.68 X as likely) to offer more correct interpretations than those in the control condition (z = 4.74, p < 0.001).\n\nwe see the estimates for the 2 intercepts, which are sometimes called cutpoints.\nThe intercepts indicate where the latent variable is cut to make the three groups that we observe in our data.\nNote that this latent variable is continuous. In general, these are not used in the interpretation of the results.\nThe cutpoints are closely related to thresholds, which are reported by other statistical packages.\nfor k groups there will be k-1 intercepts (cutpoints)\nconfirm that the CI does not include 0 (the units are ordered logits [ordered log odds])\nas with logistic regression we exponentiate the coefficients and confints to get odds ratio\n\n\n\nVisualize Model\n\n\nCODE\n# sjPlot::tab_model(om)\nsjPlot::plot_model(om)\n\n\n\n\n\nCODE\nsjPlot::plot_model(om, type = \"eff\")\n\n\n$condition\n\n\n\n\n\nCODE\n                   # show.data = TRUE, jitter = TRUE)\n\n\n\n\nDiagnostics\n\n\nCODE\n#:: ASSESS FIT\nperformance(om)\n\n\nCan't calculate log-loss.\nCan't calculate proper scoring rules for ordinal, multinomial or cumulative link models.\n\n\n# Indices of model performance\n\nAIC     |     BIC | Nagelkerke's R2 |  RMSE | Sigma\n---------------------------------------------------\n225.090 | 233.599 |           0.216 | 1.629 | 1.648\n\n\nCODE\n# #test proporitional odds assumption \nbrant(m) #only works for polyr type model not clm type model\n\n\n---------------------------------------------------- \nTest for        X2  df  probability \n---------------------------------------------------- \nOmnibus         15.6    1   0\nconditionimpasse    15.6    1   0\n---------------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nCODE\n# # A p-value of less than 0.05 on this test—particularly on the Omnibus plus at least one of the variables—should be interpreted as a failure of the proportional odds assumption.\n\n#test proportional odds assumption\nlibrary(pomcheckr)\n# https://cran.r-project.org/web/packages/pomcheckr/pomcheckr.pdf\n(p <- pomcheck( scaled ~ condition , data = df))\n\n\n[[1]]\n# A tibble: 2 × 6\n# Groups:   condition [2]\n  condition `scaled_>=1` `scaled_>=2` `scaled_>=4` `scaled_>=5` `scaled_>=3`\n  <fct>            <dbl>        <dbl>        <dbl>        <dbl>        <dbl>\n1 control            Inf        -1.43       -1.53        -1.65        NA    \n2 impasse            Inf        NA          -0.379       -0.862        0.788\n\nattr(,\"class\")\n[1] \"pomcheck\" \"list\"    \n\n\nCODE\nplot(p)\n\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\nThe output of the graphical test for proportional odds assumption suggests that the proportional odds assumption may be unreasonable for this dataset. Also, inspecting the output table, we see the coefficients for each level of the scaled variable are quite different.\nThus, an alternative approach may be more appropriate:\n\nBaseline logistic model. This model is the same as the multinomial regression model covered in the previous chapter, using the lowest ordinal value as the reference.\nAdjacent-category logistic model. This model compares each level of the ordinal variable to the next highest level, and it is a constrained version of the baseline logistic model. The brglm2 package in R offers a function bracl() for calculating an adjacent category logistic model.\nContinuation-ratio logistic model. This model compares each level of the ordinal variable to all lower levels. This can be modeled using binary logistic regression techniques, but new variables need to be constructed from the data set to allow this. The R package rms has a function cr.setup() which is a utility for preparing an outcome variable for a continuation ratio model.\n\n*Note: for multiple regression, the ordinal package offers a parameter (nominal = ~predictors) that allow you to designate some predictors as nominal rather than ordinal. But this is not appropriate for this use case._"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#categorical-outcome",
    "href": "analysis/utils/modelling_ref.html#categorical-outcome",
    "title": "Modelling Reference",
    "section": "CATEGORICAL OUTCOME",
    "text": "CATEGORICAL OUTCOME\nDoes CONDITION affect the Q1 [ordered] type of response given?\n\n\nCODE\n#SETUP DATA\ndf <- df_items %>% filter(q==1)  %>% filter(mode == \"lab-synch\") \n#scaled has already been set as an ordered factor of score_SCALED\n\n#::::::::::::DESCRIPTIVES\nprop.table(table(df$scaled, df$condition))\n\n\n           \n            control impasse\n  orth      0.39683 0.15873\n  unknown   0.00794 0.00000\n  uncertain 0.00000 0.14286\n  lines     0.00794 0.05556\n  tri       0.07937 0.15079\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\ngf_props(~scaled, fill= ~condition, data = df) %>% \n  gf_facet_grid(condition ~ .) + easy_remove_legend()\n\n\n\n\n\nCODE\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = condition,\n                       fill = scaled)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"\",\n       title = \"Q1 Interpretation\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nDoes CONDITION affect the Q1 [ordered] state of understanding?\n\n\nCODE\n#SETUP DATA\ndf <- df_items %>% filter(q==1)  %>% filter(mode == \"lab-synch\") \n#scaled has already been set as an ordered factor of score_SCALED\n\n#::::::::::::DESCRIPTIVES\ntable(df$state, df$condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse    Sum\n  orth-like  0.8065  0.3125 0.5556\n  unknown    0.0161  0.2812 0.1508\n  tri-like   0.1774  0.4062 0.2937\n  Sum        1.0000  1.0000 1.0000\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\ngf_props(~state, fill= ~condition, data = df) %>% \n  gf_facet_grid(condition ~ .) + easy_remove_legend()\n\n\n\n\n\nCODE\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"\",\n       title = \"Q1 State\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#GGSTATSPLOT\nggbarstats(df, state, condition)\n\n\n\n\n\n\nCHI SQUARE\nDoes CONDITION affect the Q1 [categorical] type of response given?\n\n\nCODE\nCrossTable( x = df$condition, y = df$scaled, \n             fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\nWarning in chisq.test(t, correct = FALSE, ...): Chi-squared approximation may be\nincorrect\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  126 \n\n \n             | df$scaled \ndf$condition |      orth |   unknown | uncertain |     lines |       tri | Row Total | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n     control |        50 |         1 |         0 |         1 |        10 |        62 | \n             |    34.444 |     0.492 |     8.857 |     3.937 |    14.270 |           | \n             |     7.025 |     0.524 |     8.857 |     2.191 |     1.278 |           | \n             |     0.806 |     0.016 |     0.000 |     0.016 |     0.161 |     0.492 | \n             |     0.714 |     1.000 |     0.000 |     0.125 |     0.345 |           | \n             |     0.397 |     0.008 |     0.000 |     0.008 |     0.079 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n     impasse |        20 |         0 |        18 |         7 |        19 |        64 | \n             |    35.556 |     0.508 |     9.143 |     4.063 |    14.730 |           | \n             |     6.806 |     0.508 |     8.580 |     2.122 |     1.238 |           | \n             |     0.312 |     0.000 |     0.281 |     0.109 |     0.297 |     0.508 | \n             |     0.286 |     0.000 |     1.000 |     0.875 |     0.655 |           | \n             |     0.159 |     0.000 |     0.143 |     0.056 |     0.151 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\nColumn Total |        70 |         1 |        18 |         8 |        29 |       126 | \n             |     0.556 |     0.008 |     0.143 |     0.063 |     0.230 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  39.1     d.f. =  4     p =  6.55e-08 \n\n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nAlternative hypothesis: two.sided\np =  1.01e-09 \n\n \n\n\n\n\nCODE\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Interpretation on First Question by Condition\",\n            data = df, condition ~ scaled, \n            shade = T)\n\n\n\n\n\nCODE\n#::::::::::::WITH STATISTICS\nggbarstats(data = df, x = condition, y = scaled,\n           type = \"nonparametric\") \n\n\n\n\n\n\n\nCODE\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Response Type on First Question by Condition\",\n            data = df, condition ~ scaled, \n            shade = T)\n\n\n\n\n\nCODE\nCrossTable( x = df$condition, y = df$scaled, \n            fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\nWarning in chisq.test(t, correct = FALSE, ...): Chi-squared approximation may be\nincorrect\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  126 \n\n \n             | df$scaled \ndf$condition |      orth |   unknown | uncertain |     lines |       tri | Row Total | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n     control |        50 |         1 |         0 |         1 |        10 |        62 | \n             |    34.444 |     0.492 |     8.857 |     3.937 |    14.270 |           | \n             |     7.025 |     0.524 |     8.857 |     2.191 |     1.278 |           | \n             |     0.806 |     0.016 |     0.000 |     0.016 |     0.161 |     0.492 | \n             |     0.714 |     1.000 |     0.000 |     0.125 |     0.345 |           | \n             |     0.397 |     0.008 |     0.000 |     0.008 |     0.079 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n     impasse |        20 |         0 |        18 |         7 |        19 |        64 | \n             |    35.556 |     0.508 |     9.143 |     4.063 |    14.730 |           | \n             |     6.806 |     0.508 |     8.580 |     2.122 |     1.238 |           | \n             |     0.312 |     0.000 |     0.281 |     0.109 |     0.297 |     0.508 | \n             |     0.286 |     0.000 |     1.000 |     0.875 |     0.655 |           | \n             |     0.159 |     0.000 |     0.143 |     0.056 |     0.151 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\nColumn Total |        70 |         1 |        18 |         8 |        29 |       126 | \n             |     0.556 |     0.008 |     0.143 |     0.063 |     0.230 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  39.1     d.f. =  4     p =  6.55e-08 \n\n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nAlternative hypothesis: two.sided\np =  1.01e-09 \n\n \n\n\nCODE\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"scaled\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n\n\n\n \n scaled\n pretty_condition\n Total\n \n \n\n control\n impasse\n \n \n \north\n503471.4 %80.6 %\n203628.6 %31.2 %\n7070100 %55.6 % \n\n \n \nunknown\n10100 %1.6 %\n010 %0 %\n11100 %0.8 % \n\n \n \nuncertain\n090 %0 %\n189100 %28.1 %\n1818100 %14.3 % \n\n \n \nlines\n1412.5 %1.6 %\n7487.5 %10.9 %\n88100 %6.3 % \n\n \n \ntri\n101434.5 %16.1 %\n191565.5 %29.7 %\n2929100 %23 % \n\n \n \nTotal\n626249.2 %100 %\n646450.8 %100 %\n126126100 %100 % \n\nχ2=39.128 · df=4 · Cramer's V=0.557 · Fisher's p=0.000 \n\n \n \n observed values\n expected values\n % within scaled\n % within pretty_condition\n \n\n\n\nTODO INFERENCE\n\n\nMULTINOMIAL REGRESSION\nTODO RECONCILE actual predictions with coefficients and visualization of predictions\nDoes condition affect the response state of Q1?\n\nhttps://peopleanalytics-regression-book.org/multinomial-logistic-regression-for-nominal-category-outcomes.html#running-a-multinomial-regression-model\nhttps://bookdown.org/chua/ber642_advanced_regression/multinomial-logistic-regression.html\nhttps://www.youtube.com/watch?v=JcCBIPqcwFo&list=PLzv58M2GAfm50X_Twskr1aXaV5qMuIszx&ab_channel=NCRMUK\nCan use nnet package multinom() or mclogit package mblogit() [“baseline logit model”] or brms with family = “categorical”\n\nFit a logistic regression predicting interpretation (k=#response categories) by condition. (k = 2).\n\n2 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer)\n(from experience, seems) Each cell must have at least one observation (if one cell is blank, then it seems to be incorrectly estimated, see predicting high_interpretation vs. state. suggests nonsignificant OR for ‘unknown’ category, when infact that difference drives the effect\nFor each equation:\n\n\\(\\beta_{0}\\) = Log Odds of [this category type vs. reference category type) response in CONTROL condition\n\n\\(e^{\\beta_{0}}\\) = ODDS of [this category type vs. reference category type] response in CONTROL condition\n\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for [this category] type response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of [this. vs reference category type] response in IMPASSE (vs) CONTROL\nNull hypothesis: \\(\\beta_{impasse} \\le 0\\) the odds for [this category of response vs. reference] does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\gt 0\\) the odds of [this category of response vs. reference] increases\n\n\n\n\n\nCODE\n#::::::::::::DESCRIPTIVES RESPONSE STATE\n\ntable(df$state, df$condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse    Sum\n  orth-like  0.8065  0.3125 0.5556\n  unknown    0.0161  0.2812 0.1508\n  tri-like   0.1774  0.4062 0.2937\n  Sum        1.0000  1.0000 1.0000\n\n\nCODE\n(t <- table(df$state, df$condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n           \n            control impasse Sum\n  orth-like      50      20  70\n  unknown         1      18  19\n  tri-like       11      26  37\n  Sum            62      64 126\n\n\n\nFit Model\n\n\nCODE\nlibrary(nnet)\n\n#check reference level \nlevels(df$state)\n\n\n[1] \"orth-like\" \"unknown\"   \"tri-like\" \n\n\nCODE\n#FIT EMPTY MODEL\ncatm.0 <- multinom(state ~ 1, data = df)\n\n\n# weights:  6 (2 variable)\ninitial  value 138.425148 \nfinal  value 122.428550 \nconverged\n\n\nCODE\n# summary(catm.0)\n\n#FIT PREDICTOR MODEL\ncatm <- multinom(formula = state ~ condition, data = df, model = TRUE)\n\n\n# weights:  9 (4 variable)\ninitial  value 138.425148 \niter  10 value 103.421004\niter  10 value 103.421004\nfinal  value 103.421004 \nconverged\n\n\nCODE\n# summary(catm)\n\n#COMPARE MODEL FIT\ntest_lrt(catm.0, catm)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |      p\n-------------------------------------------------\ncatm.0 | multinom |  2 |         |       |       \ncatm   | multinom |  4 |       2 | 38.02 | < .001\n\n\nCODE\n##compare bayesian version\n#library(brms)\n# bm1 <- brm( state ~ pretty_condition, data = df, family = \"categorical\")\n# summary(bm1)\n# plot_model(bm1)\n# report(bm1)\n# coefficient estimates are very simliar to catm. super cool!\n\n##compare mclogit version\n#\"baseline-category logit model\n# https://www.elff.eu/software/mclogit/manual/mblogit/\n# blm1 <- mblogit(state ~ pretty_condition , data = df)\n# summary(blm1)\n#identical to catm. super cool!\n\n\nLikelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.\n\n\nInterpretation\n\n\nCODE\n#::::::::INTERPRETATION\npaste(\"MODEL SUMMARY\")\n\n\n[1] \"MODEL SUMMARY\"\n\n\nCODE\nsummary(catm)\n\n\nCall:\nmultinom(formula = state ~ condition, data = df, model = TRUE)\n\nCoefficients:\n         (Intercept) conditionimpasse\nunknown        -3.91             3.81\ntri-like       -1.51             1.78\n\nStd. Errors:\n         (Intercept) conditionimpasse\nunknown        1.010            1.061\ntri-like       0.333            0.447\n\nResidual Deviance: 207 \nAIC: 215 \n\n\nCODE\n# calculate z-statistics of coefficients\n(z_stats <- summary(catm)$coefficients/summary(catm)$standard.errors)\n\n\n         (Intercept) conditionimpasse\nunknown        -3.87             3.59\ntri-like       -4.55             3.98\n\n\nCODE\n# convert to p-values\np_values <- (1 - pnorm(abs(z_stats)))*2\n# display p-values in transposed data frame\np_values <- data.frame(p = (p_values))\n# display odds ratios in transposed data frame\n\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\nodds_ratios <- data.frame(OR = exp(summary(catm)$coefficients))\n\n# options(scipen = 3)\n(results <- cbind(odds_ratios, p_values))\n\n\n         OR..Intercept. OR.conditionimpasse p..Intercept. p.conditionimpasse\nunknown            0.02               44.99      1.07e-04          0.0003330\ntri-like           0.22                5.91      5.45e-06          0.0000692\n\n\nLearning Notes\n\nModel estimates encompass two equations:\neffect of predictor on log odds of being in [unknown] instead of reference category [orth-like]\neffect of predictor on log odds of being in [tri-like] instead of reference category [orth-like]\n[need to to double check interpretation, but I think that the OR intercepts converted to probabilities equate to the marginal probability of being in each state in the [reference] control condition. which makes sense. I think]\nIF I change reference category for condition… then the intercepts should no longer be significant. The b1 coefficients should still be significant, but with changed sign (much less likely) [Yup! this works!]\n\n\n\nInference\n\nBeing in the IMPASSE condition increases the odds of giving ‘unknown/uncertain’ response rather than an orthogonal (or satisficing) response by a factor of 50 (z = 3.59, p < 0.001 ).\nBeing in the IMPASSE condition increases the odds of giving an ‘triangular or line-driven’ response rather than an orthogonal (or satisficing) response by a factor of 6 (z = 3.98, p <0.001 )\n\n\n\nVisualize\n\n\nCODE\nplot_model(catm, vline.color = 'red')\n\n\n\n\n\nCODE\nplot_model(catm, type = \"eff\")\n\n\n$condition\n\n\n\n\n\n\n\nDiagnostics\n\n\nCODE\n#EXAMINE PREDICTIONS\n#create sample data frame\ntest <- data.frame(condition = c(\"control\", \"impasse\"))\npred <- predict(catm, newdata = test, \"probs\")\npaste(\"Predicted Probability of Being in Each State\")\n\n\n[1] \"Predicted Probability of Being in Each State\"\n\n\nCODE\n(cbind(test, pred))\n\n\n  condition orth-like unknown tri-like\n1   control     0.806  0.0161    0.177\n2   impasse     0.312  0.2812    0.406\n\n\nCODE\n#performance\nperformance(catm)\n\n\nCan't calculate log-loss.\nCan't calculate proper scoring rules for ordinal, multinomial or cumulative link models.\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n-----------------------------------------------------\n214.842 | 226.187 | 0.155 |     0.147 | 0.404 | 1.302\n\n\nCODE\nDescTools::PseudoR2(catm, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n\n\nRegistered S3 method overwritten by 'DescTools':\n  method         from \n  reorder.factor gdata\n\n\n  McFadden   CoxSnell Nagelkerke \n     0.155      0.260      0.304 \n\n\nCODE\n#General Goodness of Fit\n# library(generalhoslem)\n# logitgof(df$state, catm$fitted.values, g = 3)\n# hoslem.test(x = df$state, y = catm$fitted.values, g =  10)\n#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).\n#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#mixed-continuous",
    "href": "analysis/utils/modelling_ref.html#mixed-continuous",
    "title": "Modelling Reference",
    "section": "MIXED — CONTINUOUS",
    "text": "MIXED — CONTINUOUS"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#mixed-binomial",
    "href": "analysis/utils/modelling_ref.html#mixed-binomial",
    "title": "Modelling Reference",
    "section": "MIXED — BINOMIAL",
    "text": "MIXED — BINOMIAL\n\nMixed Logistic Regression\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on test phase questions by condition; accounting for random effects of subject.\n\nFit Model\n\n\nCODE\n#SETUP DATA \nn_items = 8 #number of items in test\n\n#item level\ndf = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  accuracy = as.factor(score_niceABS),\n  q = as.factor(q)\n) %>% filter(mode ==\"lab-synch\")\n\nlibrary(lmerTest) #for CIs in glmer \n\n## 1 | SETUP RANDOM EFFECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \n\n#:: RANDOM INTERCEPT SUBJECT\nmm.rS <- glmer(accuracy ~ (1|subject), data = df,family = \"binomial\")\n\n# :: TEST random effect\npaste(\"AIC with random effect is lower than glm empty model?\", m0$aic > AIC(logLik(mm.rS)))\n\n\n[1] \"AIC with random effect is lower than glm empty model? TRUE\"\n\n\nCODE\ntest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName  |    Model | df | df_diff |   Chi2 |      p\n-------------------------------------------------\nm0    |      glm |  1 |         |        |       \nmm.rS | glmerMod |  2 |       1 | 678.41 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  1.4837179793132e-149\"\n\n\nCODE\n## 2 | ADD FIXED EFFECT\n\n# SUBJECT INTERCEPT | FIXED CONDITION \nmm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject), \n                data = df,family = \"binomial\")\n\n# :: TEST fixed factor \npaste(\"AIC with fixed effect is lower than random intercept only model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )\n\n\n[1] \"AIC with fixed effect is lower than random intercept only model? TRUE\"\n\n\nCODE\ntest_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff | Chi2 |     p\n-----------------------------------------------\nmm.rS  | glmerMod |  2 |         |      |      \nmm.CrS | glmerMod |  3 |       1 | 3.91 | 0.048\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS,mm.CrS))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.047907883063779\"\n\n\n\n\nVisualize\n\n\nCODE\n#: PRINT MODEL \nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsumm(mm.CrS)\n\n\n\n\n  \n    Observations \n    1008 \n  \n  \n    Dependent variable \n    accuracy \n  \n  \n    Type \n    Mixed effects generalized linear model \n  \n  \n    Family \n    binomial \n  \n  \n    Link \n    logit \n  \n\n \n\n  \n    AIC \n    582.02 \n  \n  \n    BIC \n    596.77 \n  \n  \n    Pseudo-R² (fixed effects) \n    0.07 \n  \n  \n    Pseudo-R² (total) \n    0.95 \n  \n\n \n \nFixed Effects\n  \n      \n    Est. \n    S.E. \n    z val. \n    p \n  \n \n\n  \n    (Intercept) \n    -7.46 \n    1.20 \n    -6.21 \n    0.00 \n  \n  \n    pretty_conditionimpasse \n    4.03 \n    1.67 \n    2.42 \n    0.02 \n  \n\n \n \nRandom Effects\n  \n    Group \n    Parameter \n    Std. Dev. \n  \n \n\n  \n    subject \n    (Intercept) \n    7.36 \n  \n\n \n \nGrouping Variables\n  \n    Group \n    # groups \n    ICC \n  \n \n\n  \n    subject \n    126 \n    0.94 \n  \n\n\n\n\nCODE\n#: INTERPRET COEFFICIENTS\n\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(mm.CrS)\n\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n----------------------------------------------------------------------------------------------------------------------\n582.024 | 582.048 | 596.771 |      0.946 |      0.066 | 0.943 | 0.213 | 1.000 |    0.145 |      -Inf |           0.021\n\n\nCODE\nprint(\"SANITY CHECK REPORTING\")\n\n\n[1] \"SANITY CHECK REPORTING\"\n\n\nCODE\nreport(mm.CrS)\n\n\nPackage 'merDeriv' needs to be installed to compute confidence intervals\n  for random effect parameters.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nPackage 'merDeriv' needs to be installed to compute confidence intervals\n  for random effect parameters.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model included subject as random effect (formula: ~1 | subject). The model's total explanatory power is substantial (conditional R2 = 0.95) and the part related to the fixed effects alone (marginal R2) is of 0.07. The model's intercept, corresponding to pretty_condition = control, is at -7.46 (95% CI [-9.81, -5.11], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 4.03, 95% CI [0.76, 7.29], p = 0.016; Std. beta = 4.03, 95% CI [0.76, 7.29])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\n# se <- sqrt(diag(stats::vcov(m1)))\n# # table of estimates with 95% CI\n# (tab <- cbind(Est = fixef(m1), LL = fixef(m1) - 1.96 * se, UL = fixef(m1) + 1.96 *\n#     se))\n# (e <- exp(tab))\n\n#: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(mm.CrS, type=\"std2\", vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(mm.CrS, type=\"pred\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\n# ggeffect(model = mm.CrS) %>% plot()\n\n#SANITY CHECK SJPLOT\n# library(effects)\n# plot(allEffects(mm.CrS))\n\n\n\n\nDiagnostics\n\n\nCODE\ncheck_model(mm.CrS)\n\n\n\n\n\nCODE\nbinned_residuals(mm.CrS)\n\n\nWarning: Probably bad model fit. Only about 64% of the residuals are inside the error bounds.\n\n\n\n\nInference\nWe fit a mixed-effect binomial logistic regression model with random intercepts for subjects to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(3): 4.98, p < 0.05). Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 5 over the control condition \\(e^{\\beta_1}\\) = 5.11, 95% CI [1.17,22,36], p < 0.05.\n\n\nCODE\n# PRETTY TABLE SJPLOT\ntab_model(mm.CrS)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.01\n<0.001\n\n\npretty condition[impasse]\n56.09\n2.14 – 1472.88\n0.016\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 subject\n54.11\n\n\nICC\n0.94\n\n\nN subject\n126\n\nObservations\n1008\n\n\nMarginal R2 / Conditional R2\n0.066 / 0.946"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#mixed-ordinal",
    "href": "analysis/utils/modelling_ref.html#mixed-ordinal",
    "title": "Modelling Reference",
    "section": "MIXED — ORDINAL",
    "text": "MIXED — ORDINAL"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#mixed-categorical",
    "href": "analysis/utils/modelling_ref.html#mixed-categorical",
    "title": "Modelling Reference",
    "section": "MIXED — CATEGORICAL",
    "text": "MIXED — CATEGORICAL\n\n\nCODE\n#BAYESIAN MIXED VERSION\n#df <- df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% filter(term==\"spring18\")\n# mm1 <- brm( state ~ condition + (1|subject), data = df, family = \"categorical\", \n# file = \"analysis/models/sgc3a_brms_mixedcat_teststate.rds\" # cache model (can be removed)))\n# summary(mm1)\n# performance(mm1)\n# plot(mm1)\n# #report(mm1)\n# #check_posterior_predictions(mm1, draws=100)\n# # library(bayesplot)\n# library(bayestestR)\n# plot(rope(mm1))"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#wip-unknown",
    "href": "analysis/utils/modelling_ref.html#wip-unknown",
    "title": "Modelling Reference",
    "section": "WIP UNKNOWN",
    "text": "WIP UNKNOWN\n\nCummulative Ordinal (Bayesian) — Equal Variance\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\n\n\nCODE\nlibrary(brms)\n\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.17.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:mosaic':\n\n    mm\n\n\nThe following object is masked from 'package:gamlss':\n\n    cs\n\n\nThe following objects are masked from 'package:VGAM':\n\n    acat, cratio, cumulative, dfrechet, dirichlet, exponential,\n    frechet, geometric, lognormal, multinomial, negbinomial, pfrechet,\n    qfrechet, rfrechet, s, sratio\n\n\nThe following objects are masked from 'package:ordinal':\n\n    ranef, VarCorr\n\n\nThe following object is masked from 'package:lme4':\n\n    ngrps\n\n\nThe following objects are masked from 'package:ggdist':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\n\nThe following object is masked from 'package:survival':\n\n    kidney\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\nCODE\n#DEFINE DATA \ndf <- df_items %>% filter(q==1) %>% filter(mode ==\"lab-synch\")\n  \n#TODO: why is this probit instead of logit?\ncumord <- brm( formula = state ~ condition,\n               data = df,\n               family = cumulative(\"probit\"),\n               file = \"analysis/models/sgc3a_brms_cumord_q1state.rds\" # cache model (can be removed)\n)\n\nsummary(cumord)\n\n\n Family: cumulative \n  Links: mu = probit; disc = identity \nFormula: state ~ condition \n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]         0.75      0.18     0.41     1.11 1.00     3054     2790\nIntercept[2]         1.22      0.19     0.85     1.60 1.00     3305     2732\nconditionimpasse     1.11      0.23     0.66     1.55 1.00     3192     2953\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nconditional_effects(cumord, \"condition\", categorical = TRUE)\n\n\n\n\n\nCODE\n#SJPLOT\nplot_model(cumord)\n\n\n\n\n\nCODE\nplot(cumord)\n\n\n\n\n\nCODE\nplot(rope(cumord))\n\n\nPossible multicollinearity between b_conditionimpasse and b_Intercept[2] (r = 0.76). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\n\n\n\nCODE\n#REPORT\nreport(cumord)\n\n\nPossible multicollinearity between b_conditionimpasse and b_Intercept[2] (r = 0.76). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nPossible multicollinearity between b_conditionimpasse and b_Intercept[2] (r = 0.76). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nThe desired updates require recompiling the model\n\n\nCompiling Stan program...\n\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nclang -mmacosx-version-min=10.13 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include '/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:88:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name 'namespace'\nnamespace Eigen {\n^\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected ';' after top level declarator\nnamespace Eigen {\n               ^\n               ;\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found\n#include <complex>\n         ^~~~~~~~~\n3 errors generated.\nmake: *** [foo.o] Error 1\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL '3a68d5685b54180e73711b90f350310a' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.59 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.10493 seconds (Warm-up)\nChain 1:                0.112528 seconds (Sampling)\nChain 1:                0.217458 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '3a68d5685b54180e73711b90f350310a' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.110388 seconds (Warm-up)\nChain 2:                0.108933 seconds (Sampling)\nChain 2:                0.219321 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '3a68d5685b54180e73711b90f350310a' NOW (CHAIN 3).\nChain 3: Rejecting initial value:\nChain 3:   Log probability evaluates to log(0), i.e. negative infinity.\nChain 3:   Stan can't start sampling from this initial value.\nChain 3: \nChain 3: Gradient evaluation took 2.4e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.104081 seconds (Warm-up)\nChain 3:                0.104063 seconds (Sampling)\nChain 3:                0.208144 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '3a68d5685b54180e73711b90f350310a' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.108595 seconds (Warm-up)\nChain 4:                0.093594 seconds (Sampling)\nChain 4:                0.202189 seconds (Total)\nChain 4: \n\n\nPossible multicollinearity between b_conditionimpasse and b_Intercept[2] (r = 0.76). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is likely invalid for ordinal families.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a Bayesian probit model (estimated using MCMC sampling with 4 chains of 2000 iterations and a warmup of 1000) to predict state with condition (formula: state ~ condition). Priors over parameters were set as uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50) and uniform (location = , scale = ) distributions. The model's explanatory power is moderate (R2 = 0.18, 95% CI [0.08, 0.28]).  Within this model:\n\n  - The effect of b Intercept[1] (Median = 0.74, 95% CI [0.41, 1.11]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 99.58% of being large (> 0.30). The estimation successfully converged (Rhat = 1.001) and the indices are reliable (ESS = 3139)\n  - The effect of b Intercept[2] (Median = 1.22, 95% CI [0.85, 1.60]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30). The estimation successfully converged (Rhat = 1.001) and the indices are reliable (ESS = 3040)\n  - The effect of b conditionimpasse (Median = 1.11, 95% CI [0.66, 1.55]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 99.98% of being large (> 0.30). The estimation successfully converged (Rhat = 1.002) and the indices are reliable (ESS = 3314)\n\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT) framework, we report the median of the posterior distribution and its 95% CI (Highest Density Interval), along the probability of direction (pd), the probability of significance and the probability of being large. The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.05| and |0.30|. Convergence and stability of the Bayesian sampling has been assessed using R-hat, which should be below 1.01 (Vehtari et al., 2019), and Effective Sample Size (ESS), which should be greater than 1000 (Burkner, 2017).\n\n\nCODE\n# ord_cum %>%\n#   spread_draws(b_Intercept, r_condition[condition,]) %>%\n#   mutate(condition_mean = b_Intercept + r_condition) %>%\n#   ggplot(aes(y = condition, x = condition_mean)) +\n#   stat_halfeye()\n\n# performance(cumord)\n# plot(cumord)\n\n\n\n\nCummulative Ordinal (Bayesian) — Unequal Variance\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\n\n\nCODE\nlibrary(brms)\n\n\n#DEFINE DATA \ndf <- df_items %>% filter(q==1) %>% filter(mode ==\"lab-synch\")\n  \n#TODO: why is this probit instead of logit?\nu.cumord <- brm( \n  formula = bf(state ~ condition) +\n               lf(disc ~ 0 + condition, cmc = FALSE),\n               data = df,\n               family = cumulative(\"probit\"),\n               file = \"analysis/models/sgc3a_brms_ucumord_q1state.rds\" # cache model (can be removed)\n)\n\nsummary(u.cumord)\n\n\nWarning: There were 97 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See http://mc-stan.org/misc/\nwarnings.html#divergent-transitions-after-warmup\n\n\n Family: cumulative \n  Links: mu = probit; disc = log \nFormula: state ~ condition \n         disc ~ 0 + condition\n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept[1]              0.84      0.18     0.48     1.19 1.00      960\nIntercept[2]              1.01      0.19     0.64     1.39 1.01      908\nconditionimpasse          0.96      0.19     0.60     1.32 1.01      954\ndisc_conditionimpasse     1.52      0.61     0.43     2.68 1.02      313\n                      Tail_ESS\nIntercept[1]              1061\nIntercept[2]               764\nconditionimpasse           848\ndisc_conditionimpasse      227\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nconditional_effects(u.cumord, \"condition\", categorical = TRUE)\n\n\n\n\n\nCODE\n#SJPLOT\nplot_model(u.cumord)\n\n\n\n\n\nCODE\nplot(u.cumord)\n\n\n\n\n\nCODE\nplot(rope(u.cumord))\n\n\n\n\n\nCODE\n#REPORT\nreport(u.cumord)\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nThe desired updates require recompiling the model\n\n\nCompiling Stan program...\n\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nclang -mmacosx-version-min=10.13 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include '/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:88:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name 'namespace'\nnamespace Eigen {\n^\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected ';' after top level declarator\nnamespace Eigen {\n               ^\n               ;\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found\n#include <complex>\n         ^~~~~~~~~\n3 errors generated.\nmake: *** [foo.o] Error 1\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL '8051b427c5dbb5a865c588f34a9e215a' NOW (CHAIN 1).\nChain 1: Rejecting initial value:\nChain 1:   Log probability evaluates to log(0), i.e. negative infinity.\nChain 1:   Stan can't start sampling from this initial value.\nChain 1: Rejecting initial value:\nChain 1:   Log probability evaluates to log(0), i.e. negative infinity.\nChain 1:   Stan can't start sampling from this initial value.\nChain 1: \nChain 1: Gradient evaluation took 6.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.68 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.42362 seconds (Warm-up)\nChain 1:                0.151596 seconds (Sampling)\nChain 1:                0.575216 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '8051b427c5dbb5a865c588f34a9e215a' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.419343 seconds (Warm-up)\nChain 2:                0.37644 seconds (Sampling)\nChain 2:                0.795783 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '8051b427c5dbb5a865c588f34a9e215a' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.398055 seconds (Warm-up)\nChain 3:                0.388631 seconds (Sampling)\nChain 3:                0.786686 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '8051b427c5dbb5a865c588f34a9e215a' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3.1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.399437 seconds (Warm-up)\nChain 4:                0.464814 seconds (Sampling)\nChain 4:                0.864251 seconds (Total)\nChain 4: \n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is likely invalid for ordinal families.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a Bayesian probit model (estimated using MCMC sampling with 4 chains of 2000 iterations and a warmup of 1000) to predict state with condition (formula: state ~ condition). Priors over parameters were set as uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), uniform (location = , scale = ) and uniform (location = , scale = ) distributions. The model's explanatory power is moderate (R2 = 0.17, 95% CI [0.07, 0.26]).  Within this model:\n\n  - The effect of b Intercept[1] (Median = 0.84, 95% CI [0.48, 1.19]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 99.98% of being large (> 0.30). The estimation successfully converged (Rhat = 1.000) but the indices are unreliable (ESS = 953)\n  - The effect of b Intercept[2] (Median = 1.01, 95% CI [0.64, 1.39]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30). The estimation successfully converged (Rhat = 1.008) but the indices are unreliable (ESS = 298)\n  - The effect of b conditionimpasse (Median = 0.95, 95% CI [0.60, 1.32]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30). The estimation successfully converged (Rhat = 1.000) but the indices are unreliable (ESS = 952)\n  - The effect of b disc conditionimpasse (Median = 1.48, 95% CI [0.43, 2.68]) has a 99.80% probability of being positive (> 0), 99.72% of being significant (> 0.05), and 98.67% of being large (> 0.30). The estimation successfully converged (Rhat = 1.000) but the indices are unreliable (ESS = 905)\n\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT) framework, we report the median of the posterior distribution and its 95% CI (Highest Density Interval), along the probability of direction (pd), the probability of significance and the probability of being large. The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.05| and |0.30|. Convergence and stability of the Bayesian sampling has been assessed using R-hat, which should be below 1.01 (Vehtari et al., 2019), and Effective Sample Size (ESS), which should be greater than 1000 (Burkner, 2017).\n\n\nCODE\n# ord_cum %>%\n#   spread_draws(b_Intercept, r_condition[condition,]) %>%\n#   mutate(condition_mean = b_Intercept + r_condition) %>%\n#   ggplot(aes(y = condition, x = condition_mean)) +\n#   stat_halfeye()\n\n# performance(cumord)\n# plot(cumord)\n\n\nIn brms, the parameter related to latent variances is called disc (short for “discrimination”), following conventions in item response theory. Note that disc is not the variance itself, but the inverse of the standard deviation, s. That is, s = 1/disc. Further, because disc must be strictly positive, it is by default modeled on the log scale.\n\n\nAdjacent-Category Ordinal (Bayesian)\n\n\nCODE\ndf <- df_items %>% filter(q==1) %>% filter(mode ==\"lab-synch\")\n\n\n# # To specify an adjacent-category model, we use family = acat() instead of family = cumulative() as an argument to the brm() function. Then, to model condition with possible category-specific effects, we wrap this variable in cs() in the model’s formula:\n\nadjcat <- brm( formula = state ~ cs(condition),\n               data = df,\n               family = acat(\"probit\"),\n               file = \"analysis/models/sgc3a_brms_adjcat_q1state.rds\" # cache model (can be removed)\n)\n \nsummary(adjcat)\n\n\n Family: acat \n  Links: mu = probit; disc = identity \nFormula: state ~ cs(condition) \n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]            1.99      0.37     1.38     2.80 1.00      637      822\nIntercept[2]           -1.26      0.47    -2.29    -0.41 1.00      644      812\nconditionimpasse[1]     1.92      0.42     1.18     2.79 1.00      656      827\nconditionimpasse[2]    -1.02      0.50    -2.08    -0.14 1.00      652      732\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nconditional_effects(cumord, \"condition\", categorical = TRUE)\n\n\n\n\n\nCODE\nconditional_effects(adjcat, \"condition\", categorical = TRUE)\n\n\n\n\n\nCODE\n#WHICH IS BETTER? cumulative or adjacent?\nplot(compare_performance(cumord, adjcat))\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\n\n\n\nCODE\ncompare_performance(cumord, adjcat)\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\n# Comparison of Model Performance Indices\n\nName   |   Model |     ELPD | ELPD_SE |   LOOIC | LOOIC weights | LOOIC_SE |    WAIC | WAIC weights |    R2\n-----------------------------------------------------------------------------------------------------------\ncumord | brmsfit | -113.628 |   7.485 | 227.256 |         0.062 |   14.970 | 227.249 |        0.002 | 0.179\nadjcat | brmsfit | -107.434 |   7.111 | 214.868 |         0.938 |   14.223 | 214.718 |        0.998 | 0.169\n\n\nCODE\n# #TIDYBAYES VISUALIZATION\n# library(tidybayes)\n# adjcat %>%\n#   spread_draws(b_Intercept, r_condition[condition,]) %>%\n#   mutate(condition_mean = b_Intercept + r_condition) %>%\n#   ggplot(aes(y = condition, x = condition_mean)) +\n#   stat_halfeye()\n\nplot(cumord)\n\n\n\n\n\nCODE\nplot(adjcat)\n\n\n\n\n\nCODE\nreport(adjcat)\n\n\nPossible multicollinearity between bcs_conditionimpasse[1] and b_Intercept[2] (r = 0.81), bcs_conditionimpasse[2] and bcs_conditionimpasse[1] (r = 0.86). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nPossible multicollinearity between bcs_conditionimpasse[1] and b_Intercept[2] (r = 0.81), bcs_conditionimpasse[2] and bcs_conditionimpasse[1] (r = 0.86). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nThe desired updates require recompiling the model\n\n\nCompiling Stan program...\n\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nclang -mmacosx-version-min=10.13 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include '/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:88:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name 'namespace'\nnamespace Eigen {\n^\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected ';' after top level declarator\nnamespace Eigen {\n               ^\n               ;\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found\n#include <complex>\n         ^~~~~~~~~\n3 errors generated.\nmake: *** [foo.o] Error 1\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'ebf0c4c0e51ade2b00ca4594e215d3ad' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000553 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 5.53 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.47723 seconds (Warm-up)\nChain 1:                1.73124 seconds (Sampling)\nChain 1:                3.20846 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'ebf0c4c0e51ade2b00ca4594e215d3ad' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9.8e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.98 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.45574 seconds (Warm-up)\nChain 2:                1.74882 seconds (Sampling)\nChain 2:                3.20456 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'ebf0c4c0e51ade2b00ca4594e215d3ad' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000105 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.05 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.46164 seconds (Warm-up)\nChain 3:                1.42022 seconds (Sampling)\nChain 3:                2.88186 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'ebf0c4c0e51ade2b00ca4594e215d3ad' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.0001 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.42222 seconds (Warm-up)\nChain 4:                1.37809 seconds (Sampling)\nChain 4:                2.80032 seconds (Total)\nChain 4: \n\n\nPossible multicollinearity between bcs_conditionimpasse[1] and b_Intercept[2] (r = 0.83), bcs_conditionimpasse[2] and bcs_conditionimpasse[1] (r = 0.87). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is likely invalid for ordinal families.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a Bayesian probit model (estimated using MCMC sampling with 4 chains of 2000 iterations and a warmup of 1000) to predict state with condition (formula: state ~ cs(condition)). Priors over parameters were set as uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), NA (NA) and NA (NA) distributions. The model's explanatory power is moderate (R2 = 0.17, 95% CI [0.07, 0.27]).  Within this model:\n\n  - The effect of b Intercept[1] (Median = 1.94, 95% CI [1.38, 2.80]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30). The estimation successfully converged (Rhat = 1.004) but the indices are unreliable (ESS = 598)\n  - The effect of b Intercept[2] (Median = -1.22, 95% CI [-2.29, -0.41]) has a 99.85% probability of being negative (< 0), 99.85% of being significant (< -0.05), and 98.90% of being large (< -0.30). The estimation successfully converged (Rhat = 1.002) but the indices are unreliable (ESS = 612)\n  - The effect of bcs conditionimpasse[1] (Median = 1.88, 95% CI [1.18, 2.79]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30). The estimation successfully converged (Rhat = 1.004) but the indices are unreliable (ESS = 617)\n  - The effect of bcs conditionimpasse[2] (Median = -0.99, 95% CI [-2.08, -0.14]) has a 99.12% probability of being negative (< 0), 98.70% of being significant (< -0.05), and 93.70% of being large (< -0.30). The estimation successfully converged (Rhat = 1.003) but the indices are unreliable (ESS = 620)\n\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT) framework, we report the median of the posterior distribution and its 95% CI (Highest Density Interval), along the probability of direction (pd), the probability of significance and the probability of being large. The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.05| and |0.30|. Convergence and stability of the Bayesian sampling has been assessed using R-hat, which should be below 1.01 (Vehtari et al., 2019), and Effective Sample Size (ESS), which should be greater than 1000 (Burkner, 2017).\n\n\nConverges, but estimates are unreliable?"
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html",
    "href": "analysis/SGC2/1_sgc2_introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In Study Two we examine if scaffolding is effective in aiding untrained students to understand the Triangular Model (TM) graph. We know that students are unlikely to construct the correct interpretation of the TM without assistance. Guided by the results of the Study One Design Task, we created four scaffolds. We test the effectiveness of these scaffolds by seeking to replicate the Qiang et.al (2014) finding that after 20 minutes of video training, students perform faster and more accurately with the unconventional TM than the conventional Linear Model (LM). Will our participants show similar performance on the TM with scaffolds rather than formal instruction? Further, will engagement with the TM in a reading task be sufficient for students to reproduce the graph in a subsequent drawing task?\nTo try the study yourself: http://morning-gorge-17056.herokuapp.com/\nEnter “github” as your session code, and number of the condition you wish to test\n0 = control (no-scaffold), 1 = “what-text”, 2 = “how-text”, 3 = “static-image”, 4 = “interactive-image”"
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html#methods",
    "href": "analysis/SGC2/1_sgc2_introduction.html#methods",
    "title": "1  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a 5 (scaffold: none-control, what-text, how-text, static image, interactive image) x 2 (graph: LM, TM) mixed design, with scaffold as a between-subjects variable and graph as a within-subject variable. To test our hypothesis that exposure to the conventional LM acts as a scaffold for the TM, we counterbalanced the order of graph-reading tasks (order: LM-first, TM-first). For each task we measured response accuracy and time. For the follow-up graph-drawing task, a team of raters coded the type of graph produced by each participant.\n\n\n\nMaterials\n\nScaffolds\nFor the first five questions of each graph-reading task, participants saw their assigned scaffold along with the designated graph. On the following ten questions, the scaffold was not present. Examples of each scaffold-condition for the TM and LM graphs are shown in the table above.\n\n\nThe Graph Drawing Task\nIn the  graph drawing task participants were given a sheet of isometric dot paper and a table containing a set of 10 time intervals. Isometric dot paper equally supports the construction of lines at 0, 45 and 90 degrees, thus minimizing any biasing effects of the paper on the type of graph the participants chose to draw. Participants were directed to draw a triangular graph of the data (“like the triangle graph you saw in the previous task”), using the pencil, eraser and ruler provided.\n \n\n\n\nProcedure\nParticipants completed the study individually in a computer lab. Each participant was randomly assigned to one of five conditions which determined what additional information (scaffold) they received while solving the first five problems with each graph: no-scaffold (control), ‘what’ text, ‘how’-text, static-image, and interactive-image. After a short introduction they continued to the first of two graph reading tasks (graph order counterbalanced). After completing the first graph reading task, they were introduced to the second scenario, and completed the second graph reading task with the remaining graph. Finally, participants completed the graph drawing task. They finished the study by completing a short demographic survey, and reading the debriefing text. The runtime of the entire study ranged from 20 to 60 minutes.\n\n\nSample\nData was collected by convenience sample of a university subject pool. Data were collected in the Spring of 2017 with, in-person, with large groups of students simultaneously completing the study (independently) in a computer lab."
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html#analysis",
    "href": "analysis/SGC2/1_sgc2_introduction.html#analysis",
    "title": "1  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\nnote: Unlike studies SGC3 and onwards, scoring for SGC2 is already included in the raw data files.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(mbp)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC2/data/2-scored-data/sgc2_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC2/data/2-scored-data/sgc2_items.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \nrio::export(df_subjects, \"analysis/SGC2/data/2-scored-data/sgc2_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC2/data/2-scored-data/sgc2_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html#resources",
    "href": "analysis/SGC2/1_sgc2_introduction.html#resources",
    "title": "1  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     \n [5] readr_2.1.2      tidyr_1.2.0      tibble_3.1.7     tidyverse_1.3.1 \n [9] Hmisc_4.7-0      ggplot2_3.3.6    Formula_1.2-4    survival_3.3-1  \n[13] lattice_0.20-45  kableExtra_1.3.4 codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            lubridate_1.8.0     bit64_4.0.5        \n [4] webshot_0.5.3       RColorBrewer_1.1-3  httr_1.4.3         \n [7] tools_4.2.1         backports_1.4.1     utf8_1.2.2         \n[10] R6_2.5.1            rpart_4.1.16        DBI_1.1.3          \n[13] colorspace_2.0-3    nnet_7.3-17         withr_2.5.0        \n[16] tidyselect_1.1.2    gridExtra_2.3       bit_4.0.4          \n[19] curl_4.3.2          compiler_4.2.1      cli_3.3.0          \n[22] rvest_1.0.2         htmlTable_2.4.0     xml2_1.3.3         \n[25] scales_1.2.0        checkmate_2.1.0     systemfonts_1.0.4  \n[28] digest_0.6.29       foreign_0.8-82      rmarkdown_2.14     \n[31] svglite_2.1.0       rio_0.5.29          base64enc_0.1-3    \n[34] jpeg_0.1-9          pkgconfig_2.0.3     htmltools_0.5.2    \n[37] labelled_2.9.1      dbplyr_2.2.1        fastmap_1.1.0      \n[40] highr_0.9           htmlwidgets_1.5.4   rlang_1.0.3        \n[43] readxl_1.4.0        rstudioapi_0.13     generics_0.1.2     \n[46] jsonlite_1.8.0      vroom_1.5.7         zip_2.2.0          \n[49] magrittr_2.0.3      Matrix_1.4-1        Rcpp_1.0.8.3       \n[52] munsell_0.5.0       fansi_1.0.3         lifecycle_1.0.1    \n[55] stringi_1.7.6       yaml_2.3.5          grid_4.2.1         \n[58] parallel_4.2.1      crayon_1.5.1        haven_2.5.0        \n[61] splines_4.2.1       hms_1.1.1           knitr_1.39         \n[64] pillar_1.7.0        reprex_2.0.1        glue_1.6.2         \n[67] evaluate_0.15       latticeExtra_0.6-29 data.table_1.14.2  \n[70] modelr_0.1.8        png_0.1-7           vctrs_0.4.1        \n[73] tzdb_0.3.0          cellranger_1.1.0    gtable_0.3.0       \n[76] assertthat_0.2.1    xfun_0.31           openxlsx_4.2.5     \n[79] broom_0.8.0         viridisLite_0.4.0   cluster_2.1.3      \n[82] ellipsis_0.3.2"
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html",
    "href": "analysis/SGC2/3_sgc2_description.html",
    "title": "2  Description",
    "section": "",
    "text": "The purpose of this notebook is describe the distributions of dependent variables for Study SGC2."
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#sample",
    "href": "analysis/SGC2/3_sgc2_description.html#sample",
    "title": "2  Description",
    "section": "SAMPLE",
    "text": "SAMPLE\n\nData Collection\nData was initially collected (in person, SONA groups in computer lab) in Spring 2017.\n\n\nCODE\ntitle = \"Participants by Condition and (counterbalanced) Task-order\"\ncols = c(\"Control\",\"Text[what]\",\"Text[how]\",\"Image[static]\", \"Image[ixv]\",\"Total\")\ncont <- table(df_subjects$order, df_subjects$pretty_condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and (counterbalanced) Task-order\n \n  \n      \n    Control \n    Text[what] \n    Text[how] \n    Image[static] \n    Image[ixv] \n    Total \n  \n \n\n  \n    LM-First \n    29 \n    31 \n    30 \n    30 \n    34 \n    154 \n  \n  \n    TM-First \n    32 \n    28 \n    36 \n    32 \n    34 \n    162 \n  \n  \n    Sum \n    61 \n    59 \n    66 \n    62 \n    68 \n    316 \n  \n\n\n\n\n\n\n\nParticipants\n\n\nCODE\n#Describe participants\nsubject.stats <- df_subjects %>% dplyr::select(AGE) %>% unlist() %>% favstats() \n\nsubject.stats$percent.female <- df_subjects %>% filter(GENDER==\"Female\") %>% count() %>% pull()/nrow(df_subjects)\n\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()%>% \n  footnote(general = \"Age in Years\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    percent.female \n  \n \n\n  \n     \n    17 \n    19 \n    20 \n    21 \n    33 \n    20.5 \n    2.2 \n    316 \n    0 \n    0.69 \n  \n\n\nNote:   Age in Years\n\n\n\n\nFor in person data collection 316 participants (69 % female ) undergraduate STEM majors at a public American University participated in exchange for course credit (age: 17 - 33 years)."
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#response-accuracy",
    "href": "analysis/SGC2/3_sgc2_description.html#response-accuracy",
    "title": "2  Description",
    "section": "RESPONSE ACCURACY",
    "text": "RESPONSE ACCURACY\n\nBlock Scores\nSubject level scores summarize the the response accuracy by a particular participant across all blocks of the two graph comprehension tasks. The task score refers to the number of questions correct (absolute scoring) in each block (linear graph, triangular graph) of the graph comprehension task.\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy by Block (Total Absolute Score)\"\nabs.stats <- rbind(\n  \"linear.block\"= df_subjects %>% dplyr::select(linear_score) %>% unlist() %>% favstats(),\n  \"triangular.block\" = df_subjects %>% dplyr::select(triangular_score) %>% unlist() %>% favstats(),\n  \"block.differences\" = df_subjects %>% dplyr::select(score_diff) %>% unlist() %>% favstats(),\n  \"total\" = df_subjects %>% dplyr::select(totalScore) %>% unlist() %>% favstats()\n)\n\nabs.stats %>% kbl (caption = title) %>% kable_classic() %>%\n  footnote(general = \"block # questions correct [0,15]; DIFF = triangular - linear\",\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nDescriptive Statistics of Response Accuracy by Block (Total Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    linear.block \n    2 \n    10 \n    11 \n    13 \n    15 \n    10.95 \n    2.13 \n    316 \n    0 \n  \n  \n    triangular.block \n    0 \n    5 \n    10 \n    12 \n    15 \n    8.77 \n    4.45 \n    316 \n    0 \n  \n  \n    block.differences \n    -13 \n    -5 \n    -1 \n    1 \n    5 \n    -2.18 \n    4.11 \n    316 \n    0 \n  \n  \n    total \n    6 \n    16 \n    22 \n    26 \n    31 \n    20.98 \n    6.00 \n    316 \n    0 \n  \n\n\nNote:   block # questions correct [0,15]; DIFF = triangular - linear\n\n\n\n\nTotal absolute scores for the LINEAR graph (n = 316) range from 2 to 15 with a mean score of (M = 10.95, SD = 2.13).\nTotal absolute scores for the TRIANGULAR graph (n = 316) range from 0 to 15 with a mean score of (M = 8.77, SD = 4.45).\nTotal absolute scores across the ENTIRE TASK (n = 316) range from 6 to 31 with a mean score of (M = 20.98, SD = 6).\nDifference scores (difference between TRIANGULAR and LINEAR) scores for each participant (n = 316) range from -13 to 5 with a mean score of (M = -2.18, SD = 4.11). (note: negative difference scores indicate the participant performed better on the linear block than the triangular block.)\n\nBy Block\n\n\nCODE\n#DATA SETUP\nlong_scores <- df_subjects %>% dplyr::select(subject,pretty_condition,order,lm_scenarios,tm_scenarios,linear_score, triangular_score) %>% pivot_longer(\n  cols = ends_with(\"score\"),\n  names_to = \"graph\",\n  values_to = \"score\"\n)\n\n#DISTRIBUTION OF SCORES BY GRAPH\nstats <- favstats(score~graph, data = long_scores)\ngf_dhistogram(~score, fill = ~graph, binwidth = 0.5,data = long_scores) %>%\n  gf_vline(xintercept = ~mean, color = ~graph, data = stats) %>%\n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(~ graph) +\n  labs( title = \"Distribution of Score (by Block)\",\n        subtitle =\"Performance on Linear Graph is better than Triangular\",\n        x = \"Block Score (# correct)\", y = \"proportion of subjects\") +\n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_scores, aes(x = graph, y = score,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Score (by Block) \",\n    x = \"Condition\", y = \"Score (# correct)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nTODO explore interactions\n\n\nCODE\n# \n# \n# library(interactions)\nm = lm(score ~ graph + pretty_condition + order + tm_scenarios, data = long_scores)\nm2 = lm(score ~ graph * pretty_condition * order * tm_scenarios, data = long_scores)\n# \n# cat_plot(model = m, pred = graph, modx = order, mod2 = pretty_condition,\n#          INT.TYPE = \"confidence\", int.width = 0.95,\n#          rug = TRUE)\n# \n# cat_plot(model = m2, pred = graph, modx = tm_scenarios, mod2 = pretty_condition,\n#          INT.TYPE = \"confidence\", int.width = 0.95,\n#          rug = TRUE)\n# \n# # cat_plot(model = m, pred = graph, modx = tm_scenarios, mod2 = pretty_condition,\n# #          INT.TYPE = \"confidence\", int.width = 0.95,\n# #          rug = TRUE)\n\n\n\n\nBy Condition\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\nstats <- favstats(score ~ pretty_condition+graph, data = long_scores)\ngf_dhistogram(~score, fill = ~pretty_condition, binwidth = 0.5,data = long_scores) %>% \n  gf_dens(color = ~pretty_condition) %>%  \n  # gf_vline(xintercept = ~mean, data = stats) %>% \n  gf_facet_grid(pretty_condition ~ graph) +\n  labs( title = \"Distribution of Score (by Condition)\",\n        subtitle =\"\",\n        x = \"Block Score (# correct)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_scores, aes(x = graph, y = score,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~pretty_condition) + labs( \n    title = \"Distribution of Score (by Condition) \",\n    x = \"Condition\", y = \"Score (# correct)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nBy Order\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~score, fill = ~graph, binwidth = 0.5,data = long_scores) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~order) +\n  labs( title = \"Distribution of Score (by Order)\",\n        subtitle =\"\",\n        x = \"Block Score (# correct)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_scores, aes(x = graph, y = score,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~order) + labs( \n    title = \"Distribution of Score (by Order)\",\n    x = \"Graph\", y = \"Score (# correct)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nBy Scenario\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~score, fill = ~graph, binwidth = 0.5,data = long_scores) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~tm_scenarios) +\n  labs( title = \"Distribution of Score (by Scenario)\",\n        subtitle =\"\",\n        x = \"Block Score (# correct)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_scores, aes(x = graph, y = score,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~tm_scenarios) + labs( \n    title = \"Distribution of Score (by Scenario) \",\n    x = \"Graph\", y = \"Score (# correct)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\n\nBlock Difference Scores\n\n\nCODE\n#DIFFERENCE SCORE BY SUBJECT\ngf_line(score~graph, group=~subject, color = ~order, data = long_scores) %>% \n  gf_facet_grid(order~pretty_condition) + \n  labs(title = \"Block Scores by Condition\") + easy_remove_legend()\n\n\n\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~score_diff, fill = ~pretty_condition, binwidth = 0.5,data = df_subjects) %>% \n  # gf_dens(color = ~graph) %>%  \n  gf_facet_grid(order~pretty_condition) +\n  labs( title = \"Block Difference Score (by Condition)\",\n        subtitle =\"\",\n        x = \"Difference Score (Triangular - Linear)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\n\n\nItem Scores\nTask Accuracy summarized over items rather than subjects\n\n\nCODE\ndf <- df_items %>% filter(graph %in% c(\"linear\",\"triangular\"))\n\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM by Condition\n# gf_props(~correct, data = df) %>% \n#   gf_facet_grid(pretty_condition~graph, labeller = label_both) +\n#   labs(x = \"Item Accuracy\",\n#        title = \"Item Accuracy by Graph and Condition\",\n#        subtitle=\"\")\n\n#STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = graph,\n                       fill = score)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_condition) +\n   labs(y = \"Proportion of Items\",\n       title = \"Item Accuracy by Graph and Condition\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#HISTOGRAM\n# gf_props(~correct, data = df) %>% \n#   gf_facet_grid(scenario~graph, labeller = label_both) +\n#   labs(x = \"Item Accuracy\",\n#        title = \"Item Accuracy by Graph and (TM Graph) Scenario\",\n#        subtitle=\"\")\n\n#STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = graph,\n                       fill = score)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~scenario) +\n   labs(y = \"Proportion of Items\",\n       title = \"Item Accuracy by Graph and (TM Graph) Scenario\",\n       x = \"TM Graph Scenario\",\n       fill = \"\",\n       subtitle=\"If the scenarios are of equal difficulty, these should be the same\")\n\n\n\n\n\nCODE\n#HISTOGRAM\n# gf_props(~correct, data = df) %>% \n#   gf_facet_grid(order~graph, labeller = label_both) +\n#   labs(x = \"Item Accuracy\",\n#        title = \"Item Accuracy by Graph and Block Order\",\n#        subtitle=\"\")\n\n#STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = graph,\n                       fill = score)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~order) +\n   labs(y = \"Proportion of Items\",\n       title = \"Item Accuracy by Graph and Block Order\",\n       x = \"Block Order\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = graph,\n                       fill = score)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~q) +\n   labs( \n     #y = \"Proportion of Items\",\n       title = \"Item Accuracy by Graph and Question Number\",\n       x = \"Question Number\",\n       fill = \"\",\n       subtitle=\"\")"
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#response-latency",
    "href": "analysis/SGC2/3_sgc2_description.html#response-latency",
    "title": "2  Description",
    "section": "RESPONSE LATENCY",
    "text": "RESPONSE LATENCY\n\nTime on Block\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Time by Block\"\ntime.stats <- rbind(\n  \"linear.block\"= df_subjects %>% dplyr::select(LM_T_M) %>% unlist() %>% favstats(),\n  \"triangular.block\" = df_subjects %>% dplyr::select(TM_T_M) %>% unlist() %>% favstats(),\n  \"block.differences\" = df_subjects %>% dplyr::select(DIFF_T_M) %>% unlist() %>% favstats(),\n  \"total\" = df_subjects %>% dplyr::select(TOTAL_T_M) %>% unlist() %>% favstats()\n)\n\ntime.stats %>% kbl (caption = title) %>% kable_classic() %>%\n  footnote(general = \"time in minutes; DIFF = triangular - linear\",\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nDescriptive Statistics of Response Time by Block\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    linear.block \n    3.92 \n    7.63 \n    8.87 \n    10.440 \n    23.1 \n    9.20 \n    2.53 \n    316 \n    0 \n  \n  \n    triangular.block \n    3.14 \n    8.82 \n    10.40 \n    12.502 \n    26.8 \n    10.85 \n    3.28 \n    316 \n    0 \n  \n  \n    block.differences \n    -12.66 \n    -3.15 \n    -1.42 \n    0.472 \n    12.9 \n    -1.64 \n    3.09 \n    316 \n    0 \n  \n  \n    total \n    21.91 \n    34.66 \n    39.66 \n    45.853 \n    66.1 \n    40.42 \n    8.54 \n    316 \n    0 \n  \n\n\nNote:   time in minutes; DIFF = triangular - linear\n\n\n\n\nResponse time (in minutes) for the LINEAR graph (n = 316) range from 3.92 to 23.06 with a mean time of (M = 9.2, SD = 2.53).\nResponse time (in minutes) for the TRIANGULAR graph (n = 316) range from 3.14 to 26.82 with a mean time of (M = 10.85, SD = 3.28).\nResponse time (in minutes) across the ENTIRE TASK (n = 316) range from 21.91 to 66.12 with a mean time of (M = 40.42, SD = 8.54).\nDifference in response time (in minutes) (difference between TRIANGULAR - LINEAR) for each participant (n = 316) range from -12.66 to 12.95 with a mean difference in time of (M = -1.64, SD = 3.09). (note: negative difference scores indicate the participant performed faster on the linear block than the triangular block.)\n\n\nCODE\n#DATA SETUP\nlong_times <- df_subjects %>% dplyr::select(subject,pretty_condition,order,lm_scenarios,tm_scenarios, LM_T_M, TM_T_M) %>% pivot_longer(\n  cols = ends_with(\"M\"),\n  names_to = \"graph\",\n  values_to = \"time\") %>% mutate(\n    graph = recode(graph, \"LM_T_M\" = \"Linear Graph\", \"TM_T_M\" = \"Triangular Graph\")\n  )\n\n#DISTRIBUTION OF SCORES BY GRAPH\nstats <- favstats(time~graph, data = long_times)\ngf_dhistogram(~time, fill = ~graph, binwidth = 0.5,data = long_times) %>%\n  gf_vline(xintercept = ~mean, color = ~graph, data = stats) %>%\n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(~ graph) +\n  labs( title = \"Distribution of Response Time\",\n        subtitle =\"Performance on Linear Graph is faster than Triangular\",\n        x = \"Block Score (# correct)\", y = \"number of subjects\") +\n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_times, aes(x = graph, y = time,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Response Time by Graph \",\n    x = \"Condition\", y = \"Response Time (minutes)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF TIME BY GRAPH\nstats <- favstats(time ~ pretty_condition+graph, data = long_times)\ngf_dhistogram(~time, fill = ~pretty_condition, binwidth = 0.5,data = long_times) %>% \n  gf_dens(color = ~pretty_condition) %>%  \n  gf_facet_grid(pretty_condition ~ graph) +\n  labs( title = \"Distribution of Response Time (by Condition)\",\n        subtitle =\"\",\n        x = \"Response Time (minutes)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_times, aes(x = graph, y = time,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~pretty_condition) + labs( \n    title = \"Distribution of Response Time by Graph \",\n    x = \"Condition\", y = \"Response Time (minutes)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF TIMES BY GRAPH\ngf_dhistogram(~time, fill = ~graph, binwidth = 0.5,data = long_times) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~order) +\n  labs( title = \"Distribution of Response Time (by Order)\",\n        subtitle =\"\",\n        x = \"Response Time (minutes)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_times, aes(x = graph, y = time,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~order) + labs( \n    title = \"Distribution of Response Time (by Order)\",\n    x = \"\", y = \"Response Time (minutes)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~time, fill = ~graph, binwidth = 0.5,data = long_times) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~tm_scenarios) +\n  labs( title = \"Distribution of Response Time (by Scenario)\",\n        subtitle =\"\",\n        x = \"Response Time (minutes)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_times, aes(x = graph, y = time,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~tm_scenarios) + labs( \n    title = \"Distribution of Response Time (by Scenario) \",\n    x = \"TM Scenario\", y = \"Respone Time (minutes)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DIFFERENCE TIME BY SUBJECT\ngf_line(time~graph, group=~subject, color = ~order, data = long_times) %>% \n  gf_facet_grid(order~pretty_condition) + \n  labs(title = \"Response Times by Condition\") + easy_remove_legend()\n\n\n\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~DIFF_T_M, fill = ~pretty_condition, binwidth = 0.5,data = df_subjects) %>% \n  # gf_dens(color = ~graph) %>%  \n  gf_facet_grid(order~pretty_condition) +\n  labs( title = \"Block Time Difference (by Condition)\",\n        subtitle =\"\",\n        x = \"Difference Time (Triangular - Linear)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\n\n\nTime on Item\n\n\nCODE\ntitle = \"Descriptive Statistics of Item Response Time by Block\"\ntime.stats <- rbind(\n  \"linear.block\"= df_items %>% filter(graph == \"linear\") %>% dplyr::select(rt_sec) %>% unlist() %>% favstats(),\n  \"triangular.block\" = df_items %>%  filter(graph == \"triangular\") %>% dplyr::select(rt_sec) %>% unlist() %>% favstats()\n)\n\ntime.stats %>% kbl (caption = title) %>% kable_classic() %>%\n  footnote(general = \"time in minutes; DIFF = triangular - linear\",\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nDescriptive Statistics of Item Response Time by Block\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    linear.block \n    4 \n    21 \n    31 \n    45 \n    302 \n    36.8 \n    24.2 \n    4740 \n    0 \n  \n  \n    triangular.block \n    2 \n    23 \n    36 \n    55 \n    401 \n    43.4 \n    30.8 \n    4739 \n    0 \n  \n\n\nNote:   time in minutes; DIFF = triangular - linear\n\n\n\n\nItem Response time (in seconds) for the LINEAR graph (n = 4740) range from 4 to 302 with a mean time of (M = 36.81, SD = 24.21).\nItem Response time (in seconds) for the TRIANGULAR graph (n = 4739) range from 2 to 401 with a mean time of (M = 43.38, SD = 30.83).\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\nstats <- favstats(rt_sec~graph, data = df)\ngf_dhistogram(~rt_sec, fill = ~graph, binwidth = 0.5,data = df) %>%\n  gf_vline(xintercept = ~mean, color = ~graph, data = stats) %>%\n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(~ graph) +\n  labs( title = \"Distribution of Response Time\",\n        subtitle =\"Performance on Linear Graph is faster than Triangular\",\n        x = \"Block Score (# correct)\", y = \"number of subjects\") +\n  easy_remove_legend()\n\n\nWarning: Removed 1 rows containing missing values (geom_vline).\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = graph, y = rt_sec,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec, color = graph),\n    size = 1.3,\n    alpha = .05,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Item Response Time by Graph \",\n    x = \"Condition\", y = \"Response Time (seconds)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF TIME BY GRAPH\nstats <- favstats(rt_sec ~ pretty_condition+graph, data = df)\ngf_dhistogram(~rt_sec, fill = ~pretty_condition, binwidth = 0.5,data = df) %>% \n  gf_dens(color = ~pretty_condition) %>%  \n  gf_facet_grid(pretty_condition ~ graph) +\n  labs( title = \"Distribution of Item Response Time (by Condition)\",\n        subtitle =\"\",\n        x = \"Response Time (seconds)\", y = \"proportion of items\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = graph, y = rt_sec,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~pretty_condition) + labs( \n    title = \"Distribution of Item Response Time by Graph \",\n    x = \"Condition\", y = \"Response Time (seconds)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF TIMES BY GRAPH\ngf_dhistogram(~rt_sec, fill = ~graph, binwidth = 0.5,data = df) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~order) +\n  labs( title = \"Distribution of Item Response Time (by Order)\",\n        subtitle =\"\",\n        x = \"Item Response Time (sec)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = graph, y = rt_sec,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~order) + labs( \n    title = \"Distribution of Item Response Time (by Order)\",\n    x = \"\", y = \"Response Time (seconds)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~rt_sec, fill = ~graph, binwidth = 0.5,data = df) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~scenario) +\n  labs( title = \"Distribution of Item Response Time (by Scenario)\",\n        subtitle =\"\",\n        x = \"Response Time (seconds)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = graph, y = rt_sec,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~scenario) + labs( \n    title = \"Distribution of Item Response Time (by Scenario) \",\n    x = \"TM Scenario\", y = \"Item Response Time (seconds)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")"
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#drawing-task",
    "href": "analysis/SGC2/3_sgc2_description.html#drawing-task",
    "title": "2  Description",
    "section": "DRAWING TASK",
    "text": "DRAWING TASK\nFinally, we explore the distribution of graph types produced by participants during the graph drawing task.\n\n\nCODE\ngf_props(~draw_type, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Type of Graph drawn by Participant\"\n  )"
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#exploring-relationships",
    "href": "analysis/SGC2/3_sgc2_description.html#exploring-relationships",
    "title": "2  Description",
    "section": "EXPLORING RELATIONSHIPS",
    "text": "EXPLORING RELATIONSHIPS\n\nBlock Accuracy\n\n\nCODE\n#SCATTERPLOT \ngf_jitter( linear_score ~ triangular_score, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Relationship between Linear and Triangular Block Scores\",\n    subtitle = \"\", \n    x = \"Linear Score\", y = \"Triangular Score\"\n  ) + easy_remove_legend()\n\n\n\n\n\n\n\nTime + Accuracy\n\n\nCODE\nq.stats <- df %>%  dplyr::group_by(graph, q, pretty_condition, score) %>% dplyr::summarise(\n  m = mean(rt_sec),\n  sd = sd(rt_sec),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score)\n)\n\ngf_line( m ~ q, group = ~group,  color = ~score, data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~q) %>% \n  gf_facet_grid(graph~pretty_condition) + scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Absolute Score\",\n       subtitle = \"\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\ndf %>%\n  ggplot(aes(y = rt_sec, x = q,  fill = pretty_condition)) +\n  stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + facet_grid(graph~pretty_condition)+labs(title = \"Average Item Response Time by Absolute Score\",\n       subtitle = \"\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")"
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#resources",
    "href": "analysis/SGC2/3_sgc2_description.html#resources",
    "title": "2  Description",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] forcats_0.5.1      stringr_1.4.0      purrr_0.3.4        readr_2.1.2       \n [5] tidyr_1.2.0        tibble_3.1.6       tidyverse_1.3.1    performance_0.8.0 \n [9] fitdistrplus_1.1-8 MASS_7.3-55        multimode_1.5      ggeasy_0.1.3      \n[13] ggdist_3.1.1       ggpubr_0.4.0       vcd_1.4-9          kableExtra_1.3.4  \n[17] mosaic_1.8.3       ggridges_0.5.3     mosaicData_0.20.2  ggformula_0.10.1  \n[21] ggstance_0.3.5     dplyr_1.0.8        Matrix_1.4-0       Hmisc_4.6-0       \n[25] ggplot2_3.3.5      Formula_1.2-4      survival_3.3-1     lattice_0.20-45   \n\nloaded via a namespace (and not attached):\n  [1] colorspace_2.0-3     ggsignif_0.6.3       ellipsis_0.3.2      \n  [4] mclust_5.4.10        leaflet_2.1.0        htmlTable_2.4.0     \n  [7] fs_1.5.2             base64enc_0.1-3      ggdendro_0.1.23     \n [10] rstudioapi_0.13      farver_2.1.0         ggrepel_0.9.1       \n [13] lubridate_1.8.0      mvtnorm_1.1-3        fansi_1.0.2         \n [16] xml2_1.3.3           codetools_0.2-18     splines_4.1.1       \n [19] rootSolve_1.8.2.3    knitr_1.38           polyclip_1.10-0     \n [22] jsonlite_1.8.0       broom_0.7.12         dbplyr_2.1.1        \n [25] cluster_2.1.2        png_0.1-7            ggforce_0.3.3       \n [28] compiler_4.1.1       httr_1.4.2           backports_1.4.1     \n [31] assertthat_0.2.1     fastmap_1.1.0        cli_3.2.0           \n [34] tweenr_1.0.2         htmltools_0.5.2      tools_4.1.1         \n [37] gtable_0.3.0         glue_1.6.2           Rcpp_1.0.8.3        \n [40] carData_3.0-5        cellranger_1.1.0     vctrs_0.3.8         \n [43] svglite_2.1.0        crosstalk_1.2.0      insight_0.18.0      \n [46] lmtest_0.9-39        xfun_0.30            rvest_1.0.2         \n [49] lifecycle_1.0.1      mosaicCore_0.9.0     rstatix_0.7.0       \n [52] zoo_1.8-9            scales_1.1.1         hms_1.1.1           \n [55] RColorBrewer_1.1-2   yaml_2.3.5           gridExtra_2.3       \n [58] labelled_2.9.0       rpart_4.1.16         latticeExtra_0.6-29 \n [61] stringi_1.7.6        highr_0.9            checkmate_2.0.0     \n [64] rlang_1.0.2          pkgconfig_2.0.3      systemfonts_1.0.4   \n [67] distributional_0.3.0 pracma_2.3.8         evaluate_0.15       \n [70] labeling_0.4.2       ks_1.13.5            htmlwidgets_1.5.4   \n [73] tidyselect_1.1.2     plyr_1.8.6           magrittr_2.0.2      \n [76] R6_2.5.1             generics_0.1.2       DBI_1.1.2           \n [79] pillar_1.7.0         haven_2.4.3          foreign_0.8-82      \n [82] withr_2.5.0          abind_1.4-5          nnet_7.3-17         \n [85] modelr_0.1.8         crayon_1.5.0         car_3.0-12          \n [88] KernSmooth_2.23-20   utf8_1.2.2           tzdb_0.2.0          \n [91] rmarkdown_2.13       jpeg_0.1-9           readxl_1.3.1        \n [94] data.table_1.14.2    reprex_2.0.1         diptest_0.76-0      \n [97] digest_0.6.29        webshot_0.5.2        munsell_0.5.0       \n[100] viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_introduction.html",
    "href": "analysis/SGC3A/1_sgc3A_introduction.html",
    "title": "3  Introduction",
    "section": "",
    "text": "In Study 3A we explore the extent to which confronting a learner with an implicit obstacle (a mental impasse) influences their interpretation of the underlying coordinate system. This is a hypothesis that emerged from analysis of Study 2, leading us to suspect that presenting a learner with a situation that induces a state of impasse will increase the probability that learners experience a moment of insight, and in turn restructure their interpretation of the coordinate system.\nIn the context of Study 2, an impasse state was (unintentionally) induced when the combination of question + data set yielded no available answer in the incorrect (cartesian) interpretation of the graph. In Study 3A, we test this hypothesis by comparing performance between a (treatment) group receiving impasse-inducing questions followed by normal questions, and a non-impasse control."
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_introduction.html#methods",
    "href": "analysis/SGC3A/1_sgc3A_introduction.html#methods",
    "title": "3  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Scaffold: control,impasse)\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 16.1. The list of questions can be found here.\n\n\n\nFigure 3.2: Sample Question (Q=1) for Graph Comprehension Task\n\n\nNote that across both control and impasse conditions, both the question, response options and graph structure were identical. The experimental manipulation (posing a mental impasse) was accomplished by changing the position of datapoints in the impasse-condition graph, such that for any given question, there was no available response option if the reader were to interpret the graph as cartesian (making an orthogonal rather than diagonal projection from the x-axis.)\nThe green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. We hypothesize that this presents the reader with an obstacle, at which point they are forced to confront their interpretation of the coordinate system and (ideally) develop a new strategy.\n\n\n\nFigure 3.3: Sample Question (Q=1) graphs for each condition\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items.\n(3A) The first five items in the task are defined as the SCAFFOLDING block. In the IMPASSE condition, the first five questions included an IMPASSE problem state. For participants in the CONTROL condition, the dataset was structure such that there was always an available ‘orthogonal answer’ for the first 5 questions.\n(3B) The remaining 10 items are defined as the TESTING block. In both conditions, these questions were not structured as impasse (i.e. contained an available orthogonal answer)\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData was collected by convenience sample of a university subject pool. Initial data (Fall 2017, Spring 2018) were collected in-person, with large groups of students simultaneously completing the study (independently) in a computer lab. In Fall 2021 and Winter 2022 we collected additional data to replicate results in a remote format (students completing the study asynchronously on their own computers)."
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_introduction.html#analysis",
    "href": "analysis/SGC3A/1_sgc3A_introduction.html#analysis",
    "title": "3  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\nBefore analysis, data files from individual data collection periods are harmonized into a common data format.\n\n\n\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nspring17_clean_data.Rmd  spring18_clean_data.Rmd  fall21_clean_data.Rmd  winter2022_clean_sgc3a.Rmd\n2_sgc3A_scoring.qmd\n\n\n\nData for study SGC_3A were collected across four time periods, interrupted by the Covid-19 pandemic.\n\n\n\nPeriod\nModality\n\n\n\n\nFall 2017\nin person, SONA groups in computer lab\n\n\nSpring 2018\nin person, SONA groups in computer lab\n\n\nFall 2021\nasynchronous, online, SONA\n\n\nWinter 2022\nasynchronous, online, SONA\n\n\n\nData collected in Fall 2017 (in person pilot) were analyzed and published as a Cognitive Science Society conference paper (“When Graph Comprehension is an Insight Problem). Additional data were collected in person in Spring 2018. Combined, Fall 2017 and Spring 2018 constitute the original SGC_3A study, conducted in person. Data collected in Fall 2021, Winter 2022 constitute the web-based replication, conducted online (asynchronously). In all cases, the experiment was administered via a web application. The replication study was conducted to validate the use of remote, asynchronous data collection during the Covid-19 pandemic.\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3A/data/0-session-level/fall17_sgc3a_participants.csv\" #COGSCI18 data\nspring18 <- \"analysis/SGC3A/data/0-session-level/spring18_sgc3a_participants.csv\"\nfall21 <- \"analysis/SGC3A/data/0-session-level/fall21_sgc3a_participants.csv\"\nwinter22 <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_participants.rds\"\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_subjects_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% \n  dplyr::select(\n  subject,condition,term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m, \n  # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n  #exploratory factors\n  violations, browser, width, height\n)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\",\n    violations = \"NULL\",\n    browser = \"NULL\",\n    width = \"NULL\",\n    height = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, \n                # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                #exploratory factors\n                violations, browser, width, height)\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_winter22_q16 <- df_subjects_winter22 %>% \n  dplyr::select(subject, condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% dplyr::select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects_winter22 <- df_subjects_winter22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors \n                 violations, browser, width, height)\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_winter22, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\"),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#IMPORT OSPAN DATA \nospan <- read_csv(\"analysis/SGC3A/data/0-session-level/fall21_scored_ospan.csv\") %>% mutate(\n  subject = SUBJECTID\n) %>% dplyr::select(-SUBJECTID)\n\n#MERGE OSPAN DATA WITH SGC DATA \n#special dataframe with just ospan subjects\n#should be 133 subjects. Some of the 200 who completed the task failed the \n#attention check question. Others were allocated to SGC4A pilot. \n#note that rather than adding OSPAN data to main dataframe, the after-scored data\n#will be manually joined to df_ospan during exploratory analysis\ndf_ospan <- df_subjects %>% filter(\n  subject %in% ospan$subject\n) %>% merge(ospan)  \n\n\n#CLEANUP\nrm(df_subjects_fall17,df_subjects_fall21, df_subjects_spring18, df_subjects_winter22,df_subjects_before)\nrm(fall17,fall21,spring18,winter22)\n\n\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_subjects$term, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    fall17 \n    27 \n    27 \n    54 \n  \n  \n    spring18 \n    35 \n    37 \n    72 \n  \n  \n    fall21 \n    68 \n    71 \n    139 \n  \n  \n    winter22 \n    28 \n    37 \n    65 \n  \n  \n    Sum \n    158 \n    172 \n    330 \n  \n\n\n\n\n\nCODE\ntitle = \"Subset of Participants who completed OSPAN TASK [Fall 2021]\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_ospan$mode, df_ospan$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nSubset of Participants who completed OSPAN TASK [Fall 2021]\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    lab-synch \n    0 \n    0 \n    0 \n  \n  \n    asynch \n    65 \n    68 \n    133 \n  \n  \n    Sum \n    65 \n    68 \n    133 \n  \n\n\n\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3A/data/0-session-level/fall17_sgc3a_blocks.csv\"\nspring18 <- \"analysis/SGC3A/data/0-session-level/spring18_sgc3a_blocks.csv\"\nfall21 <- \"analysis/SGC3A/data/0-session-level/fall21_sgc3a_blocks.csv\"\nwinter22 <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_items.rds\"\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_items_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- df_items_winter22 %>% group_by(q) %>% dplyr::select(q,relation) %>% unique()\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- df_items_winter22 %>% filter(condition=='X') %>% dplyr::select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n  \n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18, df_items_fall21) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  dplyr::select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n  \n#reduce data collected using new webapp\ndf_items_winter22 <- df_items_winter22 %>% \n  dplyr::select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_winter22,df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% dplyr::select(-question,-correct,-rt_s,-num_o)\n#add data from wi22 [stored on subject data]\ndf_freeresponse <- rbind(df_freeresponse, df_winter22_q16)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n    question = \"Please describe how to determine what event(s) start at 12pm?\",\n    response = as.character(response) #doesn't need to be factor\n  ) \n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#add back pretty condition \ndf_items <- df_items %>% mutate(\n  pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n  pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_items_fall17,df_items_fall21, df_items_spring18, df_items_winter22, df_items_before, df_winter22_q16)\nrm(fall17,fall21,spring18,winter22, map_relations)\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3A/data/1-study-level/sgc3a_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3A/data/1-study-level/sgc3a_items.csv\", row.names = FALSE)\nwrite.csv(df_freeresponse,\"analysis/SGC3A/data/1-study-level/sgc3a_freeresponse.csv\", row.names = FALSE)\nwrite.csv(df_ospan,\"analysis/SGC3A/data/1-study-level/sgc3a_ospan.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3A/data/1-study-level/sgc3a_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3A/data/1-study-level/sgc3a_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html",
    "title": "4  Response Scoring",
    "section": "",
    "text": "TODO\nThe purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC3A study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#score-sgc-data",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#score-sgc-data",
    "title": "4  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#backup <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#summarize-by-subject",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#summarize-by-subject",
    "title": "4  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#explore-distributions",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#explore-distributions",
    "title": "4  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"Impasse Condition (blue) yields more correct responses across the entire task\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(pretty_condition~q) + \n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields more correct responses on each item\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>%\ngf_facet_wrap(~pretty_condition)+\n  labs( x = \"Total Absolute Score\",\n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher total absolute scores\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE TEST PHASE\ngf_props(~item_test_NABS, fill = ~pretty_condition, \n             data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Absolute Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#QUICK PEEK\nresult <- two_sample_test(data = df_subjects, x = pretty_condition, y = s_NABS,\n                          type = \"nonparametric\", var.equal = FALSE,alternative = \"less\",\n                          k = 2L, conf.level = 0.89, effsize.type = \"g\",\n                          bf.prior = 0.707, tr = 0.2, nboot = 100L)\n\nggbetweenstats( data = df_subjects,\n                x = pretty_condition, y = s_NABS,\n                type = \"non-parametric\",\n                title = \"Total Absolute Score [directional test]\",\n                results.subtitle = FALSE,\n                subtitle = result$expression[[1]])\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores across the entire task\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(q~pretty_condition) + \n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores on each item\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) + \n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>% \n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\", \n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher cumulative scaled scores\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE TEST PHASE\ngf_histogram(~item_test_SCALED, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Scaled Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Scaled Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#QUICK PEEK\nresult <- two_sample_test(data = df_subjects, x = pretty_condition, y = s_SCALED,\n                          type = \"nonparametric\", var.equal = FALSE,alternative = \"less\",\n                          k = 2L, conf.level = 0.89, effsize.type = \"g\",\n                          bf.prior = 0.707, tr = 0.2, nboot = 100L)\n\nggbetweenstats( data = df_subjects,\n                x = pretty_condition, y = s_SCALED,\n                type = \"non-parametric\",\n                title = \"Total Scaled Score [directional test]\",\n                results.subtitle = FALSE,\n                subtitle = result$expression[[1]])\n\n\n\n\n\n\nTODO: INVESTIGATE if some of the scores assigned to 0 should be assigned to -0.5 to balance\nTODO: INVESTIGATE DISTRIBUTIONS of each subscore type\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"Impasse condition (blue) yields fewer Orthogonal and more Triangular responses\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more Triangular responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more positive trending responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"Impasse shifts density toward higher Triagular scores\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"Impasse shifts density toward lower Orthogonal scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#explore-responses",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#explore-responses",
    "title": "4  Response Scoring",
    "section": "EXPLORE RESPONSES",
    "text": "EXPLORE RESPONSES\nIn this section we explore responses given by participants to each particular item in the graph comprehension task, indicate how each response was scored, and what interpretation of the graph is indicated by different responses.\n\nScaffold Phase\nThe first five questions constitute the ‘scaffold’ (or learning) phase, where participants see a different version of the stimulus (specifically a different dataset is visualized) invoking a different experimental condition.\n\nQuestion #1\n\nQ1. Control Condition\nWe start by exploring the range of response options checked by participants on Question 1, for those assigned to the control (non-impasse) condition (condition = 111).\n\n\n\nFigure 4.1: Question 1 — Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==1)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q1 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q1 Control Condition :  Which shift(s) start at 11 am?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n    Z \n  \n  \n    Orthgonal \n    A \n    OI \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    CF \n    Z \n  \n  \n    Tversky [start diagonal] \n    F \n    Z \n  \n  \n    Tversky [end diagonal] \n    C \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nHere we summarize the distinct response options given by participants on this item. Each letter in response indicates a checkbox selected by the participant (See Figure 4.1 ). n indicates the number of participants who gave this response, while interpretation indicates the graph interpretation most consistent with that response. At the right of this table are the Absolute, followed by Partial Credit subscores for each response. NA indicates that there is no score calculated (occurs when there is no subset of response options that accord with that interpretation for this question).\nNotice that for this Question, the Triangular answer is the same as the Tversky [start diagonal] answer. In fact, for most questions, one of the Tversky sub-types will match the correct response.\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #1 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 1 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 1) %>% \n  pack_rows(\"Lines-Connect\", 2, 2) %>% \n  pack_rows(\"Orthogonal\", 3, 3) %>% \n  pack_rows(\"Other\", 4, 4)  %>% \n  pack_rows(\"Unknown\", 5, 7)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #1 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    22 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.083 \n    1.0 \n  \n  Lines-Connect\n\n    CF \n    3 \n    Tversky \n    0 \n    0.923 \n    1.000 \n    NA \n    -0.167 \n    0.5 \n  \n  Orthogonal\n\n    A \n    129 \n    Orthogonal \n    0 \n    -0.077 \n    -0.071 \n    NA \n    1.000 \n    -1.0 \n  \n  Other\n\n    AF \n    1 \n    ? \n    0 \n    0.923 \n    0.923 \n    NA \n    0.917 \n    -0.5 \n  \n  Unknown\n\n    DIJ \n    1 \n    ? \n    0 \n    -0.231 \n    -0.214 \n    NA \n    -0.167 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.077 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    0.000 \n    0.000 \n    NA \n    -0.083 \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nWe see that nearly all of the subjects selected a response consistent with one of the identified interpretations. Responses that do not accord with any interpretation are indicated as ? .\n\n\n\n\n\n\n\nWhich shifts start at 11am?\n\n\n\n\n\n\nResponse: A\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, projecting an invisible orthogonal line upward, and locating data point A.\n\n\n\n\nResponse: F\n\nindicates the triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following the right-diagonal gridline, identifying data point F.\n\n\n\n\nResponse: C, F\n\nindicates a maximal-Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following both the right-diagonal and left-diagonal gridlines, identifying both datapoints F and C.\n\n\n\n\nResponse: A , F\n\nThe reader selects both triangular and orthogonal-consistent data points\nPossibly indicates uncertainty or confusion\n\n\n\n\nThree responses were given that were not consistent with any of the identified interpretations. Note that options highlighted in light grey are considered within the range of ‘visual error’, defined by 0.5hr offset from the interpretation-specific projection.\n\n\n\n\n\n\n\n\nD I J\nX\nZ\nTODO find this person, did they subsequently give tri answers?\n\n\n\n\n\n\n\n\n\n\n\n\nQ1. Impasse Condition\nNext we explore the range of response options checked by participants on Question 1, for those assigned to the control (non-impasse) condition (condition = 111).\n\n\n\nFigure 4.2: Question 1 — Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==1)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q1 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q1 Impasse Condition :  Which shift(s) start at 11 am?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    O \n     \n  \n  \n    Satisficing [right] \n    AI \n     \n  \n  \n    Tversky [maximal] \n    CF \n     \n  \n  \n    Tversky [start diagonal] \n    F \n     \n  \n  \n    Tversky [end diagonal] \n    C \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nNotice that there is no orthogonal answer for this question. This is the purpose of the impasse condition, to remove the possibility of selecting the orthogonal answer, we expect learners will be more likely to restructure their understanding of the coordinate system, and arrive at a correct (triangular) interpretation.\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #1 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 1 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 1) %>% \n  pack_rows(\"Lines-Connect\", 2, 4) %>% \n  pack_rows(\"Satisfice\", 5, 9) %>% \n  pack_rows(\"Other\", 10, 10) %>% \n  pack_rows(\"Unknown\", 11, 12) %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #1 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    49 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.071 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    CF \n    14 \n    Tversky \n    0 \n    0.929 \n    1.000 \n    -0.143 \n    NA \n    0.5 \n  \n  \n    C \n    3 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    -0.071 \n    NA \n    0.5 \n  \n  \n    CO \n    1 \n    Tversky \n    0 \n    -0.143 \n    0.929 \n    0.929 \n    NA \n    0.5 \n  \n  Satisfice\n\n    O \n    28 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AI \n    9 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    A \n    4 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    AO \n    2 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    0.929 \n    NA \n    -1.0 \n  \n  \n    I \n    2 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  Other\n\n     \n    57 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  Unknown\n\n    E \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.071 \n    NA \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.071 \n    NA \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nWe see that nearly all of the subjects selected a response consistent with one of the identified interpretations. Responses that do not accord with any interpretation are indicated as ? .\nTODO ADJUST ‘both’ to select for both tri/satisfice or both tri/orth\n\n\n\n\n\n\n\nWhich shifts start at 11am?\n\n\n\n\n\n\nResponse: F\n\nindicates the triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following the right-diagonal gridline, identifying data point F.\n\n\n\n\nResponse: [C, F]\n\nindicates a maximal-Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following both the right-diagonal and left-diagonal gridlines, identifying both datapoints F and C gridline.\n\n\n\n\nResponses: [AOI]\n\nindicates a satisficing strategy\nConsistent with the reader identifying the datapoints nearest to the orthogonal projection from the reference point point\n\n\n\n\nTwo responses were given that were not consistent with any of the identified interpretations.\n\n\n\n\n\n\n[E],[X]\n\n\n\n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==1)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q1 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==1)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q1 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #2\n\nQ2. Control Condition\n\n\n\nFigure 4.3: Q2—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==2)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q2 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q2 Control Condition :  Which shift(s) start at the same time as D?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    K \n    Z \n  \n  \n    Orthgonal \n    E \n    G \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    AKJX \n    Z \n  \n  \n    Tversky [start diagonal] \n    AK \n    Z \n  \n  \n    Tversky [end diagonal] \n    X \n     \n  \n  \n    Tversky [duration line] \n    J \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #2 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 2 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 4) %>% \n  pack_rows(\"Orthogonal\", 5, 7) %>%\n  pack_rows(\"Other\", 8, 8)  %>% \n  pack_rows(\"Unknown\", 9, 10)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #2 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    K \n    24 \n    Triangular \n    1 \n    1.000 \n    0.500 \n    NA \n    -0.083 \n    1.0 \n  \n  \n    DK \n    1 \n    Triangular \n    1 \n    1.000 \n    0.500 \n    NA \n    -0.083 \n    1.0 \n  \n  Lines-Connect\n\n    J \n    4 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    NA \n    -0.083 \n    0.5 \n  \n  \n    AK \n    1 \n    Tversky \n    0 \n    0.917 \n    1.000 \n    NA \n    -0.167 \n    0.5 \n  \n  Orthogonal\n\n    E \n    121 \n    Orthogonal \n    0 \n    -0.083 \n    -0.077 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    DE \n    3 \n    Orthogonal \n    0 \n    -0.083 \n    -0.077 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EG \n    1 \n    Orthogonal \n    0 \n    -0.167 \n    -0.154 \n    NA \n    1.000 \n    -1.0 \n  \n  Other\n\n    D \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    B \n    1 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    NA \n    -0.083 \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nAgain, we see that most subjects selected a response consistent with one of the identified interpretations. (note, when the question stem includes a data point rather than time as reference, we do not penalize respondents for selecting the reference data point in addition to an interpretation consistent response. For example, in this question, we do not penalize respondents for selecting option D, the reference point in the question. )\n\n\n\n\n\n\n\nWhich shift(s) start at the same time as D?\n\n\n\n\nReponse: E (also EG, DE)\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (D) on the graph, projecting an invisible orthogonal line through it, and locating data point E.\n\n\n\n\nResponse: K (also KD)\n\nindicates an triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (D) on the graph, and following its descending-leftward diagonal gridline, and locating data point K.\n\n\n\n\nResponse: AK\n\nindicates an Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (D) on the graph, and following its descending-leftward diagonal gridline, and locating data point K then continuing along the connecting ascending leftward diagonal locating data point A.\n\n\n\n\nResponse: J\n\nindicates an Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (D) on the graph, and following its horizontal gridline to the y-axis, locating data point J.\n\n\n\n\nResponse: D\n\nthe reader selected only the reference point\nConsistent with the reader identifying the reference point (D) on the graph\nPossibly indicates uncertainty or confusion\n\n\n\n\n\n\n\n\n\n\n\nB\nC\n\n\n\n\n\n\n\n\n\n\n\nQ2. Impasse Condition\n\n\n\nFigure 4.4: Q2—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==2)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q2 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q2 Impasse Condition :  Which shift(s) start at the same time as D?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    K \n    Z \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n    G \n     \n  \n  \n    Tversky [maximal] \n    JKE \n    Z \n  \n  \n    Tversky [start diagonal] \n    K \n    Z \n  \n  \n    Tversky [end diagonal] \n    E \n     \n  \n  \n    Tversky [duration line] \n    J \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #2 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 2 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 10) %>% \n  pack_rows(\"Satisfice\", 11, 12) %>%\n  pack_rows(\"Other\", 13, 16)  %>% \n  pack_rows(\"Unknown\", 17, 18)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #2 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    K \n    69 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    DK \n    1 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    J \n    12 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    EK \n    3 \n    Tversky \n    0 \n    0.917 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    EX \n    2 \n    Tversky \n    0 \n    -0.167 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BEG \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.846 \n    0.846 \n    NA \n    0.5 \n  \n  \n    E \n    1 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    EKX \n    1 \n    Tversky \n    0 \n    0.833 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    HJZ \n    1 \n    Tversky \n    0 \n    -0.167 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    JK \n    1 \n    Tversky \n    0 \n    0.917 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  Satisfice\n\n    G \n    19 \n    Satisfice \n    0 \n    -0.083 \n    -0.077 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    BG \n    2 \n    Satisfice \n    0 \n    -0.167 \n    -0.154 \n    0.923 \n    NA \n    -1.0 \n  \n  Other\n\n    D \n    7 \n    reference \n    0 \n    0.000 \n    NA \n    0.000 \n    NA \n    0.0 \n  \n  \n     \n    43 \n    blank \n    0 \n    0.000 \n    NA \n    0.000 \n    NA \n    0.0 \n  \n  \n    ACDFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.250 \n    0.250 \n    -0.846 \n    NA \n    -0.5 \n  \n  \n    BEGKUZ \n    1 \n    frenzy \n    0 \n    0.667 \n    0.667 \n    0.615 \n    NA \n    -0.5 \n  \n  Unknown\n\n    C \n    6 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    FO \n    1 \n    ? \n    0 \n    -0.167 \n    -0.154 \n    -0.154 \n    NA \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==2)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q2 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==2)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q2 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #3\n\nQ3. Control Condition\n\n\n\nFigure 4.5: Q3—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==3)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q3 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q3 Control Condition :  Which shift(s) begin when C ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n    Z \n  \n  \n    Orthgonal \n    Z \n    FIO \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    AUBFOJ \n     \n  \n  \n    Tversky [start diagonal] \n    OJ \n     \n  \n  \n    Tversky [end diagonal] \n    F \n    Z \n  \n  \n    Tversky [duration line] \n    AUB \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #3 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 3 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 7) %>% \n  pack_rows(\"Orthogonal\", 8, 8) %>% \n  pack_rows(\"Other\", 9, 10) %>% \n  pack_rows(\"Unknown\", 11, 17)  \n\n\n\nFrequency of Selected Response Options for Question #3 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    24 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    0.0 \n    1.0 \n  \n  \n    EFK \n    1 \n    Triangular \n    0 \n    0.833 \n    0.833 \n    NA \n    -0.2 \n    1.0 \n  \n  Lines-Connect\n\n    ABU \n    4 \n    Tversky \n    0 \n    -0.250 \n    1.000 \n    NA \n    -0.3 \n    0.5 \n  \n  \n    O \n    3 \n    Tversky \n    0 \n    -0.083 \n    0.500 \n    NA \n    0.0 \n    0.5 \n  \n  \n    JO \n    2 \n    Tversky \n    0 \n    -0.167 \n    1.000 \n    NA \n    -0.1 \n    0.5 \n  \n  \n    DJO \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.917 \n    NA \n    -0.2 \n    0.5 \n  \n  \n    KO \n    1 \n    Tversky \n    0 \n    -0.167 \n    0.417 \n    NA \n    -0.1 \n    0.5 \n  \n  Orthogonal\n\n    Z \n    94 \n    Orthogonal \n    0 \n    0.000 \n    0.000 \n    NA \n    1.0 \n    -1.0 \n  \n  Other\n\n    C \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.0 \n    0.0 \n  \n  \n    ABDEFGHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.000 \n    NA \n    NA \n    0.0 \n    -0.5 \n  \n  Unknown\n\n    A \n    18 \n    ? \n    0 \n    -0.083 \n    0.333 \n    NA \n    -0.1 \n    -0.5 \n  \n  \n    K \n    3 \n    ? \n    0 \n    -0.083 \n    -0.083 \n    NA \n    -0.1 \n    -0.5 \n  \n  \n    AH \n    1 \n    ? \n    0 \n    -0.167 \n    0.242 \n    NA \n    -0.2 \n    -0.5 \n  \n  \n    DE \n    1 \n    ? \n    0 \n    -0.167 \n    -0.167 \n    NA \n    -0.2 \n    -0.5 \n  \n  \n    E \n    1 \n    ? \n    0 \n    -0.083 \n    -0.083 \n    NA \n    -0.1 \n    -0.5 \n  \n  \n    EU \n    1 \n    ? \n    0 \n    -0.167 \n    0.242 \n    NA \n    -0.2 \n    -0.5 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.083 \n    0.333 \n    NA \n    -0.1 \n    -0.5 \n  \n\n\n\n\n\nTODO\n\naddress RESPONSE FKE which is classified as Triangular but doesn’t seem to fit this interpretation?\nShould O,K be considered Tvresky ?\nconsider adding trapdoor on n_q, such that score is penalized (OR interpretation is not predicted?) if the Ss selects more than 1 extra options, or is missing more than 2 options?\nLEFT OFF HERE\n\n\n\n\n\n\n\n\nWhat shift(s) begin when C ends?\n\n\n\n\n\n\nResponse: Z\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (C) then using the duration encoded on the y-axis (2) , project along the horizontal gridline by two hours, and then project an invisible orthogonal line through that time (12PM) locating data point Z.\n\n\n\n\nResponse: F\n\nindicates a (correct) triangular interpretation of the coordinate system\nConsistent with the reader identifying the reference point (C) on the graph, and following the descending gridline to the x-axis to identify the end-time (11AM) and then following the ascending gridline to identify datapoints starting at 11AM and locating data point F.\n\n\n\n\nResponse: AUB (also A)\n\nindicates a Tversky strategy following connecting lines (duration)\nConsistent with the reader identifying the reference point (C) on the graph, and following the horizontal y-axis gridline and locating data points A U B.\n\n\n\n\nResponse: OJ\n\nindicates a Tversky strategy following connecting lines (start-time)\nConsistent with the reader identifying the reference point (C) on the graph, and following the ascending diagonal gridline and locating data points O J.\n\n\n\n\nResponse: C\n\nthe participant selected the point referenced in the question\npossibly indicates confusion or uncertainty\n\n\n\n\nResponse: AIOZFHJXKUDEGB\n\nthe participant selects all (or nearly all) the data points\npossibly indicates confusion or uncertainty\n\n\n\n\nSix responses (from 9 participants) appear inconsistent with any interpretation.\n\n\n\n\n\n\n\n\nK (n=3)\nAH (n=1)\nDE (n=1)\n\n\n\n\n\n\n\n\n\nUE (n=1)\nU (n=1)\nE (n=1)\n\n\n\n\n\n\n\n\n\n\nQ3. Impasse Condition\n\n\n\nFigure 4.6: Q3—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==3)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q3 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q3 Impasse Condition :  Which shift(s) begin when C ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    AI \n     \n  \n  \n    Satisficing [right] \n    F \n     \n  \n  \n    Tversky [maximal] \n    BJ \n     \n  \n  \n    Tversky [start diagonal] \n    J \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n    B \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nTODO investigate these responses 17 at O?\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #3 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 3 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 5) %>% \n  pack_rows(\"Lines-Connect\", 3, 5) %>% \n  pack_rows(\"Satisfice\", 6, 15) %>% \n  pack_rows(\"Other\", 16, 21) %>% \n  pack_rows(\"Unknown\", 22, 29) \n\n\n\nFrequency of Selected Response Options for Question #3 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    61 \n    Triangular \n    1 \n    1.000 \n    -0.077 \n    1.000 \n    NA \n    1.0 \n  \n  \n    AF \n    5 \n    Triangular \n    0 \n    0.923 \n    -0.154 \n    0.923 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    AFG \n    1 \n    Triangular \n    0 \n    0.846 \n    -0.231 \n    0.846 \n    NA \n    1.0 \n  \n  \n    B \n    8 \n    Tversky \n    0 \n    -0.077 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    J \n    3 \n    Tversky \n    0 \n    -0.077 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  Satisfice\n\n    BE \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BJ \n    1 \n    Tversky \n    0 \n    -0.154 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    HJZ \n    1 \n    Tversky \n    0 \n    -0.231 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    A \n    7 \n    Satisfice \n    0 \n    -0.077 \n    -0.077 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    AH \n    5 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    0.417 \n    NA \n    -1.0 \n  \n  \n    AI \n    3 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AOU \n    3 \n    Satisfice \n    0 \n    -0.231 \n    -0.231 \n    0.333 \n    NA \n    -1.0 \n  \n  \n    AFI \n    2 \n    Satisfice \n    0 \n    0.846 \n    -0.231 \n    0.917 \n    NA \n    -1.0 \n  \n  \n    AIO \n    2 \n    Satisfice \n    0 \n    -0.231 \n    -0.231 \n    0.917 \n    NA \n    -1.0 \n  \n  \n    AO \n    2 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    0.417 \n    NA \n    -1.0 \n  \n  Other\n\n    C \n    2 \n    reference \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  \n     \n    36 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  \n    ABDEFGHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    -0.5 \n  \n  \n    ABDEFGHJKUZ \n    1 \n    frenzy \n    0 \n    0.231 \n    0.250 \n    0.231 \n    NA \n    -0.5 \n  \n  \n    BDEFGHJKU \n    1 \n    frenzy \n    0 \n    0.385 \n    0.417 \n    0.385 \n    NA \n    -0.5 \n  \n  \n    BDEFGHJKUXZ \n    1 \n    frenzy \n    0 \n    0.231 \n    0.250 \n    0.231 \n    NA \n    -0.5 \n  \n  Unknown\n\n    O \n    17 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    DK \n    2 \n    ? \n    0 \n    -0.154 \n    -0.154 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    FJZ \n    1 \n    ? \n    0 \n    0.846 \n    0.846 \n    0.846 \n    NA \n    -0.5 \n  \n  \n    K \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    KO \n    1 \n    ? \n    0 \n    -0.154 \n    -0.154 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==3)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q3 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==3)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q3 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #4\n[PLACEHOLDER — NOT YET CONSIDERED THIS QUESTION]\n\nQ4. Control Condition\n\n\n\nFigure 4.7: Q4—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==4)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q4 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q4 Control Condition :  Which shift(s) end at 4 pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    H \n     \n  \n  \n    Orthgonal \n    U \n    OF \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    BH \n     \n  \n  \n    Tversky [start diagonal] \n    B \n     \n  \n  \n    Tversky [end diagonal] \n    H \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #4 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 4 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 3) %>% \n  pack_rows(\"Orthogonal\", 4, 8) %>% \n  pack_rows(\"Other\", 9, 10) %>% \n  pack_rows(\"Unknown\", 11, 16) \n\n\n\nFrequency of Selected Response Options for Question #4 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    H \n    29 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.083 \n    1.0 \n  \n  \n    AH \n    1 \n    Triangular \n    0 \n    0.929 \n    0.929 \n    NA \n    -0.167 \n    1.0 \n  \n  Lines-Connect\n\n    B \n    3 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    NA \n    -0.083 \n    0.5 \n  \n  Orthogonal\n\n    U \n    87 \n    Orthogonal \n    0 \n    -0.071 \n    -0.071 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    FU \n    2 \n    Orthogonal \n    0 \n    -0.143 \n    -0.143 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    DEOU \n    1 \n    Orthogonal \n    0 \n    -0.286 \n    -0.286 \n    NA \n    0.833 \n    -1.0 \n  \n  \n    DEU \n    1 \n    Orthogonal \n    0 \n    -0.214 \n    -0.214 \n    NA \n    0.833 \n    -1.0 \n  \n  \n    KU \n    1 \n    Orthogonal \n    0 \n    -0.143 \n    -0.143 \n    NA \n    0.917 \n    -1.0 \n  \n  Other\n\n     \n    6 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n    ACFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.286 \n    0.286 \n    NA \n    0.333 \n    -0.5 \n  \n  Unknown\n\n    DE \n    14 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    NA \n    -0.167 \n    -0.5 \n  \n  \n    E \n    6 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    O \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    0.000 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    G \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n\n\n\n\n\n\nTBL4 test\n\n\n\n\n\n\nOrthogonal\nOrthogonal-LinesConnecting\n\n\n\n\n |\n\n\n\nIf the subject calculates end time for each data point (using duration on the y axis), they find that an (incorrect) projection of point U ‘end time’ intersects with the (incorrect) orthogonal projection of 4:00PM.\nAlternatively, some subjects selected points E and D which intersect with an orthogonal projection from 4:00pm. We call this an ’orthogonal-lines connect” strategy, because it (incorrectly) adapts the orthogonal procedure for finding events that start at 4:00pm in order to find those that end at 4:00pm, thus selecting any data point with an orthogonal intersection with 4:00pm.\n\n\n\n\n\nQ4. Impasse Condition\n\n\n\nFigure 4.8: Q4—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==4)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q4 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q4 Impasse Condition :  Which shift(s) end at 4 pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    H \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    FO \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    BH \n     \n  \n  \n    Tversky [start diagonal] \n    B \n     \n  \n  \n    Tversky [end diagonal] \n    H \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nTODO investigate D? add to tversky or orth?\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #4 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 4 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 6) %>% \n  pack_rows(\"Satisfice\", 7, 10) %>% \n  pack_rows(\"Other\", 11, 12) %>% \n  pack_rows(\"Unknown\", 13, 19) \n\n\n\nFrequency of Selected Response Options for Question #4 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    H \n    64 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    DH \n    1 \n    Triangular \n    0 \n    0.929 \n    0.929 \n    -0.154 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    B \n    6 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    BD \n    2 \n    Tversky \n    0 \n    -0.143 \n    0.929 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BH \n    2 \n    Tversky \n    0 \n    0.929 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BDEG \n    1 \n    Tversky \n    0 \n    -0.286 \n    0.786 \n    -0.308 \n    NA \n    0.5 \n  \n  Satisfice\n\n    O \n    11 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    F \n    8 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    FO \n    7 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AFG \n    1 \n    Satisfice \n    0 \n    -0.214 \n    -0.214 \n    0.346 \n    NA \n    -1.0 \n  \n  Other\n\n     \n    20 \n    blank \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n    ACFHIJKOUZ \n    1 \n    frenzy \n    0 \n    0.357 \n    0.357 \n    0.385 \n    NA \n    -0.5 \n  \n  Unknown\n\n    D \n    35 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    A \n    5 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    K \n    3 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    G \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    AI \n    1 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    DK \n    1 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    J \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==4)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q4 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==4)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q4 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #5\n\nQ5. Control Condition\n\n\n\nFigure 4.9: Q5—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==5)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q5 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q5 Control Condition :  Coffee breaks happen halfway through a shift. Which shift(s) share a break with I?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    O \n    AZ \n  \n  \n    Orthgonal \n    U \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    UGX \n    AZKD \n  \n  \n    Tversky [start diagonal] \n    X \n     \n  \n  \n    Tversky [end diagonal] \n    UG \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #5 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 5 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>% \n  pack_rows(\"Lines-Connect\", 5, 7) %>% \n  pack_rows(\"Orthogonal\", 8, 9) %>% \n  pack_rows(\"Other\", 10, 11) %>% \n  pack_rows(\"Unknown\", 12, 22) \n\n\n\nFrequency of Selected Response Options for Question #5 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    O \n    50 \n    Triangular \n    1 \n    1.000 \n    -0.077 \n    NA \n    -0.077 \n    1.0 \n  \n  \n    FO \n    3 \n    Triangular \n    0 \n    0.909 \n    -0.154 \n    NA \n    -0.154 \n    1.0 \n  \n  \n    HO \n    1 \n    Triangular \n    0 \n    0.909 \n    -0.154 \n    NA \n    -0.154 \n    1.0 \n  \n  \n    KO \n    1 \n    Triangular \n    0 \n    0.909 \n    -0.143 \n    NA \n    -0.154 \n    1.0 \n  \n  Lines-Connect\n\n    FG \n    2 \n    Tversky \n    0 \n    -0.182 \n    0.417 \n    NA \n    -0.154 \n    0.5 \n  \n  \n    G \n    1 \n    Tversky \n    0 \n    -0.091 \n    0.500 \n    NA \n    -0.077 \n    0.5 \n  \n  \n    X \n    1 \n    Tversky \n    0 \n    -0.091 \n    1.000 \n    NA \n    -0.077 \n    0.5 \n  \n  Orthogonal\n\n    U \n    64 \n    Orthogonal \n    0 \n    -0.091 \n    0.500 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    HU \n    1 \n    Orthogonal \n    0 \n    -0.182 \n    0.417 \n    NA \n    0.923 \n    -1.0 \n  \n  Other\n\n    I \n    1 \n    reference \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n     \n    6 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    F \n    10 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    H \n    3 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    B \n    2 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    DJ \n    2 \n    ? \n    0 \n    -0.182 \n    -0.143 \n    NA \n    -0.154 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.091 \n    0.000 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    DEHJ \n    1 \n    ? \n    0 \n    -0.364 \n    -0.308 \n    NA \n    -0.308 \n    -0.5 \n  \n  \n    FK \n    1 \n    ? \n    0 \n    -0.182 \n    -0.143 \n    NA \n    -0.154 \n    -0.5 \n  \n  \n    HJ \n    1 \n    ? \n    0 \n    -0.182 \n    -0.154 \n    NA \n    -0.154 \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    0.000 \n    0.000 \n    NA \n    -0.077 \n    -0.5 \n  \n\n\n\n\n\nTODO note the compelling cases of internal inconsistency (HJDE)\n\n\nQ5. Impasse Condition\n\n\n\nFigure 4.10: Q5—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==5)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q5 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q5 Impasse Condition :  Coffee breaks happen halfway through a shift. Which shift(s) share a break with I?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    A \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    K \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    OX \n     \n  \n  \n    Tversky [start diagonal] \n    OX \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #5 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 5 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 7) %>% \n  pack_rows(\"Lines-Connect\", 8, 13) %>% \n  pack_rows(\"Orthogonal\", 14, 16) %>% \n  pack_rows(\"Other\", 17, 21) %>% \n  pack_rows(\"Unknown\", 22, 31) \n\n\n\nFrequency of Selected Response Options for Question #5 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    A \n    83 \n    Triangular \n    1 \n    1.000 \n    -0.083 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    AFG \n    5 \n    Triangular \n    0 \n    0.846 \n    -0.250 \n    -0.231 \n    NA \n    1.0 \n  \n  \n    AF \n    4 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AO \n    2 \n    Triangular \n    0 \n    0.923 \n    0.417 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AI \n    1 \n    Triangular \n    1 \n    1.000 \n    -0.083 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    AU \n    1 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AZ \n    1 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    O \n    6 \n    Tversky \n    0 \n    -0.077 \n    0.500 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    CO \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.417 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    JO \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.417 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    OX \n    1 \n    Tversky \n    0 \n    -0.154 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    UXZ \n    1 \n    Tversky \n    0 \n    -0.231 \n    0.333 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    X \n    1 \n    Tversky \n    0 \n    -0.077 \n    0.500 \n    -0.077 \n    NA \n    0.5 \n  \n  Orthogonal\n\n    K \n    5 \n    Satisfice \n    0 \n    -0.077 \n    -0.083 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    HK \n    3 \n    Satisfice \n    0 \n    -0.154 \n    -0.167 \n    0.923 \n    NA \n    -1.0 \n  \n  \n    HKUZ \n    1 \n    Satisfice \n    0 \n    -0.308 \n    -0.333 \n    0.769 \n    NA \n    -1.0 \n  \n  Other\n\n    I \n    2 \n    reference \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n     \n    24 \n    blank \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n    ABCFGUZ \n    1 \n    frenzy \n    0 \n    0.538 \n    -0.583 \n    -0.538 \n    NA \n    -0.5 \n  \n  \n    ACDEFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.154 \n    0.167 \n    0.154 \n    NA \n    -0.5 \n  \n  \n    FHJKX \n    1 \n    frenzy \n    0 \n    -0.385 \n    0.167 \n    0.692 \n    NA \n    -0.5 \n  \n  Unknown\n\n    H \n    11 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    C \n    2 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    DJ \n    2 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    F \n    2 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    FU \n    2 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    DG \n    1 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    FHZ \n    1 \n    ? \n    0 \n    -0.231 \n    -0.250 \n    -0.231 \n    NA \n    -0.5 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==5)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q5 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==5)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q5 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\n\nTesting Phase\nThe following 10 questions were the same for both conditions.\n\nQuestion #6 NONDISCRIM\n\n\n\nFigure 4.11: Q6-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==6)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) are six hours long?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    EG \n     \n  \n  \n    Orthgonal \n    EG \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\nTODO discuss non discriminant\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #6\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 6) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) \n\n\n\nFrequency of Selected Response Options for Question #6\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  \n    EG \n    330 \n    both tri + orth \n    1 \n    1 \n    NA \n    NA \n    1 \n    0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==6)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q6 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==6)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q6 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #7\n\n\n\nFigure 4.12: Q7-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==7)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which 2 shifts less than 5 hours long start at the same time?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    OX \n     \n  \n  \n    Orthgonal \n    FB \n    M \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    IJZNCHOX \n     \n  \n  \n    Tversky [start diagonal] \n    OX \n     \n  \n  \n    Tversky [end diagonal] \n    IJZN \n     \n  \n  \n    Tversky [duration line] \n    CH \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #7\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 7) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 5) %>%\n  pack_rows(\"Lines-Connect\", 6, 9) %>%\n  pack_rows(\"Orthogonal\", 10, 13) %>%\n  pack_rows(\"Other\", 14, 14) %>%\n  pack_rows(\"Unknown\", 15, 17)\n\n\n\nFrequency of Selected Response Options for Question #7\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    OX \n    93 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MO \n    2 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    AX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MOX \n    1 \n    Triangular \n    0 \n    0.938 \n    0.938 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.067 \n    1.0 \n  \n  Lines-Connect\n\n    IJ \n    3 \n    Tversky \n    0 \n    -0.125 \n    0.500 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    CH \n    1 \n    Tversky \n    0 \n    -0.125 \n    1.000 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    DJNX \n    1 \n    Tversky \n    0 \n    0.312 \n    0.357 \n    NA \n    -0.267 \n    0.5 \n  \n  \n    HK \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.438 \n    NA \n    -0.133 \n    0.5 \n  \n  Orthogonal\n\n    BF \n    203 \n    Orthogonal \n    0 \n    -0.125 \n    -0.125 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    FZ \n    16 \n    Orthogonal \n    0 \n    -0.125 \n    0.179 \n    NA \n    0.433 \n    -1.0 \n  \n  \n    B \n    1 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    1 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    0.500 \n    -1.0 \n  \n  Other\n\n     \n    2 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    GK \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.125 \n    0.179 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    KM \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 7)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q7 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 7)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q7 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #8\n\n\n\nFigure 4.13: Q8-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==8)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q: \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q:  Which shift(s) under 7 hours long starts before B starts, and ends after X ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    G \n     \n  \n  \n    Orthgonal \n    E \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #8\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 8) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 10) %>%\n  pack_rows(\"Orthogonal\", 11, 16) %>%\n  pack_rows(\"Other\", 17, 21) %>%\n  pack_rows(\"Unknown\", 22, 45)\n\n\n\nFrequency of Selected Response Options for Question #8\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    G \n    64 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    -0.067 \n    1.0 \n  \n  \n    AGK \n    4 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1.0 \n  \n  \n    CG \n    3 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    FG \n    3 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    AG \n    2 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    CFGO \n    2 \n    Triangular \n    0 \n    0.800 \n    NA \n    NA \n    -0.267 \n    1.0 \n  \n  \n    ACGP \n    1 \n    Triangular \n    0 \n    0.800 \n    NA \n    NA \n    -0.267 \n    1.0 \n  \n  \n    CFG \n    1 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1.0 \n  \n  \n    CGM \n    1 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1.0 \n  \n  \n    GM \n    1 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  Orthogonal\n\n    E \n    157 \n    Orthogonal \n    0 \n    -0.067 \n    NA \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EIJ \n    5 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.867 \n    -1.0 \n  \n  \n    EFIJ \n    3 \n    Orthogonal \n    0 \n    -0.267 \n    NA \n    NA \n    0.800 \n    -1.0 \n  \n  \n    EF \n    2 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.933 \n    -1.0 \n  \n  \n    EI \n    2 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.933 \n    -1.0 \n  \n  \n    EFI \n    1 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.867 \n    -1.0 \n  \n  Other\n\n     \n    12 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    DEHIJNOZ \n    2 \n    frenzy \n    0 \n    -0.533 \n    NA \n    NA \n    0.533 \n    -0.5 \n  \n  \n    EFGIJ \n    2 \n    frenzy \n    0 \n    0.733 \n    NA \n    NA \n    0.733 \n    -0.5 \n  \n  \n    CDGHLNOXZ \n    1 \n    frenzy \n    0 \n    0.533 \n    NA \n    NA \n    -0.533 \n    -0.5 \n  \n  \n    DEIJN \n    1 \n    frenzy \n    0 \n    -0.333 \n    NA \n    NA \n    0.733 \n    -0.5 \n  \n  Unknown\n\n    IJ \n    17 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    I \n    7 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    EFG \n    3 \n    ? \n    0 \n    0.867 \n    NA \n    NA \n    0.867 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    O \n    3 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    A \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    AK \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    C \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DN \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    F \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    IJM \n    2 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    L \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    M \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CM \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    CX \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DHNZ \n    1 \n    ? \n    0 \n    -0.267 \n    NA \n    NA \n    -0.267 \n    -0.5 \n  \n  \n    DIJN \n    1 \n    ? \n    0 \n    -0.267 \n    NA \n    NA \n    -0.267 \n    -0.5 \n  \n  \n    EFGI \n    1 \n    ? \n    0 \n    0.800 \n    NA \n    NA \n    0.800 \n    -0.5 \n  \n  \n    HLO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    IO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    KL \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 8)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q8 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 8)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q8 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #9 NONDISCRIM\n\n\n\nFigure 4.14: Q9-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==9)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) begins before J begins and ends during B?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    I \n     \n  \n  \n    Orthgonal \n    I \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #9\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q ==9) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Other\", 1, 2) %>%\n  pack_rows(\"Unknown\", 3, 19)\n\n\n\nFrequency of Selected Response Options for Question #9\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Other\n\n    I \n    247 \n    both tri + orth \n    1 \n    1.000 \n    NA \n    NA \n    1.000 \n    0.5 \n  \n  \n    IJ \n    1 \n    both tri + orth \n    1 \n    1.000 \n    NA \n    NA \n    1.000 \n    0.5 \n  \n  Unknown\n\n     \n    23 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    E \n    29 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    F \n    6 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    M \n    4 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    EI \n    3 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    FI \n    3 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    AGN \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    B \n    1 \n    ? \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CHO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DK \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    IM \n    1 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    IO \n    1 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 9)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q9 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 9)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q9 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #10\n\n\n\nFigure 4.15: Q10-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==10)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) end at the same time as F?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    E \n     \n  \n  \n    Orthgonal \n    X \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    EGZ \n     \n  \n  \n    Tversky [start diagonal] \n    G \n     \n  \n  \n    Tversky [end diagonal] \n    E \n     \n  \n  \n    Tversky [duration line] \n    Z \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #10\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 10) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 7) %>%\n  pack_rows(\"Orthogonal\", 8, 11) %>%\n  pack_rows(\"Other\", 12, 14) %>%\n  pack_rows(\"Unknown\", 15, 27)\n\n\n\nFrequency of Selected Response Options for Question #10\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    E \n    103 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    EF \n    1 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  Lines-Connect\n\n    Z \n    23 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    XZ \n    2 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    0.938 \n    0.5 \n  \n  \n    CG \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    -0.125 \n    0.5 \n  \n  \n    G \n    1 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    HLPZ \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.812 \n    NA \n    -0.250 \n    0.5 \n  \n  Orthogonal\n\n    X \n    139 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BX \n    2 \n    Orthogonal \n    0 \n    -0.125 \n    -0.125 \n    NA \n    0.938 \n    -1.0 \n  \n  \n    FX \n    2 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    AMX \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    -0.188 \n    NA \n    0.875 \n    -1.0 \n  \n  Other\n\n    F \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n     \n    6 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    CEGIO \n    1 \n    frenzy \n    0 \n    0.750 \n    0.750 \n    NA \n    -0.312 \n    -0.5 \n  \n  Unknown\n\n    B \n    27 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    J \n    6 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    IJ \n    2 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    P \n    2 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    BO \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    HLP \n    1 \n    ? \n    0 \n    -0.188 \n    -0.188 \n    NA \n    -0.188 \n    -0.5 \n  \n  \n    I \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    K \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    L \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    O \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 10)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q10 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 10)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q10 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #11\n\n\n\nFigure 4.16: Q11-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==11)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) start at 12pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    ML \n     \n  \n  \n    Orthgonal \n    FB \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #11\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 11) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>%\n  pack_rows(\"Orthogonal\", 5, 9) %>%\n  pack_rows(\"Other\", 10, 12) %>%\n  pack_rows(\"Unknown\", 13, 17)\n\n\n\nFrequency of Selected Response Options for Question #11\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    LM \n    99 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    -0.125 \n    1.0 \n  \n  \n    M \n    7 \n    Triangular \n    0 \n    0.500 \n    NA \n    NA \n    -0.062 \n    1.0 \n  \n  \n    BLM \n    2 \n    Triangular \n    0 \n    0.938 \n    NA \n    NA \n    0.375 \n    1.0 \n  \n  \n    EKM \n    1 \n    Triangular \n    0 \n    0.375 \n    NA \n    NA \n    -0.188 \n    1.0 \n  \n  Orthogonal\n\n    BF \n    201 \n    Orthogonal \n    0 \n    -0.125 \n    NA \n    NA \n    1.000 \n    -1.0 \n  \n  \n    B \n    4 \n    Orthogonal \n    0 \n    -0.062 \n    NA \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    2 \n    Orthogonal \n    0 \n    -0.062 \n    NA \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BFXZ \n    1 \n    Orthogonal \n    0 \n    -0.250 \n    NA \n    NA \n    0.875 \n    -1.0 \n  \n  \n    BH \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    NA \n    NA \n    0.438 \n    -1.0 \n  \n  Other\n\n     \n    4 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    ACDGHKLMNOPXZ \n    1 \n    frenzy \n    0 \n    0.312 \n    NA \n    NA \n    -0.812 \n    -0.5 \n  \n  \n    DHLMNOXZ \n    1 \n    frenzy \n    0 \n    0.625 \n    NA \n    NA \n    -0.500 \n    -0.5 \n  \n  Unknown\n\n    J \n    2 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    CX \n    1 \n    ? \n    0 \n    -0.125 \n    NA \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    XZ \n    1 \n    ? \n    0 \n    -0.125 \n    NA \n    NA \n    -0.125 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 11)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q11 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 11)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q11 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #12\n\n\n\nFigure 4.17: Q12-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==12)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) start at the same time as F?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    G \n     \n  \n  \n    Orthgonal \n    B \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    GZ \n     \n  \n  \n    Tversky [start diagonal] \n    G \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n    Z \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #12\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 12) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 3) %>%\n  pack_rows(\"Lines-Connect\", 4, 6) %>%\n  pack_rows(\"Orthogonal\", 7, 8) %>%\n  pack_rows(\"Other\", 9, 10) %>%\n  pack_rows(\"Unknown\", 11, 14)\n\n\n\nFrequency of Selected Response Options for Question #12\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    G \n    98 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    FG \n    3 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    GP \n    1 \n    Triangular \n    0 \n    0.938 \n    0.938 \n    NA \n    -0.125 \n    1.0 \n  \n  Lines-Connect\n\n    Z \n    4 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    BZ \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    0.938 \n    0.5 \n  \n  \n    B \n    206 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  Orthogonal\n\n    BF \n    5 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n     \n    3 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Other\n\n    CEGIO \n    1 \n    frenzy \n    0 \n    0.750 \n    0.750 \n    NA \n    -0.312 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  Unknown\n\n    E \n    2 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    FM \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 12)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q12 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 12)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q12 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #13\n\n\n\nFigure 4.18: Q13-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==13)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which 2 shifts end when Z begins?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    EF \n     \n  \n  \n    Orthgonal \n    FX \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #13\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 13) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 3) %>%\n  pack_rows(\"Orthogonal\", 4, 13) %>%\n  pack_rows(\"Other\", 14, 14) %>%\n  pack_rows(\"Unknown\", 15, 36)\n\n\n\nFrequency of Selected Response Options for Question #13\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    EF \n    91 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    0.433 \n    1.0 \n  \n  \n    CE \n    1 \n    Triangular \n    0 \n    0.433 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    E \n    1 \n    Triangular \n    0 \n    0.500 \n    NA \n    NA \n    -0.067 \n    1.0 \n  \n  Orthogonal\n\n    FX \n    141 \n    Orthogonal \n    0 \n    0.433 \n    NA \n    NA \n    1.000 \n    -1.0 \n  \n  \n    X \n    9 \n    Orthogonal \n    0 \n    -0.067 \n    NA \n    NA \n    0.500 \n    -1.0 \n  \n  \n    OX \n    4 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    KX \n    3 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    ACX \n    1 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.367 \n    -1.0 \n  \n  \n    BX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    CX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    DJNX \n    1 \n    Orthogonal \n    0 \n    -0.267 \n    NA \n    NA \n    0.300 \n    -1.0 \n  \n  \n    GX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    JX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  Other\n\n     \n    5 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    HN \n    13 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    BF \n    11 \n    ? \n    0 \n    0.433 \n    NA \n    NA \n    0.433 \n    -0.5 \n  \n  \n    F \n    10 \n    ? \n    0 \n    0.500 \n    NA \n    NA \n    0.500 \n    -0.5 \n  \n  \n    EX \n    6 \n    ? \n    0 \n    0.433 \n    NA \n    NA \n    0.433 \n    -0.5 \n  \n  \n    HL \n    5 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    HLP \n    5 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    BM \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    CO \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    DN \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    AG \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    BO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CG \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    CGO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    CH \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DKM \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    HZ \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    LP \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    NO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    NZ \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 13)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q13 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 13)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q13 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #14\n\n\n\nFigure 4.19: Q14-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==14)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) end at 3pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    X \n     \n  \n  \n    Orthgonal \n    B \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    XJND \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n    X \n     \n  \n  \n    Tversky [duration line] \n    JND \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #14\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 14) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>%\n  pack_rows(\"Orthogonal\", 5, 7) %>%\n  pack_rows(\"Other\", 8, 9) %>%\n  pack_rows(\"Unknown\", 10, 22)\n\n\n\nFrequency of Selected Response Options for Question #14\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    X \n    107 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.059 \n    1.0 \n  \n  \n    FX \n    2 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  \n    EX \n    1 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  \n    OX \n    1 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  Orthogonal\n\n    B \n    150 \n    Orthogonal \n    0 \n    -0.059 \n    -0.059 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BF \n    12 \n    Orthogonal \n    0 \n    -0.118 \n    -0.118 \n    NA \n    0.941 \n    -1.0 \n  \n  \n    BIO \n    2 \n    Orthogonal \n    0 \n    -0.176 \n    -0.176 \n    NA \n    0.882 \n    -1.0 \n  \n  Other\n\n     \n    29 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n    O \n    5 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  Unknown\n\n    F \n    3 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    G \n    3 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    A \n    2 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    BX \n    2 \n    ? \n    0 \n    0.941 \n    0.941 \n    NA \n    0.941 \n    -0.5 \n  \n  \n    HLP \n    2 \n    ? \n    0 \n    -0.176 \n    -0.176 \n    NA \n    -0.176 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    AH \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    DHO \n    1 \n    ? \n    0 \n    -0.176 \n    0.200 \n    NA \n    -0.176 \n    -0.5 \n  \n  \n    FG \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    HL \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    IJ \n    1 \n    ? \n    0 \n    -0.118 \n    0.267 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    M \n    1 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    P \n    1 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 14)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q14 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 14)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q14 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #15\n\n\n\nFigure 4.20: Q15-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==15)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Coffee breaks happen halfway through a shift. Which shifts share a break at 2pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    XK \n     \n  \n  \n    Orthgonal \n    EF \n    B \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    XKZ \n     \n  \n  \n    Tversky [start diagonal] \n    Z \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #15\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 15) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 10) %>%\n  pack_rows(\"Lines-Connect\", 11, 13) %>%\n  pack_rows(\"Orthogonal\", 14, 22) %>%\n  pack_rows(\"Other\", 23, 23) %>%\n  pack_rows(\"Unknown\", 24, 44)\n\n\n\nFrequency of Selected Response Options for Question #15\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    KX \n    100 \n    Triangular \n    1 \n    1.000 \n    0.667 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    X \n    6 \n    Triangular \n    0 \n    0.500 \n    0.333 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    CX \n    2 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    DJNX \n    2 \n    Triangular \n    0 \n    0.312 \n    0.133 \n    NA \n    -0.267 \n    1.0 \n  \n  \n    AKPX \n    1 \n    Triangular \n    0 \n    0.875 \n    0.533 \n    NA \n    -0.267 \n    1.0 \n  \n  \n    CK \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    GK \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    JX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    K \n    1 \n    Triangular \n    0 \n    0.500 \n    0.333 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    LX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  Lines-Connect\n\n    FZ \n    3 \n    Tversky \n    0 \n    -0.125 \n    0.941 \n    NA \n    0.433 \n    0.5 \n  \n  \n    OZ \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.941 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    Z \n    1 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.067 \n    0.5 \n  \n  Orthogonal\n\n    EF \n    118 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BF \n    17 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    13 \n    Orthogonal \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    E \n    8 \n    Orthogonal \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BE \n    4 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BEF \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    -0.176 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EFZ \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    0.882 \n    NA \n    0.933 \n    -1.0 \n  \n  \n    EI \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.433 \n    -1.0 \n  \n  \n    FI \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.433 \n    -1.0 \n  \n  Other\n\n     \n    11 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    G \n    4 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    B \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.000 \n    -0.5 \n  \n  \n    C \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    O \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    AG \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    BM \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CG \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    M \n    2 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    A \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    BG \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DN \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    FK \n    1 \n    ? \n    0 \n    0.438 \n    0.267 \n    NA \n    0.433 \n    -0.5 \n  \n  \n    FX \n    1 \n    ? \n    0 \n    0.438 \n    0.267 \n    NA \n    0.433 \n    -0.5 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    HN \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    HO \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    I \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    IJ \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    J \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    L \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 15)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q15 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 15)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q15 \") + \n  theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#export",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#export",
    "title": "4  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC3A/data/2-scored-data/sgc3a_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC3A/data/2-scored-data/sgc3a_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures\n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#resources",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#resources",
    "title": "4  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\nset operations\nhttps://stat.ethz.ch/R-manual/R-devel/library/base/html/sets.html\nkableExtra tables\nhttps://haozhu233.github.io/kableExtra/awesome_table_in_html.html#grouped_columns__rows\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1          stringr_1.4.0          dplyr_1.0.9           \n [4] purrr_0.3.4            readr_2.1.2            tidyr_1.2.0           \n [7] tibble_3.1.7           tidyverse_1.3.1        statsExpressions_1.3.2\n[10] ggstatsplot_0.9.3      Hmisc_4.7-0            Formula_1.2-4         \n[13] survival_3.3-1         lattice_0.20-45        pbapply_1.5-0         \n[16] ggformula_0.10.1       ggridges_0.5.3         scales_1.2.0          \n[19] ggstance_0.3.5         ggplot2_3.3.6          kableExtra_1.3.4      \n\nloaded via a namespace (and not attached):\n  [1] TH.data_1.1-1       colorspace_2.0-3    rio_0.5.29         \n  [4] ellipsis_0.3.2      estimability_1.4    htmlTable_2.4.0    \n  [7] fs_1.5.2            parameters_0.18.1   base64enc_0.1-3    \n [10] rstudioapi_0.13     farver_2.1.0        ggrepel_0.9.1      \n [13] bit64_4.0.5         fansi_1.0.3         mvtnorm_1.1-3      \n [16] lubridate_1.8.0     xml2_1.3.3          codetools_0.2-18   \n [19] splines_4.2.1       knitr_1.39          polyclip_1.10-0    \n [22] zeallot_0.1.0       jsonlite_1.8.0      broom_0.8.0        \n [25] cluster_2.1.3       dbplyr_2.2.1        png_0.1-7          \n [28] effectsize_0.7.0    ggforce_0.3.3       compiler_4.2.1     \n [31] httr_1.4.3          emmeans_1.7.5       backports_1.4.1    \n [34] assertthat_0.2.1    Matrix_1.4-1        fastmap_1.1.0      \n [37] cli_3.3.0           tweenr_1.0.2        htmltools_0.5.2    \n [40] tools_4.2.1         coda_0.19-4         gtable_0.3.0       \n [43] glue_1.6.2          Rcpp_1.0.8.3        cellranger_1.1.0   \n [46] vctrs_0.4.1         svglite_2.1.0       insight_0.17.1     \n [49] xfun_0.31           openxlsx_4.2.5      rvest_1.0.2        \n [52] lifecycle_1.0.1     mosaicCore_0.9.0    MASS_7.3-57        \n [55] zoo_1.8-10          vroom_1.5.7         hms_1.1.1          \n [58] parallel_4.2.1      sandwich_3.0-2      rematch2_2.1.2     \n [61] RColorBrewer_1.1-3  prismatic_1.1.0     curl_4.3.2         \n [64] yaml_2.3.5          gridExtra_2.3       labelled_2.9.1     \n [67] rpart_4.1.16        latticeExtra_0.6-29 stringi_1.7.6      \n [70] highr_0.9           paletteer_1.4.0     bayestestR_0.12.1  \n [73] checkmate_2.1.0     zip_2.2.0           boot_1.3-28        \n [76] rlang_1.0.3         pkgconfig_2.0.3     systemfonts_1.0.4  \n [79] evaluate_0.15       labeling_0.4.2      patchwork_1.1.1    \n [82] htmlwidgets_1.5.4   bit_4.0.4           tidyselect_1.1.2   \n [85] plyr_1.8.7          magrittr_2.0.3      R6_2.5.1           \n [88] generics_0.1.2      multcomp_1.4-19     DBI_1.1.3          \n [91] pillar_1.7.0        haven_2.5.0         foreign_0.8-82     \n [94] withr_2.5.0         datawizard_0.4.1    nnet_7.3-17        \n [97] performance_0.9.1   modelr_0.1.8        crayon_1.5.1       \n[100] utf8_1.2.2          correlation_0.8.1   tzdb_0.3.0         \n[103] rmarkdown_2.14      jpeg_0.1-9          readxl_1.4.0       \n[106] grid_4.2.1          data.table_1.14.2   reprex_2.0.1       \n[109] digest_0.6.29       webshot_0.5.3       xtable_1.8-4       \n[112] munsell_0.5.0       viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#archive",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#archive",
    "title": "4  Response Scoring",
    "section": "ARCHIVE",
    "text": "ARCHIVE\nPrior versions of functions for for-loop version of scoring, not optimized to use mapply\n\n\nCODE\n# #CALCULATE THE TRIANGULAR, ORTHOGONAL OR TVERSKIAN SUBSCORES FROM KEYFRAME\n# calc_sub_score <- function(question, cond, response,keyframe){\n# \n#   #STEP 1 GET KEY\n#   if (question < 6) #for q1 - q5 find key for question by condition\n#   {\n#     # print(keyframe)\n#     #GET KEY FOR THIS SCORE TYPE, QUESTION AND CONDITION\n#     p =  keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#     q =  keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% unlist()\n#     pn = keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(n_p)\n#     qn = keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(n_q)\n# \n#   } else {\n#     #GET KEY FOR THIS SCORE TYPE, QUESTION\n#     p =  keyframe %>% filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#     q =  keyframe %>% filter(Q == question) %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% unlist()\n#     pn = keyframe %>% filter(Q == question) %>% select(n_p)\n#     qn = keyframe %>% filter(Q == question) %>% select(n_q)\n#   }\n# \n#   #STEP 2 CALC INTERSECTIONS BETWEEN RESPONSE AND KEY\n#   \n#   #if response is not empty, split apart response for set comparison\n#     if(response != \"\")\n#     { response = response %>% str_split(\"\") %>% unlist()}\n#     \n#   ps = length(intersect(response,p))\n#   qs = length(intersect(response,q))\n#   # df_items[x,'tri_ps'] = tri_ps\n#   # df_items[x,'tri_qs'] = tri_qs\n# \n#   #STEP 3 CALC f_partialP schema SCORE FOR THIS INTERSECTION\n#   x = f_partialP(ps,pn,qs,qn) %>% unlist() %>% as.numeric()\n#   \n#   #cleanup\n#   rm(p,q,pn,qn,ps,qs)\n#   return(x)\n# \n# }\n# \n# #CALCULATE THE REFERENCE SCORES\n# calc_ref_score <- function(question, cond, response){\n#   \n#     #1. GET reference point from REF_POINT column in raw keys\n#     ref_p = keys_raw %>% filter(Q == question) %>% filter(condition == cond) %>% select(REF_POINT) %>% pull(REF_POINT) %>% str_split(\"\") %>% unlist()\n#      \n#     #2. if response has more than one character, it can't be correct\n#     #there is only ever 1 reference character\n#     n = nchar(response)\n#     if (n == 0) {x = 0}\n#     else if(n>1) {x = 0}\n#     else {\n#       #3 is the response PRECISELY the REFERENCE POINT?\n#       x = ref_p == response\n#       x = as.numeric(x)  \n#     }\n#     \n#     #cleanup\n#     rm(ref_p, response, question, cond)   \n#     return(x) #1 = match, 0 = not match\n# }\n# \n# \n# #CALCULATE SCORE BASED ON UNION OF ORTH & TRI (SUBJECT SELECTS BOTH ANSWERS )\n# calc_both_score <- function(question, cond, response){\n#   \n#TRAPDOOR \n#   #since no orth responses exist for impasse condition q1 - q5, set to 0\n#   if (question < 6 & cond == 121) {x = NA}\n#   \n#   #ELSE \n#   #calculate union of ORTH and TRI\n#   else {\n#     if (question < 6 & cond == 111) #for q1 - q5 find key for question by condition\n#   {\n#      #grab the tri and orth keys for this question as well as N option set\n#      tri_p =  keys_tri %>%  filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      orth_p = keys_orth %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      set_n =  keys_tri %>%  filter(Q == question) %>% filter(condition == cond) %>% select(set_n) %>% pull(set_n) %>% str_split(\"\") %>% unlist() \n#      #1. calc answer that is both tri and orth and only these --> union of tri_p and orth_p\n#      both_p = union(tri_p, orth_p) #the selection of tri and p\n#      #2. calc answers that should't be selected as diffrence between N [same for all keys] and both_p\n#      both_q = setdiff(set_n,both_p)\n#      both_pn = length(both_p)\n#      both_qn = length(both_q)\n#   } else{\n#     \n#      #grab the tri and orth keys for this question as well as N option set\n#      tri_p =  keys_tri %>%  filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      orth_p = keys_orth %>% filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      set_n =  keys_tri %>%  filter(Q == question) %>% select(set_n) %>% pull(set_n) %>% str_split(\"\") %>% unlist() \n#      #1. calc answer that is both tri and orth and only these --> union of tri_p and orth_p\n#      both_p = union(tri_p, orth_p) #the selection of tri and p\n#      #2. calc answers that shouldn't be selected as difference between N [same for all keys] and both_p\n#      both_q = setdiff(set_n,both_p)\n#      both_pn = length(both_p)\n#      both_qn = length(both_q)\n#   }\n#     \n#   #STEP 2 CALC INTERSECTIONS BETWEEN RESPONSE AND KEY\n#   \n#   #if response is not empty, split apart response for set comparison\n#     if(response != \"\")\n#     { response = response %>% str_split(\"\") %>% unlist()}\n#   \n#     both_ps = length(intersect(response,both_p))\n#     both_qs = length(intersect(response,both_q))\n#   \n#  \n#   #STEP 3 CALC f_partialP schema SCORE FOR THIS INTERSECTION \n#   x = f_partialP(both_ps,both_pn,both_qs,both_qn)%>% unlist() %>% as.numeric()\n#   \n#   #cleanup\n#   rm(both_p,both_q,both_pn,both_qn,both_ps,both_qs, question, cond, response )   \n#   }\n#   \n#   return(x) #true correct, trues, false correct, falses\n# }\n\n\nLooping to do the scoring (not using MAPPLY)\n\n\nCODE\n#RUN THIS OR THE CALCULATE-SCORES-MAPPLY\n# df_items = trad \n# \n# pb <- timerProgressBar() \n# on.exit(close(pb)) \n#  \n# #CALCULATE SUBSCORES (in loop)\n# \n# for (x in 1:nrow(df_items)) {\n#   \n#   #show progress bar \n#   setTimerProgressBar(pb, x) \n#   \n#   #PREPARE ITEMS FOR SCORING\n#   #sort response vectors alphabetically\n#   #doesn't impact scoring, but does impact response display tables\n#    df_items[x,'response'] <-  df_items[x,'response'] %>% str_split(\"\") %>% unlist() %>% sort() %>% str_c(collapse=\"\")\n# \n#   #get properties of the RESPONSE ITEM\n#   qu = df_items[x,'q'] %>% as.numeric()\n#   cond = as.character(df_items[x,'condition']) %>% as.numeric()\n#   r = df_items[x,'response'] \n# \n#   #calculate the main subscores\n#   df_items[x,'score_TRI'] = calc_sub_score(qu, cond, r,keys_tri)\n#   df_items[x,'score_ORTH'] = calc_sub_score(qu, cond, r,keys_orth)\n#   df_items[x,'score_SATISFICE'] = calc_sub_score(qu, cond, r,keys_satisfice)\n#   df_items[x,'score_TV_max'] = calc_sub_score(qu, cond, r,keys_tversky_max)\n#   df_items[x,'score_TV_start'] = calc_sub_score(qu, cond, r,keys_tversky_start)\n#   df_items[x,'score_TV_end'] = calc_sub_score(qu, cond, r,keys_tversky_end)\n#   df_items[x,'score_TV_duration'] = calc_sub_score(qu, cond, r, keys_tversky_duration)\n#   \n#   #calculate special subscores\n#   df_items[x,'score_REF'] = calc_ref_score(qu, cond, r)\n#   df_items[x,'score_BOTH'] = calc_both_score(qu, cond, r)\n# }\n# \n# #CALCULATE ABSOLUTE SCORES\n# #calculate absolute scores dichotomous\n# df_items$score_ABS = as.integer(df_items$correct)\n# #niceABS indicates if the response is correct without penalizing the allowable triangular options(ie. the ref point)\n# df_items$score_niceABS  <- as.integer((df_items$score_TRI == 1))\n#  \n# #cleanup\n# rm(qu,cond,r, x)\n\n# trad_scored = df_items\n\n\nsanity check equivalence of for-loop and mapply scoring\n\n\nCODE\n#CHECK EQUIVALENCE OF LOOP AND MAPPLY SCORING \n# tests = data.frame (\n#   alt_tri = alt_scored$score_TRI,\n#   trad_tri = trad_scored$score_TRI,\n#   alt_orth = alt_scored$score_ORTH,\n#   trad_orth = trad_scored$score_ORTH,\n#   alt_ref = alt_scored$score_REF,\n#   trad_ref = trad_scored$score_REF,\n#   alt_tv_max = alt_scored$score_TV_max,\n#   trad_tv_max = trad_scored$score_TV_max,\n#   alt_tv_dur = alt_scored$score_TV_duration,\n#   trad_tv_dur = trad_scored$score_TV_duration,\n#   alt_tv_start = alt_scored$score_TV_start,\n#   trad_tv_start = trad_scored$score_TV_start,\n#   alt_tv_end = alt_scored$score_TV_end,\n#   trad_tv_end = trad_scored$score_TV_end,\n#   alt_both = alt_scored$score_BOTH,\n#   trad_both = trad_scored$score_BOTH,\n#   trad_response = trad_scored$response,\n#   alt_response = alt_scored$response,\n#   q_match = trad_scored$q == alt_scored$q,\n#   q = trad_scored$q,\n#   c_match = trad_scored$condition == alt_scored$condition,\n#   condition = trad_scored$condition\n# )\n# \n# tests$tri = tests$alt_tri == tests$trad_tri\n# tests$orth = tests$alt_orth == tests$trad_orth\n# tests$ref = tests$alt_ref == tests$trad_ref\n# tests$tvdur = tests$alt_tv_dur == tests$trad_tv_dur\n# tests$tvstart = tests$alt_tv_start == tests$trad_tv_start\n# tests$tvend = tests$alt_tv_end == tests$trad_tv_end\n# tests$both = tests$alt_both == tests$trad_both\n# \n# #CHECKS \n# unique(tests$tri)\n# unique(tests$orth)\n# unique(tests$ref)\n# unique(tests$tvdur)\n# unique(tests$tvstart)\n# unique(tests$tvend)\n# unique(tests$both)\n# \n# unique(alt_scored$score_ABS == trad_scored$score_ABS)\n# unique(alt_scored$score_niceABS == trad_scored$score_niceABS)\n\n\nPrior inline version of derive interpretation, before externalizing to a function in the scoring script. ::: {.cell}\n\nCODE\n# threshold_range = 0.5 #set required variance in subscores to be discriminant\n# threshold_frenzy = 4\n# \n# for (x in 1:nrow(df_items)) {\n#   \n#   #CALCULATE MAX TVERSKY SUBSCORE\n#   t = df_items[x,] %>% select(score_TV_max, score_TV_start, score_TV_end, score_TV_duration) #reshape\n#   t.long = gather(t,score, value, 1:4)\n#   t.long[t.long == \"\"] = NA #replace empty scores with NA so we can ignore them\n#   if(length(unique(t.long$value)) == 1 ){\n#     if(is.na(unique(t.long$value))){\n#       df_items[x,'score_TVERSKY'] = NA\n#       df_items[x,'tv_type'] = NA   \n#     }\n#   } else {\n#     df_items[x,'score_TVERSKY'] = as.numeric(max(t.long$value,na.rm = TRUE))\n#     df_items[x,'tv_type'] = t.long[which.max(t.long$value),'score']\n#   }\n#   \n#   #CALCULATE MAX SATISFICING SUBSCORE\n#   t = df_items[x,] %>% select(score_SAT_left, score_SAT_right)\n#   t.long = gather(t,score, value, 1:2)\n#   t.long[t.long == \"\"] = NA #replace empty scores\n#   if(length(unique(t.long$value)) == 1 ){\n#     if(is.na(unique(t.long$value))){\n#       df_items[x,'score_SATISFICE'] = NA\n#       df_items[x,'sat_type'] = NA   \n#     }\n#   } else {\n#     df_items[x,'score_SATISFICE'] = as.numeric(max(t.long$value,na.rm = TRUE))\n#     df_items[x,'sat_type'] = t.long[which.max(t.long$value),'score']  \n#   }\n#   \n#   #NOW CALCULATE RANGE AMONG SUBSCORES\n#   #order of this selection matters in breaking ties! \n#   t = df_items[x,] %>% select(score_TRI, score_TVERSKY, score_SATISFICE, score_ORTH)\n#   t.long = gather(t,score, value, 1:4)\n#   t.long[t.long == \"\"] = NA\n#   \n#   df_items[x,'top_score'] = as.numeric(max(t.long$value,na.rm = TRUE))\n#   df_items[x,'top_type'] = t.long[which.max(t.long$value),'score']\n#   \n#   #calculate the range between highest and lowest scores \n#   r = as.numeric(range(t.long$value,na.rm = TRUE))\n#   r = diff(r)\n#   df_items[x,'range'] = r\n#   \n#   #DISCRIMINANT BETWEEN SUBSCORES TO PREDICT BEST FIT INTERPRETATION\n#   \n#   if (r < threshold_range) {\n#       #then we can't predict the interpretation, leave it as \"?\"\n#     df_items[x,'best'] = \"?\"\n#   } else {\n#       p =  df_items[x,'top_type']\n#       if (p == \"score_TRI\") {df_items[x,'best'] = \"Triangular\"\n#       } else if(p == \"score_ORTH\") {df_items[x,'best'] = \"Orthogonal\"\n#       } else if(p == \"score_TVERSKY\") {df_items[x,'best'] = \"Tversky\"\n#       } else if(p == \"score_SATISFICE\") {df_items[x,'best'] = \"Satisfice\"}\n#   }\n#   \n#   #CHECK SPECIAL SITUATIONS\n# \n#   #BOTH TRI AND ORTH?  \n#   if (!is.na(df_items[x,'score_BOTH'])) { #only check if both is not null\n#       if( df_items[x,'score_BOTH'] == 1) {\n#         df_items[x,'best'] = \"both tri + orth\"}\n#   }\n#   \n#   #IS BLANK?\n#   if( df_items[x,'num_o'] == 0) {  \n#     df_items[x,'best'] = \"blank\"\n#   }\n#   \n#   #IS FRENZY?\n#   if( df_items[x,'num_o'] > threshold_frenzy) { \n#       df_items[x,'best'] = \"frenzy\"\n#   }\n# \n#   #IS REF POINT?\n#   if (!is.na(df_items[x,'score_REF'])) { #only check if the score is NOT null\n#       if( df_items[x,'score_REF'] == 1) {\n#           df_items[x,'best'] = \"reference\"\n#       }\n#   }\n# \n# }#end loop\n# \n# #cleanup \n# rm(t, t.long, x, r,p)\n# rm(threshold_frenzy, threshold_range)\n# \n# #set order of levels for response exploration table\n# df_items$int2 <- factor(df_items$best,\n#                                   levels = c(\"Triangular\", \"Tversky\",\n#                                              \"Satisfice\", \"Orthogonal\", \"reference\", \"both tri + orth\", \"blank\",\"frenzy\",\"?\"))\n# \n# #set order of levels\n# df_items$interpretation <- factor(df_items$best,\n#                                   levels = c(\"Orthogonal\",\"Satisfice\", \"frenzy\",\"?\",\"reference\",\"blank\",\n#                                                \"both tri + orth\", \"Tversky\",\"Triangular\"))\n# \n# #collapsed representation of scale of interpretations\n# df_items$high_interpretation <- fct_collapse(df_items$interpretation,\n#   orthogonal = c(\"Satisfice\", \"Orthogonal\"),\n#   neg.trans = c(\"frenzy\",\"?\"),\n#   neutral = c(\"reference\",\"blank\"),\n#   pos.trans = c(\"Tversky\",\"both tri + orth\"),\n#   triangular = \"Triangular\"\n# ) \n# \n# #reorder levels\n# df_items$high_interpretation = factor(df_items$high_interpretation, levels= c(\"orthogonal\", \"neg.trans\",\"neutral\",\"pos.trans\",\"triangular\"))\n# \n# #cleanup \n# df_items <- df_items %>% dplyr::select(-best)\n# \n# #recode as numeric inase they are char \n# # df_items$score_TV_duration <- df_items$score_TV_duration %>% as.numeric()\n# # df_items$score_SATISFICE <- df_items$score_SATISFICE %>% as.numeric()\n\n:::\nOld inline calculation of score_SCALED before externalizing as function ::: {.cell}\n\nCODE\n# df_items$score_SCALED <- recode(df_items$interpretation,\n#                           \"Orthogonal\" = -1,\n#                           \"Satisfice\" = -1,\n#                           \"frenzy\" = -0.5,\n#                           \"?\" = -0.5,\n#                           \"reference\" = 0,\n#                           \"blank\" = 0, \n#                           \"both tri + orth\" = 0.5,\n#                           \"Tversky\" = 0.5,\n#                           \"Triangular\" = 1)\n\n:::\nOriginal summary by subject before externalizing as function ::: {.cell}\n\nCODE\n# #prep items\n# df_items <- df_items %>% mutate(\n#   tv_type = as.factor(tv_type),\n#   top_type = as.factor(top_type)\n# )\n# \n# #summarize SCORES and TIME by subject\n# subjects_summary <- df_items %>% filter(q %nin% c(6,9)) %>% group_by(subject) %>% dplyr::summarise (\n#   subject = as.character(subject),\n#   pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n#   s_TRI = sum(score_TRI,na.rm=TRUE),\n#   s_ORTH = sum(score_ORTH,na.rm=TRUE),\n#   s_TVERSKY = sum(score_TVERSKY,na.rm=TRUE),\n#   s_SATISFICE = sum(score_SATISFICE, na.rm=TRUE),\n#   s_REF = sum(score_REF,na.rm=TRUE),\n#   s_ABS = sum(score_ABS,na.rm=TRUE),\n#   s_NABS = sum(score_niceABS,na.rm=TRUE),\n#   s_SCALED = sum(score_SCALED,na.rm=TRUE),\n#   DV_percent_NABS = s_NABS/13,\n#   rt_m = sum(rt_s)/60,\n#   item_avg_rt = mean(rt_s),\n#   item_min_rt = min(rt_s),\n#   item_max_rt = max(rt_s),\n#   item_n_TRI = sum(interpretation == \"Triangular\"),\n#   item_n_ORTH = sum(interpretation == \"Orthogonal\"),\n#   item_n_TV = sum(interpretation == \"Tversky\"),\n#   item_n_SAT = sum(interpretation == \"Satisfice\"),\n#   item_n_OTHER = sum(interpretation %nin% c(\"Triangular\",\"Orthogonal\",\"Tversky\",\"Satisfice\")),\n#   item_n_POS = sum(high_interpretation == \"pos.trans\"),\n#   item_n_NEG = sum(high_interpretation == \"neg.trans\"),\n#   item_n_NEUTRAL = sum(high_interpretation == \"neutral\")\n# ) %>% arrange(subject) %>% slice(1L)\n# \n# #summarize first scaffold item of interest by subject\n# subjects_q1 <- df_items %>% filter(q == 1) %>% mutate(\n#   item_q1_NABS = score_niceABS,\n#   item_q1_SCALED = score_SCALED,\n#   item_q1_interpretation = interpretation,\n#   item_q1_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q1_NABS, item_q1_SCALED, item_q1_interpretation,item_q1_rt) %>% arrange(subject)\n# \n# #summarize last test item of interest by subject\n# subjects_q5 <- df_items %>% filter(q == 5) %>% mutate(\n#   item_q5_NABS = score_niceABS,\n#   item_q5_SCALED = score_SCALED,\n#   item_q5_interpretation = interpretation,\n#   item_q5_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q5_NABS, item_q5_SCALED, item_q5_interpretation,item_q5_rt) %>% arrange(subject)\n# \n# #summarize first test item of interest by subject\n# subjects_q7 <- df_items %>% filter(q == 7) %>% mutate(\n#   item_q7_NABS = score_niceABS,\n#   item_q7_interpretation = interpretation,\n#   item_q7_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q7_NABS, item_q7_interpretation,item_q7_rt) %>% arrange(subject)\n# \n# #summarize last test item of interest by subject\n# subjects_q15 <- df_items %>% filter(q == 15) %>% mutate(\n#   item_q15_NABS = score_niceABS,\n#   item_q15_interpretation = interpretation,\n#   item_q15_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q15_NABS, item_q15_interpretation,item_q15_rt) %>% arrange(subject)\n# \n# #summarize scaffold phase performance\n# subjects_scaffold <- df_items %>% filter(q<6)  %>% group_by(subject) %>% dplyr::summarise (\n#   item_scaffold_NABS = sum(score_niceABS),\n#   item_scaffold_SCALED = sum(score_SCALED),\n#   item_scaffold_rt = sum(rt_s)\n# )%>% dplyr::select(subject, item_scaffold_NABS, item_scaffold_SCALED, item_scaffold_rt) %>% arrange(subject)\n# \n# #summarize test phase performance\n# subjects_test <- df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% group_by(subject) %>% dplyr::summarise (\n#   item_test_NABS = sum(score_niceABS),\n#   item_test_SCALED = sum(score_SCALED),\n#   item_test_rt = sum(rt_s)\n# )%>% dplyr::select(subject, item_test_NABS, item_test_SCALED, item_test_rt) %>% arrange(subject)\n# \n# #import subjects\n# df_subjects <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n# \n# #SANITY CHECK SUBJECT ORDER BEFORE MERGE; BOTH SHOULD BE TRUE\n# unique(subjects_summary$subject == df_subjects$subject)\n# unique(subjects_summary$subject == subjects_q1$subject)\n# unique(subjects_summary$subject == subjects_q5$subject)\n# unique(subjects_summary$subject == subjects_q7$subject)\n# unique(subjects_summary$subject == subjects_q15$subject)\n# unique(subjects_summary$subject == subjects_scaffold$subject)\n# unique(subjects_summary$subject == subjects_test$subject)\n# \n# #CAREFULLY CHECK THIS — RELIES ON \n# x = merge(df_subjects, subjects_summary)\n# x = merge(x, subjects_q1)\n# x = merge(x, subjects_q5)\n# x = merge(x, subjects_q7)\n# x = merge(x, subjects_q15)\n# x = merge(x, subjects_scaffold)\n# x = merge(x, subjects_test)\n# df_subjects <- x %>% dplyr::select(-absolute_score) #drop absolute score from webapp that includes Q6 and Q9\n# \n# #cleanup\n# rm(subjects_q1, subjects_q5, subjects_q7, subjects_q15, subjects_scaffold, subjects_test, subjects_summary, x)\n\n:::\nSummarize Cummulative Progress versions before functionize\n\n\nCODE\n# #SUMMARIZE-CUMULATIVE ABSOLUTE PROGRESS\n# \n# \n# #filter for valid items\n# x <- df_items %>% filter(q %nin% c(6,9)) %>% dplyr::select(subject,mode, pretty_condition, q,score_niceABS) \n# \n# #pivot wider\n# wide <- x %>% pivot_wider(names_from=q, names_glue = \"q_{q}\", values_from = score_niceABS)\n# \n# #calc stepwise cumulative score\n# wide$c1 = wide$q_1\n# wide$c2 = wide$c1 + wide$q_2\n# wide$c3 = wide$c2 + wide$q_3\n# wide$c4 = wide$c3 + wide$q_4\n# wide$c5 = wide$c4 + wide$q_5\n# wide$c6 = wide$c5 + wide$q_7\n# wide$c7 = wide$c6 + wide$q_8\n# wide$c8 = wide$c7 + wide$q_10\n# wide$c9 = wide$c8 + wide$q_11\n# wide$c10 = wide$c9 + wide$q_12\n# wide$c11 = wide$c10 + wide$q_13\n# wide$c12 = wide$c11 + wide$q_14\n# wide$c13 = wide$c12 + wide$q_15\n# wide <- wide %>% dplyr::select(subject,mode, pretty_condition,c1,c2,c3,c4,c5,c6, c7,c8,c9, c10,c11,c12,c13)\n# \n# #lengthen \n# df_absolute_progress <- wide %>% pivot_longer(cols= c1:c13, names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n# df_absolute_progress$question <- as.integer(df_absolute_progress$question)\n# \n# \n# #cleanup \n# rm(x,wide)\n#   \n# # SUMMARIZE-CUMULATIVE SCALED PROGRESS\n# \n# #filter for valid items\n# x <- df_items %>% filter(q %nin% c(6,9)) %>% select(subject,mode, pretty_condition, q,score_SCALED)\n# \n# #pivot wider\n# wide <- x %>% pivot_wider(names_from=q, names_glue = \"q_{q}\", values_from = score_SCALED)\n# \n# #calc stepwise cumulative score\n# wide$c1 = wide$q_1\n# wide$c2 = wide$c1 + wide$q_2\n# wide$c3 = wide$c2 + wide$q_3\n# wide$c4 = wide$c3 + wide$q_4\n# wide$c5 = wide$c4 + wide$q_5\n# wide$c6 = wide$c5 + wide$q_7\n# wide$c7 = wide$c6 + wide$q_8\n# wide$c8 = wide$c7 + wide$q_10\n# wide$c9 = wide$c8 + wide$q_11\n# wide$c10 = wide$c9 + wide$q_12\n# wide$c11 = wide$c10 + wide$q_13\n# wide$c12 = wide$c11 + wide$q_14\n# wide$c13 = wide$c12 + wide$q_15\n# wide <- wide %>% select(subject,mode, pretty_condition,c1,c2,c3,c4,c5,c6, c7,c8,c9, c10,c11,c12,c13)\n# \n# #lengthen \n# df_scaled_progress <- wide %>% pivot_longer(cols= c1:c13, names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n# df_scaled_progress$question <- as.integer(df_scaled_progress$question)\n# \n# #cleanup \n# rm(x,wide)"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html",
    "href": "analysis/SGC3A/3_sgc3A_description.html",
    "title": "5  Description",
    "section": "",
    "text": "The purpose of this notebook is describe the distributions of dependent variables for Study SGC3A."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#sample",
    "href": "analysis/SGC3A/3_sgc3A_description.html#sample",
    "title": "5  Description",
    "section": "SAMPLE",
    "text": "SAMPLE\n\nData Collection\nData was initially collected (in person, SONA groups in computer lab) in Fall 2017. In Spring 2018, additional data were collected after small modifications were made to the experimental platform to increase the size of multiple-choice input buttons, and to add an additional free-response question following the main task block. In Fall 2021, the study was replicated using asynchronous, online SONA pool, with additional participants collected in Winter 2022.\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_subjects$pretty_mode, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    laboratory \n    62 \n    64 \n    126 \n  \n  \n    online-replication \n    96 \n    108 \n    204 \n  \n  \n    Sum \n    158 \n    172 \n    330 \n  \n\n\n\n\n\n\n\nParticipants\n\n\nCODE\n#Describe participants\nsubject.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(age) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% filter(mode == \"asynch\") %>% dplyr::select(age) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(age) %>% unlist() %>% favstats()\n) \nsubject.stats$percent.female <- c(\n  (df_lab %>%  filter(gender==\"Female\") %>% count())$n/count(df_lab) %>% unlist(),\n  (df_online %>% filter(gender==\"Female\") %>% count())$n/count(df_online) %>% unlist(),\n  (df_subjects %>% filter(gender==\"Female\") %>% count())$n/count(df_subjects) %>% unlist()\n)\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()%>% \n  footnote(general = \"Age in Years\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    percent.female \n  \n \n\n  \n    lab \n    18 \n    19 \n    20 \n    21 \n    33 \n    20.4 \n    2.12 \n    126 \n    0 \n    0.619 \n  \n  \n    online \n    18 \n    20 \n    20 \n    21 \n    31 \n    20.6 \n    2.00 \n    204 \n    0 \n    0.672 \n  \n  \n    combined \n    18 \n    19 \n    20 \n    21 \n    33 \n    20.5 \n    2.05 \n    330 \n    0 \n    0.652 \n  \n\n\nNote:   Age in Years\n\n\n\n\nFor in-person collection, 126 participants (62 % female ) undergraduate STEM majors at a public American University participated in person in exchange for course credit (age: 18 - 33 years). Participants were randomly assigned to one of two experimental groups.\nFor online replication 204 participants (67 % female ) undergraduate STEM majors at a public American University participated online, asynchronously in exchange for course credit (age: 18 - 31 years). Participants were randomly assigned to one of two experimental groups.\nCombined overall 330 participants (65 % female ) undergraduate STEM majors at a public American University participated in exchange for course credit (age: 18 - 33 years)."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#response-accuracy",
    "href": "analysis/SGC3A/3_sgc3A_description.html#response-accuracy",
    "title": "5  Description",
    "section": "RESPONSE ACCURACY",
    "text": "RESPONSE ACCURACY\n\nSubject Level Scores\nSubject level scores summarize the the response accuracy by a particular participant across all discriminant items in the graph comprehension task.\n\nTest Phase Absolute Score\nRecall from Section 3.1.2.1 that the absolute score (following the dichotomous scoring approach) s_NABS indicates if the subject’s response for a particular item was perfectly correct: whether they selected all correct answer options and no others (excluding certain allowed exceptions, such as also selecting the data point referenced in the question). The absolute score for an individual item is either 0 or 1. When summarized across the entire set of discriminant items, the total absolute score for an individual subject ranges from [0,13]. When summarized across just the test phase (final items following scaffolding phase) scores for an individual subject range from [0,8]. First we examine performance on the test phase (final 8 questions, appears after scaffolding phase). This tells us how the participants perform after exposure to the 5 scaffolding questions (in the impasse condition).\n\n\nCODE\ntitle = \"Descriptive Statistics of TEST PHASE Response Accuracy (Total Absolute Score)\"\nabs.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_test_NABS) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(item_test_NABS) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(item_test_NABS) %>% unlist() %>% favstats()\n) \nabs.stats %>% kbl (caption = title) %>% kable_classic() %>% \n  footnote(general = \"# questions correct [0,8]\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of TEST PHASE Response Accuracy (Total Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    0 \n    0 \n    0 \n    6 \n    8 \n    2.53 \n    3.32 \n    126 \n    0 \n  \n  \n    online \n    0 \n    0 \n    0 \n    6 \n    8 \n    2.16 \n    3.19 \n    204 \n    0 \n  \n  \n    combined \n    0 \n    0 \n    0 \n    6 \n    8 \n    2.30 \n    3.24 \n    330 \n    0 \n  \n\n\nNote:   # questions correct [0,8]\n\n\n\n\nFor in person collection, total absolute scores in the TEST phase (n = 126) range from 0 to 8 with a mean score of (M = 2.53, SD = 3.32).\nFor online replication, (online) total absolute scores in the TEST phase (n = 204) range from 0 to 8 with a slightly lower mean score of (M = 2.16, SD = 3.19).\nWhen combined overall, total absolute accuracy scores in the TEST phase (n = 330) range from 0 to 8 with a slightly lower mean score of (M = 2.3, SD = 3.24).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL ABSOLUTE\n  gf_props(~item_test_NABS, data = df_subjects) + \n  labs(x = \"number of correct responses (test phase)\",\n       y = \"% of subjects\",\n       title = \"Distribution of TEST Absolute Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_test_NABS\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of TEST Absolute Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"Total Absolute Score (Test Phase)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_test_NABS,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_NABS),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_NABS, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbetweenstats(y = item_test_NABS, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(item_test_NABS)) + \n  stat_ecdf(geom = \"step\") +\n  facet_grid(pretty_condition~pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — TEST Absolute Score \",\n        x = \"Total Absolute Score (Test Phase) [0,8]\", \n        y = \"Cumulative Probability\")\n\n\n\n\n\nCODE\n#NOTE this is clobbered by the shift function imports; so I load those later\n\n\nVisual inspection of this distribution suggests it is not normal, and likely bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018). TODO REFERENCE\n\n\nCODE\nmultimode::modetest(df_subjects$item_test_NABS)\n\n\nWarning in multimode::modetest(df_subjects$item_test_NABS): A modification of\nthe data was made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$item_test_NABS\nExcess mass = 0.1, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$item_test_NABS, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$item_test_NABS,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$item_test_NABS, mod0 = n_modes, : If\nthe density function has an unbounded support, artificial modes may have been\ncreated in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is infact multimodal (m = 0.1, p < 0.001), with two identifiable modes at 0.013 and 7.894, and an antimode at 2.867.\n\n\n\n\n\n\nNote\n\n\n\nCondition appears (through visual inspection) to yield a positive influence on Total Absolute Score in the TEST Phase, across data collection modalities.\n\n\n\n\nTest Phase Scaled Scores\nThe test phase scaled score s_SCALED summarizes the scaled score on the 8 strategy-discriminant questions in the test phase, for each subject. This score ranges from from -8 (all orthogonal) to 8 (all triangular). Recall that the s_SCALED score for an item is a numeric representation of the strategy-consistent response, scaled from -1 to +1 (see Section 4.1.4)\nMost importantly, the Scaled score gives us a way of quantitatively examining how correctly a participant interpreted the coordinate system across the entire set of items. It offers a more nuanced look into performance than absolute score.\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total Scaled Score)\"\nscaled.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_test_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(item_test_SCALED) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(item_test_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -8 \n    -8.0 \n    -6.00 \n    6 \n    8 \n    -2.11 \n    6.69 \n    126 \n    0 \n  \n  \n    online \n    -8 \n    -7.5 \n    -5.75 \n    5 \n    8 \n    -2.32 \n    6.29 \n    204 \n    0 \n  \n  \n    combined \n    -8 \n    -8.0 \n    -6.00 \n    6 \n    8 \n    -2.24 \n    6.43 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, TEST phase scaled scores (n = 126) range from -8 to 8 with a mean score of (M = -2.11, SD = 6.69).\nFor online replication, TEST phase scaled scores (n = 204) range from -8 to 8 with a slightly lower mean score of (M = -2.32, SD = 6.29).\nWhen combined overall, TEST phase scaled scores (n = 330) range from -8 to 8 with a slightly lower mean score of (M = -2.24, SD = 6.44).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL SCALED\ngf_props(~item_test_SCALED, data = df_subjects) +\n  labs(x = \"total scaled score (test phase)\",\n       y = \"% of subjects\",\n       title = \"Distribution of TEST Scaled Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_test_SCALED\",binwidth=1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) + \n  labs( title = \"Distribution of TEST Scaled Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"total scaled score (test phase)\", y = \"number of participants\") + \n theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_test_SCALED,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_SCALED),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_SCALED, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Scaled Score \",\n    x = \"Condition\", y = \"Total Scaled Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n#EASY STATS\nggbetweenstats(y = item_test_SCALED, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(item_test_SCALED)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Test Phase Scaled Score\",\n        x = \"Test Phase Scaled Score [-8,8]\", \n        y = \"Cumulative Probability\") \n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and perhaps perhaps bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018).\n\n\nCODE\nmultimode::modetest(df_subjects$item_test_SCALED)\n\n\nWarning in multimode::modetest(df_subjects$item_test_SCALED): A modification of\nthe data was made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$item_test_SCALED\nExcess mass = 0.2, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$item_test_SCALED, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$item_test_SCALED,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$item_test_SCALED, mod0 = n_modes, :\nIf the density function has an unbounded support, artificial modes may have been\ncreated in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is in fact multimodal (m = 0.1, p < 0.001), with two identifiable modes at -7.721 and 7.822, and an antimode at 1.93.\n\n\n\nFirst Item Scores\nNext we consider the response accuracy on just the first question of the graph comprehension task: a subject’s first exposure to the TM graph.\n\nFirst Item Absolute Score\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Lab)\"\nitem.contingency <- df_lab %>%  dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>%  addmargins(2) %>% prop.table(margin=2) %>% addmargins(1)\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Lab)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    0 \n    0.839 \n    0.703 \n    0.77 \n  \n  \n    1 \n    0.161 \n    0.297 \n    0.23 \n  \n  \n    Sum \n    1.000 \n    1.000 \n    1.00 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Online)\"\nitem.contingency <- df_online %>%  dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>%  addmargins(2) %>% prop.table(margin=2) %>% addmargins(1)\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Online)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    0 \n    0.875 \n    0.722 \n    0.794 \n  \n  \n    1 \n    0.125 \n    0.278 \n    0.206 \n  \n  \n    Sum \n    1.000 \n    1.000 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Combined)\"\nitem.contingency <- df_subjects %>%  dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>%  addmargins(2) %>% prop.table(margin=2) %>% addmargins(1)\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Combined)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    0 \n    0.861 \n    0.715 \n    0.785 \n  \n  \n    1 \n    0.139 \n    0.285 \n    0.215 \n  \n  \n    Sum \n    1.000 \n    1.000 \n    1.000 \n  \n\n\n\n\n\nAcross data collection sessions, first-item accuracy is consistent across experimental conditions. Incorrect answers are far more frequent (78%) than correct answers (22%). Accuracy is somewhat improved in the IMPASSE condition, with roughly 28% of all IMPASSE-condition questions answered correctly, compared to only 14% in the CONTROL condition.\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_props(~item_q1_NABS, data = df_subjects) +\n  labs(x = \"response accuracy\",\n       y = \"% subjects\",\n       title = \"Proportion of Correct Responses on First Item\",\n       subtitle=\"\")+\n  theme(legend.position = \"none\")+theme_ggdist()\n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_subjects %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(item_q1_NABS))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n  labs(x = \"response accuracy\",\n       title = \"Proportion of Correct Responses on First Item (by Modality and Condition)\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Proportion of Correct Responses on First Item\",\n            data = df_subjects, item_q1_NABS ~ pretty_condition, rot_labels=c(0,90,0,0), \n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\"))) \n\n\n\n\n\nCODE\n#STATSPLOT\nggbarstats(\n  x = item_q1_NABS,\n  y = pretty_condition, \n  data = df_subjects\n)\n\n\n\n\n\n\n\nFirst Item Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1. (note: we evaluate scaled_score on the first item rather than interpretation, because no orthogonal interpretation is available in the impasse condition)\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (First Item Scaled Score)\"\nfirstscaled.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats()\n) \nfirstscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (First Item Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -1 \n    -1 \n    -1 \n    0.5 \n    1 \n    -0.298 \n    0.849 \n    126 \n    0 \n  \n  \n    online \n    -1 \n    -1 \n    -1 \n    0.5 \n    1 \n    -0.287 \n    0.812 \n    204 \n    0 \n  \n  \n    combined \n    -1 \n    -1 \n    -1 \n    0.5 \n    1 \n    -0.291 \n    0.825 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, first item scaled scores (n = 126) range from -1 to 1 with a mean score of (M = -0.3, SD = 0.85).\nFor online replication, (online) first item scaled scores (n = 204) range from -1 to 1 with a slightly lower mean score of (M = -0.29, SD = 0.81).\nWhen combined overall, first item scaled scores (n = 330) range from -1 to 1 with a slightly lower mean score of (M = -0.29, SD = 0.83).\n\n\nCODE\n#GGFORMULA | PROPORTIONAL HISTOGRAM SUBJECT FIRST SCALED\ngf_props(~item_q1_SCALED, data = df_subjects) +\n  labs(x = \"scaled score (first item)\",\n       y = \"% of subjects\",\n       title = \"Distribution of First Item Scaled Score\",\n       subtitle = \"\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_q1_SCALED\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) + \n  labs( title = \"Distribution of First Item Scaled Score (by Mode and Condition)\",\n        subtitle =\"Impasse condition yields more intermediate scores (indicating uncertainty)\",\n        x = \"scaled score (firt item) \", y = \"number of participants\") + \n  theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_subjects %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(item_q1_SCALED))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n  labs(x = \"response accuracy\",\n       title = \"Type of Responses on First Item (by Modality and Condition)\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbarstats(\n  x = item_q1_SCALED,\n  y = pretty_condition, \n  data = df_subjects\n)\n\n\n\n\n\n\n\n\nInterpretation Scores\nNext we consider the the interpretations assigned to each response. For each response given by a participant to a question, we assign an interpretation label based on the interpretation the response most closely matches (see Section 3.2.3).\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Lab)\"\nitem.contingency <- df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(interpretation, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Lab)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.297 \n    0.116 \n    0.414 \n  \n  \n    Satisfice \n    0.000 \n    0.028 \n    0.028 \n  \n  \n    frenzy \n    0.002 \n    0.005 \n    0.007 \n  \n  \n    ? \n    0.026 \n    0.053 \n    0.079 \n  \n  \n    reference \n    0.001 \n    0.004 \n    0.005 \n  \n  \n    blank \n    0.008 \n    0.034 \n    0.042 \n  \n  \n    both tri + orth \n    0.060 \n    0.056 \n    0.116 \n  \n  \n    Tversky \n    0.004 \n    0.017 \n    0.021 \n  \n  \n    Triangular \n    0.094 \n    0.195 \n    0.288 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Online)\"\nitem.contingency <- df_items %>% filter(mode == \"asynch\") %>% dplyr::select(interpretation, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Online)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.260 \n    0.122 \n    0.382 \n  \n  \n    Satisfice \n    0.000 \n    0.024 \n    0.024 \n  \n  \n    frenzy \n    0.002 \n    0.001 \n    0.003 \n  \n  \n    ? \n    0.050 \n    0.066 \n    0.116 \n  \n  \n    reference \n    0.000 \n    0.002 \n    0.002 \n  \n  \n    blank \n    0.013 \n    0.055 \n    0.068 \n  \n  \n    both tri + orth \n    0.056 \n    0.061 \n    0.117 \n  \n  \n    Tversky \n    0.011 \n    0.023 \n    0.035 \n  \n  \n    Triangular \n    0.078 \n    0.175 \n    0.253 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Combined)\"\nitem.contingency <- df_items %>%  dplyr::select(interpretation, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Combined)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.274 \n    0.120 \n    0.394 \n  \n  \n    Satisfice \n    0.000 \n    0.025 \n    0.025 \n  \n  \n    frenzy \n    0.002 \n    0.003 \n    0.004 \n  \n  \n    ? \n    0.041 \n    0.061 \n    0.102 \n  \n  \n    reference \n    0.001 \n    0.002 \n    0.003 \n  \n  \n    blank \n    0.011 \n    0.047 \n    0.058 \n  \n  \n    both tri + orth \n    0.058 \n    0.059 \n    0.117 \n  \n  \n    Tversky \n    0.009 \n    0.021 \n    0.029 \n  \n  \n    Triangular \n    0.084 \n    0.183 \n    0.267 \n  \n  \n    Sum \n    0.479 \n    0.521 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_propsh(~interpretation, data = df_items, fill = ~pretty_condition) %>% \n  gf_facet_grid(pretty_condition~pretty_mode) +\n  labs(x = \"% of items\",\n       title = \"Proportion of Interpretations Across Items\",\n       subtitle=\"Impasse Condition yields shift from Orthogonal to alternative interpretations\")+\n  theme(legend.position = \"none\")\n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_items %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(interpretation))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n  labs(x = \"response accuracy\",\n       title = \"Response Types on All Items (by Modality and Condition)\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Proportion of Interpretations across Conditions\",\n            data = df_items, pretty_condition ~ interpretation, rot_labels=c(0,90,0,0),\n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\")))\n\n\n\n\n\nCODE\n#STATSPLOT\nggbarstats(\n  x = high_interpretation,\n  y = pretty_condition, \n  data = df_items\n)\n\n\n\n\n\n\n\nCumulative Task Performance\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#response-latency",
    "href": "analysis/SGC3A/3_sgc3A_description.html#response-latency",
    "title": "5  Description",
    "section": "RESPONSE LATENCY",
    "text": "RESPONSE LATENCY\n\nTime on First Item\nHere we consider the time spent on just the first individual item (first exposure to graph).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_lab%>% dplyr::select(item_q1_rt) %>% unlist()  %>%  favstats(),\n  \"online\"= df_online %>% dplyr::select(item_q1_rt) %>% unlist() %>% favstats(),\n  \"combined\"= df_subjects %>% dplyr::select(item_q1_rt) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of First Response Time (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of First Response Time (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    7.22 \n    26.6 \n    39.3 \n    52.2 \n    161 \n    44.5 \n    26.2 \n    126 \n    0 \n  \n  \n    online \n    4.84 \n    19.9 \n    31.0 \n    48.9 \n    306 \n    43.3 \n    41.3 \n    204 \n    0 \n  \n  \n    combined \n    4.84 \n    22.3 \n    34.0 \n    50.7 \n    306 \n    43.8 \n    36.2 \n    330 \n    0 \n  \n\n\n\n\n\nResponse time on the first item for in person subjects (n = 126) ranged from 7.22 to 161.36 minutes with a mean duration of (M = 44.53, SD = 26.22).\nResponse time on the first item for online replication subjects (n = 204) ranged from 4.84 to 305.94 minutes with a mean duration of (M = 43.32, SD = 41.27).\nResponse time on the first item for combined subjects (n = 330) ranged from 4.84 to 305.94 minutes with a mean duration of (M = 43.78, SD = 36.23).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_q1_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of First Item Response Time (seconds)\", subtitle = \"fit by gamma distribution\", x = \"First Item Response Time (seconds)\", y = \"% items\")\n\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_q1_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"First Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#recode as boolean correct\ndf_subjects <- df_subjects %>% mutate(\n  item_q1_NABS = as.logical(item_q1_NABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_q1_rt, color = item_q1_NABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .1\n  )) + \n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"First Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\n\n\nCODE\n# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n\n\nTime on Item\nHere we consider the time spent on an individual item (across all items).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(rt_s) %>% unlist()  %>%  favstats(),\n  \"online\"= df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(rt_s) %>% unlist() %>% favstats(),\n  \"combined\"= df_items %>%   dplyr::select(rt_s) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of Item Response Latency (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Item Response Latency (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    1.264 \n    13.8 \n    24.9 \n    46.1 \n    336 \n    35.5 \n    33.1 \n    1890 \n    0 \n  \n  \n    online \n    1.264 \n    13.8 \n    24.9 \n    46.1 \n    336 \n    35.5 \n    33.1 \n    1890 \n    0 \n  \n  \n    combined \n    0.003 \n    12.5 \n    23.7 \n    43.9 \n    532 \n    35.2 \n    37.2 \n    4950 \n    0 \n  \n\n\n\n\n\nTime on an individual item for in person subjects (n = 1890) ranged from 1.26 to 336.03 minutes with a mean duration of (M = 35.47, SD = 33.12).\nTime on an individual item for online replication subjects (n = 1890) ranged from 1.26 to 336.03 minutes with a mean duration of (M = 35.47, SD = 33.12).\nTime on an individual item for combined subjects (n = 4950) ranged from 0 to 531.52 minutes with a mean duration of (M = 35.24, SD = 37.21).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~rt_s, data = df_items) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of Item Response Time (seconds)\", \n       subtitle = \"fit by gamma distribution\", x = \"Item Response Time (seconds)\", y = \"% items\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_items, x = \"rt_s\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#recode as boolean correct\ndf_items <- df_items %>% mutate(\n  score_niceABS = as.logical(score_niceABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_items, aes(x = pretty_condition, y = rt_s, color = score_niceABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    # position = position_dodgejust(),\n    justification = 1.5, \n    # adjust = .5, \n    width = .5, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA,\n    position = position_dodge2()\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitterdodge(\n      # seed = 1,\n      dodge.width = 0.5,\n      jitter.width = 0.075\n  )) +\n  labs( title = \"Distribution of Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\n\n\nCODE\n# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n\n\nTime on SCAFFOLD Phase\nHere we consider just the time spent on the first five items of the task (the scaffold phase).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_scaffold_rt) %>% unlist()  %>%  favstats(),\n  \"online\"= df_online %>% dplyr::select(item_scaffold_rt) %>% unlist() %>% favstats(),\n  \"combined\"= df_subjects %>% dplyr::select(item_scaffold_rt) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of SCAFFOLD Phase Response Latency (minutes)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of SCAFFOLD Phase Response Latency (minutes)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    1.235 \n    2.66 \n    3.71 \n    4.92 \n    11.1 \n    4.03 \n    1.88 \n    126 \n    0 \n  \n  \n    online \n    0.614 \n    2.10 \n    2.92 \n    4.31 \n    15.4 \n    3.52 \n    2.26 \n    204 \n    0 \n  \n  \n    combined \n    0.614 \n    2.29 \n    3.25 \n    4.58 \n    15.4 \n    3.72 \n    2.13 \n    330 \n    0 \n  \n\n\n\n\n\nTotal time on SCAFFOLD phase for in person subjects (n = 126) ranged from 1.24 to 11.1 minutes with a mean duration of (M = 4.03, SD = 1.88).\nTotal time on SCAFFOLD phase for online replication subjects (n = 204) ranged from 0.61 to 15.39 minutes with a mean duration of (M = 3.52, SD = 2.26).\nTotal time on SCAFFOLD phase for combined subjects (n = 330) ranged from 0.61 to 15.39 minutes with a mean duration of (M = 3.72, SD = 2.13).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_scaffold_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of SCAFFOLD Phase Response Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Scaffold Phase Time (minutes)\", y = \"% subjects\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_scaffold_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of SCAFFOLD Phase Response Time (minutes)\",\n        subtitle =\"\",\n        x = \"Scaffold Phase Time (minutes)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_scaffold_rt, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_scaffold_rt),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_scaffold_rt, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  ))+ labs( title = \"Distribution of SCAFFOLD Phase Response Time (minutes)\",\n        subtitle =\"\",\n        y = \"Total Study Time (minutes)\", x = \"Condition\") +\n  theme_ggdist() + theme(legend.position = \"blank\") #+\n\n\n\n\n\nCODE\n  # coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\nTime on TEST Phase\nHere we consider just the time spent on the remaining eight discriminant items of the task (the test phase).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_test_rt) %>% unlist()  %>%  favstats(),\n  \"online\"= df_online %>% dplyr::select(item_test_rt) %>% unlist() %>% favstats(),\n  \"combined\"= df_subjects %>% dplyr::select(item_test_rt) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of TEST Phase Response Latency (minutes)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of TEST Phase Response Latency (minutes)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    1.022 \n    2.97 \n    3.75 \n    4.76 \n    10.8 \n    4.00 \n    1.37 \n    126 \n    0 \n  \n  \n    online \n    0.703 \n    3.10 \n    3.89 \n    5.17 \n    13.5 \n    4.41 \n    2.24 \n    204 \n    0 \n  \n  \n    combined \n    0.703 \n    3.03 \n    3.80 \n    4.99 \n    13.5 \n    4.26 \n    1.96 \n    330 \n    0 \n  \n\n\n\n\n\nTotal time on TEST phase for in person subjects (n = 126) ranged from 1.02 to 10.85 minutes with a mean duration of (M = 4, SD = 1.37).\nTotal time on TEST phase for online replication subjects (n = 204) ranged from 0.7 to 13.49 minutes with a mean duration of (M = 4.41, SD = 2.24).\nTotal time on TEST phase for combined subjects (n = 330) ranged from 0.7 to 13.49 minutes with a mean duration of (M = 4.26, SD = 1.96).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_test_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of TEST Phase Response Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Test Phase Time (minutes)\", y = \"% subjects\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_test_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of TEST Phase Response Time (minutes)\",\n        subtitle =\"\",\n        x = \"Test Phase Time (minutes)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_test_rt, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_rt),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_rt, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  ))+ \n  labs( title = \"Distribution of TEST Phase Response Time (minutes)\",\n        subtitle =\"\",\n        y = \"Total Study Time (minutes)\", x = \"Condition\") +\n  theme_ggdist() + theme(legend.position = \"blank\") #+\n\n\n\n\n\nCODE\n  # coord_cartesian(xlim = c(0.5, NA), clip = \"off\")"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#exploring-relationships",
    "href": "analysis/SGC3A/3_sgc3A_description.html#exploring-relationships",
    "title": "5  Description",
    "section": "EXPLORING RELATIONSHIPS",
    "text": "EXPLORING RELATIONSHIPS\n\nACCURACY (VS) LATENCY\n\nTotal Task\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ rt_m, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by TOTAL Item Response Time\",\n    subtitle = \"\", \n    x = \"Total Item Response Time (minutes)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_avg_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MEAN Item Response Time\",\n    subtitle = \"\", \n    x = \"Average Item Response Time (seconds)\", y = \"Total Scaled Score\"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_max_rt, data = df_subjects %>% filter(item_max_rt < 400), alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MAX Item Response Time\",\n    subtitle = \"\", \n    x = \"MAX Item Response Time (s)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#NOTE: LOG transforms of the RT do not yield linear relationships\n\n\n\n\nPhase Specific\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_scaffold_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"TOTAL Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( item_scaffold_NABS ~ item_scaffold_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"SCAFFOLD (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"SCAFFOLD Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( item_test_NABS ~ item_scaffold_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"TEST (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"TEST Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( item_test_NABS ~ item_avg_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"TEST (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"AVERAGE Item Response Time (minutes)\", y = \"TEST Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#NOTE: LOG transforms of the RT do not yield linear relationships\n\n\n\n\nAverage Item RT by Accuracy\n\n\nCODE\nq.stats <- df_items %>% filter(q != 6) %>% dplyr::group_by(q, pretty_condition, score_niceABS) %>% dplyr::summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score_niceABS)\n)\n\ngf_line( m ~ q, group = ~group,  color = ~as.factor(score_niceABS),data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~q) %>% \n  gf_facet_wrap(~pretty_condition) + scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Absolute Score\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\n# df_items %>%\n#   ggplot(aes(y = rt_s, x = q,  fill = pretty_condition)) +\n#   stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + facet_wrap(~pretty_condition)\n\n\n\n\nCODE\nq.stats <- df_items %>% filter(q %nin% c(6,9)) %>% dplyr::group_by(q, pretty_condition, interpretation) %>% dplyr::summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score_SCALED)\n)\n\ngf_line( m ~ as.factor(q), group = ~group,  color = ~interpretation,data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~as.factor(q)) %>% \n  gf_facet_grid(interpretation~pretty_condition) + #+ scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Interpretation\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       caption=\"NOTE: Points with no ribbon indicate singular response\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Interpretation\")\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\ndf_items %>% filter(q %nin% c(6,9)) %>% mutate( interpretation = recode(interpretation, \"reference\" = \"blank\", \"frenzy\" = \"?\")) %>% \n  ggplot(aes(y = rt_s, x = q,  fill = interpretation)) +\n  stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + \n  facet_grid(interpretation ~ pretty_condition) + \n  labs( title = \"Average Response Time by Question Interpretation\", x = \"Question\", y=\"Averate Item Response Time (s)\")"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#replication-check",
    "href": "analysis/SGC3A/3_sgc3A_description.html#replication-check",
    "title": "5  Description",
    "section": "REPLICATION CHECK",
    "text": "REPLICATION CHECK\n\nData Collection Mode on Absolute Score\nDoes Mode Change Effect of Condition on Score?\nTo verify that the data collected in the lab and remotely online are comparable, we perform a t-test on group means of ABSOLUTE SCORE for each condition, and examine whether data collection modality is a significant predictor of variance in absolute score\n\n\nCODE\npaste(\"Two Sample T-Test for S_ABS LAB vs. ONLINE control condition\")\n\n\n[1] \"Two Sample T-Test for S_ABS LAB vs. ONLINE control condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 111), s_ABS ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_ABS by mode\nt = 0.5, df = 120, p-value = 0.6\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -1.09  1.84\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                   2.68                    2.30 \n\n\nCODE\npaste(\"Two Sample T-Test for S_ABS LAB vs. ONLINE impasse condition\")\n\n\n[1] \"Two Sample T-Test for S_ABS LAB vs. ONLINE impasse condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 121), s_ABS ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_ABS by mode\nt = 1, df = 135, p-value = 0.3\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -0.727  2.435\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                   5.44                    4.58 \n\n\nCODE\npaste(\"OLS Linear Regression Predicting Absolute Score by Data Collection Mode\")\n\n\n[1] \"OLS Linear Regression Predicting Absolute Score by Data Collection Mode\"\n\n\nCODE\nsummary(lm(data = df_subjects, formula = s_ABS ~ mode ))\n\n\n\nCall:\nlm(formula = s_ABS ~ mode, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.08  -3.51  -2.51   4.49   9.49 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.08       0.44    9.27   <2e-16 ***\nmodeasynch     -0.57       0.56   -1.02     0.31    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.94 on 328 degrees of freedom\nMultiple R-squared:  0.00314,   Adjusted R-squared:  0.000105 \nF-statistic: 1.03 on 1 and 328 DF,  p-value: 0.31\n\n\nBoth t-tests are non-significant with 95% confidence intervals including 0. Further, an OLS linear regression model predicting cumulative absolute score indicates that data collection mode is not a significant predictor, explaining only 0.01% of variance in absolute score, F(1,328) = 1.03, p > 0.05.\n\n\n\n\n\n\nDecision\n\n\n\nIt is reasonable to infer that data from both in-person and remote studies arise from the same data generating process.\n\n\n\n\nData Collection Mode on Cumulative Score\nAre the by-condition group means significantly different by data collection modality?\nTo verify that the data collected in the lab and remotely online are comparable, we perform a t-test on group means of SCALED SCORE for each condition.\n\n\nCODE\npaste(\"Two Sample T-Test for s_SCALED LAB vs. ONLINE control condition\")\n\n\n[1] \"Two Sample T-Test for s_SCALED LAB vs. ONLINE control condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 111), s_SCALED ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by mode\nt = -0.1, df = 117, p-value = 0.9\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -3.15  2.83\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                  -6.52                   -6.36 \n\n\nCODE\npaste(\"Two Sample T-Test for s_SCALED LAB vs. ONLINE impasse condition\")\n\n\n[1] \"Two Sample T-Test for s_SCALED LAB vs. ONLINE impasse condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 121), s_SCALED ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by mode\nt = 0.5, df = 130, p-value = 0.6\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -2.08  3.49\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                  1.008                   0.306 \n\n\nCODE\npaste(\"OLS Linear Regression Predicting Scaled Score by Data Collection Mode\")\n\n\n[1] \"OLS Linear Regression Predicting Scaled Score by Data Collection Mode\"\n\n\nCODE\nsummary(lm(data = df_subjects, formula = s_SCALED ~ mode ))\n\n\n\nCall:\nlm(formula = s_SCALED ~ mode, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10.30  -7.80  -4.42  10.33  15.83 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   -2.698      0.853   -3.16   0.0017 **\nmodeasynch    -0.135      1.085   -0.12   0.9011   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.58 on 328 degrees of freedom\nMultiple R-squared:  4.71e-05,  Adjusted R-squared:  -0.003 \nF-statistic: 0.0155 on 1 and 328 DF,  p-value: 0.901\n\n\nBoth t-tests are non-significant with 95% confidence intervals including 0. Further, an OLS linear regression model predicting cumulative scaled score indicates that data collection mode is not a significant predictor, explaining less than 0.001% of variance in absolute score, F(1,328) = 0.0078, p > 0.05.\n\n\n\n\n\n\nDecision\n\n\n\nIt is reasonable to infer that data from both in-person and remote studies arise from the same data generating process."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#archive",
    "href": "analysis/SGC3A/3_sgc3A_description.html#archive",
    "title": "5  Description",
    "section": "ARCHIVE",
    "text": "ARCHIVE\nSample ridgeplot code\n\n\nCODE\n#RIDGEPLOT\n# ggplot(data = df_subjects, aes(x = s_NABS, y = mode)) +\n#   geom_density_ridges() + xlim(0,13)+\n#   facet_wrap(~condition, labeller = label_both) +\n# labs(x = \"total number correct \",\n# y = \"proportion of subjects\",\n#        title = \"Subject Cumulative Score (Absolute)\",\n#        subtitle = \"Score distributions are comparable across modalities and different across conditions\") +\n#   theme_minimal()\n\n\n\nWhat Kind of Distribution is Total Score?\nWhat kind of distribution is the Total Absolute Score (TEST Phase) data? We use the fitdistrplus package to compare the distribution of this variable to a variety of probability distribution families. First, we transform the # correct items to % correct items by dividing it by the total number of items (n = 8).\n\n\nCODE\n#describe the distribution\ndescdist(data = df_subjects$item_test_NABS/8, discrete = FALSE, boot = 1000)\n\n\n\n\n\nsummary statistics\n------\nmin:  0   max:  1 \nmedian:  0 \nmean:  0.287 \nestimated sd:  0.405 \nestimated skewness:  0.876 \nestimated kurtosis:  1.93 \n\n\nCODE\nprint(\"FIT A NORMAL DISTRIBUTION\")\n\n\n[1] \"FIT A NORMAL DISTRIBUTION\"\n\n\nCODE\nnormal_ = fitdist(df_subjects$item_test_NABS/8,\"norm\")\nplot(normal_)\n\n\n\n\n\nCODE\nprint(\"FIT A BETA DISTRIBUTION\")\n\n\n[1] \"FIT A BETA DISTRIBUTION\"\n\n\nCODE\nbeta_ = fitdist(df_subjects$item_test_NABS/8,\"beta\", method=\"mme\" )\nplot(beta_)\n\n\n\n\n\nCODE\nsummary(beta_)\n\n\nFitting of the distribution ' beta ' by matching moments \nParameters : \n       estimate\nshape1   0.0721\nshape2   0.1786\nLoglikelihood:  Inf   AIC:  -Inf   BIC:  -Inf \n\n\nInterpreting the Cullen and Frey graph, it appears that number percentage of correct responses per subject may follow a beta distribution (u-shape tpe). If we fit this variable using both a normal and beta distribution (using method of moments), it appears that the beta distribution provides a much better fit. The parameter estimates for the beta distribution are: shape1 = 0.072, shape2 = 0.179. The beta distribution is a flexible distribution insofar as it can model a wide range of shapes with its two parameters. TODO: HOW might this be applied to the total score data?\nAnalysis Notes - This distribution is very bimodal, so OLS linear regression estimating means may not be informative, as the mean actually falls near the location of the anitmode (least common value) - Should investigate log transform to see if residuals of LM will be normal (no) - Should investigate beta regression\n\n\nWhole Task Scores\n\nAbsolute Score\nTotal Scores that include both Scaffolding Phase as well as Test Phase performance.\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total Absolute Score)\"\nabs.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(s_NABS) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(s_NABS) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(s_NABS) %>% unlist() %>% favstats()\n) \nabs.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    0 \n    0 \n    1 \n    9.00 \n    13 \n    4.11 \n    5.09 \n    126 \n    0 \n  \n  \n    online \n    0 \n    0 \n    1 \n    8.00 \n    13 \n    3.52 \n    4.89 \n    204 \n    0 \n  \n  \n    combined \n    0 \n    0 \n    1 \n    8.75 \n    13 \n    3.75 \n    4.97 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, total absolute scores (n = 126) range from 0 to 13 with a mean score of (M = 4.11, SD = 5.09).\nFor online replication, (online) total absolute accuracy scores (n = 204) range from 0 to 13 with a slightly lower mean score of (M = 3.52, SD = 4.89).\nWhen combined overall, total absolute accuracy scores (n = 330) range from 0 to 13 with a slightly lower mean score of (M = 3.75, SD = 4.97).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL ABSOLUTE\n  gf_props(~s_NABS, data = df_subjects) + \n  labs(x = \"number of correct responses\",\n       y = \"% of subjects\",\n       title = \"Distribution of Task Absolute Score\",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") + \n  theme_minimal()\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"s_NABS\", binwidth = 1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of Task Absolute Score (by Mode and Condition)\",\n        subtitle =\"Pattern of response is the same across data collection modes but differs by condition\",\n        x = \"Total Absolute Score\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = s_NABS, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    adjust = .5, \n    width = .6, \n    .width = 0, \n    justification = -.3, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter(\n      seed = 1, width = .1\n    )\n  ) + labs(\n    title = \"Distribution of Task Absolute Score\",\n    x = \"Condition\", y = \"Total Absolute Score\"\n  ) + theme_ggdist() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(s_NABS)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Task Absolute Score\",\n        x = \"Task Absolute Score [0,13]\", \n        y = \"Cumulative Probability\") + theme_minimal()\n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and likely bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018). TODO REFERENCE\n\n\nCODE\nmultimode::modetest(df_subjects$s_NABS)\n\n\nWarning in multimode::modetest(df_subjects$s_NABS): A modification of the data\nwas made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$s_NABS\nExcess mass = 0.1, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$s_NABS, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$s_NABS,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$s_NABS, mod0 = n_modes, display =\nTRUE): If the density function has an unbounded support, artificial modes may\nhave been created in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is infact multimodal (m = 0.1, p < 0.001), with two identifiable modes at 0.26 and 12.261, and an antimode at 6.985.\n\n\n\n\n\n\nNote\n\n\n\nCondition appears (through visual inspection) to yield a positive influence on Total Absolute Score (across the entire task), across data collection modalities.\n\n\n\n\nScaled Score\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total Scaled Score)\"\nscaled.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -13 \n    -12.0 \n    -7.50 \n    8.75 \n    13 \n    -2.70 \n    10.08 \n    126 \n    0 \n  \n  \n    online \n    -13 \n    -10.0 \n    -7.00 \n    6.62 \n    13 \n    -2.83 \n    9.26 \n    204 \n    0 \n  \n  \n    combined \n    -13 \n    -10.5 \n    -7.25 \n    7.50 \n    13 \n    -2.78 \n    9.56 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, total scaled scores (n = 126) range from -13 to 13 with a mean score of (M = -2.7, SD = 10.08).\nFor online replication, total scaled scores (n = 204) range from -13 to 13 with a slightly lower mean score of (M = -2.83, SD = 9.26).\nWhen combined overall, total scaled scores (n = 330) range from -13 to 13 with a slightly lower mean score of (M = -2.78, SD = 9.56).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL SCALED\ngf_props(~s_SCALED, data = df_subjects) +\n  labs(x = \"total scaled score\",\n       y = \"% of subjects\",\n       title = \"Distribution of Total Scaled Score\",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") + \n  theme_minimal()\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"s_SCALED\",binwidth=1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) + \n  labs( title = \"Distribution of Total Scaled Score (by Condition and Mode)\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"total scaled score\", y = \"number of participants\") + \n  theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = s_SCALED, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    adjust = .5, \n    width = .6, \n    .width = 0, \n    justification = -.3, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter(\n      seed = 1, width = .1\n    )\n  ) + labs(\n    title = \"Distribution of Task Scaled Score \",\n    x = \"Condition\", y = \"Total Scaled Score\"\n  ) + theme_ggdist() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(s_SCALED)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Task Scaled Score\",\n        x = \"Task Scaled Score [-13, 13]\", \n        y = \"Cumulative Probability\") + theme_minimal()\n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and perhaps perhaps bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018).\n\n\nCODE\nmultimode::modetest(df_subjects$s_SCALED)\n\n\nWarning in multimode::modetest(df_subjects$s_SCALED): A modification of the data\nwas made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$s_SCALED\nExcess mass = 0.1, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$s_SCALED, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$s_SCALED,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$s_SCALED, mod0 = n_modes, display =\nTRUE): If the density function has an unbounded support, artificial modes may\nhave been created in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is in fact multimodal (m = 0.1, p < 0.001), with two identifiable modes at -11.195 and 12.103, and an antimode at 2.942.\nAnalysis Notes - As with absolute score, the distribution of scaled score is very bimodal - Same need to investigate transformations and alternative distributions for regression\n\n\n\n\n\n\nNote\n\n\n\nCondition appears (through visual inspection) to yield a positive influence on Total Scaled Score across data collection modalities.\n\n\n\n\n\nItem Level Scores\n\nItem Absolute Score\nWhole Task Accuracy summarized over items rather than subjects\n\n\nCODE\nx <- df_items %>% mutate(score = as.logical(score_ABS))\n\ntitle = \"Proportion of Correct Items By Condition (Lab)\"\n\nitem.contingency <- df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(score_ABS, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Items By Condition (Lab)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    0 \n    0.344 \n    0.268 \n    0.613 \n  \n  \n    1 \n    0.148 \n    0.240 \n    0.387 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Items By Condition (Online)\"\nitem.contingency <- df_items %>% filter(mode == \"asynch\") %>% dplyr::select(score_ABS, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Items By Condition (Online)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    0 \n    0.342 \n    0.307 \n    0.649 \n  \n  \n    1 \n    0.128 \n    0.223 \n    0.351 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM\nstats = df_items %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(score_niceABS))\ngf_props(~score_niceABS, data = df_items) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) +\n  labs(x = \"Item Absolute Score\",\n       title = \"Item Absolute Score\",\n       subtitle=\"Across modalities, the impasse condition yielded more correct responses\")+\n  theme_minimal()\n\n\n\n\n\n\n\nItem Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1.\n\n\nCODE\ntitle = \"Descriptive Statistics of Item Response Accuracy (Scaled Score)\"\nscaled.stats.items <- rbind(\n  \"lab\"= df_items %>% filter(mode == 'lab-synch') %>% dplyr::select(score_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_items %>% filter(mode == \"asynch\") %>% dplyr::select(score_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats.items %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Item Response Accuracy (Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -1 \n    -1 \n    -0.5 \n    1 \n    1 \n    -0.128 \n    0.877 \n    1890 \n    0 \n  \n  \n    online \n    -1 \n    -1 \n    -0.5 \n    1 \n    1 \n    -0.136 \n    0.842 \n    3060 \n    0 \n  \n\n\n\n\n\n\n\nCODE\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM\nstats = df_items %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(score_SCALED))\ngf_props(~score_SCALED, data = df_items) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Scaled Score for Item\",\n       y = \"Proportion of Items\",\n       title = \"Distribution of Accuracy per Item (Scale Score)\",\n       subtitle=\"The impasse condition shifts density toward the positive score\")+\n  theme_minimal()"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#resources",
    "href": "analysis/SGC3A/3_sgc3A_description.html#resources",
    "title": "5  Description",
    "section": "RESOURCES",
    "text": "RESOURCES\n\nhttps://rpkgs.datanovia.com/ggpubr/reference/index.html\nAppropriate models for response time data. (see: https://lindeloev.github.io/shiny-rt/)]{style=“color: red;”}.\nEspecially see https://lindeloev.github.io/shiny-rt/ for ideas on modelling reaction time data\n\n\n\nCODE\nsessionInfo()\n\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] forcats_0.5.1      stringr_1.4.0      purrr_0.3.4        readr_2.1.2       \n [5] tidyr_1.2.0        tibble_3.1.6       tidyverse_1.3.1    performance_0.8.0 \n [9] fitdistrplus_1.1-8 MASS_7.3-55        multimode_1.5      ggeasy_0.1.3      \n[13] ggstatsplot_0.9.3  ggdist_3.1.1       ggpubr_0.4.0       vcd_1.4-9         \n[17] kableExtra_1.3.4   mosaic_1.8.3       ggridges_0.5.3     mosaicData_0.20.2 \n[21] ggformula_0.10.1   ggstance_0.3.5     dplyr_1.0.8        Matrix_1.4-0      \n[25] Hmisc_4.6-0        ggplot2_3.3.5      Formula_1.2-4      survival_3.3-1    \n[29] lattice_0.20-45   \n\nloaded via a namespace (and not attached):\n  [1] readxl_1.3.1           backports_1.4.1        systemfonts_1.0.4     \n  [4] plyr_1.8.6             splines_4.1.1          crosstalk_1.2.0       \n  [7] leaflet_2.1.0          TH.data_1.1-0          digest_0.6.29         \n [10] htmltools_0.5.2        fansi_1.0.2            magrittr_2.0.2        \n [13] checkmate_2.0.0        paletteer_1.4.0        cluster_2.1.2         \n [16] ks_1.13.5              tzdb_0.2.0             mosaicCore_0.9.0      \n [19] modelr_0.1.8           vroom_1.5.7            sandwich_3.0-1        \n [22] svglite_2.1.0          jpeg_0.1-9             colorspace_2.0-3      \n [25] rvest_1.0.2            ggrepel_0.9.1          haven_2.4.3           \n [28] xfun_0.30              prismatic_1.1.0        crayon_1.5.0          \n [31] jsonlite_1.8.0         zeallot_0.1.0          zoo_1.8-9             \n [34] glue_1.6.2             polyclip_1.10-0        gtable_0.3.0          \n [37] emmeans_1.7.2          MatrixModels_0.5-0     webshot_0.5.2         \n [40] statsExpressions_1.3.2 distributional_0.3.0   car_3.0-12            \n [43] abind_1.4-5            scales_1.1.1           mvtnorm_1.1-3         \n [46] DBI_1.1.2              rstatix_0.7.0          Rcpp_1.0.8.3          \n [49] viridisLite_0.4.0      xtable_1.8-4           htmlTable_2.4.0       \n [52] bit_4.0.4              mclust_5.4.10          foreign_0.8-82        \n [55] datawizard_0.4.1       htmlwidgets_1.5.4      httr_1.4.2            \n [58] RColorBrewer_1.1-2     ellipsis_0.3.2         pkgconfig_2.0.3       \n [61] farver_2.1.0           dbplyr_2.1.1           nnet_7.3-17           \n [64] utf8_1.2.2             effectsize_0.6.0.1     labeling_0.4.2        \n [67] tidyselect_1.1.2       rlang_1.0.2            cellranger_1.1.0      \n [70] munsell_0.5.0          tools_4.1.1            cli_3.2.0             \n [73] generics_0.1.2         broom_0.7.12           evaluate_0.15         \n [76] fastmap_1.1.0          ggdendro_0.1.23        yaml_2.3.5            \n [79] rematch2_2.1.2         bit64_4.0.5            fs_1.5.2              \n [82] knitr_1.38             rootSolve_1.8.2.3      pbapply_1.5-0         \n [85] pracma_2.3.8           xml2_1.3.3             correlation_0.8.1     \n [88] compiler_4.1.1         rstudioapi_0.13        png_0.1-7             \n [91] ggsignif_0.6.3         reprex_2.0.1           tweenr_1.0.2          \n [94] stringi_1.7.6          highr_0.9              parameters_0.18.1     \n [97] vctrs_0.3.8            pillar_1.7.0           lifecycle_1.0.1       \n[100] lmtest_0.9-39          estimability_1.3       data.table_1.14.2     \n[103] insight_0.18.0         patchwork_1.1.1        R6_2.5.1              \n[106] latticeExtra_0.6-29    KernSmooth_2.23-20     gridExtra_2.3         \n[109] BayesFactor_0.9.12-4.3 codetools_0.2-18       gtools_3.9.2          \n[112] boot_1.3-28            assertthat_0.2.1       withr_2.5.0           \n[115] multcomp_1.4-18        parallel_4.1.1         diptest_0.76-0        \n[118] bayestestR_0.12.1      hms_1.1.1              rpart_4.1.16          \n[121] labelled_2.9.0         coda_0.19-4            rmarkdown_2.13        \n[124] carData_3.0-5          ggforce_0.3.3          lubridate_1.8.0       \n[127] base64enc_0.1-3"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html",
    "title": "6  Hypothesis Testing",
    "section": "",
    "text": "TODO\nThe purpose of this notebook is test the hypotheses that determined the design of the SGC3A study.\nResearch Questions\nIn SGC3A we set out to answer the following question: Does posing a mental impasse improve performance on the interval graph comprehension task?\nExperimental Hypothesis\nLearners posed with scenario designed to evoke a mental impasse will be more likely to correct interpret the graph.\nNull Hypothesis\nNo significant differences in performance will exist between learners in the IMPASSE and CONTROL conditions."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1a-q1-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1a-q1-accuracy",
    "title": "6  Hypothesis Testing",
    "section": "H1A | Q1 ACCURACY",
    "text": "H1A | Q1 ACCURACY\nDo Ss in the IMPASSE condition have a higher likelihood of producing a correct response to the first question?\nThe graph comprehension task includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their first exposure to the unconventional triangular coordinate system.\n\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of correctly responding to the first question than those in the CONTROL condition\n\n\nData\n\ndata: df_items where q == 1\noutcome: accuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nLogistic Regression on accuracy predicted by condition\n\naccount for difference in odds of correct score by condition\n\n\nAlternatives:\n\nChi-Square test of independence on outcome accuracy by condition\n\n\n\nNotes\n\nCHIQ SQR is simplest method to examine independence of two categorical factors; LOGISTIC REGRESSION is recommended for binomial ~ continuous; though with regression we can quantify the size of the effect and overall model fit\nindependence assumption : (CHI SQR) as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\ncell frequency : (CHI SQR) expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\n\n\n\n\nCODE\n#:::::::: PREP DATA\ndf <- df_items %>% filter(q==1) %>% mutate(\n  accuracy = recode_factor(score_niceABS, \"0\" =\"incorrect\",\"1\"=\"correct\")\n) \n\n# #:::::::: GROUPED PROPORTIONAL BAR CHART\n# gf_props(~accuracy, fill = ~pretty_condition,\n#        position = position_dodge(), data = df) %>%\n#   gf_facet_grid(~pretty_mode) +\n#    labs(x = \"Correct Response on Q 1\",\n#        title = \"Accuracy on First Question by Condition\",\n#        subtitle=\"Impasse Condition yields a greater proportion of correct responses\") #theme(legend.position = \"none\")\n\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Accuracy on First Question by Condition\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nA proportional bar chart visualizing the proportion of incorrect (vs) correct responses in each condition for each data collection modality (left/right facet) reveals that the pattern of responses appear the same regardless of the data collection modality. In both data collection sessions, the proportion of incorrect responses is much greater than the proportion of correct responses, regardless of condition (marginal probability of incorrect). In the impasse condition, the difference in proportions is smaller than the control condition (conditional probability of success in impasse; (i.e) There are more correct responses in the impasse condition than the control condition).\n\n(LAB)\n\n\nCODE\n#:::::::: IN PERSON ONLY\ndf <- df_items %>% filter(mode == \"lab-synch\") %>% filter(q==1) %>% mutate(\n  accuracy = recode_factor(score_niceABS, \"0\" =\"incorrect\",\"1\"=\"correct\")\n) \n\n#::::::::::::DESCRIPTIVES\n\ntable(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n              111   121   Sum\n  incorrect 0.839 0.703 0.770\n  correct   0.161 0.297 0.230\n  Sum       1.000 1.000 1.000\n\n\nCODE\ntable(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            111 121 Sum\n  incorrect  52  45  97\n  correct    10  19  29\n  Sum        62  64 126\n\n\n\nLOGISTIC REGRESSION\nFit a logistic regression predicting accuracy (absolute score) (n = 126) by condition (k = 2).\n\n\nParameter estimate: \\(\\beta_{0}\\) = Log Odds of (correct) responses in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of correct response in CONTROL condition\nParameter estimate: \\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for correct response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of correct response in IMPASSE (vs) CONTROL\nNull hypothesis:\\(\\beta_{impasse} \\le 0\\) the odds for a correct response does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\gt 0\\) the odds of a correct response increases\n\n\nFit Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\n\nCODE\n# MODEL FITTING ::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\n# print(\"EMPTY MODEL\")\n# summary(m0)\n\n#: 2 CONDITION model\nm1 <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\n# print(\"PREDICTOR MODEL\")\n# summary(m1)\n\n#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > m1$aic)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff | Chi2 |     p\n------------------------------------------\nm0   |   glm |  1 |         |      |      \nm1   |   glm |  2 |       1 | 3.31 | 0.069\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.0687084837283363\"\n\n\nThe Condition predictor decreases AIC, but the Likelihood Ratio Test is marginal. We proceed to examine the predictor model, as we plan to do a 1-tailed NHST .\n\n\nDescribe\n\n\nCODE\n# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL [default two-tailed sig test]\")\n\n\n[1] \"PREDICTOR MODEL [default two-tailed sig test]\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.839  -0.839  -0.593  -0.593   1.910  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.649      0.345   -4.77  1.8e-06 ***\npretty_conditionimpasse    0.786      0.441    1.79    0.074 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 135.95  on 125  degrees of freedom\nResidual deviance: 132.63  on 124  degrees of freedom\nAIC: 136.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\n# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: \n\n# one-sided (right tail) z test for B COEFFICIENT\n#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients\n\n#SANITY CHECK 2-tailed test should match the model output\ntt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"p value for two-tailed test, null B = 0 : \",round(tt,3))\n\n\n[1] \"p value for two-tailed test, null B = 0 :  0.074\"\n\n\nCODE\not <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"BUT we want a one tailed directional, null: B <= 0: \",round(ot,3))\n\n\n[1] \"BUT we want a one tailed directional, null: B <= 0:  0.037\"\n\n\nCODE\npaste(\"adjusted confint for directional hypothesis\")\n\n\n[1] \"adjusted confint for directional hypothesis\"\n\n\nCODE\n(dcint <- confint(m1, level = 0.90)) # get 90% for right side))\n\n\nWaiting for profiling to be done...\n\n\n                            5 %  95 %\n(Intercept)             -2.2578 -1.11\npretty_conditionimpasse  0.0749  1.53\n\n\nCODE\n# https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte\n\n#:::::::: INTERPRET COEFFICIENTS\n\n# print(\"Confidence Interval —- LOG ODDS\")\n# confint(m1) #not adjusted for 1-tailed\nprint(\"Coefficients —- ODDS RATIOS\")\n\n\n[1] \"Coefficients —- ODDS RATIOS\"\n\n\nCODE\n# (e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiated, not adjusted\n(e <- cbind( exp(coef(m1)), exp(dcint))) #exponentiated, adjusted\n\n\n                                5 %  95 %\n(Intercept)             0.192 0.105 0.329\npretty_conditionimpasse 2.196 1.078 4.631\n\n\nCODE\nprint(\"MODEL PREDICTIONS\")\n\n\n[1] \"MODEL PREDICTIONS\"\n\n\nCODE\n# Retrieve predictions as probabilities \n# (for each level of the predictor)\npred.control <- predict(m1,data.frame(pretty_condition=\"control\"),type=\"response\")\npaste(\"Probability of success in control,\", pred.control)\n\n\n[1] \"Probability of success in control, 0.161290322580645\"\n\n\nCODE\npred.impasse <- predict(m1,data.frame(pretty_condition=\"impasse\"),type=\"response\")\npaste(\"Probability of success in impasse,\", pred.impasse)\n\n\n[1] \"Probability of success in impasse, 0.296875000000275\"\n\n\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \nggcoefstats(m1, output = \"plot\", \n              conf.level = 0.90) + \n  labs(x = \"Log Odds Estimate\", \n       title = \"LOGODDS | Q1 Accuracy ~ condition\",\n       subtitle = \"(p is for two tailed test)\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m1, type = \"est\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) +  #manually adjusted for directional test   \n  labs(title = \"ODDS RATIO | Q1 Accuracy ~ condition\",\n       subtitle = \"(p for one sided test)\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m1, type=\"eff\",\n           show.data = TRUE, jitter = TRUE,\n           title = \"MODEL PREDICTION | Q1 Accuracy ~ condition\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\n# tab_model(m1)\n\n\n\n\nDiagnostics\n\n\nCODE\nprint(\"SANITY CHECK REPORTING\")\n\n\n[1] \"SANITY CHECK REPORTING\"\n\n\nCODE\nreport(m1)\n\n\nWe fitted a logistic model (estimated using ML) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model's explanatory power is weak (Tjur's R2 = 0.03). The model's intercept, corresponding to pretty_condition = control, is at -1.65 (95% CI [-2.39, -1.02], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically non-significant and positive (beta = 0.79, 95% CI [-0.06, 1.68], p = 0.074; Std. beta = 0.79, 95% CI [-0.06, 1.68])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nCODE\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(m1)\n\n\n# Indices of model performance\n\nAIC     |     BIC | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n----------------------------------------------------------------------------------------------\n136.632 | 142.305 |     0.026 | 0.415 | 1.034 |    0.526 |    -7.184 |           0.046 | 0.655\n\n\nCODE\nprint(\"MODEL DIAGNOSTICS\")\n\n\n[1] \"MODEL DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m1)\n\n\n\n\n\n\n\nInference\nWe fit a logistic regression model to explore the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 1.79, p = 0.04, one-tailed). The model predicts that the odds of a correct response on the first question in the impasse condition increase by nearly 120% (\\(e^{beta_{impasse}}\\) = 2.19, 95% CI [1.08, 4.63]) over the control condition. The intercept \\(\\beta_{0}\\) parameter is also significant, (\\(e^{b_{0}}\\) = 0.19, p < 0.001, 95% CI [0.11, 0.33]) indicating that the odds of a correct response in the control condition are significantly less than even (less than 50/50 chance of correct response in control condition).\nEquivalent statements:\n\nbeing in impasse condition increases log odds of correct response by 0.79 (over control)\nbeing in impasse increases odds of correct response in impasse over control increases by factor of 2.19\nprobability of correct response in impasse predicted as 30%, vs only 16% in control condition\n\n\n\nTODO\n\nAre these residuals OK? I didn’t think normally distributed residuals were an assumption for logistic regression.\ninterpretation/reporting of model fit?\nsanity check correct interpretation of coefficients & reporting\nSANITY CHECK:: meaning of b0 estimate\n\nb0 parameter is odds of (+) response on reference [control]\nsignificant b0 indicates that odds of a (+) are (significantly) different from 1:1 (i.e. not an equal probability of correct vs incorrect responses in control)\n\n\n\n\n\n\n(ONLINE REPLICATION)\n\n\nCODE\n#:::::::: ONLINE ONLY\ndf <- df_items %>% filter(mode == \"asynch\") %>% filter(q==1) %>% mutate(\n  accuracy = recode_factor(score_niceABS, \"0\" =\"incorrect\",\"1\"=\"correct\")\n) \n\n#::::::::::::DESCRIPTIVES\n\ntable(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n              111   121   Sum\n  incorrect 0.875 0.722 0.794\n  correct   0.125 0.278 0.206\n  Sum       1.000 1.000 1.000\n\n\nCODE\ntable(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            111 121 Sum\n  incorrect  84  78 162\n  correct    12  30  42\n  Sum        96 108 204\n\n\n\nLOGISTIC REGRESSION\nFit a logistic regression predicting accuracy (absolute score) (n = r nrow(df)) by condition. (k = 2).\n\nParameter estimate: \\(\\beta_{0}\\) = Log Odds of (correct) responses in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of correct response in CONTROL condition Parameter estimate:\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for correct response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of correct response in IMPASSE (vs) CONTROL\nNull hypothesis: \\(\\beta_{impasse} \\le 0\\) the odds for a correct response does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\gt 0\\) the odds of a correct response increases\n\n\nFit Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\n\nCODE\n# MODEL FITTING ::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\n# print(\"EMPTY MODEL\")\n# summary(m0)\n\n#: 2 CONDITION model\nm1 <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\n# print(\"PREDICTOR MODEL\")\n# summary(m1)\n\n#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > m1$aic)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff | Chi2 |     p\n------------------------------------------\nm0   |   glm |  1 |         |      |      \nm1   |   glm |  2 |       1 | 7.49 | 0.006\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.00621908608981449\"\n\n\nThe Condition predictor decreases AIC, and the Likelihood Ratio Test indicates the predictor model is a better fit.\n\n\nDescribe\n\n\nCODE\n# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL [default two-tailed sig test]\")\n\n\n[1] \"PREDICTOR MODEL [default two-tailed sig test]\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.807  -0.807  -0.517  -0.517   2.039  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.946      0.309   -6.31  2.9e-10 ***\npretty_conditionimpasse    0.990      0.376    2.63   0.0084 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 207.45  on 203  degrees of freedom\nResidual deviance: 199.96  on 202  degrees of freedom\nAIC: 204\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\n# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: \n\n# one-sided (right tail) z test for B COEFFICIENT\n#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients\n\n#SANITY CHECK 2-tailed test should match the model output\ntt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"p value for two-tailed test, null B = 0 : \",round(tt,3))\n\n\n[1] \"p value for two-tailed test, null B = 0 :  0.008\"\n\n\nCODE\not <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"BUT we want a one tailed directional, null: B <= 0: \",round(ot,3))\n\n\n[1] \"BUT we want a one tailed directional, null: B <= 0:  0.004\"\n\n\nCODE\npaste(\"adjusted confint for directional hypothesis\")\n\n\n[1] \"adjusted confint for directional hypothesis\"\n\n\nCODE\n(dcint <- confint(m1, level = 0.90)) # get 90% for right side))\n\n\nWaiting for profiling to be done...\n\n\n                           5 %  95 %\n(Intercept)             -2.489 -1.47\npretty_conditionimpasse  0.388  1.63\n\n\nCODE\n# https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte\n\n\n#:::::::: INTERPRET COEFFICIENTS\n\n# print(\"Confidence Interval —- LOG ODDS\")\n# confint(m1) #not adjusted for 1-tailed\nprint(\"Coefficients —- ODDS RATIOS\")\n\n\n[1] \"Coefficients —- ODDS RATIOS\"\n\n\nCODE\n# (e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiated, not adjusted\n(e <- cbind( exp(coef(m1)), exp(dcint))) #exponentiated, adjusted\n\n\n                                5 %  95 %\n(Intercept)             0.143 0.083 0.231\npretty_conditionimpasse 2.692 1.473 5.109\n\n\nCODE\nprint(\"MODEL PREDICTIONS\")\n\n\n[1] \"MODEL PREDICTIONS\"\n\n\nCODE\n# Retrieve predictions as probabilities \n# (for each level of the predictor)\npred.control <- predict(m1,data.frame(pretty_condition=\"control\"),type=\"response\")\npaste(\"Probability of success in control,\", pred.control)\n\n\n[1] \"Probability of success in control, 0.125000000004466\"\n\n\nCODE\npred.impasse <- predict(m1,data.frame(pretty_condition=\"impasse\"),type=\"response\")\npaste(\"Probability of success in impasse,\", pred.impasse)\n\n\n[1] \"Probability of success in impasse, 0.277777777778245\"\n\n\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \nggcoefstats(m1, output = \"plot\", \n              conf.level = 0.90) + \n  labs(x = \"Log Odds Estimate\", \n       title = \"LOGODDS | Q1 Accuracy ~ condition\",\n       subtitle = \"(p is for two tailed test)\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m1, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) +  #manually adjusted for directional test   \n  labs(title = \"ODDS RATIO | Q1 Accuracy ~ condition\",\n       subtitle = \"(p for one sided test)\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m1, type=\"eff\",\n           show.data = TRUE, jitter = TRUE,\n           title = \"MODEL PREDICTION | Q1 Accuracy ~ condition\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\n# tab_model(m1)\n\n\n\n\nDiagnostics\n\n\nCODE\nprint(\"SANITY CHECK REPORTING\")\n\n\n[1] \"SANITY CHECK REPORTING\"\n\n\nCODE\nreport(m1)\n\n\nWe fitted a logistic model (estimated using ML) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model's explanatory power is weak (Tjur's R2 = 0.04). The model's intercept, corresponding to pretty_condition = control, is at -1.95 (95% CI [-2.60, -1.38], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.99, 95% CI [0.27, 1.76], p = 0.008; Std. beta = 0.99, 95% CI [0.27, 1.76])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nCODE\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(m1)\n\n\n# Indices of model performance\n\nAIC     |     BIC | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n----------------------------------------------------------------------------------------------\n203.962 | 210.598 |     0.036 | 0.397 | 0.995 |    0.490 |    -9.361 |           0.034 | 0.685\n\n\nCODE\nprint(\"MODEL DIAGNOSTICS\")\n\n\n[1] \"MODEL DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m1)\n\n\n\n\n\n\n\nInference\nWe fit a logistic regression model to explore the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 2.63, p = 0.004, one-tailed). The model predicts that the odds of a correct response on the first question in the impasse condition increase by nearly 170% (\\(e^{beta_{impasse}}\\) = 2.69, 95% CI [1.47, 5,11]) over the control condition.The intercept \\(\\beta_{0}\\) parameter is also significant, (\\(e^{b_{0}}\\) = 0.14, p < 0.001, 95% CI [0.08, 0.23]) indicating that the odds of a correct response in the control condition are significantly less than even (less than 50/50 chance of correct response in control condition).\nEquivalent statements:\n\nbeing in impasse condition increases log odds of correct response by 0.99 (over control)\nbeing in impasse increases odds of correct response in impasse over control increases by factor of 2.69\nprobability of correct response in impasse predicted as 28%, vs only 12% in control condition"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1a-q1-interpretation-state",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1a-q1-interpretation-state",
    "title": "6  Hypothesis Testing",
    "section": "H1A | Q1 INTERPRETATION STATE",
    "text": "H1A | Q1 INTERPRETATION STATE\nDo Ss in the IMPASSE condition offer less-orthogonal interpretations on first question?\nWhile absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn’t allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject’s response, and determine if these interpretations differ by experimental condition. State is a 3-category derived response variable that groups the following interpretations:\n\n“orthogonal-like” [reference category] includes orthogonal and satisficing responses ==> indicates a primarily orthogonal state of coordinate system understanding\n“unknown” includes: blank, reference point, responses that can’t be classified (including selecting all datapoints) => indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly not orthogonal nor triangular\n“triangle-like” includes correct triangular and ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n\n\n\n\n\n\n\n\nResearch Question\nDoes Ss in the impasse condition produce less orthogonal responses on the first question?\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of producing unknown and and triangle-like response states, relative to orthogonal response states, on the first question\n\n\nData\n\ndata: df_items where q == 1\noutcome: state ( 3 level factor from 5 level high_interpretation )\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMultinomial (Logistic) Regression on state predicted by condition\n\nAlternative:\n\nOrdinal regression on state; but model doesn’t satisfy proportional odds assumption (parallel slopes)\nMultinomial or Ordinal regression on high_interpretation (5 category interpretation state which distinguishes between uncertain (blank, reference) unclassifiable, triangle-like and true triangular.) There are some cells with zeros, however (no uncertain responses in control) which means the model can’t accurately estimate those comparisons\n\n\n\n\n\n\nCODE\n#:::::::: PREP DATA\ndf <- df_items %>% filter(q==1) \n\n# #:::::::: GROUPED PROPORTIONAL BAR CHART\n# gf_props(~high_interpretation, fill = ~pretty_condition,\n#        position = position_dodge(), data = df) %>%\n#   gf_facet_grid(~pretty_mode) +\n#    labs(x = \"Correct Response on Q 1\",\n#        title = \"Interpretation on First Question by Condition\",\n#        subtitle=\"\") #theme(legend.position = \"none\")\n\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Interpretation on First Question by Condition\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nA proportional bar chart visualizing the proportion of each interpretations type by condition for each data collection modality (left/right facet) reveals that the pattern of responses are similar regardless of the data collection modality, by differ by condition. In the impasse condition, there are more triangular responses than in control. We see that around half of the ‘incorrect’ (i.e. not triangular) responses in the impasse condition are not orthogonal-like, but “other/unknown”.\n\n(LAB)\n\n\nCODE\n#:::::::: IN PERSON ONLY\ndf <- df_items %>% filter(mode == \"lab-synch\") %>% filter(q==1) \n\n#::::::::::::DESCRIPTIVES\n\ntable(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse    Sum\n  orth-like  0.8065  0.3125 0.5556\n  unknown    0.0161  0.2812 0.1508\n  tri-like   0.1774  0.4062 0.2937\n  Sum        1.0000  1.0000 1.0000\n\n\nCODE\n(t <- table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n           \n            control impasse Sum\n  orth-like      50      20  70\n  unknown         1      18  19\n  tri-like       11      26  37\n  Sum            62      64 126\n\n\n\nMULTINOMIAL REGRESSION\nDoes condition affect the response state of Q1?\nFit a logistic regression predicting interpretation state (k=3) by condition(k = 2).\n\n2 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) [essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing [reference category] vs [this category])\nFor each equation:\n\n\\(\\beta_{0}\\) = Log Odds of [this category type vs. reference category type) response in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of [this category type vs. reference category type] response in CONTROL condition\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for [this category] type response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of [this. vs reference category type] response in IMPASSE (vs) CONTROL\nTwo-tailed NHST Null hypothesis: \\(\\beta_{impasse} = 0\\) the odds for [this category of response vs. reference] are not different for IMPASSE condition\nAlternative hypothesis: \\(\\beta_{impasse} \\ne 0\\) the odds of [this category of response vs. reference] increases or decreases for IMPASSE condition\n\n\n\nFit Model\n\n\nCODE\n#check reference level \nprint(\"Categories (first is reference)\")\n\n\n[1] \"Categories (first is reference)\"\n\n\nCODE\nlevels(df$state)\n\n\n[1] \"orth-like\" \"unknown\"   \"tri-like\" \n\n\nCODE\n#FIT EMPTY MODEL\n# print(\"EMPTY MODEL\")\ncatm.0 <- multinom(state ~ 1, data = df)\n\n\n# weights:  6 (2 variable)\ninitial  value 138.425148 \nfinal  value 122.428550 \nconverged\n\n\nCODE\n# summary(catm.0)\n\n#FIT PREDICTOR MODEL\n# print(\"PREDICTOR MODEL\")\ncatm <- multinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\n\n# weights:  9 (4 variable)\ninitial  value 138.425148 \niter  10 value 103.421004\niter  10 value 103.421004\nfinal  value 103.421004 \nconverged\n\n\nCODE\n# summary(catm)\n\n#COMPARE MODEL FIT\npaste(\"AIC wth predictor is lower than empty model?\", catm.0$AIC > catm$AIC)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(catm.0, catm)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |      p\n-------------------------------------------------\ncatm.0 | multinom |  2 |         |       |       \ncatm   | multinom |  4 |       2 | 38.02 | < .001\n\n\nCODE\n##compare bayesian version\n#library(brms)\n# bm1 <- brm( state ~ pretty_condition, data = df, family = \"categorical\")\n# summary(bm1)\n# plot_model(bm1)\n# report(bm1)\n# coefficient estimates are very simliar to catm. super cool!\n\n##compare mclogit version\n#\"baseline-category logit model\n# https://www.elff.eu/software/mclogit/manual/mblogit/\n# blm1 <- mblogit(state ~ pretty_condition , data = df)\n# summary(blm1)\n#identical to catm. super cool!\n\n\nAIC in predictor model is less than empty model, and likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.\n\n\nDescribe\n\n\nCODE\n#::::::::INTERPRETATION\npaste(\"MODEL SUMMARY\")\n\n\n[1] \"MODEL SUMMARY\"\n\n\nCODE\nsummary(catm)\n\n\nCall:\nmultinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\nCoefficients:\n         (Intercept) pretty_conditionimpasse\nunknown        -3.91                    3.81\ntri-like       -1.51                    1.78\n\nStd. Errors:\n         (Intercept) pretty_conditionimpasse\nunknown        1.010                   1.061\ntri-like       0.333                   0.447\n\nResidual Deviance: 207 \nAIC: 215 \n\n\nCODE\n# calculate z-statistics of coefficients\nz_stats <- summary(catm)$coefficients/summary(catm)$standard.errors\n# convert to p-values\np_values <- (1 - pnorm(abs(z_stats)))*2\n# display p-values in transposed data frame\n(p_values <- data.frame(p = (p_values)))\n\n\n         p..Intercept. p.pretty_conditionimpasse\nunknown       1.07e-04                 0.0003330\ntri-like      5.45e-06                 0.0000692\n\n\nCODE\n# display odds ratios in transposed data frame\n\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\nodds_ratios <- data.frame(OR = exp(summary(catm)$coefficients))\noptions(scipen = 2)\n(results <- cbind(odds_ratios, p_values))\n\n\n         OR..Intercept. OR.pretty_conditionimpasse p..Intercept.\nunknown            0.02                      44.99    0.00010722\ntri-like           0.22                       5.91    0.00000545\n         p.pretty_conditionimpasse\nunknown                  0.0003330\ntri-like                 0.0000692\n\n\n\n\nInference\n\nBeing in the IMPASSE condition increases the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 45 (z = 3.58, p < 0.001) . Participants in the impasse condition were 45x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 5.90 (z = 3.98, p < 0.001 ). Participants in the impasse condition were almost 6x as likely to give an a triangular response rather than an orthogonal response compared to participants in control.\nAs with the (binary) logistic regression on accuracy ~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 in the control condition. (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)\n[need to to double check interpretation, but I think that the OR intercepts converted to probabilities equate to the marginal probability of being in each state in the control condition. which makes sense. I think.?]\nIF I change reference category for condition… then the intercepts should no longer be significant. The b1 coefficients should still be significant, but with changed sign (much less likely) [Yup! this works!]\n\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \nggcoefstats(catm, output = \"plot\", \n              # conf.level = 0.90,\n              exclude.intercept = FALSE) + \n  labs(x = \"Log Odds Estimate\", \n       title = \"LOGODDS | Q1 State ~ condition\",\n       subtitle = \"(p is for two tailed test)\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(catm, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.05, #can manually adjust to account for directional test\n           ci.lvl = 0.95 ) +  #can manually adjusted for directional test   \n  labs(title = \"ODDS RATIO | Q1 State ~ condition\",\n       subtitle = \"(p for one two test)\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(catm, type=\"eff\",\n           title = \"MODEL PREDICTION | Q1 State ~ condition\",\n           axis.title = c(\"Condition\",\"Probability of Response State\"))\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\n#tab_model(catm)\n\n\n\n\nDiagnostics\n\n\nCODE\n#EXAMINE PREDICTIONS\n#create sample data frame\ntest <- data.frame(pretty_condition = c(\"control\", \"impasse\"))\npred <- predict(catm, newdata = test, \"probs\")\npaste(\"Predicted Probability of Being in Each State\")\n\n\n[1] \"Predicted Probability of Being in Each State\"\n\n\nCODE\n(cbind(test, pred))\n\n\n  pretty_condition orth-like unknown tri-like\n1          control     0.806  0.0161    0.177\n2          impasse     0.312  0.2812    0.406\n\n\nCODE\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(catm)\n\n\nCan't calculate log-loss.\nCan't calculate proper scoring rules for ordinal, multinomial or cumulative link models.\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n-----------------------------------------------------\n214.842 | 226.187 | 0.155 |     0.147 | 0.404 | 1.302\n\n\nCODE\nDescTools::PseudoR2(catm, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n\n\nRegistered S3 method overwritten by 'DescTools':\n  method         from \n  reorder.factor gdata\n\n\n  McFadden   CoxSnell Nagelkerke \n     0.155      0.260      0.304 \n\n\nCODE\n#General Goodness of Fit\n#library(generalhoslem)\n#logitgof(df$state, catm$fitted.values, g = 3)\n#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).\n#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables\nchisq.test(df$state,predict(catm)) #actual states VS predicted states\n\n\n\n    Pearson's Chi-squared test\n\ndata:  df$state and predict(catm)\nX-squared = 34, df = 2, p-value = 4e-08\n\n\nCODE\n# The chi-square test tests the decrease in unexplained variance from the baseline model to the final model\n\n# print(\"MODEL DIAGNOSTICS\")\n#check_model(catm) can't do overall diagnostics, have to do them on individual model equations\n\n\n\n\nTODO\n\ninterpretation/reporting of model fit?\nsanity check correct interpretation of coefficients & reporting\ndiagnostics on individual model equations\ncalculate 1-tailed p values to match directional hypothesis\n\n\n\n\n\n(ONLINE REPLICATION)\n\n\nCODE\n#:::::::: ONLINE ONLY\ndf <- df_items %>% filter(mode == \"asynch\") %>% filter(q==1) \n\n#::::::::::::DESCRIPTIVES\n\ntable(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse    Sum\n  orth-like  0.8229  0.2315 0.5098\n  unknown    0.0312  0.3889 0.2206\n  tri-like   0.1458  0.3796 0.2696\n  Sum        1.0000  1.0000 1.0000\n\n\nCODE\n(t <- table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n           \n            control impasse Sum\n  orth-like      79      25 104\n  unknown         3      42  45\n  tri-like       14      41  55\n  Sum            96     108 204\n\n\n\nMULTINOMIAL REGRESSION\nDoes condition affect the response state of Q1?\nFit a logistic regression predicting interpretation (k=3) by condition(k = 2).\n\n2 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) [essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing [reference category] vs [this category])\nFor each equation:\n\n\\(\\beta_{0}\\) = Log Odds of [this category type vs. reference category type) response in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of [this category type vs. reference category type] response in CONTROL condition\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for [this category] type response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of [this. vs reference category type] response in IMPASSE (vs) CONTROL\nTwo-tailed NHST Null hypothesis: \\(\\beta_{impasse} = 0\\) the odds for [this category of response vs. reference] does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\ne 0\\) the odds of [this category of response vs. reference] increases\n\n\n\nFit Model\n\n\nCODE\n#check reference level \nprint(\"Categories (first is reference)\")\n\n\n[1] \"Categories (first is reference)\"\n\n\nCODE\nlevels(df$state)\n\n\n[1] \"orth-like\" \"unknown\"   \"tri-like\" \n\n\nCODE\n#FIT EMPTY MODEL\n# print(\"EMPTY MODEL\")\ncatm.0 <- multinom(state ~ 1, data = df)\n\n\n# weights:  6 (2 variable)\ninitial  value 224.116907 \nfinal  value 210.176688 \nconverged\n\n\nCODE\n# summary(catm.0)\n\n#FIT PREDICTOR MODEL\n# print(\"PREDICTOR MODEL\")\ncatm <- multinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\n\n# weights:  9 (4 variable)\ninitial  value 224.116907 \niter  10 value 168.708105\niter  10 value 168.708105\nfinal  value 168.708105 \nconverged\n\n\nCODE\n# summary(catm)\n\n#COMPARE MODEL FIT\npaste(\"AIC wth predictor is lower than empty model?\", catm.0$AIC > catm$AIC)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(catm.0, catm)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |      p\n-------------------------------------------------\ncatm.0 | multinom |  2 |         |       |       \ncatm   | multinom |  4 |       2 | 82.94 | < .001\n\n\nCODE\n##compare bayesian version\n#library(brms)\n# m1 <- brm( state ~ condition, data = df, family = \"categorical\")\n# summary(m1)\n# plot_model(m1)\n# report(m1)\n\n\nAIC in predictor model is less than empty model, and likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.\n\n\nDescribe\n\n\nCODE\n#::::::::INTERPRETATION\npaste(\"MODEL SUMMARY\")\n\n\n[1] \"MODEL SUMMARY\"\n\n\nCODE\nsummary(catm)\n\n\nCall:\nmultinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\nCoefficients:\n         (Intercept) pretty_conditionimpasse\nunknown        -3.27                    3.79\ntri-like       -1.73                    2.22\n\nStd. Errors:\n         (Intercept) pretty_conditionimpasse\nunknown        0.588                   0.640\ntri-like       0.290                   0.385\n\nResidual Deviance: 337 \nAIC: 345 \n\n\nCODE\n# calculate z-statistics of coefficients\nz_stats <- summary(catm)$coefficients/summary(catm)$standard.errors\n# convert to p-values\np_values <- (1 - pnorm(abs(z_stats)))*2\n# display p-values in transposed data frame\n(p_values <- data.frame(p = (p_values)))\n\n\n         p..Intercept. p.pretty_conditionimpasse\nunknown       2.68e-08                  3.22e-09\ntri-like      2.41e-09                  7.73e-09\n\n\nCODE\n# display odds ratios in transposed data frame\n\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\nodds_ratios <- data.frame(OR = exp(summary(catm)$coefficients))\noptions(scipen = 2)\n(results <- cbind(odds_ratios, p_values))\n\n\n         OR..Intercept. OR.pretty_conditionimpasse p..Intercept.\nunknown           0.038                      44.22      2.68e-08\ntri-like          0.177                       9.25      2.41e-09\n         p.pretty_conditionimpasse\nunknown                   3.22e-09\ntri-like                  7.73e-09\n\n\n\n\nInference\n\nBeing in the IMPASSE condition increases the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 44 (z = 5.92, p < 0.001) . Participants in the impasse condition were 44x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 9.25 (z = 5.77, p < 0.001 ). Participants in the impasse condition were moret than 9x as likely to give an a triangular response rather than an orthogonal response compared to participants in control.\nAs with the (binary) logistic regression on accuracy ~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 in the control condition. (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)\n\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \nggcoefstats(catm, output = \"plot\", \n              # conf.level = 0.90,\n              exclude.intercept = FALSE) + \n  labs(x = \"Log Odds Estimate\", \n       title = \"LOGODDS | Q1 State ~ condition\",\n       subtitle = \"(p is for two tailed test)\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(catm, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.05, #can manually adjust to account for directional test\n           ci.lvl = 0.95 ) +  #can manually adjusted for directional test   \n  labs(title = \"ODDS RATIO | Q1 State ~ condition\",\n       subtitle = \"(p for one two test)\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(catm, type=\"eff\",\n           title = \"MODEL PREDICTION | Q1 State ~ condition\",\n           axis.title = c(\"Condition\",\"Probability of Response State\"))\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\n#tab_model(catm)\n\n\n\n\nDiagnostics\n\n\nCODE\n#EXAMINE PREDICTIONS\n#create sample data frame\ntest <- data.frame(pretty_condition = c(\"control\", \"impasse\"))\npred <- predict(catm, newdata = test, \"probs\")\npaste(\"Predicted Probability of Being in Each State\")\n\n\n[1] \"Predicted Probability of Being in Each State\"\n\n\nCODE\npaste(\"[these should be consistent with effects plot, above\")\n\n\n[1] \"[these should be consistent with effects plot, above\"\n\n\nCODE\n(cbind(test, pred))\n\n\n  pretty_condition orth-like unknown tri-like\n1          control     0.823  0.0313    0.146\n2          impasse     0.232  0.3889    0.380\n\n\nCODE\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(catm)\n\n\nCan't calculate log-loss.\nCan't calculate proper scoring rules for ordinal, multinomial or cumulative link models.\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n-----------------------------------------------------\n345.416 | 358.689 | 0.197 |     0.193 | 0.403 | 1.299\n\n\nCODE\nDescTools::PseudoR2(catm, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n\n\n  McFadden   CoxSnell Nagelkerke \n     0.197      0.334      0.383 \n\n\nCODE\n#General Goodness of Fit\n#library(generalhoslem)\n#logitgof(df$state, catm$fitted.values, g = 3)\n#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).\n#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables\nchisq.test(df$state,predict(catm)) #actual states VS predicted states\n\n\n\n    Pearson's Chi-squared test\n\ndata:  df$state and predict(catm)\nX-squared = 75, df = 2, p-value <2e-16\n\n\nCODE\n# The chi-square test tests the decrease in unexplained variance from the baseline model to the final model\n\n# print(\"MODEL DIAGNOSTICS\")\n#check_model(catm) can't do overall diagnostics, have to do them on individual model equations"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1b-test-phase-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1b-test-phase-accuracy",
    "title": "6  Hypothesis Testing",
    "section": "H1B | TEST PHASE ACCURACY",
    "text": "H1B | TEST PHASE ACCURACY\n\n\n\n\n\n\n\nResearch Question\nDo Ss in the IMPASSE condition score higher across the entire task than those in the CONTROL group?\n\n\n\n\nHypothesis\n(H1B) Participants in the IMPASSE condition will have higher test phase performance than those in the CONTROL condition.\n\n\nData\ndata: df_items where q nin 1,2,3,4,5,6,9 (the 8 discriminating test phase Qs ), df_subjects\noutcome:\n\n[at item level] : accuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\n[subject level]: accuracy (number of test phase qs correct from total s_NABS)\n\npredictor: condition [between-subjects factor]\n\n\nAnalysis Strategy\n\nWilcoxon-Rank Sum (Mann-Whitney) test on subject-level total accuracy of test phase (item_test_NABS)\nMixed Logistic Regression\naccuracy ~ condition + (1 | subject )\nmodel effect of condition on probability of correct response [during test phase] while accounting for subject (and item-level?) effects\n\n\n\nAlternatives\n\nOrdinal Mixed Logistic Regression\ninterpretation ~ condition + (1 | subject )\nmodel effect of condition on [ordered correctness of interpretation] [during test phase] while accounting for subject (and item-level?) effects\nShift in Modal Mass (descriptive)\ndescribe & visualize shift in deciles between conditions for scaled_score (at subject level)\nOLS Linear Regression: bimodal distribution at tails makes the mean a poor predictor; LMs violate assumptions of normally distributed residuals; both absolute and scaled scores yield non-normal residuals; no transformation of the outcome variables yield normal residuals\n\n\n\nNotes\nAlso exploring:\n\nHurdle model (mixture model w/ binomial + [poisson or negbinom count; 0s from 1 DGP)\nZero Inflated model (mixture model w/ binomial + poisson or negbinom count; 0s from 2 DGPs)\nBeta regression hurdle model? (mixture with location and scale parameters [mean, variance] and hurdles for floor and ceiling effects)\nOther way to account for the severe bimodality?\n\n\n\n\n\n\nCODE\n#item level\ndf = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  accuracy = recode_factor(score_niceABS, \"0\" =\"incorrect\",\"1\"=\"correct\"),\n  q = as.factor(q)\n)\n\n#:::::::: STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(title = \"Test Phase Accuracy\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nCODE\n#:::::::: FACETED HISTOGRAM\nstats = df_subjects %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))\ngf_props(~item_test_NABS, \n         fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_grid(pretty_condition ~ pretty_mode) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"# Correct\",\n       y = \"proportion of subjects\",\n       title = \"Test Phase Absolute Score (# Correct)\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n\n\n\n\n\n\n(LAB)\n\n\nCODE\n#:::::::: IN PERSON ONLY\ndf_i <- df_items %>% filter(mode == \"lab-synch\") %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  accuracy = recode_factor(score_niceABS, \"0\" =\"incorrect\",\"1\"=\"correct\")\n) \n\ndf_s <- df_subjects %>% filter(mode == \"lab-synch\") %>% mutate(\n  test_score = item_test_NABS\n)\n\n\n\nWILCOXON RANK SUM (Mann-Whitney Test)\n\nNon parametric alternative to t-test; compares median rather than mean by ranking data\nDoes not assume normality\nDoes not assume equal variance of samples (homogeneity of variance)\n\n\nTest\n\n\nCODE\n(w <- wilcox.test(df_s$test_score ~ df_s$condition,\n                 paired = FALSE, alternative = \"less\")) #less, greater\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df_s$test_score by df_s$condition\nW = 1438, p-value = 0.002\nalternative hypothesis: true location shift is less than 0\n\n\nCODE\nreport(w)\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum test with continuity correction testing the difference in ranks between df_s$test_score and df_s$condition suggests that the effect is negative, statistically significant, and medium (W = 1438.00, p = 0.002; r (rank biserial) = -0.28, 95% CI [-1.00, -0.11])\n\n\n\n\nVisualize\n\n\nCODE\n#[manual one-sided test]\n(results <- statsExpressions::two_sample_test(y = test_score, x = condition, data = df_s,\n                                type = \"nonparametric\", alternative = \"less\",\n                                var.equal = FALSE))\n\n\n# A tibble: 1 × 14\n  parameter1 parameter2 statistic p.value method alternative effectsize estimate\n  <chr>      <chr>          <dbl>   <dbl> <chr>  <chr>       <chr>         <dbl>\n1 test_score condition       1438 0.00161 Wilco… less        r (rank b…   -0.275\n# … with 6 more variables: conf.level <dbl>, conf.low <dbl>, conf.high <dbl>,\n#   conf.method <chr>, n.obs <int>, expression <list>\n\n\nCODE\n#:::::::: STATSPLOT | VIOLIN\nggbetweenstats(y = test_score, x = condition, data = df_s,\n               results.subtitle = FALSE, #override default [two tailed] test dsiplay\n               subtitle = results$expression[[1]]\n              )\n\n\n\n\n\n\n\nInference\nA Mann-Whitney (Wilcoxon Rank Sum) test evaluating the difference in median accuracy score in the test phase of the graph comprehension task indicates that performance was better in the impasse (vs) control condition. [report stats]\n\n\n\nMIXED LOGISTIC REGRESSION\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.\nTODO: Do I need to add a random effect for item? I have no reason to suspect that accuracy changes by item, only response time.\n\nFit Model\n\n\nCODE\n## 1 | SETUP RANDOM INTERCEPT SUBJECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nprint(\"Empty fixed model\")\n\n\n[1] \"Empty fixed model\"\n\n\nCODE\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df_i) \n# summary(m0)\n\n#:: RANDOM INTERCEPT SUBJECT\nprint(\"Subject intercept random model\")\n\n\n[1] \"Subject intercept random model\"\n\n\nCODE\nmm.rS <- glmer(accuracy ~ (1|subject), data = df_i, family = \"binomial\")\n# summary(mm.rS)\n\n# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", m0$aic > AIC(logLik(mm.rS)))\n\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\n\nCODE\ntest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName  |    Model | df | df_diff |   Chi2 |      p\n-------------------------------------------------\nm0    |      glm |  1 |         |        |       \nmm.rS | glmerMod |  2 |       1 | 678.41 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  1.4837179793132e-149\"\n\n\nCODE\n## 2 | ADD RANDOM INTERCEPT ITEM?\n\n#:: RANDOM INTERCEPT SUBJECT + INTERCEPT Q\n# print(\"Subject & Question random intercepts\")\n# mm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = \"binomial\")\n# # summary(mm.rSQ)\n# \n# # :: TEST random effect\n# paste(\"AIC decreases w/ new model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)) )\n# test_lrt(mm.rS,mm.rSQ) #same as anova(m0, m1, test = \"Chi\")\n# paste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS,mm.rSQ))$p[2])\n\n## 3 | ADD FIXED EFFECT CONDITION\n\n# print(\"FIXED Condition + Subject & Question random intercepts\")\n# mm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q), \n#                 data = df_i, family = \"binomial\")\n# summary(mm.CrSQ)\n# \n# paste(\"AIC decreases w/ new model\", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.CrSQ)) )\n# test_lrt(mm.rSQ,mm.CrSQ) #same as anova(m0, m1, test = \"Chi\")\n# paste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rSQ,mm.CrSQ))$p[2])\n\nprint(\"FIXED Condition + Subject random intercepts\")\n\n\n[1] \"FIXED Condition + Subject random intercepts\"\n\n\nCODE\nmm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject) ,\n                data = df_i, family = \"binomial\")\n# summary(mm.CrS)\n\npaste(\"AIC decreases w/ new model\", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )\n\n\n[1] \"AIC decreases w/ new model TRUE\"\n\n\nCODE\ntest_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff | Chi2 |     p\n-----------------------------------------------\nmm.rS  | glmerMod |  2 |         |      |      \nmm.CrS | glmerMod |  3 |       1 | 3.91 | 0.048\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS,mm.CrS))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.047907883063779\"\n\n\n\n\nDescribe\n\n\nCODE\n# best model\nm1 <- mm.CrS\n\n#::::::::: PRINT MODEL \n\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m1)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition + (1 | subject)\n   Data: df_i\n\n     AIC      BIC   logLik deviance df.resid \n     582      597     -288      576     1005 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5079 -0.0674 -0.0217  0.1326  2.8082 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 54.1     7.36    \nNumber of obs: 1008, groups:  subject, 126\n\nFixed effects:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                -7.46       1.20   -6.21  5.2e-10 ***\npretty_conditionimpasse     4.03       1.67    2.42    0.016 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nprtty_cndtn 0.156 \n\n\nCODE\n#:::::::: MANUAL ONE-SIDED SIGTEST \n\n# one-sided (right tail) z test for B COEFFICIENT\n#SANITY CHECK 2-tailed test should match the model output\ntt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"p value for two-tailed test, null B = 0 : \",round(tt,5))\n\n\n[1] \"p value for two-tailed test, null B = 0 :  0.01573\"\n\n\nCODE\not <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"BUT we want a one tailed directional, null: B <= 0: \",round(ot,5))\n\n\n[1] \"BUT we want a one tailed directional, null: B <= 0:  0.00787\"\n\n\nCODE\n#:::::::: INTERPRET COEFFICIENTS\n\nse <- sqrt(diag(stats::vcov(m1)))\n# table of estimates with 95% CI\npaste(\"LOG ODDS\")\n\n\n[1] \"LOG ODDS\"\n\n\nCODE\n(tab <- cbind(Est = fixef(m1), LL = fixef(m1) - 1.96 * se, UL = fixef(m1) + 1.96 *\n    se))\n\n\n                          Est     LL    UL\n(Intercept)             -7.46 -9.813 -5.11\npretty_conditionimpasse  4.03  0.759  7.30\n\n\nCODE\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\n(e <- exp(tab))\n\n\n                              Est        LL         UL\n(Intercept)              0.000576 0.0000548    0.00606\npretty_conditionimpasse 56.086699 2.1356333 1472.96720\n\n\n\n\nInference\nTo investigate the effect of condition on test phase item accuracy, we fit a mixed-effect binomial logistic regression model with random intercepts for subjects. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(4): 9.81, p < 0.002). The explanatory power of the entire model is substantial (conditional R2 = 0.74) though the part related to the fixed effects (marginal R2) is 0.02. Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 56 over the control condition \\(e^{\\beta_1}\\) = 56, p < 0.01, one-tailed, 90% CI [2.14, 1473].\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \nggcoefstats(m1, output = \"plot\", \n              conf.level = 0.90) + \n  labs(x = \"Log Odds Estimate\", \n       subtitle = \"p is for two tailed test\")\n\n\nWarning: Removed 2 rows containing missing values (geom_errorbarh).\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m1, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) + #manually adjusted for directional test   \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m1, type=\"eff\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\n#tab_model(m1)\n\n\n\n\nDiagnostics\n\n\nCODE\nprint(\"SANITY CHECK REPORTING\")\n\n\n[1] \"SANITY CHECK REPORTING\"\n\n\nCODE\nreport(m1)\n\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model included subject as random effect (formula: ~1 | subject). The model's total explanatory power is substantial (conditional R2 = 0.95) and the part related to the fixed effects alone (marginal R2) is of 0.07. The model's intercept, corresponding to pretty_condition = control, is at -7.46 (95% CI [-9.81, -5.11], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 4.03, 95% CI [0.76, 7.29], p = 0.016; Std. beta = 4.03, 95% CI [0.76, 7.29])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(m1)\n\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n----------------------------------------------------------------------------------------------------------------------\n582.024 | 582.048 | 596.771 |      0.946 |      0.066 | 0.943 | 0.213 | 1.000 |    0.145 |      -Inf |           0.021\n\n\nCODE\nprint(\"DIAGNOSTICS\")\n\n\n[1] \"DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m1)\n\n\n\n\n\nCODE\nbinned_residuals(m1)\n\n\nWarning: Probably bad model fit. Only about 64% of the residuals are inside the error bounds.\n\n\n\n\nTODO\n\nsanity check interpretation\nsanity check random effects structure :\n\ndo I need to have ITEM as random intercept? What does it mean to have two random intercepts?\n\nDIAGNOSTICS: What in the world is happening with the normality of random effects plot? Do the fixed effects residuals need to be normally distributed?\nAre there other plots or recommended diagnostics for mixed log regression\nconsider multiple regression with rt, sequence cluster, confidence, etc.\nWhat else needs to be interpreted with respect to the item and subject random effects?\nDouble check: can’t have condition by subject or item slope bc subjects are nested in conditions, not crossed\n\n\n\n\n\nTODO (ONLINE REPLICATION)\n**TODO after verify correctness of approach for the lab-based sample (above)\n\n\nSHIFT IN MODAL MASS\nThe Effect of Condition on Total Scaled Score can be described as a ‘shift’ in mass between the low and high modes of each distribution.\nFirst, we use the Kolmogorov-Smirnov test as a Robust alternative to the t-test to test if the two distributions likely come from different populations.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(mbp)\n\n#(requires shift function files loaded)\n#LOAD MODAL SHIFT FUNCTION RESOURCES\nsource(\"analysis/utils/shift_function/Rallfun-v30.txt\")\nsource(\"analysis/utils/shift_function/wilcox_modified.txt\")\nsource(\"analysis/utils/shift_function/rgar_visualisation.txt\")\nsource(\"analysis/utils/shift_function/rgar_utils.txt\")\n#NOTE: something in these breaks the stat_ecdf in ggplot2\n\n#PREP DATA \ndf <- df_subjects %>%\n  dplyr::select(s_SCALED, pretty_condition) %>%\n  mutate(\n    data = as.numeric(s_SCALED),\n    #flip order levels to correctly orient graph\n    # gr = recode_factor(pretty_condition, \"impasse\" = \"impasse\", \"control\"=\"control\")\n    gr = as.character(pretty_condition)\n  ) %>% dplyr::select(data,gr)\n\n\ng1 <- df %>% filter(gr == \"control\") %>% dplyr::pull(data)\ng2 <- df %>% filter(gr == \"impasse\") %>% dplyr::pull(data)\n\n\n#COMPARE DISTRIBUTIONS WITH ROBUST TESTS\n\n#What do common tests say about the difference?\n\n# Kolmogorov-Smirnov test\n#If y is numeric, a two-sample (Smirnov) test of the null hypothesis that x and y \n#were drawn from the same continuous distribution is performed. Alternatively, y ...\n\n#null is X is drawn from CDF EQUAL TO Y\nks.test(g1,g2) \n\n\nWarning in ks.test.default(g1, g2): p-value will be approximate in the presence\nof ties\n\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  g1 and g2\nD = 0.5, p-value <2e-16\nalternative hypothesis: two-sided\n\n\nCODE\nprint(\"SUGGESTS that impasse and control come from different population distributions\")\n\n\n[1] \"SUGGESTS that impasse and control come from different population distributions\"\n\n\nCODE\n# #null is X is NOT LESS THAN Y\nks.test(g1,g2, alternative = \"greater\") \n\n\nWarning in ks.test.default(g1, g2, alternative = \"greater\"): p-value will be\napproximate in the presence of ties\n\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  g1 and g2\nD^+ = 0.5, p-value <2e-16\nalternative hypothesis: the CDF of x lies above that of y\n\n\nCODE\nprint(\"SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]\")\n\n\n[1] \"SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]\"\n\n\nCODE\n#REGULAR T-TEST\nt.test(g1,g2) # regular Welsh t-test\n\n\n\n    Welch Two Sample t-test\n\ndata:  g1 and g2\nt = -7, df = 325, p-value = 7e-12\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -8.93 -5.06\nsample estimates:\nmean of x mean of y \n   -6.427     0.567 \n\n\n\n\nCODE\n#IF THIS ERRORS, consider loadling plyr (older than dplyr)\n# kernel density estimate + rug plot + superimposed deciles\nkde <- plot.kde_rug_dec2(df)\n\n\nLoading required package: plyr\n\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\n\nThe following object is masked from 'package:vcdExtra':\n\n    summarise\n\n\nThe following object is masked from 'package:ggpubr':\n\n    mutate\n\n\nThe following objects are masked from 'package:Hmisc':\n\n    is.discrete, summarize\n\n\nCODE\n# kde\n\n# compute shift function\nout <- shifthd( g1, g2, nboot=200)\n\n# plot shift function\nsf <- plot.sf(data=out) # function from rgar_visualisation.txt\n# sf\n\n# combine KDE + SF\ncowplot::plot_grid(kde, sf, labels=c(\"A\", \"B\"), ncol = 1, nrow = 2, rel_heights = c(1.5, 1),label_size = 18,hjust = -1,scale=.95)"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1b-test-phase-interpretation-state",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1b-test-phase-interpretation-state",
    "title": "6  Hypothesis Testing",
    "section": "H1B | TEST PHASE INTERPRETATION STATE",
    "text": "H1B | TEST PHASE INTERPRETATION STATE\nDo Ss in the IMPASSE condition offer less-orthogonal interpretations across the test phase questions?\nWhile absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn’t allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject’s response, and determine if these interpretations differ by experimental condition. State is a 3-category variable that groups the following interpretations:\n\n“orthogonal-like” [reference category] includes orthogonal and satisficing responses ==> indicates a primarily orthogonal state of coordinate system understanding\n“unknown” includes: blank, reference point, responses that can’t be classified (including selecting all datapoints) => indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly not orthogonal nor triangular\n“triangle-like” includes correct triangular and ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n\n\n\n\n\n\n\n\nResearch Question\nDoes Ss in the impasse condition produce less orthogonal responses across questions in the test phase?\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of producing unknown and triangle-like response states across test phase items\n\n\nData\n\ndata: df_items where q nin 1,2,3,4,5,6,9 (8 discriminant test phase items)\noutcome: state ( 3 level factor from high_interpretation )\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMIXED Multinomial (Logistic) Regression on state predicted by condition\n\nAlternative:\n\nMIXED Ordinal regression on state (doesn’t meet proportional odds assumption-I think)\nMIXED Multinomial or Ordinal regression on high_interpretation (some cells are 0, produces problems)\n\n\n\n\n\n\nCODE\n#:::::::: PREP DATA\ndf = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  q = as.factor(q),\n  subject = as.factor(subject)\n)\n\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(title = \"Test Phase Interpretation\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nA proportional bar chart visualizing the proportion of each interpretations type by condition for each data collection modality (left/right facet) reveals that the pattern of responses are similar regardless of the data collection modality, by differ by condition. In the impasse condition, there are more triangular responses than in control. In the impasse condition, there are also more positive transition (i.e. triangle-like) and neutral (ie. blank or uncertain response types) than in the control condition.\n\n(LAB)\n\n\nCODE\n#:::::::: IN PERSON ONLY\ndf <- df_items %>% filter(mode == \"lab-synch\") %>% filter(q %nin% c(1,2,3,4,5,6,9)) \n\n#::::::::::::DESCRIPTIVES\n\ntable(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse    Sum\n  orth-like  0.7077  0.4297 0.5665\n  unknown    0.0605  0.1191 0.0903\n  tri-like   0.2319  0.4512 0.3433\n  Sum        1.0000  1.0000 1.0000\n\n\nCODE\n(t <- table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n           \n            control impasse  Sum\n  orth-like     351     220  571\n  unknown        30      61   91\n  tri-like      115     231  346\n  Sum           496     512 1008\n\n\n\nMIXED MULTINOMIAL REGRESSION\nDoes condition affect the response state of of items in the test phase?\nFit a MIXED logistic regression predicting interpretation state (k=3) by condition(k = 2).\n\nCan use mclogit mblogit() with random effect or bayesian brms package b/c nlme, lme4 don’t support random effects on multinomial (ie no categorical family on glmer())\nAlternative would be to manually run 2 X binomial mixed models [should compare outcomes]\n2 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) [essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing [reference category] vs [this category])\nFor each equation:\n\n\\(\\beta_{0}\\) = Log Odds of [this category type vs. reference category type) response in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of [this category type vs. reference category type] response in CONTROL condition\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for [this category] type response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of [this. vs reference category type] response in IMPASSE (vs) CONTROL\nTwo-tailed NHST Null hypothesis: \\(\\beta_{impasse} = 0\\) the odds for [this category of response vs. reference] are not different for IMPASSE condition\nAlternative hypothesis: \\(\\beta_{impasse} \\ne 0\\) the odds of [this category of response vs. reference] increases or decreases for IMPASSE condition\n\n\n\nFit Model [mblogit]\n\n\nCODE\n#https://www.elff.eu/software/mclogit/manual/mblogit/\n#\"baseline category logit\" model matches multinom()\n\n#check reference level \nprint(\"Categories (first is reference)\")\n\n\n[1] \"Categories (first is reference)\"\n\n\nCODE\nlevels(df$state)\n\n\n[1] \"orth-like\" \"unknown\"   \"tri-like\" \n\n\nCODE\n#FIT EMPTY MODEL\n# print(\"EMPTY MODEL\")\nm.mbl0 <- mblogit(state ~ pretty_condition ,  #no random intercepts; fixed only model \n                  data = df)\n\n\n\nIteration 1 - deviance = 1754 - criterion = 0.413\nIteration 2 - deviance = 1746 - criterion = 0.00446\nIteration 3 - deviance = 1746 - criterion = 0.0000478\nIteration 4 - deviance = 1746 - criterion = 8.67e-09\nconverged\n\n\nCODE\n#summary(m.mbl0)\n\n#FIT PREDICTOR MODEL\n# print(\"PREDICTOR MODEL\")\nm.mbl1 <- mblogit(state ~ pretty_condition , \n                  random = ~ 1 | subject , \n                  data = df)\n\n\n\nIteration 1 - deviance = 1159 - criterion = 0.757\nIteration 2 - deviance = 985 - criterion = 0.0458\nIteration 3 - deviance = 911 - criterion = 0.00786\nIteration 4 - deviance = 913 - criterion = 0.00183\nIteration 5 - deviance = 853 - criterion = 0.0256\nIteration 6 - deviance = 852 - criterion = 0.0172\nIteration 7 - deviance = 908 - criterion = 0.00226\nIteration 8 - deviance = 890 - criterion = 0.0308\nIteration 9 - deviance = 899 - criterion = 0.0106\nIteration 10 - deviance = 917 - criterion = 0.00119\nIteration 11 - deviance = 893 - criterion = 0.00737\nIteration 12 - deviance = 876 - criterion = 0.0188\nIteration 13 - deviance = 877 - criterion = 0.00489\nIteration 14 - deviance = 908 - criterion = 0.000375\nIteration 15 - deviance = 917 - criterion = 0.000056\nIteration 16 - deviance = 920 - criterion = 0.0000127\nIteration 17 - deviance = 921 - criterion = 0.00000303\nIteration 18 - deviance = 921 - criterion = 6.77e-07\nIteration 19 - deviance = 922 - criterion = 1.44e-07\nIteration 20 - deviance = 922 - criterion = 3e-08\nIteration 21 - deviance = 922 - criterion = 6.28e-09\nconverged\n\n\nCODE\n# summary(m.mbl1)\n\n#COMPARE MODEL FIT\npaste(\"AIC wth predictor is lower than empty model?\", AIC(m.mbl0) > AIC(m.mbl1))\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(m.mbl0, m.mbl1)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |    Chi2 |      p\n---------------------------------------------------\nm.mbl0 |  mblogit |  4 |         |         |       \nm.mbl1 | mmblogit |  7 |       3 | 1579.53 | < .001\n\n\nCODE\n#DESCRIBE MODEL\nsummary(m.mbl1)\n\n\nWarning in sqrt(diag(vcov.phi)): NaNs produced\n\n\n\nCall:\nmblogit(formula = state ~ pretty_condition, data = df, random = ~1 | \n    subject)\n\nEquation for unknown vs orth-like:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -2.436      0.294   -8.29  < 2e-16 ***\npretty_conditionimpasse    1.547      0.395    3.92 0.000089 ***\n\nEquation for tri-like vs orth-like:\n                        Estimate Std. Error z value  Pr(>|z|)    \n(Intercept)               -2.170      0.481   -4.51 0.0000065 ***\npretty_conditionimpasse    2.153      0.653    3.30   0.00098 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Co-)Variances:\nGrouping level: subject \n           Estimate      Std.Err. \nunknown~1   2.26          NaN     \ntri-like~1  3.36 10.05   1.57 4.61\n\nNull Deviance:     2210 \nResidual Deviance: 922 \nNumber of Fisher Scoring iterations:  21\nNumber of observations\n  Groups by subject: 126\n  Individual observations:  1008\n\n\nCODE\n#INTERPRET COEFFICIENTS\ncint <- confint(m.mbl1, level = 0.95)\nprint(\"ODDS RATIO\")\n\n\n[1] \"ODDS RATIO\"\n\n\nCODE\n(e <- cbind( exp(coef(m.mbl1)), exp(cint))) #exponentiated, adjusted\n\n\n                                         2.5 % 97.5 %\nunknown~(Intercept)              0.0875 0.0491  0.156\ntri-like~(Intercept)             0.1142 0.0445  0.294\nunknown~pretty_conditionimpasse  4.6963 2.1659 10.183\ntri-like~pretty_conditionimpasse 8.6136 2.3928 31.007\n\n\nCODE\n#PERFORMANCE\nperformance(m.mbl1)\n\n\n# Indices of model performance\n\nAIC      |      BIC |  RMSE | Sigma\n-----------------------------------\n7974.169 | 8008.579 | 0.245 | 0.958\n\n\nCODE\n#TABLE\ntab_model(m.mbl1, transform = \"exp\", title = \"Model Predicted Odds Ratio\")\n\n\n\nModel Predicted Odds Ratio\n\n \nstate\n\n\nPredictors\nEstimates\nCI\np\n\n\nunknown~(Intercept)\n0.09\n0.05 – 0.16\n<0.001\n\n\ntri-like~(Intercept)\n0.11\n0.04 – 0.29\n<0.001\n\n\nunknown~prettyconditionimpasse\n4.70\n2.17 – 10.18\n<0.001\n\n\ntri-like~prettyconditionimpasse\n8.61\n2.39 – 31.01\n0.001\n\n\n\nN subject\n126\n\nObservations\n1008\n\n\n\n\n\n\n\n\nInference\n\nBeing in the IMPASSE condition increases the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 4.7 (z = 3.92, p < 0.001) . Participants in the impasse condition were 4.7x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 8.61 (z = 3.30, p < 0.001 ). Participants in the impasse condition were more than 8.6x as likely to give an a triangular response rather than an orthogonal response compared to participants in control.\nAs with the (binary) logistic regression on accuracy ~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 in the control condition. (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)\nTODO: these estimates seem low to me, given the estimates for the Q1 only model. Also different from brms estimates (below) Suspect NaNs error thrown with mblogit() may be relevant\n\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m.mbl1, vline.color = \"red\", \n           transform = \"exp\", #for some reason have to manually add for mixed?\n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.05, #can manually adjust to account for directional test\n           ci.lvl = 0.95) +  #can manually adjusted for directional test   \n  labs(title = \"ODDS RATIO | Test Phase State ~ condition\",\n       subtitle = \"(p for  two-tailed test)\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m.mbl1)\n\n\n\n\n \nstate\n\n\nPredictors\nEstimates\nCI\np\n\n\nunknown~(Intercept)\n-2.44\n-3.01 – -1.86\n<0.001\n\n\ntri-like~(Intercept)\n-2.17\n-3.11 – -1.23\n<0.001\n\n\nunknown~prettyconditionimpasse\n1.55\n0.77 – 2.32\n<0.001\n\n\ntri-like~prettyconditionimpasse\n2.15\n0.87 – 3.43\n0.001\n\n\n\nN subject\n126\n\nObservations\n1008\n\n\n\n\n\n\n\n\nDiagnostics\n\n\nCODE\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(m.mbl1)\n\n\n# Indices of model performance\n\nAIC      |      BIC |  RMSE | Sigma\n-----------------------------------\n7974.169 | 8008.579 | 0.245 | 0.958\n\n\nCODE\n#General Goodness of Fit\n#library(generalhoslem)\n#logitgof(df$state, catm$fitted.values, g = 3)\n#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).\n#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables\n\n# print(\"MODEL DIAGNOSTICS\")\n# check_model(m.mbl1) can't do overall diagnostics, have to do them on individual model equations\n\n\n\n\nFit Model [brms]\n\n\nCODE\n#library(brms) #bayesian mixed regression models\n\n#BAYESIAN MIXED VERSION\nmixcat.1 <- brm( state ~ condition + (1|subject), data = df, family = \"categorical\",\n                                          file = \"analysis/models/sgc3a_brms_mixedcat_teststate_LAB.rds\") # cache model (can be removed)))\n\n#DESCRIBE MODEL\nsummary(mixcat.1)\n\n\n Family: categorical \n  Links: muunknown = logit; mutrilike = logit \nFormula: state ~ condition + (1 | subject) \n   Data: df (Number of observations: 1008) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~subject (Number of levels: 126) \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muunknown_Intercept)     1.89      0.32     1.32     2.61 1.00     1272\nsd(mutrilike_Intercept)     5.66      0.80     4.30     7.43 1.00     1292\n                        Tail_ESS\nsd(muunknown_Intercept)     2509\nsd(mutrilike_Intercept)     1952\n\nPopulation-Level Effects: \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nmuunknown_Intercept       -3.45      0.43    -4.35    -2.68 1.00     2508\nmutrilike_Intercept       -4.17      0.95    -6.22    -2.48 1.00      996\nmuunknown_condition121     1.91      0.53     0.90     2.97 1.00     2180\nmutrilike_condition121     3.82      1.19     1.58     6.26 1.01      827\n                       Tail_ESS\nmuunknown_Intercept        2751\nmutrilike_Intercept        1434\nmuunknown_condition121     2675\nmutrilike_condition121     1475\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n# report(mixcat.1)\n\n#VISUALIZE\n\nplot(mixcat.1)\n\n\n\n\n\n\n\n\nCODE\nplot_model(mixcat.1)\n\n\n\n\n\nCODE\ncheck_posterior_predictions(mixcat.1, draws=1000)\n\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\nCODE\nlibrary(bayesplot)\n\n\nThis is bayesplot version 1.9.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\nCODE\nlibrary(bayestestR)\n\n\n\nAttaching package: 'bayestestR'\n\n\nThe following object is masked from 'package:gmodels':\n\n    ci\n\n\nThe following object is masked from 'package:ggdist':\n\n    hdi\n\n\nCODE\nplot(rope(mixcat.1, ci = 0.89))\n\n\nPossible multicollinearity between b_mutrilike_condition121 and b_mutrilike_Intercept (r = 0.75). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\n\n\n\nCODE\n#PERFORMANCE\nperformance(mixcat.1)\n\n\n'bayes_R2' is not defined for unordered categorical models.\n\n\n# Indices of model performance\n\nELPD     | ELPD_SE |   LOOIC | LOOIC_SE |    WAIC\n-------------------------------------------------\n-449.046 |  27.678 | 898.092 |   55.355 | 877.628\n\n\nCODE\n#TABLE\ntab_model(mixcat.1) #, transform = \"exp\", title = \"Model Predicted Odds Ratio\")\n\n\n'bayes_R2' is not defined for unordered categorical models.\n\n\n\n\n \nstate: unknown\nstate: trilike\n\n\nPredictors\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\n\n\nIntercept\n0.02\n0.00 – 0.08\n0.03\n0.01 – 0.07\n\n\nID indicates randomlyassignedcondition(111->control,121->impasse):condition 121\n43.86\n4.87 – 521.75\n6.64\n2.47 – 19.42\n\n\nRandom Effects\n\n\n\nσ2\n0.21\n\n\n\nτ00\n0.65\n\n\nICC\n0.25\n\n\nN subject\n126\n\nObservations\n1008\n\n\n\n\n\n\n\n\nInference\n\nBeing in the IMPASSE condition increases the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 43.86.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 6.64.\nTODO RECONCILE:: brms model provides substantially higher estimate for blank/uncertain response (vs) the mblogit frequentist model. I think these should be similar, as they were for the non-mixed veresions. Suspect NaNs error thrown with mblogit() may be relevant\n\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(mixcat.1, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.05, #can manually adjust to account for directional test\n           ci.lvl = 0.95 ) +  #can manually adjusted for directional test   \n  labs(title = \"ODDS RATIO | Test Phase State ~ condition\",\n       subtitle = \"(p for one two test)\")\n\n\n\n\n\nCODE\nresult <- estimate_density(mixcat.1)\nplot(result, stack = FALSE, priors = TRUE)\n\n\nWarning: 'b_muunknown_condition121', or one of its 'at' groups, is empty and has\n  no density information.\n\n\nWarning: 'b_mutrilike_condition121', or one of its 'at' groups, is empty and has\n  no density information.\n\n\n\n\n\nCODE\nresult <- describe_posterior(mixcat.1)\n\n\nPossible multicollinearity between b_mutrilike_condition121 and b_mutrilike_Intercept (r = 0.75). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nCODE\nplot(result, stack = FALSE, priors = TRUE)\n\n\n\n\n\nCODE\nresult <- p_direction(mixcat.1)\nplot(result, stack = FALSE)\n\n\n\n\n\nCODE\nprior_summary(mixcat.1)\n\n\n                prior     class         coef   group resp      dpar nlpar lb ub\n               (flat)         b                           mutrilike            \n               (flat)         b condition121              mutrilike            \n               (flat)         b                           muunknown            \n               (flat)         b condition121              muunknown            \n student_t(3, 0, 2.5) Intercept                           mutrilike            \n student_t(3, 0, 2.5) Intercept                           muunknown            \n student_t(3, 0, 2.5)        sd                           mutrilike        0   \n student_t(3, 0, 2.5)        sd                           muunknown        0   \n student_t(3, 0, 2.5)        sd              subject      mutrilike        0   \n student_t(3, 0, 2.5)        sd    Intercept subject      mutrilike        0   \n student_t(3, 0, 2.5)        sd              subject      muunknown        0   \n student_t(3, 0, 2.5)        sd    Intercept subject      muunknown        0   \n       source\n      default\n (vectorized)\n      default\n (vectorized)\n      default\n      default\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n\n\nCODE\nhypothesis(mixcat.1, \"muunknown_condition121 > 0 \")\n\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (muunknown_condit... > 0     1.91      0.53     1.06     2.79       3999\n  Post.Prob Star\n1         1    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\n\nDiagnostics\n\n\nTODO\n\npriors? used default flat priors… ok?\nposterior predictive checks\ndiagnostics on random effects\nreconcilliation of mblogit() vs brms versions of the model; seems like they should yield similar estimates"
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html",
    "title": "7  Introduction",
    "section": "",
    "text": "TODO UPDATE ALL\nIn Study 3B we compare the efficacy of the explicit [interaction] scaffold and the implicit [impasse] scaffold."
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html#methods",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html#methods",
    "title": "7  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Scaffold: control,impasse)\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 16.1. The list of questions can be found here.\n\n\n\nFigure 7.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nNote that across both control and impasse conditions, both the question, response options and graph structure were identical. The experimental manipulation (posing a mental impasse) was accomplished by changing the position of datapoints in the impasse-condition graph, such that for any given question, there was no available response option if the reader were to interpret the graph as cartesian (making an orthogonal rather than diagonal projection from the x-axis.)\nThe green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. We hypothesize that this presents the reader with an obstacle, at which point they are forced to confront their interpretation of the coordinate system and (ideally) develop a new strategy.\n\n\n\nFigure 7.2: Sample Question (Q=1) graphs for each condition\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items.\n(3B) The first five items in the task are defined as the SCAFFOLDING block. In the IMPASSE condition, the first five questions included an IMPASSE problem state. For participants in the CONTROL condition, the dataset was structure such that there was always an available ‘orthogonal answer’ for the first 5 questions.\n(3B) The remaining 10 items are defined as the TESTING block. In both conditions, these questions were not structured as impasse (i.e. contained an available orthogonal answer)\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData was collected by convenience sample of a university subject pool. Initial data (Fall 2017, Spring 2018) were collected in-person, with large groups of students simultaneously completing the study (independently) in a computer lab. In Fall 2021 and Winter 2022 we collected additional data to replicate results in a remote format (students completing the study asynchronously on their own computers)."
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html#analysis",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html#analysis",
    "title": "7  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\nBefore analysis, data files from individual data collection periods are harmonized into a common data format.\n\n\n\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nspring17_clean_data.Rmd  spring18_clean_data.Rmd  fall21_clean_data.Rmd  winter2022_clean_sgc3b.Rmd\n2_sgc3B_scoring.qmd\n\n\n\nData for study SGC_3B were collected across four time periods, interrupted by the Covid-19 pandemic.\n\n\n\nPeriod\nModality\n\n\n\n\nFall 2017\nin person, SONA groups in computer lab\n\n\nSpring 2018\nin person, SONA groups in computer lab\n\n\nFall 2021\nasynchronous, online, SONA\n\n\n\nData collected in Fall 2017, Spring 2018 constitute the original SGC_3B study, conducted in person. Data collected in Fall 2021 constitute the web-based replication, conducted online (asynchronously). In all cases, the experiment was administered via a web application.\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3B/data/0-session-level/fall17_sgc3b_participants.csv\"\nspring18 <- \"analysis/SGC3B/data/0-session-level/spring18_sgc3b_participants.csv\"\nfall21 <- \"analysis/SGC3B/data/0-session-level/fall21_sgc3b_participants.csv\"\nmeta <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_participants.csv\" #FOR SCHEMA ONLY\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\nmeta <- read_csv(meta)\n\n#SAVE METADATA FROM SGC3A, but no rows \ndf_subjects <- meta %>% filter(condition=='X') %>% \n  dplyr::select(\n  subject,condition,term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m, \n  # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n  #exploratory factors\n  violations, browser, width, height\n)\n\n#COMPARE COLS\n# janitor::compare_df_cols(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21,meta)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\",\n    violations = \"NULL\",\n    browser = \"NULL\",\n    width = \"NULL\",\n    height = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, \n                # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                #exploratory factors\n                violations, browser, width, height)\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\"),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#CLEANUP\nrm(df_subjects_fall17,df_subjects_fall21, df_subjects_spring18, df_subjects_before)\nrm(fall17,fall21,spring18,meta)\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3B/data/0-session-level/fall17_sgc3b_blocks.csv\"\nspring18 <- \"analysis/SGC3B/data/0-session-level/spring18_sgc3b_blocks.csv\"\nfall21 <- \"analysis/SGC3B/data/0-session-level/fall21_sgc3b_blocks.csv\"\nmeta <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_items.rds\" #FOR SCHEMA ONLY\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\nmeta <- read_rds(meta) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- meta %>% group_by(q) %>% dplyr::select(q,relation) %>% unique()\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- meta %>% filter(condition=='X') %>% dplyr::select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n  \n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18, df_items_fall21) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  dplyr::select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n\n#COMPARE COLS\njanitor::compare_df_cols(df_items_before, df_items)\n\n\n  column_name df_items_before  df_items\n1      answer       character character\n2   condition         numeric    factor\n3     correct         logical    factor\n4        mode       character    factor\n5           q         numeric    factor\n6    question       character character\n7        rt_s         numeric   numeric\n8     subject       character    factor\n9        term       character    factor\n\n\nCODE\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% dplyr::select(-question,-correct,-rt_s,-num_o)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n  question = \"Please describe how to determine what event(s) start at 12pm?\",\n  response = as.character(response) #doesn't need to be factor\n)\n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#add back pretty condition \ndf_items <- df_items %>% mutate(\n  pretty_condition = recode_factor(condition, \n                                   \"111\" = \"none-none\", \"121\" =  \"none-impasse\", \n                                   \"211\" = \"img-none\", \"221\" =  \"img-impasse\", \n                                   \"311\" = \"ixv-none\", \"321\" =  \"ixv-impasse\", \n                                   ),\n  pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_items_fall17,df_items_fall21, df_items_spring18, df_items_before)\nrm(fall17,fall21,spring18,meta, map_relations)\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3B/data/1-study-level/sgc3b_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3B/data/1-study-level/sgc3b_items.csv\", row.names = FALSE)\nwrite.csv(df_freeresponse,\"analysis/SGC3B/data/1-study-level/sgc3b_freeresponse.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3B/data/1-study-level/sgc3b_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3B/data/1-study-level/sgc3b_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html#resources",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html#resources",
    "title": "7  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.3        bit64_4.0.5       vroom_1.5.7       jsonlite_1.8.0   \n [5] viridisLite_0.4.0 modelr_0.1.8      assertthat_0.2.1  highr_0.9        \n [9] cellranger_1.1.0  yaml_2.3.5        pillar_1.7.0      backports_1.4.1  \n[13] glue_1.6.2        digest_0.6.29     rvest_1.0.2       snakecase_0.11.0 \n[17] colorspace_2.0-3  htmltools_0.5.2   pkgconfig_2.0.3   broom_0.8.0      \n[21] labelled_2.9.1    haven_2.5.0       scales_1.2.0      webshot_0.5.3    \n[25] svglite_2.1.0     openxlsx_4.2.5    rio_0.5.29        tzdb_0.3.0       \n[29] generics_0.1.2    ellipsis_0.3.2    withr_2.5.0       janitor_2.1.0    \n[33] cli_3.3.0         magrittr_2.0.3    crayon_1.5.1      readxl_1.4.0     \n[37] evaluate_0.15     fs_1.5.2          fansi_1.0.3       xml2_1.3.3       \n[41] foreign_0.8-82    tools_4.2.1       data.table_1.14.2 hms_1.1.1        \n[45] lifecycle_1.0.1   munsell_0.5.0     reprex_2.0.1      zip_2.2.0        \n[49] compiler_4.2.1    systemfonts_1.0.4 rlang_1.0.3       grid_4.2.1       \n[53] rstudioapi_0.13   htmlwidgets_1.5.4 rmarkdown_2.14    gtable_0.3.0     \n[57] DBI_1.1.3         curl_4.3.2        R6_2.5.1          lubridate_1.8.0  \n[61] knitr_1.39        fastmap_1.1.0     bit_4.0.4         utf8_1.2.2       \n[65] stringi_1.7.6     parallel_4.2.1    Rcpp_1.0.8.3      vctrs_0.4.1      \n[69] dbplyr_2.2.1      tidyselect_1.1.2  xfun_0.31"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html",
    "title": "8  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC3B study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#score-sgc-data",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#score-sgc-data",
    "title": "8  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n# backup <- read_rds('analysis/SGC3B/data/1-study-level/sgc3b_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC3B/data/1-study-level/sgc3b_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#summarize-by-subject",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#summarize-by-subject",
    "title": "8  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC3B/data/1-study-level/sgc3b_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#explore-distributions",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#explore-distributions",
    "title": "8  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"Impasse Condition (blue) yields more correct responses across the entire task\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(pretty_condition~q) + \n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields more correct responses on each item\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>% \ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\", \n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher total absolute scores\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE TEST PHASE\ngf_histogram(~item_test_NABS, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Absolute Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores across the entire task\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(q~pretty_condition) + \n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores on each item\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) + \n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>% \n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\", \n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher cumulative scaled scores\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE TEST PHASE\ngf_histogram(~item_test_SCALED, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Scaled Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Scaled Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\nTODO: INVESTIGATE if some of the scores assigned to 0 should be assigned to -0.5 to balance\nTODO: INVESTIGATE DISTRIBUTIONS of each subscore type\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"Impasse condition (blue) yields fewer Orthogonal and more Triangular responses\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more Triangular responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more positive trending responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"Impasse shifts density toward higher Triagular scores\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"Impasse shifts density toward lower Orthogonal scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#export",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#export",
    "title": "8  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3B/data/2-scored-data/sgc3b_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3B/data/2-scored-data/sgc3b_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC3B/data/2-scored-data/sgc3b_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC3B/data/2-scored-data/sgc3b_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures\n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3B/data/2-scored-data/sgc3b_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3B/data/2-scored-data/sgc3b_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#resources",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#resources",
    "title": "8  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     \n [5] readr_2.1.2      tidyr_1.2.0      tibble_3.1.7     tidyverse_1.3.1 \n [9] Hmisc_4.7-0      Formula_1.2-4    survival_3.3-1   lattice_0.20-45 \n[13] pbapply_1.5-0    ggformula_0.10.1 ggridges_0.5.3   scales_1.2.0    \n[17] ggstance_0.3.5   ggplot2_3.3.6    kableExtra_1.3.4\n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            bit64_4.0.5         lubridate_1.8.0    \n [4] webshot_0.5.3       RColorBrewer_1.1-3  httr_1.4.3         \n [7] tools_4.2.1         backports_1.4.1     utf8_1.2.2         \n[10] R6_2.5.1            rpart_4.1.16        DBI_1.1.3          \n[13] colorspace_2.0-3    nnet_7.3-17         withr_2.5.0        \n[16] tidyselect_1.1.2    gridExtra_2.3       curl_4.3.2         \n[19] bit_4.0.4           compiler_4.2.1      cli_3.3.0          \n[22] rvest_1.0.2         htmlTable_2.4.0     xml2_1.3.3         \n[25] labeling_0.4.2      mosaicCore_0.9.0    checkmate_2.1.0    \n[28] systemfonts_1.0.4   digest_0.6.29       foreign_0.8-82     \n[31] rmarkdown_2.14      svglite_2.1.0       rio_0.5.29         \n[34] base64enc_0.1-3     jpeg_0.1-9          pkgconfig_2.0.3    \n[37] htmltools_0.5.2     labelled_2.9.1      dbplyr_2.2.1       \n[40] fastmap_1.1.0       readxl_1.4.0        htmlwidgets_1.5.4  \n[43] rlang_1.0.3         rstudioapi_0.13     farver_2.1.0       \n[46] generics_0.1.2      jsonlite_1.8.0      vroom_1.5.7        \n[49] zip_2.2.0           magrittr_2.0.3      Matrix_1.4-1       \n[52] Rcpp_1.0.8.3        munsell_0.5.0       fansi_1.0.3        \n[55] lifecycle_1.0.1     stringi_1.7.6       yaml_2.3.5         \n[58] MASS_7.3-57         plyr_1.8.7          grid_4.2.1         \n[61] parallel_4.2.1      crayon_1.5.1        haven_2.5.0        \n[64] splines_4.2.1       hms_1.1.1           knitr_1.39         \n[67] pillar_1.7.0        reprex_2.0.1        glue_1.6.2         \n[70] evaluate_0.15       latticeExtra_0.6-29 data.table_1.14.2  \n[73] modelr_0.1.8        tzdb_0.3.0          png_0.1-7          \n[76] vctrs_0.4.1         tweenr_1.0.2        cellranger_1.1.0   \n[79] gtable_0.3.0        polyclip_1.10-0     assertthat_0.2.1   \n[82] openxlsx_4.2.5      xfun_0.31           ggforce_0.3.3      \n[85] broom_0.8.0         viridisLite_0.4.0   cluster_2.1.3      \n[88] ellipsis_0.3.2     \n\n\nTODO sample from sgc3a to combine with sgc3b ::: {.cell}\n\nCODE\n# \n# table(df_subjects$condition)\n# \n# set.seed(8)\n# r111  <- df_subjects %>% filter(condition == \"111\") %>% sample(50) %>% dplyr::select(-orig.id)\n# r121  <- df_subjects %>% filter(condition == \"121\") %>% sample(50) %>% dplyr::select(-orig.id)\n# df <- df_subjects %>% filter(condition %nin% c(111,121)) \n# df <- rbind(df,r111,r121)\n# \n# df <- df %>% mutate(\n#   explicit = as_factor(str_sub(condition,1,1)),\n#   explicit = recode_factor(explicit, \"1\" = \"none\", \"2\" = \"image\", \"3\"=\"ixv\"),\n#   impasse = str_sub(condition,2,2),\n#   impasse = recode_factor(impasse, \"1\" = \"none\", \"2\"=\"impasse\")\n# )\n# \n# table(df$explicit , df$impasse)\n# table(df$condition)\n# \n# gf_histogram(~s_NABS, data = df) %>% \n#   gf_facet_grid(df$explicit~df$impasse)\n# \n# m <- lm(s_NABS ~ explicit+impasse+explicit*impasse, data = df)\n# summary(m)\n# summ(m)\n# check_model(m)\n# \n# gf_boxplot(~s_NABS, data = df) %>% gf_facet_grid(explicit~impasse)\n# \n# library(interactions)\n# library(jtools) #nice presentation of restuls\n# summ(m)\n# cat_plot(model = m, pred=explicit, modx = impasse, geom=\"line\")+\n#   lims(y=c(0,13))\n# \n# \n# means <- favstats(s_NABS~condition, data = df)\n# means\n# \n# x <- favstats(absolute_score~condition , data = df4c)\n# x\n# \n# means <- rbind(means,x)\n# means\n# \n# df <- df %>% dplyr::select(s_NABS, condition) %>% rename(score=s_NABS)\n# c4 <- df4c %>% dplyr::select(absolute_score, condition) %>% rename(score = absolute_score)\n# \n# df <- rbind(df,c4)\n# gf_boxplot(score ~ condition, data = df)\n\n:::"
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html",
    "title": "9  Introduction",
    "section": "",
    "text": "In Study 4A we explore the extent to which the design of the axes and gridlines of the graph influence how a reader interprets its underlying coordinate system.\nExperimental Hypothesis:\nWe hypothesize that the design of the major axes (specifically orthogonal) axes establish for the learner the basis of the coordinate system. Differently oriented axes should lead the reader to be more open to alternative coordinate systems.\nExploratory Questions"
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html#methods",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html#methods",
    "title": "9  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 4 levels (Graphical Framework: ORTH-FULL, ORTH-SPARSE, ORTH-GRID, TRI-SPARSE) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Graphical Framework: ORTH-FULL, ORTH-SPARSE, ORTH-GRID, TRI-SPARSE)\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 16.1. The list of questions can be found here.\n\n\n\nFigure 9.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items : the Graph Comprehension Task\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData were collected by convenience sample of a university subject pool during the winter of 2022. Participants accessed the study via a web browser (asynchronously). The stimulus application required the participant stay in full-screen mode for the entirety of the study."
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html#analysis",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html#analysis",
    "title": "9  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nwinter2022_clean_sgc4a.Rmd\n2_sgc4A_scoring.qmd\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC4A/data/0-session-level/fall17_sgc4a_participants.csv\"\nspring18 <- \"analysis/SGC4A/data/0-session-level/spring18_sgc4a_participants.csv\"\nwinter22 <- \"analysis/SGC4A/data/0-session-level/winter22_sgc4a_participants.rds\"\nsummer22 <- \"analysis/SGC4A/data/0-session-level/su22_sgc4a_participants.rds\"\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\ndf_subjects_summer22 <- read_rds(summer22) #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% \n  dplyr::select(\n  subject,condition,pretty_condition, term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m, \n  # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n  #exploratory factors\n  violations, browser, width, height\n)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    condition = as.factor(condition),\n    pretty_condition = \"NULL\",\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\",\n    violations = \"NULL\",\n    browser = \"NULL\",\n    width = \"NULL\",\n    height = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, pretty_condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, \n                # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                #exploratory factors\n                violations, browser, width, height)\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_winter22_q16 <- df_subjects_winter22 %>% \n  dplyr::select(subject, condition, pretty_condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% dplyr::select(-explanation)\n\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects_winter22 <- df_subjects_winter22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors \n                 violations, browser, width, height)\n\ndf_subjects_summer22 <- df_subjects_summer22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors \n                 violations, browser, width, height)\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#compare dataframe columns\n# janitor::compare_df_cols(df_subjects, df_subjects_winter22, df_subjects_before)\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_winter22, df_subjects_summer22, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\"),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\",\"summer22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#REFACTOR CONDITIONS\ndf_subjects <- df_subjects %>% mutate(\n    condition = recode_factor(condition, \"11111\" = \"111\", \"112\" = \"112\", \"111\" = \"111\", \"113\" = \"113\", \"114\" = \"114\", \"115\"=\"115\"),\n    pretty_condition = recode_factor(condition, \"111\" = \"Orth-Full\", \"114\" =  \"Orth-Sparse\", \"115\"=\"Orth-Grid\",\"113\"=\"Tri-Sparse\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_subjects_fall17, df_subjects_spring18, df_subjects_winter22,df_subjects_before, df_subjects_summer22)\nrm(fall17,spring18,winter22, summer22)\n\n#FINALLY DROP CONDITION 112 (partial orthog with y axis lines extending only to right end of triangle)\n#this was an incomplete [pilot only] condition collected in FA17 SP18 for pilot purposes\ndf_subjects <- df_subjects %>% filter(condition != \"112\") %>% \n  mutate(\n    condition = droplevels(condition),\n    pretty_condition = droplevels(pretty_condition)\n  )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC4A/data/0-session-level/fall17_sgc4a_blocks.csv\"\nspring18 <- \"analysis/SGC4A/data/0-session-level/spring18_sgc4a_blocks.csv\"\nwinter22 <- \"analysis/SGC4A/data/0-session-level/winter22_sgc4a_items.rds\"\nsummer22 <- \"analysis/SGC4A/data/0-session-level/su22_sgc4a_items.rds\"\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\ndf_items_summer22 <- read_rds(summer22) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- df_items_winter22 %>% group_by(q) %>% select(q,relation) %>% unique()\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- df_items_winter22 %>% filter(condition=='X') %>% select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n\n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n  \n#reduce data collected using new webapp\ndf_items_winter22 <- df_items_winter22 %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\ndf_items_summer22 <- df_items_summer22 %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\n\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_winter22, df_items_summer22, df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\",\"summer22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% select(-question,-correct,-rt_s,-num_o)\n#add data from wi22 [stored on subject data]\ndf_winter22_q16 <- df_winter22_q16 %>% dplyr::select(-pretty_condition)\ndf_freeresponse <- rbind(df_freeresponse, df_winter22_q16)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n    question = \"Please describe how to determine what event(s) start at 12pm?\",\n    response = as.character(response) #doesn't need to be factor\n  ) \n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#REFACTOR CONDITIONS\ndf_items <- df_items %>% mutate(\n    condition = recode_factor(condition, \"11111\" = \"111\", \"112\" = \"112\", \"111\" = \"111\", \"113\" = \"113\", \"114\" = \"114\", \"115\"=\"115\"),\n    pretty_condition = recode_factor(condition, \"111\" = \"Orth-Full\", \"114\" =  \"Orth-Sparse\", \"115\"=\"Orth-Grid\",\"113\"=\"Tri-Sparse\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_items_fall17, df_items_spring18, df_items_winter22, df_items_before, df_winter22_q16, df_items_summer22)\nrm(fall17,spring18,winter22, map_relations, summer22)\n\n#FINALLY DROP CONDITION 112 (partial orthog with y axis lines extending only to right end of triangle)\n#this was an incomplete [pilot only] condition collected in FA17 SP18 for pilot purposes\ndf_items <- df_items %>% filter(condition != \"112\") %>% \n  mutate(\n    condition = droplevels(condition),\n    pretty_condition = droplevels(pretty_condition)\n  )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4A/data/1-study-level/sgc4a_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4A/data/1-study-level/sgc4a_items.csv\", row.names = FALSE)\nwrite.csv(df_freeresponse,\"analysis/SGC4A/data/1-study-level/sgc4a_freeresponse.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4A/data/1-study-level/sgc4a_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4A/data/1-study-level/sgc4a_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html#resources",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html#resources",
    "title": "9  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     \n [5] readr_2.1.2      tidyr_1.2.0      tibble_3.1.7     ggplot2_3.3.6   \n [9] tidyverse_1.3.1  kableExtra_1.3.4 codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] httr_1.4.3        highr_0.9         pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] bit_4.0.4         munsell_0.5.0     broom_0.8.0       compiler_4.2.1   \n[29] modelr_0.1.8      xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4\n[33] htmltools_0.5.2   tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3      \n[37] viridisLite_0.4.0 crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1     \n[41] withr_2.5.0       grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0     \n[45] lifecycle_1.0.1   DBI_1.1.3         magrittr_2.0.3    scales_1.2.0     \n[49] zip_2.2.0         cli_3.3.0         stringi_1.7.6     vroom_1.5.7      \n[53] fs_1.5.2          xml2_1.3.3        ellipsis_0.3.2    generics_0.1.2   \n[57] vctrs_0.4.1       openxlsx_4.2.5    tools_4.2.1       bit64_4.0.5      \n[61] glue_1.6.2        hms_1.1.1         parallel_4.2.1    fastmap_1.1.0    \n[65] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[69] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html",
    "title": "10  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC4A study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#score-sgc-data",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#score-sgc-data",
    "title": "10  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#backup <- read_rds('analysis/SGC4A/data/1-study-level/sgc4a_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC4A/data/1-study-level/sgc4a_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#summarize-by-subject",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#summarize-by-subject",
    "title": "10  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC4A/data/1-study-level/sgc4a_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#explore-distributions",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#explore-distributions",
    "title": "10  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_condition = as.factor(condition),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n                \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(pretty_condition~q) +\n  labs( x = \"Absolute Score\",\n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>%\ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\",\n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(q~pretty_condition) +\n  labs( x = \"Scaled Score\",\n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) +\n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>%\n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\",\n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.05), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.05), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"Impasse shifts density toward higher Triagular scores\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"Impasse shifts density toward lower Orthogonal scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#peeking",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#peeking",
    "title": "10  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\nm1 <- lm(s_SCALED ~ pretty_condition, data = df_subjects)\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ pretty_condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8.34  -6.11  -3.61   1.96  21.37 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   -6.893      0.693   -9.94   <2e-16 ***\npretty_conditionOrth-Sparse    0.740      1.140    0.65    0.517    \npretty_conditionOrth-Grid     -1.479      1.103   -1.34    0.180    \npretty_conditionTri-Sparse     2.237      0.990    2.26    0.024 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.49 on 476 degrees of freedom\nMultiple R-squared:  0.0245,    Adjusted R-squared:  0.0183 \nF-statistic: 3.98 on 3 and 476 DF,  p-value: 0.00806\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n                  Df Sum Sq Mean Sq F value Pr(>F)   \npretty_condition   3    861   287.0    3.98 0.0081 **\nResiduals        476  34308    72.1                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with pretty_condition (formula: s_SCALED ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.02, F(3, 476) = 3.98, p = 0.008, adj. R2 = 0.02). The model's intercept, corresponding to pretty_condition = Orth-Full, is at -6.89 (95% CI [-8.26, -5.53], t(476) = -9.94, p < .001). Within this model:\n\n  - The effect of pretty condition [Orth-Sparse] is statistically non-significant and positive (beta = 0.74, 95% CI [-1.50, 2.98], t(476) = 0.65, p = 0.517; Std. beta = 0.09, 95% CI [-0.18, 0.35])\n  - The effect of pretty condition [Orth-Grid] is statistically non-significant and negative (beta = -1.48, 95% CI [-3.65, 0.69], t(476) = -1.34, p = 0.180; Std. beta = -0.17, 95% CI [-0.43, 0.08])\n  - The effect of pretty condition [Tri-Sparse] is statistically significant and positive (beta = 2.24, 95% CI [0.29, 4.18], t(476) = 2.26, p = 0.024; Std. beta = 0.26, 95% CI [0.03, 0.49])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#export",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#export",
    "title": "10  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4A/data/2-scored-data/sgc4a_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4A/data/2-scored-data/sgc4a_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC4A/data/2-scored-data/sgc4a_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC4A/data/2-scored-data/sgc4a_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4A/data/2-scored-data/sgc4a_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4A/data/2-scored-data/sgc4a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#resources",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#resources",
    "title": "10  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n  [1] TH.data_1.1-1       colorspace_2.0-3    rio_0.5.29         \n  [4] ellipsis_0.3.2      estimability_1.4    htmlTable_2.4.0    \n  [7] parameters_0.18.1   base64enc_0.1-3     fs_1.5.2           \n [10] rstudioapi_0.13     farver_2.1.0        bit64_4.0.5        \n [13] fansi_1.0.3         mvtnorm_1.1-3       lubridate_1.8.0    \n [16] xml2_1.3.3          codetools_0.2-18    splines_4.2.1      \n [19] knitr_1.39          polyclip_1.10-0     jsonlite_1.8.0     \n [22] broom_0.8.0         cluster_2.1.3       dbplyr_2.2.1       \n [25] png_0.1-7           effectsize_0.7.0    ggforce_0.3.3      \n [28] compiler_4.2.1      httr_1.4.3          emmeans_1.7.5      \n [31] backports_1.4.1     assertthat_0.2.1    Matrix_1.4-1       \n [34] fastmap_1.1.0       cli_3.3.0           tweenr_1.0.2       \n [37] htmltools_0.5.2     tools_4.2.1         coda_0.19-4        \n [40] gtable_0.3.0        glue_1.6.2          Rcpp_1.0.8.3       \n [43] cellranger_1.1.0    vctrs_0.4.1         svglite_2.1.0      \n [46] insight_0.17.1      xfun_0.31           openxlsx_4.2.5     \n [49] rvest_1.0.2         lifecycle_1.0.1     mosaicCore_0.9.0   \n [52] zoo_1.8-10          MASS_7.3-57         vroom_1.5.7        \n [55] hms_1.1.1           sandwich_3.0-2      parallel_4.2.1     \n [58] RColorBrewer_1.1-3  curl_4.3.2          yaml_2.3.5         \n [61] gridExtra_2.3       labelled_2.9.1      rpart_4.1.16       \n [64] latticeExtra_0.6-29 stringi_1.7.6       bayestestR_0.12.1  \n [67] checkmate_2.1.0     zip_2.2.0           rlang_1.0.3        \n [70] pkgconfig_2.0.3     systemfonts_1.0.4   evaluate_0.15      \n [73] htmlwidgets_1.5.4   labeling_0.4.2      bit_4.0.4          \n [76] tidyselect_1.1.2    plyr_1.8.7          magrittr_2.0.3     \n [79] R6_2.5.1            generics_0.1.2      multcomp_1.4-19    \n [82] DBI_1.1.3           pillar_1.7.0        haven_2.5.0        \n [85] foreign_0.8-82      withr_2.5.0         datawizard_0.4.1   \n [88] nnet_7.3-17         modelr_0.1.8        crayon_1.5.1       \n [91] utf8_1.2.2          tzdb_0.3.0          rmarkdown_2.14     \n [94] jpeg_0.1-9          grid_4.2.1          readxl_1.4.0       \n [97] data.table_1.14.2   reprex_2.0.1        digest_0.6.29      \n[100] webshot_0.5.3       xtable_1.8-4        munsell_0.5.0      \n[103] viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html",
    "href": "analysis/SGC4A/3_sgc4A_description.html",
    "title": "11  Description",
    "section": "",
    "text": "TODO check term cell counts and decide if data is pilot or included ? The purpose of this notebook is describe the distributions of dependent variables for Study SGC4A.\nTEMP REMOVE IN PERSON DATA ::: {.cell}\n:::"
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html#sample",
    "href": "analysis/SGC4A/3_sgc4A_description.html#sample",
    "title": "11  Description",
    "section": "SAMPLE",
    "text": "SAMPLE\n\nData Collection\nData were collected online in Winter 2022.\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Orth-Full\", \"Orth-Sparse\",\"Orth-Grid\", \"Tri-Sparse\",\"Total for Period\")\ncont <- table(df_subjects$term, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Orth-Full \n    Orth-Sparse \n    Orth-Grid \n    Tri-Sparse \n    Total for Period \n  \n \n\n  \n    winter22 \n    88 \n    86 \n    88 \n    98 \n    360 \n  \n  \n    Sum \n    88 \n    86 \n    88 \n    98 \n    360 \n  \n\n\n\n\n\n\n\nParticipants\n\n\nCODE\n#Describe participants\nsubject.stats <-df_subjects %>% dplyr::select(age) %>% unlist() %>% favstats()\nsubject.stats$percent.female <- df_subjects %>% filter(gender==\"Female\") %>% count() %>% unlist()/nrow(df_subjects)\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()%>% \n  footnote(general = \"Age in Years\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    percent.female \n  \n \n\n  \n     \n    18 \n    19 \n    20 \n    21 \n    37 \n    20.5 \n    2.29 \n    360 \n    0 \n    0.722 \n  \n\n\nNote:   Age in Years\n\n\n\n\n360 participants (72 % female ) undergraduate STEM majors at a public American University participated in exchange for course credit (age: 18 - 37 years)."
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html#response-accuracy",
    "href": "analysis/SGC4A/3_sgc4A_description.html#response-accuracy",
    "title": "11  Description",
    "section": "RESPONSE ACCURACY",
    "text": "RESPONSE ACCURACY\n\nSubject Level Scores\nSubject level scores summarize the the response accuracy by a particular participant across all discriminant items in the graph comprehension task.\n\nTask Absolute Score\nRecall from Section 3.1.2.1 that the absolute score (following the dichotomous scoring approach) s_NABS indicates if the subject’s response for a particular item was perfectly correct: whether they selected all correct answer options and no others (excluding certain allowed exceptions, such as also selecting the data point referenced in the question). Across the entire task, there ae 13 strategy discriminating questions.\n\n\nCODE\ntitle = \"Descriptive Statistics of Task Response Accuracy (Total Absolute Score)\"\nabs.stats <- df_subjects %>% dplyr::select(s_NABS) %>% unlist() %>% favstats()\nabs.stats %>% kbl (caption = title) %>% kable_classic() %>% \n  footnote(general = \"# questions correct [0,13]\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Task Response Accuracy (Total Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    0 \n    0 \n    0 \n    2 \n    13 \n    2.22 \n    4.02 \n    360 \n    0 \n  \n\n\nNote:   # questions correct [0,13]\n\n\n\n\nWhen combined overall, total absolute accuracy scores in the TEST phase (n = 360) range from 0 to 13 with a slightly lower mean score of (M = 2.22, SD = 4.02).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL ABSOLUTE\n  gf_props(~s_NABS, data = df_subjects) + \n  labs(x = \"number of correct responses\",\n       y = \"% of subjects\",\n       title = \"Distribution of Task Absolute Score \",\n       subtitle = \"\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"s_NABS\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of Task Absolute Score\",\n        subtitle =\"\",\n        x = \"Total Absolute Score\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = s_NABS,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_NABS),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_NABS, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Task Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score \") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbetweenstats(y = s_NABS, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(s_NABS)) + \n  stat_ecdf(geom = \"step\") +\n  facet_grid(pretty_condition~pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Task Absolute Score \",\n        x = \"Total Absolute Score [0,13]\", \n        y = \"Cumulative Probability\")\n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and perhaps bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018).\n\n\nCODE\nmultimode::modetest(df_subjects$s_NABS)\n\n\nWarning in multimode::modetest(df_subjects$s_NABS): A modification of the data\nwas made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$s_NABS\nExcess mass = 0.06, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\n# n_modes = multimode::nmodes(df_subjects$s_NABS, bw=2) #bw = 2questions/15 = 0.15%\n# l_modes = multimode::locmodes(df_subjects$s_NABS,mod0 =  n_modes, display = TRUE)\n\n\nThe excess mass test for multimodality suggests there is not enough mass at the positive end of the score distribution to be considered multimodal.\n\n\nTask Scaled Scores\nThe total scaled score s_SCALED summarizes the scaled score on the 13 strategy-discriminant questions, for each subject. This score ranges from from -13 (all orthogonal) to 13 (all triangular). Recall that the s_SCALED score for an item is a numeric representation of the strategy-consistent response, scaled from -1 to +1 (see Section 4.1.4)\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy\"\nscaled.stats <- df_subjects %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats()\nscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    -13 \n    -12.5 \n    -10.5 \n    -5 \n    13 \n    -6.58 \n    8.23 \n    360 \n    0 \n  \n\n\n\n\n\nOverall, task scaled scores (n = 360) range from -13 to 13 with a slightly lower mean score of (M = -6.58, SD = 8.23).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL SCALED\ngf_props(~item_test_SCALED, data = df_subjects) +\n  labs(x = \"total scaled score\",\n       y = \"% of subjects\",\n       title = \"Distribution of Scaled Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"s_SCALED\",binwidth=1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\")) + \n  labs( title = \"Distribution of Scaled Score\",\n        subtitle =\"\",\n        x = \"total scaled score\", y = \"number of participants\") + \n theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = s_SCALED,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_SCALED),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_SCALED, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Task Scaled Score\",\n    x = \"Condition\", y = \"Total Scaled \") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbetweenstats(y = s_SCALED, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(s_SCALED)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Task Scaled Score\",\n        x = \"Test Phase Scaled Score [-13,13]\", \n        y = \"Cumulative Probability\") \n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and perhaps perhaps bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018).\n\n\nCODE\nmultimode::modetest(df_subjects$s_SCALED)\n\n\nWarning in multimode::modetest(df_subjects$s_SCALED): A modification of the data\nwas made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$s_SCALED\nExcess mass = 0.07, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\n# n_modes = multimode::nmodes(df_subjects$s_SCALED, bw=2) #bw = 2questions/15 = 0.15%\n# l_modes = multimode::locmodes(df_subjects$s_SCALED,mod0 =  n_modes, display = TRUE)\n\n\nThe excess mass test for multimodality suggests there is not enough mass at the positive end of the score distribution to be considered multimodal.\n\n\n\nFirst Item Scores\nNext we consider the response accuracy on just the first question of the graph comprehension task: a subject’s first exposure to the TM graph.\n\nFirst Item Absolute Score\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Combined)\"\nitem.contingency <- df_subjects %>%  dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>%  addmargins(2) %>% prop.table(margin=2) %>% addmargins(1)\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Combined)\n \n  \n      \n    Orth-Full \n    Orth-Sparse \n    Orth-Grid \n    Tri-Sparse \n    Sum \n  \n \n\n  \n    0 \n    0.943 \n    0.886 \n    0.959 \n    0.86 \n    0.914 \n  \n  \n    1 \n    0.057 \n    0.114 \n    0.041 \n    0.14 \n    0.086 \n  \n  \n    Sum \n    1.000 \n    1.000 \n    1.000 \n    1.00 \n    1.000 \n  \n\n\n\n\n\nAcross data collection sessions, first-item accuracy is consistent across experimental conditions. Incorrect answers are far more frequent (91%) than correct answers (9%). Highest accuracy is achieved in the Triangular gridlines condition, with roughly 14% correct response rate, compared to only 6% in the orthogonal axis control.\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_props(~item_q1_NABS, data = df_subjects) +\n  labs(x = \"response accuracy\",\n       y = \"% subjects\",\n       title = \"Proportion of Correct Responses on First Item\",\n       subtitle=\"\")+\n  theme(legend.position = \"none\")+theme_ggdist()\n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_subjects %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(item_q1_NABS))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  labs(x = \"response accuracy\",\n       title = \"Proportion of Correct Responses on First Item\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Proportion of Correct Responses on First Item\",\n            data = df_subjects, item_q1_NABS ~ pretty_condition, rot_labels=c(0,90,0,0), \n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\"))) \n\n\n\n\n\nCODE\n#STATSPLOT\nggbarstats(\n  x = item_q1_NABS,\n  y = pretty_condition, \n  data = df_subjects\n)\n\n\n\n\n\n\n\nFirst Item Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1. (note: we evaluate scaled_score on the first item rather than interpretation, because no orthogonal interpretation is available in the impasse condition)\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (First Item Scaled Score)\"\nfirstscaled.stats <- df_subjects %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats()\nfirstscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (First Item Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    -0.751 \n    0.621 \n    360 \n    0 \n  \n\n\n\n\n\nWhen combined overall, first item scaled scores (n = 360) range from -1 to 1 with a slightly lower mean score of (M = -0.75, SD = 0.62).\n\n\nCODE\n#GGFORMULA | PROPORTIONAL HISTOGRAM SUBJECT FIRST SCALED\ngf_props(~item_q1_SCALED, data = df_subjects) +\n  labs(x = \"scaled score (first item)\",\n       y = \"% of subjects\",\n       title = \"Distribution of First Item Scaled Score\",\n       subtitle = \"\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_q1_SCALED\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\")) + \n  labs( title = \"Distribution of First Item Scaled Score (by Mode and Condition)\",\n        subtitle =\"\",\n        x = \"scaled score (firt item) \", y = \"number of participants\") + \n  theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_subjects %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(item_q1_SCALED))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  labs(x = \"response accuracy\",\n       title = \"Type of Responses on First Item (by Modality and Condition)\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbarstats(\n  x = item_q1_SCALED,\n  y = pretty_condition, \n  data = df_subjects\n)\n\n\n\n\n\n\n\n\nInterpretation Scores\nNext we consider the the interpretations assigned to each response. For each response given by a participant to a question, we assign an interpretation label based on the interpretation the response most closely matches (see Section 3.2.3).\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition\"\nitem.contingency <- df_items %>%  dplyr::select(interpretation, pretty_condition) %>% table() %>%  addmargins(2) %>% prop.table(margin=2) %>% addmargins(1)\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition\n \n  \n      \n    Orth-Full \n    Orth-Sparse \n    Tri-Sparse \n    Orth-Grid \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.568 \n    0.558 \n    0.484 \n    0.608 \n    0.556 \n  \n  \n    frenzy \n    0.005 \n    0.008 \n    0.004 \n    0.005 \n    0.006 \n  \n  \n    ? \n    0.125 \n    0.089 \n    0.114 \n    0.114 \n    0.111 \n  \n  \n    reference \n    0.000 \n    0.002 \n    0.001 \n    0.003 \n    0.001 \n  \n  \n    blank \n    0.018 \n    0.023 \n    0.025 \n    0.035 \n    0.026 \n  \n  \n    both tri + orth \n    0.120 \n    0.119 \n    0.115 \n    0.120 \n    0.118 \n  \n  \n    Tversky \n    0.025 \n    0.018 \n    0.031 \n    0.020 \n    0.024 \n  \n  \n    Triangular \n    0.139 \n    0.183 \n    0.227 \n    0.094 \n    0.158 \n  \n  \n    Sum \n    1.000 \n    1.000 \n    1.000 \n    1.000 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_propsh(~interpretation, data = df_items, fill = ~pretty_condition) %>% \n  gf_facet_grid(pretty_condition~pretty_mode) +\n  labs(x = \"% of items\",\n       title = \"Proportion of Interpretations Across Items\",\n       subtitle=\"\")+\n  theme(legend.position = \"none\")\n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_items %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(interpretation))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  labs(x = \"response accuracy\",\n       title = \"Response Types on All Items (by Condition)\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\n# vcd::mosaic(main=\"Proportion of Interpretations across Conditions\",\n#             data = df_items, pretty_condition ~ interpretation, rot_labels=c(0,90,0,0), \n#             offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n#             spacing = spacing_dimequal(unit(1:2, \"lines\"))) \n\n#STATSPLOT\nggbarstats(\n  x = interpretation,\n  y = pretty_condition, \n  data = df_items\n)\n\n\n\n\n\n\n\nCumulative Task Performance\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html#response-latency",
    "href": "analysis/SGC4A/3_sgc4A_description.html#response-latency",
    "title": "11  Description",
    "section": "RESPONSE LATENCY",
    "text": "RESPONSE LATENCY\n\nTime on First Item\nHere we consider the time spent on just the first individual item (first exposure to graph).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- df_subjects %>% dplyr::select(item_q1_rt) %>% unlist() %>% favstats()\ntitle = \"Descriptive Statistics of First Response Time (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of First Response Time (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    5.6 \n    13.8 \n    21.6 \n    35.6 \n    536 \n    31.1 \n    39.1 \n    360 \n    0 \n  \n\n\n\n\n\nResponse time on the first item for subjects (n = 360) ranged from 5.6 to 536.39 minutes with a mean duration of (M = 31.08, SD = 39.13).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_q1_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of First Item Response Time (seconds)\", subtitle = \"fit by gamma distribution\", x = \"First Item Response Time (seconds)\", y = \"% items\")\n\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\n\nWarning: Removed 360 rows containing missing values (geom_vline).\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_q1_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"First Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#recode as boolean correct\ndf_subjects <- df_subjects %>% mutate(\n  item_q1_NABS = as.logical(item_q1_NABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_q1_rt, color = item_q1_NABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .1\n  )) + \n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"First Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\n\n\nCODE\n# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n#TEMP REMOVE OUTLIERS\ndf <- df_subjects %>% filter(item_q1_rt < 300)\n\n#STATSPLOT\nggbetweenstats(y = item_q1_rt, x = pretty_condition, data = df,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\n\n\nTime on Item\nHere we consider the time spent on an individual item (across all items).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- df_items %>%   dplyr::select(rt_s) %>% unlist() %>% favstats()\ntitle = \"Descriptive Statistics of Item Response Latency (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Item Response Latency (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    0.785 \n    12.1 \n    21.8 \n    40.6 \n    536 \n    33.1 \n    36 \n    5400 \n    0 \n  \n\n\n\n\n\nTime on an individual item for subjects (n = 5400) ranged from 0.78 to 536.39 minutes with a mean duration of (M = 33.13, SD = 35.95).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~rt_s, data = df_items) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of Item Response Time (seconds)\", \n       subtitle = \"fit by gamma distribution\", x = \"Item Response Time (seconds)\", y = \"% items\") \n\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\n\nWarning: Removed 5400 rows containing missing values (geom_vline).\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_items, x = \"rt_s\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#recode as boolean correct\ndf_items <- df_items %>% mutate(\n  score_niceABS = as.logical(score_niceABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_items, aes(x = pretty_condition, y = rt_s, color = score_niceABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    # position = position_dodgejust(),\n    justification = 1.5, \n    # adjust = .5, \n    width = .5, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA,\n    position = position_dodge2()\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitterdodge(\n      # seed = 1,\n      dodge.width = 0.5,\n      jitter.width = 0.075\n  )) +\n  labs( title = \"Distribution of Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\n\n\nCODE\n# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n                \n#STATSPLOT\nggbetweenstats(y = rt_s, x = pretty_condition, data = df_items,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\n\n\nTime on Task\nHere we consider the time spent on the entire experimental task.\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- df_subjects %>% dplyr::select(rt_m) %>% unlist() %>% favstats()\ntitle = \"Descriptive Statistics of Total Task Response Latency (minutes)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Total Task Response Latency (minutes)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    1.19 \n    5.08 \n    6.85 \n    9.1 \n    26.8 \n    7.48 \n    3.52 \n    360 \n    0 \n  \n\n\n\n\n\nTotal time on task for subjects (n = 360) ranged from 1.19 to 26.82 minutes with a mean duration of (M = 7.48, SD = 3.52).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~rt_m, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of Total Response Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Total Response Time (minutes)\", y = \"% subjects\") \n\n\nWarning: Removed 360 rows containing missing values (geom_vline).\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"rt_m\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of Total Response Time (minutes)\",\n        subtitle =\"\",\n        x = \"Scaffold Phase Time (minutes)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = rt_m, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = rt_m),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = rt_m, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  ))+ labs( title = \"Distribution of Total Response Time (minutes)\",\n        subtitle =\"\",\n        y = \"Total Response Time (minutes)\", x = \"Condition\") +\n  theme_ggdist() + theme(legend.position = \"blank\") #+\n\n\n\n\n\nCODE\n  # coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n#STATSPLOT\nggbetweenstats(y = rt_m, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )"
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html#exploring-relationships",
    "href": "analysis/SGC4A/3_sgc4A_description.html#exploring-relationships",
    "title": "11  Description",
    "section": "EXPLORING RELATIONSHIPS",
    "text": "EXPLORING RELATIONSHIPS\n\nACCURACY (VS) LATENCY\n\nTotal Task\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ rt_m, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by TOTAL Item Response Time\",\n    subtitle = \"\", \n    x = \"Total Item Response Time (minutes)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_avg_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MEAN Item Response Time\",\n    subtitle = \"\", \n    x = \"Average Item Response Time (seconds)\", y = \"Total Scaled Score\"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_max_rt, data = df_subjects %>% filter(item_max_rt < 400), alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MAX Item Response Time\",\n    subtitle = \"\", \n    x = \"MAX Item Response Time (s)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#NOTE: LOG transforms of the RT do not yield linear relationships\n\n\n\n\nAverage Item RT by Accuracy\n\n\nCODE\nq.stats <- df_items %>% filter(q != 6) %>% dplyr::group_by(q, pretty_condition, score_niceABS) %>% dplyr::summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score_niceABS)\n)\n\ngf_line( m ~ q, group = ~group,  color = ~as.factor(score_niceABS),data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~q) %>% \n  gf_facet_wrap(~pretty_condition) + scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Absolute Score\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\n# df_items %>%\n#   ggplot(aes(y = rt_s, x = q,  fill = pretty_condition)) +\n#   stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + facet_wrap(~pretty_condition)\n\n\n\n\nCODE\nq.stats <- df_items %>% filter(q %nin% c(6,9)) %>% dplyr::group_by(q, pretty_condition, interpretation) %>% dplyr::summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score_SCALED)\n)\n\ngf_line( m ~ as.factor(q), group = ~group,  color = ~interpretation,data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~as.factor(q)) %>% \n  gf_facet_grid(interpretation~pretty_condition) + #+ scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Interpretation\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       caption=\"NOTE: Points with no ribbon indicate singular response\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Interpretation\")\n\n\ngeom_path: Each group consists of only one observation. Do you need to adjust\nthe group aesthetic?\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\ndf_items %>% filter(q %nin% c(6,9)) %>% mutate( interpretation = recode(interpretation, \"reference\" = \"blank\", \"frenzy\" = \"?\")) %>% \n  ggplot(aes(y = rt_s, x = q,  fill = interpretation)) +\n  stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + \n  facet_grid(interpretation ~ pretty_condition) + \n  labs( title = \"Average Response Time by Question Interpretation\", x = \"Question\", y=\"Averate Item Response Time (s)\")"
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html#resources",
    "href": "analysis/SGC4A/3_sgc4A_description.html#resources",
    "title": "11  Description",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] forcats_0.5.1      stringr_1.4.0      purrr_0.3.4        readr_2.1.2       \n [5] tidyr_1.2.0        tibble_3.1.7       tidyverse_1.3.1    performance_0.9.1 \n [9] fitdistrplus_1.1-8 MASS_7.3-57        multimode_1.5      ggeasy_0.1.3      \n[13] ggstatsplot_0.9.3  ggdist_3.1.1       ggpubr_0.4.0       vcd_1.4-10        \n[17] kableExtra_1.3.4   mosaic_1.8.3       ggridges_0.5.3     mosaicData_0.20.2 \n[21] ggformula_0.10.1   ggstance_0.3.5     dplyr_1.0.9        Matrix_1.4-1      \n[25] Hmisc_4.7-0        ggplot2_3.3.6      Formula_1.2-4      survival_3.3-1    \n[29] lattice_0.20-45   \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.2.2             ks_1.13.5              tidyselect_1.1.2      \n  [4] htmlwidgets_1.5.4      gmp_0.6-5              munsell_0.5.0         \n  [7] codetools_0.2-18       effectsize_0.7.0       withr_2.5.0           \n [10] colorspace_2.0-3       highr_0.9              knitr_1.39            \n [13] rstudioapi_0.13        ggsignif_0.6.3         labeling_0.4.2        \n [16] emmeans_1.7.5          polyclip_1.10-0        bit64_4.0.5           \n [19] farver_2.1.0           datawizard_0.4.1       coda_0.19-4           \n [22] vctrs_0.4.1            generics_0.1.2         TH.data_1.1-1         \n [25] xfun_0.31              BWStest_0.2.2          diptest_0.76-0        \n [28] R6_2.5.1               BayesFactor_0.9.12-4.4 cachem_1.0.6          \n [31] assertthat_0.2.1       scales_1.2.0           vroom_1.5.7           \n [34] multcomp_1.4-19        nnet_7.3-17            rootSolve_1.8.2.3     \n [37] gtable_0.3.0           multcompView_0.1-8     sandwich_3.0-2        \n [40] MatrixModels_0.5-0     rlang_1.0.3            zeallot_0.1.0         \n [43] systemfonts_1.0.4      PMCMRplus_1.9.5        splines_4.2.1         \n [46] rstatix_0.7.0          broom_0.8.0            prismatic_1.1.0       \n [49] mosaicCore_0.9.0       checkmate_2.1.0        yaml_2.3.5            \n [52] abind_1.4-5            modelr_0.1.8           crosstalk_1.2.0       \n [55] backports_1.4.1        tools_4.2.1            ellipsis_0.3.2        \n [58] RColorBrewer_1.1-3     ggdendro_0.1.23        Rcpp_1.0.8.3          \n [61] plyr_1.8.7             base64enc_0.1-3        rpart_4.1.16          \n [64] pbapply_1.5-0          correlation_0.8.1      zoo_1.8-10            \n [67] haven_2.5.0            ggrepel_0.9.1          cluster_2.1.3         \n [70] fs_1.5.2               magrittr_2.0.3         data.table_1.14.2     \n [73] lmtest_0.9-40          reprex_2.0.1           mvtnorm_1.1-3         \n [76] hms_1.1.1              patchwork_1.1.1        evaluate_0.15         \n [79] xtable_1.8-4           leaflet_2.1.1          jpeg_0.1-9            \n [82] mclust_5.4.10          readxl_1.4.0           gridExtra_2.3         \n [85] compiler_4.2.1         KernSmooth_2.23-20     crayon_1.5.1          \n [88] htmltools_0.5.2        tzdb_0.3.0             lubridate_1.8.0       \n [91] DBI_1.1.3              SuppDists_1.1-9.7      kSamples_1.2-9        \n [94] tweenr_1.0.2           dbplyr_2.2.1           boot_1.3-28           \n [97] car_3.1-0              cli_3.3.0              parallel_4.2.1        \n[100] insight_0.17.1         pkgconfig_2.0.3        statsExpressions_1.3.2\n[103] foreign_0.8-82         xml2_1.3.3             paletteer_1.4.0       \n[106] svglite_2.1.0          webshot_0.5.3          estimability_1.4      \n[109] rvest_1.0.2            distributional_0.3.0   digest_0.6.29         \n[112] parameters_0.18.1      pracma_2.3.8           rmarkdown_2.14        \n[115] cellranger_1.1.0       htmlTable_2.4.0        lifecycle_1.0.1       \n[118] jsonlite_1.8.0         carData_3.0-5          viridisLite_0.4.0     \n[121] fansi_1.0.3            labelled_2.9.1         pillar_1.7.0          \n[124] fastmap_1.1.0          httr_1.4.3             glue_1.6.2            \n[127] bayestestR_0.12.1      png_0.1-7              bit_4.0.4             \n[130] ggforce_0.3.3          stringi_1.7.6          rematch2_2.1.2        \n[133] latticeExtra_0.6-29    memoise_2.0.1          Rmpfr_0.8-9"
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html",
    "title": "12  Introduction",
    "section": "",
    "text": "In Study 4B we explore the extent to which the design of the marks indicating data points influence how a reader interprets its underlying coordinate system."
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html#methods",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html#methods",
    "title": "12  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 3 levels (Mark: POINT, CROSS, ARROW) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Mark Design: Point, Arrow, Cross )\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 16.1. The list of questions can be found here.\n\n\n\nFigure 12.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nIn each experimental\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a Triangular Model (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items: The Graph Comprehension Task.\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData were collected by convenience sample of a university subject pool during the winter of 2022. Participants accessed the study via a web browser (asynchronously). The stimulus application required the participant stay in full-screen mode for the entirety of the study."
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html#analysis",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html#analysis",
    "title": "12  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nwinter2022_clean_sgc4b.Rmd\n2_sgc4B_scoring.qmd\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#import file\ndf_subjects <- read_rds(\"analysis/SGC4B/data/0-session-level/sgc4b_participants.rds\") #use RDS file as it contains metadata\n\n#NO EXPLANATION COLUMN IN SGC4B DATASET; TRIAL NOT COLLECTED \n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\n# df_q16 <- df_subjects %>% \n#   select(subject, condition, term , mode, explanation) %>% \n#   mutate(\n#     q = 16,\n#     response = explanation\n#   ) %>% select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\n#drop absolute score because we rescore in 2_scoring\ndf_subjects <- df_subjects %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, study, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m,\n                 #exploratory factors\n                 violations, browser, width, height\n                 )\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#set factors\ndf_subjects <- df_subjects %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#read datafiles\ndf_items <- read_rds(\"analysis/SGC4B/data/0-session-level/sgc4b_items.rds\") #use RDS file as it contains metadata\n\n#reduce data collected using new webapp\ndf_items <- df_items %>% \n  select(subject, condition, pretty_condition, study, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  ) %>% \n  mutate(\n    response = str_remove_all(as.character(answer), \",\"),\n    num_o = str_length(response)\n  )%>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4B/data/1-study-level/sgc4b_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4B/data/1-study-level/sgc4b_items.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4B/data/1-study-level/sgc4b_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4B/data/1-study-level/sgc4b_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html#resources",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html#resources",
    "title": "12  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] highr_0.9         httr_1.4.3        pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] munsell_0.5.0     broom_0.8.0       compiler_4.2.1    modelr_0.1.8     \n[29] xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2  \n[33] tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3       viridisLite_0.4.0\n[37] crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1      withr_2.5.0      \n[41] grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1  \n[45] DBI_1.1.3         magrittr_2.0.3    scales_1.2.0      zip_2.2.0        \n[49] cli_3.3.0         stringi_1.7.6     fs_1.5.2          xml2_1.3.3       \n[53] ellipsis_0.3.2    generics_0.1.2    vctrs_0.4.1       openxlsx_4.2.5   \n[57] tools_4.2.1       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n[61] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[65] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html",
    "title": "13  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC4B study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#score-sgc-data",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#score-sgc-data",
    "title": "13  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#backup <- read_rds('analysis/SGC4B/data/1-study-level/sgc4b_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC4B/data/1-study-level/sgc4b_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#summarize-by-subject",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#summarize-by-subject",
    "title": "13  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC4B/data/1-study-level/sgc4b_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#explore-distributions",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#explore-distributions",
    "title": "13  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  pretty_condition = pretty_condition,\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(pretty_condition~q) +\n  labs( x = \"Absolute Score\",\n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>%\ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\",\n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(q~pretty_condition) +\n  labs( x = \"Scaled Score\",\n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) +\n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>%\n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\",\n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#peeking",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#peeking",
    "title": "13  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\nm1 <- lm(s_SCALED ~ pretty_condition, data = df_subjects)\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ pretty_condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8.55  -5.55  -3.72   1.78  20.62 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             -7.621      0.891   -8.55  9.2e-16 ***\npretty_conditioncross    1.344      1.290    1.04    0.299    \npretty_conditionarrow    3.172      1.237    2.56    0.011 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.5 on 269 degrees of freedom\nMultiple R-squared:  0.0241,    Adjusted R-squared:  0.0168 \nF-statistic: 3.32 on 2 and 269 DF,  p-value: 0.0376\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n                  Df Sum Sq Mean Sq F value Pr(>F)  \npretty_condition   2    480   240.0    3.32  0.038 *\nResiduals        269  19434    72.2                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with pretty_condition (formula: s_SCALED ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.02, F(2, 269) = 3.32, p = 0.038, adj. R2 = 0.02). The model's intercept, corresponding to pretty_condition = point, is at -7.62 (95% CI [-9.38, -5.87], t(269) = -8.55, p < .001). Within this model:\n\n  - The effect of pretty condition [cross] is statistically non-significant and positive (beta = 1.34, 95% CI [-1.20, 3.88], t(269) = 1.04, p = 0.299; Std. beta = 0.16, 95% CI [-0.14, 0.45])\n  - The effect of pretty condition [arrow] is statistically significant and positive (beta = 3.17, 95% CI [0.74, 5.61], t(269) = 2.56, p = 0.011; Std. beta = 0.37, 95% CI [0.09, 0.65])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#export",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#export",
    "title": "13  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4B/data/2-scored-data/sgc4b_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4B/data/2-scored-data/sgc4b_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC4B/data/2-scored-data/sgc4b_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC4B/data/2-scored-data/sgc4b_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4B/data/2-scored-data/sgc4b_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4B/data/2-scored-data/sgc4b_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#resources",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#resources",
    "title": "13  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n  [1] TH.data_1.1-1       colorspace_2.0-3    rio_0.5.29         \n  [4] ellipsis_0.3.2      estimability_1.4    htmlTable_2.4.0    \n  [7] parameters_0.18.1   base64enc_0.1-3     fs_1.5.2           \n [10] rstudioapi_0.13     farver_2.1.0        bit64_4.0.5        \n [13] fansi_1.0.3         mvtnorm_1.1-3       lubridate_1.8.0    \n [16] xml2_1.3.3          codetools_0.2-18    splines_4.2.1      \n [19] knitr_1.39          polyclip_1.10-0     jsonlite_1.8.0     \n [22] broom_0.8.0         cluster_2.1.3       dbplyr_2.2.1       \n [25] png_0.1-7           effectsize_0.7.0    ggforce_0.3.3      \n [28] compiler_4.2.1      httr_1.4.3          emmeans_1.7.5      \n [31] backports_1.4.1     assertthat_0.2.1    Matrix_1.4-1       \n [34] fastmap_1.1.0       cli_3.3.0           tweenr_1.0.2       \n [37] htmltools_0.5.2     tools_4.2.1         coda_0.19-4        \n [40] gtable_0.3.0        glue_1.6.2          Rcpp_1.0.8.3       \n [43] cellranger_1.1.0    vctrs_0.4.1         svglite_2.1.0      \n [46] insight_0.17.1      xfun_0.31           openxlsx_4.2.5     \n [49] rvest_1.0.2         lifecycle_1.0.1     mosaicCore_0.9.0   \n [52] zoo_1.8-10          MASS_7.3-57         vroom_1.5.7        \n [55] hms_1.1.1           sandwich_3.0-2      parallel_4.2.1     \n [58] RColorBrewer_1.1-3  curl_4.3.2          yaml_2.3.5         \n [61] gridExtra_2.3       labelled_2.9.1      rpart_4.1.16       \n [64] latticeExtra_0.6-29 stringi_1.7.6       bayestestR_0.12.1  \n [67] checkmate_2.1.0     zip_2.2.0           rlang_1.0.3        \n [70] pkgconfig_2.0.3     systemfonts_1.0.4   evaluate_0.15      \n [73] htmlwidgets_1.5.4   labeling_0.4.2      bit_4.0.4          \n [76] tidyselect_1.1.2    plyr_1.8.7          magrittr_2.0.3     \n [79] R6_2.5.1            generics_0.1.2      multcomp_1.4-19    \n [82] DBI_1.1.3           pillar_1.7.0        haven_2.5.0        \n [85] foreign_0.8-82      withr_2.5.0         datawizard_0.4.1   \n [88] nnet_7.3-17         modelr_0.1.8        crayon_1.5.1       \n [91] utf8_1.2.2          tzdb_0.3.0          rmarkdown_2.14     \n [94] jpeg_0.1-9          grid_4.2.1          readxl_1.4.0       \n [97] data.table_1.14.2   reprex_2.0.1        digest_0.6.29      \n[100] webshot_0.5.3       xtable_1.8-4        munsell_0.5.0      \n[103] viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html",
    "title": "14  Introduction",
    "section": "",
    "text": "TODO UPDATE\nIn Study 4C we explore the extent to which the orientation of the axes in space influence how a reader interprets its underlying coordinate system."
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html#methods",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html#methods",
    "title": "14  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 3 levels (Mark: POINT, CROSS, ARROW) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Mark Design: Point, Arrow, Cross )\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 16.1. The list of questions can be found here.\n\n\n\nFigure 14.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nIn each experimental\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a Triangular Model (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items: The Graph Comprehension Task.\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData were collected by convenience sample of a university subject pool during the winter of 2022. Participants accessed the study via a web browser (asynchronously). The stimulus application required the participant stay in full-screen mode for the entirety of the study."
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html#analysis",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html#analysis",
    "title": "14  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\n\n\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#import file\ndf_subjects <- read_rds(\"analysis/SGC4C/data/0-session-level/sgc4c_participants.rds\") #use RDS file as it contains metadata\n\n#NO EXPLANATION COLUMN IN SGC4c DATASET; TRIAL NOT COLLECTED \n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\n# df_q16 <- df_subjects %>% \n#   select(subject, condition, term , mode, explanation) %>% \n#   mutate(\n#     q = 16,\n#     response = explanation\n#   ) %>% select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\n#drop absolute score because we rescore in 2_scoring\ndf_subjects <- df_subjects %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, study, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m,\n                 #exploratory factors\n                 violations, browser, width, height\n                 )\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#set factors\ndf_subjects <- df_subjects %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#read datafiles\ndf_items <- read_rds(\"analysis/SGC4C/data/0-session-level/sgc4c_items.rds\") #use RDS file as it contains metadata\n\n#reduce data collected using new webapp\ndf_items <- df_items %>% \n  select(subject, condition, pretty_condition, study, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  ) %>% \n  mutate(\n    response = str_remove_all(as.character(answer), \",\"),\n    num_o = str_length(response)\n  )%>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4C/data/1-study-level/sgc4c_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4C/data/1-study-level/sgc4c_items.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4C/data/1-study-level/sgc4c_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4C/data/1-study-level/sgc4c_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html#resources",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html#resources",
    "title": "14  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] highr_0.9         httr_1.4.3        pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] munsell_0.5.0     broom_0.8.0       compiler_4.2.1    modelr_0.1.8     \n[29] xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2  \n[33] tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3       viridisLite_0.4.0\n[37] crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1      withr_2.5.0      \n[41] grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1  \n[45] DBI_1.1.3         magrittr_2.0.3    scales_1.2.0      zip_2.2.0        \n[49] cli_3.3.0         stringi_1.7.6     fs_1.5.2          xml2_1.3.3       \n[53] ellipsis_0.3.2    generics_0.1.2    vctrs_0.4.1       openxlsx_4.2.5   \n[57] tools_4.2.1       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n[61] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[65] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html",
    "title": "15  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC4C study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#score-sgc-data",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#score-sgc-data",
    "title": "15  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#backup <- read_rds('analysis/SGC4C/data/1-study-level/sgc4c_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC4C/data/1-study-level/sgc4c_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#summarize-by-subject",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#summarize-by-subject",
    "title": "15  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC4C/data/1-study-level/sgc4c_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#explore-distributions",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#explore-distributions",
    "title": "15  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  pretty_condition = pretty_condition,\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(pretty_condition~q) +\n  labs( x = \"Absolute Score\",\n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>%\ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\",\n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(q~pretty_condition) +\n  labs( x = \"Scaled Score\",\n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) +\n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>%\n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\",\n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#peeking",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#peeking",
    "title": "15  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\nm1 <- lm(s_SCALED ~ pretty_condition, data = df_subjects)\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ pretty_condition, data = df_subjects)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.933  -6.700   0.183   7.067  16.800 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)  \n(Intercept)                        4.43       2.45    1.81    0.081 .\npretty_conditionORTH-rotate-45    -8.23       3.46   -2.38    0.025 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.48 on 28 degrees of freedom\nMultiple R-squared:  0.168, Adjusted R-squared:  0.138 \nF-statistic: 5.65 on 1 and 28 DF,  p-value: 0.0245\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n                 Df Sum Sq Mean Sq F value Pr(>F)  \npretty_condition  1    508     508    5.65  0.025 *\nResiduals        28   2519      90                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with pretty_condition (formula: s_SCALED ~ pretty_condition). The model explains a statistically significant and moderate proportion of variance (R2 = 0.17, F(1, 28) = 5.65, p = 0.025, adj. R2 = 0.14). The model's intercept, corresponding to pretty_condition = TRI-rotate-45, is at 4.43 (95% CI [-0.58, 9.45], t(28) = 1.81, p = 0.081). Within this model:\n\n  - The effect of pretty condition [ORTH-rotate-45] is statistically significant and negative (beta = -8.23, 95% CI [-15.33, -1.14], t(28) = -2.38, p = 0.025; Std. beta = -0.81, 95% CI [-1.50, -0.11])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#export",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#export",
    "title": "15  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4C/data/2-scored-data/sgc4c_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4C/data/2-scored-data/sgc4c_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC4C/data/2-scored-data/sgc4c_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC4C/data/2-scored-data/sgc4c_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4C/data/2-scored-data/sgc4c_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4C/data/2-scored-data/sgc4c_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#resources",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#resources",
    "title": "15  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n  [1] TH.data_1.1-1       colorspace_2.0-3    rio_0.5.29         \n  [4] ellipsis_0.3.2      estimability_1.4    htmlTable_2.4.0    \n  [7] parameters_0.18.1   base64enc_0.1-3     fs_1.5.2           \n [10] rstudioapi_0.13     farver_2.1.0        bit64_4.0.5        \n [13] fansi_1.0.3         mvtnorm_1.1-3       lubridate_1.8.0    \n [16] xml2_1.3.3          codetools_0.2-18    splines_4.2.1      \n [19] knitr_1.39          polyclip_1.10-0     jsonlite_1.8.0     \n [22] broom_0.8.0         cluster_2.1.3       dbplyr_2.2.1       \n [25] png_0.1-7           effectsize_0.7.0    ggforce_0.3.3      \n [28] compiler_4.2.1      httr_1.4.3          emmeans_1.7.5      \n [31] backports_1.4.1     assertthat_0.2.1    Matrix_1.4-1       \n [34] fastmap_1.1.0       cli_3.3.0           tweenr_1.0.2       \n [37] htmltools_0.5.2     tools_4.2.1         coda_0.19-4        \n [40] gtable_0.3.0        glue_1.6.2          Rcpp_1.0.8.3       \n [43] cellranger_1.1.0    vctrs_0.4.1         svglite_2.1.0      \n [46] insight_0.17.1      xfun_0.31           openxlsx_4.2.5     \n [49] rvest_1.0.2         lifecycle_1.0.1     mosaicCore_0.9.0   \n [52] zoo_1.8-10          MASS_7.3-57         vroom_1.5.7        \n [55] hms_1.1.1           sandwich_3.0-2      parallel_4.2.1     \n [58] RColorBrewer_1.1-3  curl_4.3.2          yaml_2.3.5         \n [61] gridExtra_2.3       labelled_2.9.1      rpart_4.1.16       \n [64] latticeExtra_0.6-29 stringi_1.7.6       bayestestR_0.12.1  \n [67] checkmate_2.1.0     zip_2.2.0           rlang_1.0.3        \n [70] pkgconfig_2.0.3     systemfonts_1.0.4   evaluate_0.15      \n [73] htmlwidgets_1.5.4   labeling_0.4.2      bit_4.0.4          \n [76] tidyselect_1.1.2    plyr_1.8.7          magrittr_2.0.3     \n [79] R6_2.5.1            generics_0.1.2      multcomp_1.4-19    \n [82] DBI_1.1.3           pillar_1.7.0        haven_2.5.0        \n [85] foreign_0.8-82      withr_2.5.0         datawizard_0.4.1   \n [88] nnet_7.3-17         modelr_0.1.8        crayon_1.5.1       \n [91] utf8_1.2.2          tzdb_0.3.0          rmarkdown_2.14     \n [94] jpeg_0.1-9          grid_4.2.1          readxl_1.4.0       \n [97] data.table_1.14.2   reprex_2.0.1        digest_0.6.29      \n[100] webshot_0.5.3       xtable_1.8-4        munsell_0.5.0      \n[103] viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html",
    "title": "16  Introduction",
    "section": "",
    "text": "In Study 5A we explore the extent to which requiring mouse-cursor interaction with the graph improves interpretation of the underlying coordinate system."
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html#methods",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html#methods",
    "title": "16  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 16.1. The list of questions can be found here.\n\n\n\nFigure 16.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nNote that across both control and impasse conditions, both the question, response options and graph structure were identical. The experimental manipulation (posing a mental impasse) was accomplished by changing the position of datapoints in the impasse-condition graph, such that for any given question, there was no available response option if the reader were to interpret the graph as cartesian (making an orthogonal rather than diagonal projection from the x-axis.)\nThe green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. We hypothesize that this presents the reader with an obstacle, at which point they are forced to confront their interpretation of the coordinate system and (ideally) develop a new strategy.\n\n\n\nFigure 16.2: Sample Question (Q=1) graphs for each condition\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items : the Graph Comprehension Task\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData was collected by convenience sample …"
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html#analysis",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html#analysis",
    "title": "16  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nwinter2022_clean_sgc5a.Rmd\n2_sgc5_scoring.qmd\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import file\ndf_subjects <- read_rds(\"analysis/SGC5A/data/0-session-level/sgc5_participants.rds\") #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \n# df_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% select(\n#   subject,condition,term,mode,\n#   gender,age,language, schoolyear, country,\n#   effort,difficulty,confidence,enjoyment,other,\n#   totaltime_m,absolute_score\n# )\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_q16 <- df_subjects %>% \n  dplyr::select(subject, condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% dplyr::select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects <- df_subjects %>% \n  # mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 #absolute_score,#drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors\n                 violations, browser, width, height\n                 )\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#set factors\ndf_subjects <- df_subjects %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#read datafiles\ndf_items <- read_rds(\"analysis/SGC5A/data/0-session-level/sgc5_items.rds\") #use RDS file as it contains metadata\n\n#reduce data collected using new webapp\ndf_items <- df_items %>% \n  dplyr::select(subject, condition, pretty_condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  ) %>% \n  mutate(\n    response = str_remove_all(as.character(answer), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC5A/data/1-study-level/sgc5_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC5A/data/1-study-level/sgc5_items.csv\", row.names = FALSE)\nwrite.csv(df_q16,\"analysis/SGC5A/data/1-study-level/sgc5_freeresponse.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC5A/data/1-study-level/sgc5_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC5A/data/1-study-level/sgc5_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html#resources",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html#resources",
    "title": "16  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] highr_0.9         httr_1.4.3        pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] munsell_0.5.0     broom_0.8.0       compiler_4.2.1    modelr_0.1.8     \n[29] xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2  \n[33] tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3       viridisLite_0.4.0\n[37] crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1      withr_2.5.0      \n[41] grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1  \n[45] DBI_1.1.3         magrittr_2.0.3    scales_1.2.0      zip_2.2.0        \n[49] cli_3.3.0         stringi_1.7.6     fs_1.5.2          xml2_1.3.3       \n[53] ellipsis_0.3.2    generics_0.1.2    vctrs_0.4.1       openxlsx_4.2.5   \n[57] tools_4.2.1       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n[61] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[65] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html",
    "title": "17  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC5 study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#score-sgc-data",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#score-sgc-data",
    "title": "17  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#backup <- read_rds('analysis/SGC5A/data/1-study-level/sgc5_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC5A/data/1-study-level/sgc5_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n# extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#summarize-by-subject",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#summarize-by-subject",
    "title": "17  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC5A/data/1-study-level/sgc5_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#explore-distributions",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#explore-distributions",
    "title": "17  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_condition = recode_factor(condition, \"11115\" = \"point-click\"),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(pretty_condition~q) + \n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, data = df_subjects) %>% \n   gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Total Absolute Score\", \n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(q~pretty_condition) + \n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) + \n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>% \n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\", \n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01)) + \n geom_line(position=position_jitter(w=0.15, h=0.00), size=0.1) +\n facet_wrap( ~ pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01)) + \n geom_line(position=position_jitter(w=0.15, h=0.00), size=0.1) +\n facet_wrap( ~ pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_histogram(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#peeking",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#peeking",
    "title": "17  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\n\nsgc3a <- read_rds(\"analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds\") %>% filter(condition == \"111\") %>% dplyr::select(-pretty_mode)\n\n\ncomb <- rbind(sgc3a, df_subjects)  \n\ngf_histogram(~s_SCALED, data = comb) %>% \n  gf_facet_wrap(~pretty_condition)\n\n\n\n\n\nCODE\nm1 <- lm(s_SCALED ~ pretty_condition, data = comb)\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ pretty_condition, data = comb)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.57  -5.49  -3.57   1.51  20.51 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   -6.427      0.647   -9.93   <2e-16 ***\npretty_conditionpoint-click   -1.086      0.997   -1.09     0.28    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.14 on 271 degrees of freedom\nMultiple R-squared:  0.00436,   Adjusted R-squared:  0.000682 \nF-statistic: 1.19 on 1 and 271 DF,  p-value: 0.277\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n                  Df Sum Sq Mean Sq F value Pr(>F)\npretty_condition   1     78    78.5    1.19   0.28\nResiduals        271  17935    66.2               \n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with pretty_condition (formula: s_SCALED ~ pretty_condition). The model explains a statistically not significant and very weak proportion of variance (R2 = 4.36e-03, F(1, 271) = 1.19, p = 0.277, adj. R2 = 6.82e-04). The model's intercept, corresponding to pretty_condition = control, is at -6.43 (95% CI [-7.70, -5.15], t(271) = -9.93, p < .001). Within this model:\n\n  - The effect of pretty condition [point-click] is statistically non-significant and negative (beta = -1.09, 95% CI [-3.05, 0.88], t(271) = -1.09, p = 0.277; Std. beta = -0.13, 95% CI [-0.37, 0.11])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#export",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#export",
    "title": "17  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC5A/data/2-scored-data/sgc5a_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC5A/data/2-scored-data/sgc5a_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC5A/data/2-scored-data/sgc5a_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC5A/data/2-scored-data/sgc5a_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC5A/data/2-scored-data/sgc5a_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC5A/data/2-scored-data/sgc5a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#resources",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#resources",
    "title": "17  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n  [1] TH.data_1.1-1       colorspace_2.0-3    rio_0.5.29         \n  [4] ellipsis_0.3.2      estimability_1.4    htmlTable_2.4.0    \n  [7] parameters_0.18.1   base64enc_0.1-3     fs_1.5.2           \n [10] rstudioapi_0.13     farver_2.1.0        bit64_4.0.5        \n [13] fansi_1.0.3         mvtnorm_1.1-3       lubridate_1.8.0    \n [16] xml2_1.3.3          codetools_0.2-18    splines_4.2.1      \n [19] knitr_1.39          polyclip_1.10-0     jsonlite_1.8.0     \n [22] broom_0.8.0         cluster_2.1.3       dbplyr_2.2.1       \n [25] png_0.1-7           effectsize_0.7.0    ggforce_0.3.3      \n [28] compiler_4.2.1      httr_1.4.3          emmeans_1.7.5      \n [31] backports_1.4.1     assertthat_0.2.1    Matrix_1.4-1       \n [34] fastmap_1.1.0       cli_3.3.0           tweenr_1.0.2       \n [37] htmltools_0.5.2     tools_4.2.1         coda_0.19-4        \n [40] gtable_0.3.0        glue_1.6.2          Rcpp_1.0.8.3       \n [43] cellranger_1.1.0    vctrs_0.4.1         svglite_2.1.0      \n [46] insight_0.17.1      xfun_0.31           openxlsx_4.2.5     \n [49] rvest_1.0.2         lifecycle_1.0.1     mosaicCore_0.9.0   \n [52] zoo_1.8-10          MASS_7.3-57         vroom_1.5.7        \n [55] hms_1.1.1           sandwich_3.0-2      parallel_4.2.1     \n [58] RColorBrewer_1.1-3  curl_4.3.2          yaml_2.3.5         \n [61] gridExtra_2.3       labelled_2.9.1      rpart_4.1.16       \n [64] latticeExtra_0.6-29 stringi_1.7.6       bayestestR_0.12.1  \n [67] checkmate_2.1.0     zip_2.2.0           rlang_1.0.3        \n [70] pkgconfig_2.0.3     systemfonts_1.0.4   evaluate_0.15      \n [73] htmlwidgets_1.5.4   labeling_0.4.2      bit_4.0.4          \n [76] tidyselect_1.1.2    plyr_1.8.7          magrittr_2.0.3     \n [79] R6_2.5.1            generics_0.1.2      multcomp_1.4-19    \n [82] DBI_1.1.3           pillar_1.7.0        haven_2.5.0        \n [85] foreign_0.8-82      withr_2.5.0         datawizard_0.4.1   \n [88] nnet_7.3-17         modelr_0.1.8        crayon_1.5.1       \n [91] utf8_1.2.2          tzdb_0.3.0          rmarkdown_2.14     \n [94] jpeg_0.1-9          grid_4.2.1          readxl_1.4.0       \n [97] data.table_1.14.2   reprex_2.0.1        digest_0.6.29      \n[100] webshot_0.5.3       xtable_1.8-4        munsell_0.5.0      \n[103] viridisLite_0.4.0"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Schmidt, Dennis, Tobias Raupach, Annette Wiegand, Manfred Herrmann, and\nPhilipp Kanzow. 2021. “Relation Between Examinees’ True Knowledge\nand Examination Scores: Systematic Review and Exemplary Calculations on\nMultiple-True-False\nItems.” Educational Research Review 34 (November):\n100409. https://doi.org/10.1016/j.edurev.2021.100409."
  }
]