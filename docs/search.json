[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SGC-X",
    "section": "",
    "text": "Study SGC2 | Description\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC2 | Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC3A | (Lab) Hypothesis Testing\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC3A | (OSPAN) Hypothesis Testing\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC3A | (Relication) Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC3A | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC3A | 3 Description\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC3A | Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC4A | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4A | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4A | 3 Description\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4A | Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC4B | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4B | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC4C | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4C | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC5A | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC5A | 2 Response Scoring\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "analysis/utils/scoring.html",
    "href": "analysis/utils/scoring.html",
    "title": "Scoring Strategy",
    "section": "",
    "text": "The purpose of this notebook is to describe the strategy for assigning a score ( a measure of accuracy) to response data for the SGC studies. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.)"
  },
  {
    "objectID": "analysis/utils/scoring.html#multiple-response-scoring",
    "href": "analysis/utils/scoring.html#multiple-response-scoring",
    "title": "Scoring Strategy",
    "section": "MULTIPLE RESPONSE SCORING",
    "text": "MULTIPLE RESPONSE SCORING\nThe graph comprehension task of the SGC studies presents readers with a graph, a question, and a series of checkboxes. Participants are instructed to use the graph to answer the question, and respond by selecting all the checkboxes that apply, where each checkbox corresponds to a datapoint in the graph.\n\n\n\nFigure 1. Sample Graph Comprehension (Question # 6)\n\n\nIn the psychology and education literatures on Tests & Measures, the format of this type of question is referred to as Multiple Response (MR), (also: Multiple Choice Multiple Answer (MCMA) and Multiple Answer Multiple Choice (MAMC)). It has a number of properties that make it different from traditional Single Answer Multiple Choice (SAMC) questions, where the respondent marks a single response from a number of options. In particular, there are a number of very different ways that MAMC questions can be scored.\nIn tranditional SAMC format questions, one point is given for selecting the option designated as correct, and zero points given for marking any of the alternative (i.e. distractor) options. Individual response options on MAMC questions, however might be partially correct (\\(i\\)), while responses on other answer options within the same item might be incorrect (\\(n – i\\)). In MR, it is not obvious how to allocate points when the respondent marks a true-correct option (i.e. options that should be selected, denoted \\(p\\)), as well as one or more false-correct options (i.e. options that should not be selected, denoted \\(q\\)). Should partial credit be awarded? If so, are options that respondents false-selected and false-unselected items equally penalized?\nSchmidt et al. (2021) performed a systematic literature review of publications proposing MAMC (or equivalent) scoring schemes, ultimately synthesizing over 80 sources into 27 distinct scoring approaches. Upon reviewing the benefits of trade-offs of each approach, for this study we choose utilize two of the schemes: dichotomous scoring ( Schmidt et al. (2021) scheme #1), and partial scoring \\([-1/q,0, +1/p]\\) ( Schmidt et al. (2021) scheme #26), as well as a scaled discriminant score that leverages partial scoring to discriminate between strategy-specific patterns of response.\n\nResponse Encoding\nFirst, we note that the question type evaluated by Schmidt et al. (2021) is referred to as Multiple True-False (MTF), a variant of MAMC where respondents are presented with a question (stem) and series of response options with True/False (e.g. radio buttons) for each. Depending on the implementation of the underlying instrument, it may or may not be possible for respondents to not respond to a particular option (i.e. leave the item ‘blank’). Although MTF questions have a different underlying implementation (and potentially different psychometric properties) they are identical in their mathematical properties; that is, responses to a MAMC question of ‘select all that apply’ can be coded as a series of T/F responses to each response option\n\n\n\nFigure 3.1: Figure 2. SAMC (vs) MAMC (vs) MTF\n\n\nIn this example (Figure 3.1), we see an example of a question with four response options (\\(n=4\\)) in each question type. In the SAMC approach (at left), there are four possible responses, given explicitly by the response options (respondent can select only one) \\((\\text{number of possible responses} = n)\\). With only four possible responses, we cannot entirely discriminate between all combinations of the underlying response variants we might be interested in, and must always choose an ‘ideal subset’ of possible distractors to present as response options. In the MAMC (middle) and MTF (at right), the same number of response options (\\(n=4\\)) yield a much greater number \\((\\text{number of possible responses} = 2^{n})\\). We can also see the equivalence between a MAMC and MTF format questions with the same response options. Options the respondent selects in MAMC are can be coded as T, and options they leave unselected can be coded as F. Thus, for response options (ABCD), a response of [AB] can also be encoded as [TTFF].\n\n\nScoring Schemes\nIn the sections that follow, we use the terminology:\nProperties of the Stimulus-Question\n\\[\\begin{align}\nn &= \\text{number of response options} \\\\  \n  &= p + q \\\\\n  p &= \\text{number of true-select options (i.e. should be selected)} \\\\\n  q &= \\text{number of true-unselect options (i.e. should not be selected)}\n\\end{align}\\]\nProperties of the Subject’s Response\n\\[\\begin{align}\ni &= \\text{number of options in correct state}, (0 ≤ i ≤ n) \\\\\nf &= \\text{resulting score}\n\\end{align}\\]\n\nDichotomous Scoring\nDichotomous Scoring is the strictest scoring scheme, where a response only receives points if it is exactly correct, meaning the respondent includes only correct-select options, and does select any additional (i.e. incorrect-select) options that should not be selected. This is also known as all or nothing scoring, and importantly, it ignores any partial knowledge that a participant may be expressing through their choice of options. They may select some but not all of the correct-select options, and one or more but not all of the correct-unselect items, but receive the same score as a respondent selects none of the correct-select options, or all of the correct-unselect options. In this sense, dichotomous scoring tells us only about perfect knowledge, and ignores any indication of partial knowledge the respondent may be indicating through their selection of response options.\nIn Dichotomous Scoring\n\nscore for the question is either 0 or 1\nfull credit is only given if all responses are correct; otherwise no credit\ndoes not account for partial knowledge. - with increasing number of response options, scoring becomes stricter as each statement must be marked correctly.\n\nThe algorithm for dichotomous scoring is given by:\n\\[\\begin{gather*}\nf =\n\\begin{cases}\n  1, \\text{if } i = n \\\\    \n  0, \\text{otherwise}    \n\\end{cases}\n\\end{gather*}\\] 0 i n\n\n\nCODE\nf_dichom <- function(i, n) {\n \n  # print(paste(\"i is :\",i,\" n is:\",n)) \n  \n  #if (n == 0 ) return error \n  ifelse( (n == 0), print(\"ERROR n can't be 0\"), \"\")\n  \n  #if (i > n ) return error \n  ifelse( (i > n), print(\"i n can't > n\"), \"\")\n  \n  #if (i==n) return 1, else 0\n  return (ifelse( (i==n), 1 , 0))\n \n}\n\n\n\n\nPartial Scoring [-1/n, +1/n]\nPartial Scoring refers to a class or scoring schemes that award the respondent partial credit depending on pattern of options they select. Schmidt et al. (2021) identify twenty-six different partial credit scoring schemes in the literature, varying in the range of possible scores, and the relative weighting of incorrectly selected (vs) incorrectly unselected options.\nA particularly elegant approach to partial scoring is referred to as the \\([-1/n, +1/n]\\) approach ( Schmidt et al. (2021) #17). This approach is appealing in the context of SGC3A, because it: (1) takes into account all information provided by the respondent: the pattern of what the select, and choose not to select.\nIn Partial Scoring \\([-1/n, +1/n]\\):\n\nScores range from [-1, +1]\nOne point is awarded if all options are correct\nOne point point is subtracted if all options are incorrect.\nIntermediate results are credited as fractions accordingly (\\(+1/n\\) for each correct, \\(-1/n\\) for each incorrect)\nThis results in at chance performance (i.e. half of the given options marked correctly), being awarded 0 points are awarded\n\nThis scoring is more consistent with the motivating theory that Triangular Graph readers start out with an incorrect (i.e. orthogonal, cartesian) interpretation of the coordinate system, and transition to a correct (i.e. triangular) interpretation. But the first step in making this transition is realizing the cartesian interpretation is incorrect, which may yield blank responses where the respondent is essentially saying, ‘there is no correct answer to this question’.\nSchmidt et al. (2021) describe the Partial \\({[-1/n, +1/n]}\\) scoring scheme as the only scoring method (of the 27 described) where respondents’ scoring results can be interpreted as a percentage of their true knowledge. One important drawback of this method is that a respondent may receive credit (a great deal of credit, depending on the number of answer options n) even if she did not select any options. In the case (such as ours) where there are many more response options \\(n\\) than there are options meant to be selected \\(p\\), this partial scoring algorithm poses a challenge because the respondent can achieve an almost completely perfect score by selecting a small number of options that should not be selected.\nThe algorithm for partial scoring\\([-1/n, +1/n]\\) is given by:\n\\[\\begin{align}\nf &= (1/n * i) - (1/n * (n-i)) \\\\\n&= (2i - n)/{n}\n\\end{align}\\]\n\n\nCODE\nf_partialN <- function(i, n) {\n\n# print(paste(\"i is :\",i,\" n is:\",n))\n\n#if(n==0) return error\nifelse((n==0),print(\"ERROR: n should not be 0\"),\"\")\n\n#if(i >n ) return error\nifelse((i > n),print(\"ERROR: i CANNOT BE GREATER THAN n\"),\"\")\n\nreturn ((2*i - n) / n) \n}\n\n\n\n\nPartial Scoring [-1/q, +1/p]\nOne drawback of the Partial Scoring \\([-1/n, +1/n]\\) approach is that treats the choice to select, and choice to not select options as equally indicative of the respondent’s understanding. That is to say, incorrectly selecting one particular option is no more or less informative than incorrectly not-selecting a different item. This represents an important difference between MAMC (i.e. “select all correct options”) vs MTF (i.e. “Mark each option as true or false”) questions.\nIn our study, the selection of any particular option (remember options represent data points on the stimulus graph) is indicative of a particular interpretation of the stimulus. Incorrectly selecting an option indicates an interpretation of the graph with respect to that particular option. Alternatively, failing to select a correct option might mean the individual has a different interpretation, or that they failed to find all the data points consistent with the interpretation.\nFor this reason, we consider another alternative Partial Scoring scheme that takes into consideration only the selected statements, without penalizing statements incorrectly not selected. (See Schmidt et al. (2021) method #26; also referred to as the Morgan-Method) This partial scoring scheme takes into consideration that the most effort-free (or ‘default’) response for any given item is the null, or blank response. Blank responses indicate no understanding, perhaps confusion, or refusal to answer. These lack of responses are awarded zero credit. Whereas taking the action to select an incorrect option is effortful, and is indicative of incorrect understanding.\nPartial Scoring \\([-1/q, +1/p]\\):\n\nawards +1/p for each correctly selected option (\\(p_s\\)), and subtracts \\(1/(n-p) = 1/q\\) for each incorrectly selected option (\\(q_s\\))\nonly considers selected options; does not penalize nor reward unselected options\n\nProperties of Item\n\\[\\begin{align}\np &= \\text{number of true-select options (i.e. should be selected)} \\\\\nq &= \\text{number of true-unselect options (i.e. should not be selected)} \\\\\nn &= \\text{number of options} \\: ( n = p + q)\n\\end{align}\\]\nProperties of Response\n\\[\\begin{align}\np_s &= \\text{number of true-select options selected (i.e. number of correctly checked options)}\\\\\nq_s &= \\text{number of true-unselect options selected (i.e. number of incorrectly checked options }\n\\end{align}\\]\nThe algorithm for partial scoring \\([-1/q, +1/p]\\) is given by:\n\\[\\begin{align}\nf &= (p_s / p) - ({q_s}/{q}) \\\\\n\\end{align}\\]\n\n\nCODE\nf_partialP <- function(t,p,f,q) {\n\n  #t = number of correct-selected options\n  #p = number of true options\n  #f = number of incorrect-selected options\n  #q = number of false options\n  #n = number of options + p + q\n  \n  ifelse( (p == 0), return(NA), \"\") #handle empty response set gracefully by returning nothing rather than 0\n  ifelse( (p != 0), return( (t / p) - (f/q)), \"\")\n}\n\n\n\n\n\nComparison of Schemes\nWhich scoring scheme is most appropriate for the goals of the graph comprehension task?\nConsider the following example:\nFor a question with \\(n = 5\\) response options (data points A, B, C, D and E) with a correct response of A, the schemes under consideration yield the following scores:\n\n\nCODE\ntitle <- \"Comparison of Scoring Schemes for n = 5 options [ A,B,C,D,E ]\"\n\ncorrect <- c( \"A____\",  \n              \"A____\",      \n              \"A____\",        \n              \"A____\",        \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\" ) \n\nresponse <- c(\"A____\",  \n              \"AB___\",      \n              \"A___E\",      \n              \"AB__E\",        \n              \"____E\",\n              \"___DE\",\n              \"_BCDE\",      \n              \"ABCDE\",      \n              \"_____\" )\n\ni <- c(        5,       \n               4,              \n               4,              \n               3,               \n               \n               3,\n               2,\n               0,\n               1,\n               4)\n\nabs <- c(f_dichom(5,5), \n         f_dichom(4,5), \n         f_dichom(4,5), \n         f_dichom(3,5), \n         \n         f_dichom(3,5), \n         f_dichom(2,5),\n         f_dichom(0,5),\n         f_dichom(1,5),\n         f_dichom(4,5))\n\npartial1 <- c(f_partialN(5,5), \n              f_partialN(4,5), \n              f_partialN(4,5), \n              f_partialN(3,5), \n              \n              f_partialN(3,5), \n              f_partialN(2,5),\n              f_partialN(0,5),\n              f_partialN(1,5),\n              f_partialN(4,5))\n\npartial2 <- c(f_partialP(1,1,0,4), \n              f_partialP(1,1,1,4), \n              f_partialP(1,1,1,4), \n              f_partialP(1,1,2,4), \n              \n              f_partialP(0,1,1,4),\n              f_partialP(0,1,2,4),\n              f_partialP(0,1,4,4),\n              f_partialP(1,1,4,4), \n              f_partialP(0,1,0,4))\n\nnames = c(    \"Correct Answer\",\n              \"Response\",\n              \"i \",\n              \"Dichotomous\",\n              \"Partial [-1/n, +1/n]\",\n              \"Partial[-1/q, +1/p]\")\n\ndt <- data.frame(correct, response, i, abs, partial1 , partial2)\n\nkbl(dt, col.names = names, caption = title, digits=3) %>%\n  kable_classic() %>%\n    add_header_above(c(\"Response Scenario \" = 3, \"Scores\" = 3)) %>% \n    pack_rows(\"Perfect Response\", 1, 1) %>%\n    pack_rows(\"Correct + Extra Incorrect Selections\", 2, 4) %>%\n    pack_rows(\"Only Incorrect Selections\", 5, 6) %>%\n    pack_rows(\"Completely Inverse Response \", 7, 7) %>%\n    pack_rows(\"Selected ALL or NONE\", 8, 9) %>%\n    footnote(general = paste(\"i = number of options in correct state; _ indicates option not selected\"),\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nComparison of Scoring Schemes for n = 5 options [ A,B,C,D,E ]\n \n\nResponse Scenario \nScores\n\n  \n    Correct Answer \n    Response \n    i  \n    Dichotomous \n    Partial [-1/n, +1/n] \n    Partial[-1/q, +1/p] \n  \n \n\n  Perfect Response\n\n    A____ \n    A____ \n    5 \n    1 \n    1.0 \n    1.00 \n  \n  Correct + Extra Incorrect Selections\n\n    A____ \n    AB___ \n    4 \n    0 \n    0.6 \n    0.75 \n  \n  \n    A____ \n    A___E \n    4 \n    0 \n    0.6 \n    0.75 \n  \n  \n    A____ \n    AB__E \n    3 \n    0 \n    0.2 \n    0.50 \n  \n  Only Incorrect Selections\n\n    A____ \n    ____E \n    3 \n    0 \n    0.2 \n    -0.25 \n  \n  \n    A____ \n    ___DE \n    2 \n    0 \n    -0.2 \n    -0.50 \n  \n  Completely Inverse Response \n\n    A____ \n    _BCDE \n    0 \n    0 \n    -1.0 \n    -1.00 \n  \n  Selected ALL or NONE\n\n    A____ \n    ABCDE \n    1 \n    0 \n    -0.6 \n    0.00 \n  \n  \n    A____ \n    _____ \n    4 \n    0 \n    0.6 \n    0.00 \n  \n\n\nNote:   i = number of options in correct state; _ indicates option not selected\n\n\n\n\nCODE\n#cleanup\nrm(dt, abs, correct,i,names,partial1,partial2,response,title)\n\n\n\nWe see that in the Dichotomous scheme, only the precisely correct response (row 1) yields a score other than zero. This scheme does now allow us to differentiate between different response patters.\nThe Partial \\([-1/n, +1/n]\\) scheme yields a range from \\([-1,1]\\), differentiating between responses. However, a blank response (bottom row) receives the same score (0.6) as the selection of the correct option + 1 incorrect option (row 2), which is problematic with for the goals of this study, where we need to differentiate between states of confusion or uncertainty yielding blank responses and the intentional selection of incorrect items.\nThe Partial \\([-1/q, +1/p]\\) scheme also yields a range of scores from \\([-1,1]\\). A blank response (bottom row) yields the same score (\\(0\\)) as the selection of all answer options (row 7); both are patterns of behavior we would expect to see if a respondent is confused or uncertain that there is a correct answer to the question.\n\nNext we consider an example from our study, with \\(n = 15\\) options and \\(p = 1\\) correct option to be selected.\n\n\nCODE\ntitle <- \"Comparison of Scoring Schemes for SGC3 with n=15 and p=1 options [A,B...N,O]  \"\n\ncorrect <- c( \"A____\",  \n              \"A____\",      \n              \"A____\",      \n              \"A____\",        \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\" ) \n\nresponse <- c(\"A__...__\",  \n              \"AB_...__\",      \n              \"A__..._O\",      \n              \"AB_..._O\",        \n              \"___..._O\",      \n              \"___...NO\",      \n              \"_BC...NO\",\n              \"ABC...NO\",      \n              \"___...__\" )\n\ni <- c(        15,       \n               14,              \n               14,              \n               13,\n               13,               \n               12,          \n               0,\n               1,\n               14)\n\nabs <- c(f_dichom(15,15), \n         f_dichom(14,15), \n         f_dichom(14,15), \n         f_dichom(13,15), \n         f_dichom(13,15),\n         f_dichom(12,15),\n         f_dichom(0,15),\n         f_dichom(1,15),\n         f_dichom(14,15))\n\npartial1 <- c(f_partialN(15,15), \n              f_partialN(14,15), \n              f_partialN(14,15), \n              f_partialN(13,15), \n              f_partialN(13,15),\n              f_partialN(12,15),\n              f_partialN(0,15),\n              f_partialN(1,15),\n              f_partialN(14,15))\n\npartial2 <- c(f_partialP(1,1,0,14), \n              f_partialP(1,1,1,14), \n              f_partialP(1,1,1,14), \n              f_partialP(1,1,2,14), \n              f_partialP(0,1,1,14),\n              f_partialP(0,1,2,14),\n              f_partialP(0,1,14,14),\n              f_partialP(1,1,14,14), \n              f_partialP(0,1,0,14))\n\nnames = c(    \"Correct Answer\",\n              \"Response\",\n              \"$i$ \",\n              \"Dichotomous\",\n              \"Partial [-1/n, +1/n]\",\n              \"Partial [-1/q, +1/p]\")\n\ndt <- data.frame(correct, response, i, abs, partial1 , partial2)\n\nkbl(dt, col.names = names, caption = title, digits=3) %>%\n  kable_classic() %>%\n    add_header_above(c(\"Response Scenario \" = 3, \"Scores\" = 3)) %>% \n    pack_rows(\"Perfect Response\", 1, 1) %>%\n    pack_rows(\"Correct + Extra Incorrect Selections\", 2, 4) %>%\n    pack_rows(\"Only Incorrect Selections\", 5, 6) %>%\n    pack_rows(\"Completely Inverse Response \", 7, 7) %>%\n    pack_rows(\"Selected ALL or NONE\", 8, 9) %>%\n    footnote(general = paste(\"i = number of options in correct state; _ indicates option not selected\"),\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nComparison of Scoring Schemes for SGC3 with n=15 and p=1 options [A,B...N,O]  \n \n\nResponse Scenario \nScores\n\n  \n    Correct Answer \n    Response \n    $i$  \n    Dichotomous \n    Partial [-1/n, +1/n] \n    Partial [-1/q, +1/p] \n  \n \n\n  Perfect Response\n\n    A____ \n    A__...__ \n    15 \n    1 \n    1.000 \n    1.000 \n  \n  Correct + Extra Incorrect Selections\n\n    A____ \n    AB_...__ \n    14 \n    0 \n    0.867 \n    0.929 \n  \n  \n    A____ \n    A__..._O \n    14 \n    0 \n    0.867 \n    0.929 \n  \n  \n    A____ \n    AB_..._O \n    13 \n    0 \n    0.733 \n    0.857 \n  \n  Only Incorrect Selections\n\n    A____ \n    ___..._O \n    13 \n    0 \n    0.733 \n    -0.071 \n  \n  \n    A____ \n    ___...NO \n    12 \n    0 \n    0.600 \n    -0.143 \n  \n  Completely Inverse Response \n\n    A____ \n    _BC...NO \n    0 \n    0 \n    -1.000 \n    -1.000 \n  \n  Selected ALL or NONE\n\n    A____ \n    ABC...NO \n    1 \n    0 \n    -0.867 \n    0.000 \n  \n  \n    A____ \n    ___...__ \n    14 \n    0 \n    0.867 \n    0.000 \n  \n\n\nNote:   i = number of options in correct state; _ indicates option not selected\n\n\n\n\nCODE\n#cleanup\nrm(dt, abs, correct,i,names,partial1,partial2,response,title)\n\n\nHere again we see that the Partial \\([-1/q, +1/p]\\) scheme allows us differentiate between patterns of responses, in a way that is more sensible for the goals of the SGC3 graph comprehension task.\n\n\n\n\n\n\nDecision\n\n\n\nThe Partial \\([-1/q, +1/p]\\) scheme is more appropriate for scoring the graph comprehension task than the Partial \\([-1/n, +1/n]\\) scheme because it allows us to differentially penalize incorrectly selected and incorrectly not selected answer options."
  },
  {
    "objectID": "analysis/utils/scoring.html#sec-scoringOverview",
    "href": "analysis/utils/scoring.html#sec-scoringOverview",
    "title": "Scoring Strategy",
    "section": "SCORING SGC DATA",
    "text": "SCORING SGC DATA\nIn SGC_3A we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key.\n5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\nFor each study in the SGC project, MR data will be scored by following these steps:\n(1) Preparing answer keys: For each dataset+question set combination, an answer key is that defines the ‘correct’ answer set under each interpretation of the graph (i.e. a triangular answer, an orthogonal answer, etc).\n(2) Calculate strategy scores: Using the strategy specific answer keys, an interpretation subscore is calculated for each response for each interpretation.\n(3) Interpretation classification: The interpretation subscores are compared in order to classify each response as a particular interpretation. If no classification can be made, the response is classified as ‘?’.\n(4) Calculate Absolute and Scaled Scores: Two final scores are calculated for each response; an Absolute score that indicates if the response was precisely correct according to the triangular interpretation, and a Scaled score that assigns a numeric value to the interpretation given by the response (ranging from -1 to +1)\n\n1. Prepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#LOAD INDIVIDUAL KEY FILES \nkey_111_raw <- read_csv('analysis/utils/keys/SGCX_scaffold_111_key.csv') %>% mutate(condition = \"DEFAULT\", phase = \"scaffold\")\nkey_121_raw <- read_csv('analysis/utils/keys/SGCX_scaffold_121_key.csv')%>% mutate(condition = 121, phase = \"scaffold\")\ncs = rep('c', 23) %>% str_c(collapse=\"\") #create column spec \nkey_test_raw <- read_csv('analysis/utils/keys/SGCX_test_key.csv', col_types = cs)%>% mutate(condition = \"DEFAULT\", phase = \"test\") \n\n#JOIN THEM\nkeys_raw <- rbind(key_111_raw, key_121_raw, key_test_raw )\n\n#CLEANUP\nrm(key_111_raw, key_121_raw, key_test_raw)\n\n\nIn order to calculate scores using the \\([-1/q, +1/p]\\) algorithm, we need to define the subset of all response options (set N) that should be selected (set P) and should not be selected (set Q). In order to calculate subscores for each graph interpretation (i.e. triangular, orthogonal, tversky) we must define these sets independently for each interpretation. For each question, the keys_raw dataframe gives us set N (all response options), and a set P (options that should be selected) for each interpretation. From these we must derive set Q for each interpretation.\n\nSET \\(N\\), all response options (superset) . This set is the same across all interpretations (a property of the question) and is given in the answer key.\nSET \\(P\\), \\(P \\subset N\\) , the subset of options that should be selected (rewarded as +1/p) . This set differs by interpretation, and is given in the answer key.\nSET \\(A, A \\subset N, A \\sqcup P\\) , the subset of options that should not be selected, but if they are, aren’t penalized (i.e. these options are ignored. Not rewarded, nor penalized). These include any options referenced in the question (i.e. select shifts that start at the same time as X; don’t penalize if they also select ‘X’), as well as options within 0.5hr offset from the data point to accommodate reasonable visual errors. This set differs by interpretation, and is given in the answer key (columns REF_POINT and _also).\nSET \\(Q\\), the subset of options that should not be selected and are penalized (as -1/q). This set differs by interpretation and is not given in the answer key. We can derive set Q for each interpretation by \\(Q = N - (P \\cup A)\\)\n\nThe next step in scoring is preparing interpretation-specific answer keys that specify sets N, P, A and Q for each question.\n\nTriangular Key\nFirst we construct a key set based on the ‘Triangular’ interpretation (i.e. the actually correct answers).\n\n\nCODE\nverify_tri = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TRIANGULAR KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tri <- keys_raw %>% \n  select(Q, condition, OPTIONS, TRIANGULAR, TRI_allow, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    TRI_allow = str_replace_na(TRI_allow,\"\"),\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TRIANGULAR,\n    set_p = str_replace_na(set_p,\"\"),#replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TRI_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"),#replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DEFINE SETS N, P, A\nfor (x in 1:nrow(keys_tri)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tri[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tri[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tri[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tri[x,'set_q'] = Q\n  keys_tri[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tri[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tri <- keys_tri %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tri <- keys_tri %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup \nrm(N,A,P,Q,n_q,s,x,tempunion)\n\n\nThis leaves us a dataframe keys_tri that define the sets of response options consistent with the triangular graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each option in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nOrthogonal Key\nNext we construct a key set based on the ‘Orthogonal’ interpretation.\n\n\nCODE\nverify_orth = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT ORTHOGONAL KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_orth <- keys_raw %>% \n  select(Q, condition, OPTIONS, ORTHOGONAL, ORTH_allow, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    ORTH_allow = str_replace_na(ORTH_allow,\"\"),\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = ORTHOGONAL,\n    set_p = str_replace_na(set_p,\"\"),#replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(ORTH_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answer options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_orth)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_orth[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_orth[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_orth[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_orth[x,'set_q'] = Q\n  keys_orth[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  # print(tempunion)\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n\n  verify_orth[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_orth <- keys_orth %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_orth <- keys_orth %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x, cs)\n\n\nThis leaves us a dataframe keys_orth that define the sets of response options consistent with the orthogonal graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nTversky Keys\nNext we construct the key set based on a partial-understanding strategy we refer to as ‘Tversky’. We use the label Tversky as shorthand for a partial interpretation of the coordinate system where subjects select a set of responses that lay along a connecting line from the referenced data point or referenced time for that item. The term is named for Barbara Tversky based on her work on graphical primitives (e.g. “lines connect, arrows direct, boxes contain”).\n\n\nCODE\nverify_max = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-MAX\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_max <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_max, TV_max_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_max = str_replace_na(TV_max,\"\"),\n    TV_max_allow = str_replace_na(TV_max_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_max,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_max_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_max)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_max[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_max[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_max[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_max[x,'set_q'] = Q\n  keys_tversky_max[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_max[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_max <- keys_tversky_max %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_max <- keys_tversky_max %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\nverify_tversky_start = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-START\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_start <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_start, TV_start_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_start = str_replace_na(TV_start,\"\"),\n    TV_start_allow = str_replace_na(TV_start_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_start,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_start_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_start)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_start[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_start[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_start[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_start[x,'set_q'] = Q\n  keys_tversky_start[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_start[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_start <- keys_tversky_start %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_start <- keys_tversky_start %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\nverify_tversky_end = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-END\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_end <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_end, TV_end_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_end = str_replace_na(TV_end,\"\"),\n    TV_end_allow = str_replace_na(TV_end_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_end,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_end_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_end)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_end[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_end[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_end[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_end[x,'set_q'] = Q\n  keys_tversky_end[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_end[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_end <- keys_tversky_end %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_end <- keys_tversky_end %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\nverify_tversky_duration = c()\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-DURATION\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_duration <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_dur, TV_dur_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_dur = str_replace_na(TV_dur,\"\"),\n    TV_dur_allow = str_replace_na(TV_dur_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_dur,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_dur_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_duration)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_duration[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_duration[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_duration[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_duration[x,'set_q'] = Q\n  keys_tversky_duration[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_duration[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_duration <- keys_tversky_duration %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_duration <- keys_tversky_duration %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\nThis leaves us four dataframes, each corresponding to a different variant of a ‘lines connecting to reference point’ strategy.\n- keys_tversky_max : the superset of lines connecting options - keys_tversky_start : lines connecting to the rightward diagonal (start time) of the reference point - keys_tversky_end: lines connecting to the leftward diagonal (end time) of the reference point - keys_tversky_duration: lines connecting to the horizontal y-intercept (duration) of the reference point\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nSatisficing Key\nNext we construct two keys based on the ‘Satisficing’ strategy. Satisficing involves selecting any data points within 0.5hr visual offset of the orthogonal interpretation of the graph (because no orthogonal response option is available). One key represents selecting a point slightly to the left of the orthogonal, and the other key represents selecting a point slightly to the right of the orthogonal. The “Satisficing” strategy involves the reader selecting data points nearest to the orthogonal projection from the reference point in the question. We observe this strategy in some readers when there is no orthogonal response available (i.e. in the impasse condition), so they select the points nearest to the projection (i.e. “close enough”).\n\n\nCODE\nverify_satisfice_right = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT SATISFICE RIGHT KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_satisfice_right <- keys_raw %>% \n  select(Q, condition, OPTIONS, SATISFICE_right, REF_POINT) %>% \n  mutate(\n    #replace NAs\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n\n    #P options that SHOULD be selected (rewarded)\n    set_p = SATISFICE_right,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n\n    #A options that are ignored if selected\n    set_a = str_c(REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n\n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_satisfice_right)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_satisfice_right[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_satisfice_right[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_satisfice_right[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to data frame\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_satisfice_right[x,'set_q'] = Q\n  keys_satisfice_right[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_satisfice_right[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_satisfice_right <- keys_satisfice_right %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q)%>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_satisfice_right <- keys_satisfice_right %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\n\nverify_satisfice_left = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT SATISFICE left KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_satisfice_left <- keys_raw %>% \n  select(Q, condition, OPTIONS, SATISFICE_left, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = SATISFICE_left,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_satisfice_left)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_satisfice_left[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_satisfice_left[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_satisfice_left[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to data frame\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_satisfice_left[x,'set_q'] = Q\n  keys_satisfice_left[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_satisfice_left[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_satisfice_left <- keys_satisfice_left %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q)%>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_satisfice_left <- keys_satisfice_left %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\nThis leaves us a dataframe keys_satisfice that define the sets of response options consistent with the orthogonal graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nCODE\n#cleanup\nrm(verify_tri, verify_orth, verify_max, verify_tversky_duration, verify_tversky_end, verify_tversky_start, verify_satisfice_right, verify_satisfice_left)\n\n\nFinally, we need to clean up and generalize our answer keys to accommodate the experimental conditions for Study SGC4-SGC5. In both of these studies the answer set (and underlying graphed data set) are identical, the conditions differ only based on the structure of the gridlines or marks used to represent the data, or interactive mode of the answer format.\n\n\nCODE\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nwrite.csv(keys_raw,\"analysis/utils/keys/parsed_keys/keys_raw\", row.names = FALSE)\nwrite.csv(keys_orth,\"analysis/utils/keys/parsed_keys/keys_orth\", row.names = FALSE)\nwrite.csv(keys_tri,\"analysis/utils/keys/parsed_keys/keys_tri\", row.names = FALSE)\nwrite.csv(keys_satisfice_left,\"analysis/utils/keys/parsed_keys/keys_satisfice_left\", row.names = FALSE)\nwrite.csv(keys_satisfice_right,\"analysis/utils/keys/parsed_keys/keys_satisfice_right\", row.names = FALSE)\nwrite.csv(keys_tversky_duration,\"analysis/utils/keys/parsed_keys/keys_tversky_duration\", row.names = FALSE)\nwrite.csv(keys_tversky_end,\"analysis/utils/keys/parsed_keys/keys_tversky_end\", row.names = FALSE)\nwrite.csv(keys_tversky_max,\"analysis/utils/keys/parsed_keys/keys_tversky_max\", row.names = FALSE)\nwrite.csv(keys_tversky_start,\"analysis/utils/keys/parsed_keys/keys_tversky_start\", row.names = FALSE)\n\n\n\n\n\n2. Calculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangular interpretation?\nscore_ORTH How consistent is the response with the orthogonal interpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation?\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\nTo facilitate scoring, we import the following helper functions in each scoring script.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\nprint(calc_subscore)\n\n\nfunction (question, cond, response, keyframe) \n{\n    if (cond == 121 & question < 6) {\n        p = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% \n            unlist()\n        q = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% \n            unlist()\n        pn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(n_p)\n        qn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(n_q)\n    }\n    else {\n        p = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(set_p) %>% pull(set_p) %>% \n            str_split(\"\") %>% unlist()\n        q = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(set_q) %>% pull(set_q) %>% \n            str_split(\"\") %>% unlist()\n        pn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(n_p)\n        qn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(n_q)\n    }\n    if (response != \"\") {\n        response = response %>% str_split(\"\") %>% unlist()\n    }\n    ps = length(intersect(response, p))\n    qs = length(intersect(response, q))\n    x = f_partialP(ps, pn, qs, qn) %>% unlist() %>% as.numeric()\n    rm(p, q, pn, qn, ps, qs)\n    return(x)\n}\n\n\n\n\n3. Derive Interpretation\nNext, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\nprint(derive_interpretation)\n\n\nfunction (df) \n{\n    threshold_range = 0.5\n    threshold_frenzy = 4\n    for (x in 1:nrow(df)) {\n        t = df[x, ] %>% dplyr::select(score_TV_max, score_TV_start, \n            score_TV_end, score_TV_duration)\n        t.long = gather(t, score, value, 1:4)\n        t.long[t.long == \"\"] = NA\n        if (length(unique(t.long$value)) == 1) {\n            if (is.na(unique(t.long$value))) {\n                df[x, \"score_TVERSKY\"] = NA\n                df[x, \"tv_type\"] = NA\n            }\n        }\n        else {\n            df[x, \"score_TVERSKY\"] = as.numeric(max(t.long$value, \n                na.rm = TRUE))\n            df[x, \"tv_type\"] = t.long[which.max(t.long$value), \n                \"score\"]\n        }\n        t = df[x, ] %>% dplyr::select(score_SAT_left, score_SAT_right)\n        t.long = gather(t, score, value, 1:2)\n        t.long[t.long == \"\"] = NA\n        if (length(unique(t.long$value)) == 1) {\n            if (is.na(unique(t.long$value))) {\n                df[x, \"score_SATISFICE\"] = NA\n                df[x, \"sat_type\"] = NA\n            }\n        }\n        else {\n            df[x, \"score_SATISFICE\"] = as.numeric(max(t.long$value, \n                na.rm = TRUE))\n            df[x, \"sat_type\"] = t.long[which.max(t.long$value), \n                \"score\"]\n        }\n        t = df[x, ] %>% dplyr::select(score_TRI, score_TVERSKY, \n            score_SATISFICE, score_ORTH)\n        t.long = gather(t, score, value, 1:4)\n        t.long[t.long == \"\"] = NA\n        df[x, \"top_score\"] = as.numeric(max(t.long$value, na.rm = TRUE))\n        df[x, \"top_type\"] = t.long[which.max(t.long$value), \"score\"]\n        r = as.numeric(range(t.long$value, na.rm = TRUE))\n        r = diff(r)\n        df[x, \"range\"] = r\n        if (r < threshold_range) {\n            df[x, \"best\"] = \"?\"\n        }\n        else {\n            p = df[x, \"top_type\"]\n            if (p == \"score_TRI\") {\n                df[x, \"best\"] = \"Triangular\"\n            }\n            else if (p == \"score_ORTH\") {\n                df[x, \"best\"] = \"Orthogonal\"\n            }\n            else if (p == \"score_TVERSKY\") {\n                df[x, \"best\"] = \"Tversky\"\n            }\n            else if (p == \"score_SATISFICE\") {\n                df[x, \"best\"] = \"Satisfice\"\n            }\n        }\n        if (!is.na(df[x, \"score_BOTH\"])) {\n            if (df[x, \"score_BOTH\"] == 1) {\n                df[x, \"best\"] = \"both tri + orth\"\n            }\n        }\n        if (df[x, \"num_o\"] == 0) {\n            df[x, \"best\"] = \"blank\"\n        }\n        if (df[x, \"num_o\"] > threshold_frenzy) {\n            df[x, \"best\"] = \"frenzy\"\n        }\n        if (!is.na(df[x, \"score_REF\"])) {\n            if (df[x, \"score_REF\"] == 1) {\n                df[x, \"best\"] = \"reference\"\n            }\n        }\n    }\n    rm(t, t.long, x, r, p)\n    rm(threshold_frenzy, threshold_range)\n    df$int2 <- factor(df$best, levels = c(\"Triangular\", \"Tversky\", \n        \"Satisfice\", \"Orthogonal\", \"reference\", \"both tri + orth\", \n        \"blank\", \"frenzy\", \"?\"))\n    df$interpretation <- factor(df$best, levels = c(\"Orthogonal\", \n        \"Satisfice\", \"frenzy\", \"?\", \"reference\", \"blank\", \"both tri + orth\", \n        \"Tversky\", \"Triangular\"))\n    df$high_interpretation <- fct_collapse(df$interpretation, \n        orthogonal = c(\"Satisfice\", \"Orthogonal\"), neg.trans = c(\"frenzy\", \n            \"?\"), neutral = c(\"reference\", \"blank\"), pos.trans = c(\"Tversky\", \n            \"both tri + orth\"), triangular = \"Triangular\")\n    df$tv_type = as.factor(df$tv_type)\n    df$top_type = as.factor(df$top_type)\n    df$high_interpretation = factor(df$high_interpretation, levels = c(\"orthogonal\", \n        \"neg.trans\", \"neutral\", \"pos.trans\", \"triangular\"))\n    df <- df %>% dplyr::select(-best)\n    return(df)\n}\n\n\n\n\n4. Derive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\nprint(calc_scaled)\n\n\nfunction (v) \n{\n    v <- recode(v, Orthogonal = -1, Satisfice = -1, frenzy = -0.5, \n        `?` = -0.5, reference = 0, blank = 0, `both tri + orth` = 0.5, \n        Tversky = 0.5, Triangular = 1)\n    return(v)\n}\n\n\n\n\n5. Summarize by Subject\nThe final step in the scoring procedure is to summarise the item-level scores by subject, and save certain summaries to the subject-level record. We also construct two long-format dataframes containing cummulative progress scores (the point-in-time [absolute, scaled] scores for each subject on each question).\n\n\nCODE\nprint(summarise_bySubject)\n\n\nfunction (subjects, items) \n{\n    subjects_summary <- items %>% filter(q %nin% c(6, 9)) %>% \n        group_by(subject) %>% dplyr::summarise(subject = as.character(subject), \n        s_TRI = sum(score_TRI, na.rm = TRUE), s_ORTH = sum(score_ORTH, \n            na.rm = TRUE), s_TVERSKY = sum(score_TVERSKY, na.rm = TRUE), \n        s_SATISFICE = sum(score_SATISFICE, na.rm = TRUE), s_REF = sum(score_REF, \n            na.rm = TRUE), s_ABS = sum(score_ABS, na.rm = TRUE), \n        s_NABS = sum(score_niceABS, na.rm = TRUE), s_SCALED = sum(score_SCALED, \n            na.rm = TRUE), DV_percent_NABS = s_NABS/13, rt_m = sum(rt_s)/60, \n        item_avg_rt = mean(rt_s), item_min_rt = min(rt_s), item_max_rt = max(rt_s), \n        item_n_TRI = sum(interpretation == \"Triangular\"), item_n_ORTH = sum(interpretation == \n            \"Orthogonal\"), item_n_TV = sum(interpretation == \n            \"Tversky\"), item_n_SAT = sum(interpretation == \"Satisfice\"), \n        item_n_OTHER = sum(interpretation %nin% c(\"Triangular\", \n            \"Orthogonal\", \"Tversky\", \"Satisfice\")), item_n_POS = sum(high_interpretation == \n            \"pos.trans\"), item_n_NEG = sum(high_interpretation == \n            \"neg.trans\"), item_n_NEUTRAL = sum(high_interpretation == \n            \"neutral\")) %>% arrange(subject) %>% slice(1L)\n    subjects_q1 <- items %>% filter(q == 1) %>% mutate(item_q1_NABS = score_niceABS, \n        item_q1_SCALED = score_SCALED, item_q1_interpretation = interpretation, \n        item_q1_rt = rt_s, ) %>% dplyr::select(subject, item_q1_NABS, \n        item_q1_SCALED, item_q1_interpretation, item_q1_rt) %>% \n        arrange(subject)\n    subjects_q5 <- items %>% filter(q == 5) %>% mutate(item_q5_NABS = score_niceABS, \n        item_q5_SCALED = score_SCALED, item_q5_interpretation = interpretation, \n        item_q5_rt = rt_s, ) %>% dplyr::select(subject, item_q5_NABS, \n        item_q5_SCALED, item_q5_interpretation, item_q5_rt) %>% \n        arrange(subject)\n    subjects_q7 <- items %>% filter(q == 7) %>% mutate(item_q7_NABS = score_niceABS, \n        item_q7_interpretation = interpretation, item_q7_rt = rt_s, \n        ) %>% dplyr::select(subject, item_q7_NABS, item_q7_interpretation, \n        item_q7_rt) %>% arrange(subject)\n    subjects_q15 <- items %>% filter(q == 15) %>% mutate(item_q15_NABS = score_niceABS, \n        item_q15_interpretation = interpretation, item_q15_rt = rt_s, \n        ) %>% dplyr::select(subject, item_q15_NABS, item_q15_interpretation, \n        item_q15_rt) %>% arrange(subject)\n    subjects_scaffold <- items %>% filter(q < 6) %>% group_by(subject) %>% \n        dplyr::summarise(item_scaffold_NABS = sum(score_niceABS), \n            item_scaffold_SCALED = sum(score_SCALED), item_scaffold_rt = sum(rt_s)/60) %>% \n        dplyr::select(subject, item_scaffold_NABS, item_scaffold_SCALED, \n            item_scaffold_rt) %>% arrange(subject)\n    subjects_test <- items %>% filter(q %nin% c(1, 2, 3, 4, 5, \n        6, 9)) %>% group_by(subject) %>% dplyr::summarise(item_test_NABS = sum(score_niceABS), \n        item_test_SCALED = sum(score_SCALED), item_test_rt = sum(rt_s)/60) %>% \n        dplyr::select(subject, item_test_NABS, item_test_SCALED, \n            item_test_rt) %>% arrange(subject)\n    print(unique(subjects_summary$subject == subjects$subject))\n    print(unique(subjects_summary$subject == subjects_q1$subject))\n    print(unique(subjects_summary$subject == subjects_q5$subject))\n    print(unique(subjects_summary$subject == subjects_q7$subject))\n    print(unique(subjects_summary$subject == subjects_q15$subject))\n    print(unique(subjects_summary$subject == subjects_scaffold$subject))\n    print(unique(subjects_summary$subject == subjects_test$subject))\n    x = merge(subjects, subjects_summary, by.x = \"subject\", by.y = \"subject\")\n    x = merge(x, subjects_q1)\n    x = merge(x, subjects_q5)\n    x = merge(x, subjects_q7)\n    x = merge(x, subjects_q15)\n    x = merge(x, subjects_scaffold)\n    x = merge(x, subjects_test)\n    subjects <- x\n    rm(subjects_q1, subjects_q5, subjects_q7, subjects_q15, subjects_scaffold, \n        subjects_test, subjects_summary, x)\n    return(subjects)\n}\n\n\nCODE\nprint(progress_Absolute)\n\n\nfunction (items) \n{\n    x <- items %>% filter(q %nin% c(6, 9)) %>% dplyr::select(subject, \n        mode, pretty_condition, q, score_niceABS)\n    wide <- x %>% pivot_wider(names_from = q, names_glue = \"q_{q}\", \n        values_from = score_niceABS)\n    wide$c1 = wide$q_1\n    wide$c2 = wide$c1 + wide$q_2\n    wide$c3 = wide$c2 + wide$q_3\n    wide$c4 = wide$c3 + wide$q_4\n    wide$c5 = wide$c4 + wide$q_5\n    wide$c6 = wide$c5 + wide$q_7\n    wide$c7 = wide$c6 + wide$q_8\n    wide$c8 = wide$c7 + wide$q_10\n    wide$c9 = wide$c8 + wide$q_11\n    wide$c10 = wide$c9 + wide$q_12\n    wide$c11 = wide$c10 + wide$q_13\n    wide$c12 = wide$c11 + wide$q_14\n    wide$c13 = wide$c12 + wide$q_15\n    wide <- wide %>% dplyr::select(subject, mode, pretty_condition, \n        c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13)\n    df_absolute_progress <- wide %>% pivot_longer(cols = c1:c13, \n        names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n    df_absolute_progress$question <- as.integer(df_absolute_progress$question)\n    rm(x, wide)\n    return(df_absolute_progress)\n}\n\n\nCODE\nprint(progress_Scaled)\n\n\nfunction (items) \n{\n    x <- items %>% filter(q %nin% c(6, 9)) %>% select(subject, \n        mode, pretty_condition, q, score_SCALED)\n    wide <- x %>% pivot_wider(names_from = q, names_glue = \"q_{q}\", \n        values_from = score_SCALED)\n    wide$c1 = wide$q_1\n    wide$c2 = wide$c1 + wide$q_2\n    wide$c3 = wide$c2 + wide$q_3\n    wide$c4 = wide$c3 + wide$q_4\n    wide$c5 = wide$c4 + wide$q_5\n    wide$c6 = wide$c5 + wide$q_7\n    wide$c7 = wide$c6 + wide$q_8\n    wide$c8 = wide$c7 + wide$q_10\n    wide$c9 = wide$c8 + wide$q_11\n    wide$c10 = wide$c9 + wide$q_12\n    wide$c11 = wide$c10 + wide$q_13\n    wide$c12 = wide$c11 + wide$q_14\n    wide$c13 = wide$c12 + wide$q_15\n    wide <- wide %>% select(subject, mode, pretty_condition, \n        c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13)\n    df_scaled_progress <- wide %>% pivot_longer(cols = c1:c13, \n        names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n    df_scaled_progress$question <- as.integer(df_scaled_progress$question)\n    rm(x, wide)\n    return(df_scaled_progress)\n}\n\n\n\n\n\n\n\n\nSchmidt, Dennis, Tobias Raupach, Annette Wiegand, Manfred Herrmann, and Philipp Kanzow. 2021. “Relation Between Examinees’ True Knowledge and Examination Scores: Systematic Review and Exemplary Calculations on Multiple-True-False Items.” Educational Research Review 34 (November): 100409. https://doi.org/10.1016/j.edurev.2021.100409."
  },
  {
    "objectID": "analysis/utils/modelling_ref.html",
    "href": "analysis/utils/modelling_ref.html",
    "title": "Modelling Reference",
    "section": "",
    "text": "In this notebook we use data from study SGC3A to explore different modelling techniques and assess their suitability for the bimodal accuracy distributions in the SGC project data."
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#continuous-outcome",
    "href": "analysis/utils/modelling_ref.html#continuous-outcome",
    "title": "Modelling Reference",
    "section": "CONTINUOUS OUTCOME",
    "text": "CONTINUOUS OUTCOME\nDoes CONDITION have an effect on TEST PHASE ABSOLUTE SCORE?\n(# questions correct on test phase of task, [in lab] participants)\n(Can also be transformed to proportion or percentage).\n\n\nCODE\n#::::::::::::SETUP DATA\ndf = df_subjects %>% filter(mode == \"lab-synch\")\n\n#::::::::::::DESCRIPTIVES\nmosaic::favstats(test_score ~ condition, data = df)\n\n\n  condition min Q1 median Q3 max mean   sd  n missing\n1   control   0  0      0  1   8 1.71 3.05 62       0\n2   impasse   0  0      2  7   8 3.33 3.40 64       0\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\n\n# #GGFORMULA | FACETED HISTOGRAM\n# stats = df %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))\n# gf_props(~item_test_NABS, \n#          fill = ~pretty_condition, data = df) %>% \n#   gf_facet_grid(~pretty_condition) %>% \n#   gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n#   labs(x = \"# Correct\",\n#        y = \"proportion of subjects\",\n#        title = \"Test Phase Absolute Score (# Correct)\",\n#        subtitle = \"\") + theme(legend.position = \"blank\")\n\n##GGPUBR | HIST+DENSITY SCORE \np <- gghistogram(df, x = \"test_score\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"condition\")) +\n  labs( title = \"Distribution of TEST Absolute Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"Total Absolute Score (Test Phase)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = condition, y = test_score,\n                        fill = condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=condition, y = test_score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=condition, y = test_score, color = condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\nIndependent Samples T-Test (Student’s T)\n\nTests null hypothesis that true difference in population mean is == 0\nAssumes normally distributed variables\nAssumes equal variance of samples (homogeneity of variance)\nSGC accuracy data violate both homogeneity and normality*\n\n\n\nCODE\n(t <- t.test( test_score ~ condition,data = df,\n              paired = FALSE, var.equal = TRUE, alternative = c(\"two.sided\"))) # less, greater for one sided tests\n\n\n\n    Two Sample t-test\n\ndata:  test_score by condition\nt = -3, df = 124, p-value = 0.006\nalternative hypothesis: true difference in means between group control and group impasse is not equal to 0\n95 percent confidence interval:\n -2.759 -0.478\nsample estimates:\nmean in group control mean in group impasse \n                 1.71                  3.33 \n\n\nCODE\nreport(t)\n\n\nWarning in .effectsize_t.test(model, type = type, verbose = verbose, ...):\nUnable to retrieve data from htest object. Using t_to_d() approximation.\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Two Sample t-test testing the difference of test_score by condition (mean in group control = 1.71, mean in group impasse = 3.33) suggests that the effect is negative, statistically significant, and medium (difference = -1.62, 95% CI [-2.76, -0.48], t(124) = -2.81, p = 0.006; Cohen's d = -0.50, 95% CI [-0.86, -0.15])\n\n\n\n\nCODE\n#STATSPLOT | VIOLIN\n\n#one tailed tests must be done manually by extracting results expression and adding as subtitle\n#default is two tailed test\n# results <- two_sample_test( data = df, x = pretty_condition, \n#                             y = item_test_NABS,\n#                             alternative = \"g\")\n\nggbetweenstats(y = test_score, x = condition, data = df,\n               type = \"parametric\", var.equal = TRUE,\n               alternative  = \"g\",\n               pairwide.display = \"significant\", ) \n\n\n\n\n\nCODE\n# + labs(subtitle = results$expression[[1]])\n\n\n\n\nIndependent Samples T-Test (Welch’s T)\n\nTests null hypothesis that true difference in population mean is == 0\nAssumes normally distributed variables\nDoes not assumes equal variance of samples (homogeneity of variance)\nSGC accuracy data violates normality assumption\n\n\n\nCODE\n(t <- t.test( test_score ~ condition,data = df,\n              paired = FALSE, var.equal = FALSE))\n\n\n\n    Welch Two Sample t-test\n\ndata:  test_score by condition\nt = -3, df = 123, p-value = 0.006\nalternative hypothesis: true difference in means between group control and group impasse is not equal to 0\n95 percent confidence interval:\n -2.76 -0.48\nsample estimates:\nmean in group control mean in group impasse \n                 1.71                  3.33 \n\n\nCODE\nreport(t)\n\n\nWarning in .effectsize_t.test(model, type = type, verbose = verbose, ...):\nUnable to retrieve data from htest object. Using t_to_d() approximation.\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of test_score by condition (mean in group control = 1.71, mean in group impasse = 3.33) suggests that the effect is negative, statistically significant, and medium (difference = -1.62, 95% CI [-2.76, -0.48], t(123.25) = -2.81, p = 0.006; Cohen's d = -0.51, 95% CI [-0.86, -0.15])\n\n\n\n\nCODE\n#STATSPLOT | VIOLIN\nggbetweenstats(y = test_score, x = condition, data = df,\n               type = \"parametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\n\n\nYuen’s T-Test (Trimmed Means)\n\nRobust alternative to to t-test is Yuen’s t-test which uses trimmed means\nTrimmed means are not desireable for this research scenario because they trim data from the extremes, and in this study these are true, interesting values\nhttps://garstats.wordpress.com/2017/11/28/trimmed-means/\n\n\n\nCODE\n(y <- yuenbt( formula = test_score ~ condition, data = df,\n              side = TRUE, EQVAR = FALSE))\n\n\nCall:\nyuenbt(formula = test_score ~ condition, data = df, side = TRUE, \n    EQVAR = FALSE)\n\nTest statistic: -2.99 (df = NA), p-value = 0.00668\n\nTrimmed mean difference:  -2.53 \n95 percent confidence interval:\n-4.04     -1.02 \n\n\n\n\nCODE\n#STATSPLOT | VIOLIN\nggbetweenstats(y = test_score, x = condition, data = df,\n               type = \"robust\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\n\n\nWilcoxon Rank Sum (Mann-Whitney Test)\n\nNon parametric alternative to t-test; compares median rather than mean by ranking data\nDoes not assume normality\nDoes not assume equal variance of samples (homogeneity of variance)\nAppropriate for SGC accuracy data\nhttps://www.datanovia.com/en/lessons/wilcoxon-test-in-r/#two-sample-wilcoxon-test\nhttps://data.library.virginia.edu/the-wilcoxon-rank-sum-test/\n\n\n\nCODE\n(w <- wilcox.test(df$test_score ~ df$condition,\n                 paired = FALSE, alternative = \"two.sided\")) #less, greater\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df$test_score by df$condition\nW = 1438, p-value = 0.003\nalternative hypothesis: true location shift is not equal to 0\n\n\nCODE\nreport(w)\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum test with continuity correction testing the difference in ranks between df$test_score and df$condition suggests that the effect is negative, statistically significant, and medium (W = 1438.00, p = 0.003; r (rank biserial) = -0.28, 95% CI [-0.45, -0.08])\n\n\nCODE\n#effect size\ndf %>% wilcox_effsize(test_score ~ condition)\n\n\n# A tibble: 1 × 7\n  .y.        group1  group2  effsize    n1    n2 magnitude\n* <chr>      <chr>   <chr>     <dbl> <int> <int> <ord>    \n1 test_score control impasse   0.263    62    64 small    \n\n\n\n\nCODE\n#STATSPLOT | VIOLIN\nggbetweenstats(y = test_score, x = condition, data = df,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\n\n\nFloor/Ceiling Corrections + Wilcoxon Rank Sum\n\nThe bimodal distribution of the subject-level score data do not meet the requirements for t-tests.\nHowever, a non-parametric alternative is available (Wilcoxon rank sum test / Man-Whitney test)\nAdditional corrections are available for data with ‘floor’ and/or ‘ceiling’ effects via the ‘DACR’ package\nhttps://link.springer.com/article/10.3758/s13428-020-01407-2#Sec14 see also https://qmliu.shinyapps.io/DACFE/\n\nFor comparison we run a standard followed by a Wilcoxon rank-sum (Mann-Whitney) test that is a nonparametric alternative for non-normally distributed data.\n\n\nCODE\n(t <- wilcox.test(df$test_score ~ df$condition))\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df$test_score by df$condition\nW = 1438, p-value = 0.003\nalternative hypothesis: true location shift is not equal to 0\n\n\nNext, we calculate the t-test and ANOVA (F-test) based on a series of corrections provided for data with floor and/or ceiling effects.\n\nhttps://link.springer.com/article/10.3758/s13428-020-01407-2#Sec14\n\nUsing properties from truncated normal distributions, the authors propose an easy-to-use method for the t-test and ANOVA with ceiling/floor data.\nThe proposed method calculates the degrees of freedom based on the after-truncation sample sizes (where l = number of floor observations, and r = number of ceiling observations). The rationale was that the proposed method utilizes full information only from data points of n − r − l participants and partial information from data points of r + l participants of a group for the mean and variance estimation. Specifically, the corrected mean and variance estimates (Eqs. 14 and 15) are functions of mean and variance estimates using after-truncation data (n − r − l participants) and the standardized floor and ceiling threshold estimates. The thresholds are estimated using the ceiling and floor percentage estimates based on data points of n − r and n − l participants, respectively. This is a relatively conservative approach for calculating the degrees of freedom, which can help control the type I error rate. This feature can be beneficial, especially given the “replication crisis” in psychological and behavioral research.\n\n\nCODE\n#FLOOR-CEILING ADJUSTED T TESTS\nlibrary(DACF) #tests for data with floor and ceiling \n# https://www.rdocumentation.org/packages/DACF/versions/1.0.0\n\n#prepare data [vector of scores for each group]\nscore_111 <- df %>% filter(condition == \"control\") %>% dplyr::select(test_score) %>% pull()\nscore_121 <- df %>% filter(condition == \"impasse\") %>% dplyr::select(test_score) %>% pull()\n\n# recover the mean and variance for ceiling/floor data\na <- rec.mean.var(score_111) %>% unlist()\n# recover the mean and variance for ceiling/floor data\nb <- rec.mean.var(score_121) %>% unlist()\nr <- as.data.frame(rbind(\"control\"=a ,\"impasse\"=b))\nr\n\n\n        ceiling.percentage floor.percentage est.mean est.var\ncontrol              0.113            0.710    -7.70   207.4\nimpasse              0.172            0.422     2.39    51.8\n\n\nCODE\n# method \"a\" uses original sample size\n# method \"b\" uses after-truncation sample size\n\n# perform adjusted t test\nlw.t.test(score_111,score_121,\"b\")\n\n\n$statistic\n[1] -4.94\n\n$p.value\n[1] 0.000012\n\n$est.d\n[1] -0.887\n\n$conf.int\n[1] -14.22  -5.97\n\n\nCODE\n#FLOOR-CEILING ADJUSTED F* TEST ANOVA\nlw.f.star(df,test_score~condition,\"b\")\n\n\n$statistic\n[1] 5.85\n\n$p.value\n[1] 0.0321\n\n$est.f.squared\n[1] 0.158\n\n\nCODE\n# method \"a\" uses original sample size\n# method \"b\" uses after-truncation sample size\n\n\nThe control condition has 11% of data at ceiling and 71% at floor, with corrected mean of -7 and variance of 207. The impasse condition has 17% at ceiling, and only 42% at floor, with with corrected mean of 2.39 and variance respectively as of 58.\nA corrected t-test t statistic is -4.94, p = 0.05. The estimated Cohen’s d is -0.89 with a confidence interval 0f [-14.22, -5.97].\nA correct F-test (ANOVA) has a corrected F-statstic of 5.85, p < 0.05, and Fsquared effect size of 0.195\n\n\nLinear Regression\n\nAssumes homogeneity of variance\nAssumes normally distributed residuals\nwhen default dummy coding is used;\n\nintercept = predicted mean of first group,\npredictor coefficient = difference to mean of second group\n\n\n\n\nCODE\n#SCORE predicted by CONDITION\nlm.1 <- lm(test_score ~ condition, data = df)\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(lm.1)\n\n\n\nCall:\nlm(formula = test_score ~ condition, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3.33  -1.71  -1.71   3.67   6.29 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         1.710      0.411    4.16 0.000058 ***\nconditionimpasse    1.618      0.576    2.81   0.0058 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.23 on 124 degrees of freedom\nMultiple R-squared:  0.0598,    Adjusted R-squared:  0.0522 \nF-statistic: 7.89 on 1 and 124 DF,  p-value: 0.00579\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(lm.1)\n\n\nAnalysis of Variance Table\n\nResponse: test_score\n           Df Sum Sq Mean Sq F value Pr(>F)   \ncondition   1     82    82.5    7.89 0.0058 **\nResiduals 124   1297    10.5                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(lm.1)\n\n\n                 2.5 % 97.5 %\n(Intercept)      0.897   2.52\nconditionimpasse 0.478   2.76\n\n\nCODE\nreport(lm.1) #sanity check\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict test_score with condition (formula: test_score ~ condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.06, F(1, 124) = 7.89, p = 0.006, adj. R2 = 0.05). The model's intercept, corresponding to condition = control, is at 1.71 (95% CI [0.90, 2.52], t(124) = 4.16, p < .001). Within this model:\n\n  - The effect of condition [impasse] is statistically significant and positive (beta = 1.62, 95% CI [0.48, 2.76], t(124) = 2.81, p = 0.006; Std. beta = 0.49, 95% CI [0.14, 0.83])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\ncheck_model(lm.1)\n\n\n\n\n\n\n\nCODE\n#MODEL ESTIMATES WITH UNCERTAINTY\n\n#setup references\nm <- lm.1\ndf <- df\ncall <- m$call %>% as.character()\n\n# uncertainty model visualization\ndf  %>%\n  modelr::data_grid(condition) %>%\n  augment(lm.1, newdata = ., se_fit = TRUE) %>%\n  ggplot(aes(y = condition, color = condition)) +\n  stat_halfeye( scale = .5,\n      aes(\n        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n        fill = stat(cut_cdf_qi(cdf,\n                .width = c(.90, .95),\n                labels = scales::percent_format())))) +\n  scale_fill_brewer(direction = -1) +\n  labs (title = \"(LAB) Test Phase Accuracy ~ Condition\",\n        x = \"model predicted mean (% correct)\", y = \"Condition\", fill = \"Interval\",\n        subtitle = paste(\"lm(\",call[2],\")\")\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nCODE\n#sjPlot\np1 <- plot_model(lm.1,  type = \"eff\", \n           show.data = TRUE, jitter = TRUE,\n           show.p = TRUE) \n\n#BS TO MANUALLY ADD REGRESSION FORMULA AS SUBTITLE \n# library(equatiomatic)\n# library(latex2exp)\n# (x <- extract_eq(lm.1, use_coefs = TRUE, ital_vars=TRUE, coef_digits = 1, raw_tex = FALSE))\n# b = TeX(x)\n# p1[[\"condition\"]][[\"labels\"]][[\"subtitle\"]]= expression( paste(widehat(accuracy), \" = \", 1.7, \" + \", \"1.6\", \"*\", condition[impasse]) )\np1\n\n\n$condition\n\n\n\n\n\n\n\nCensored (Tobit) Regression\nhttps://stats.oarc.ucla.edu/r/dae/tobit-models/\nFor censored data (i.e. truncated axis). The tobit model, also called a censored regression model, is designed to estimate linear relationships between variables when there is either left- or right-censoring in the dependent variable (also known as censoring from below and above, respectively). Censoring from above takes place when cases with a value at or above some threshold, all take on the value of that threshold, so that the true value might be equal to the threshold, but it might also be higher. In the case of censoring from below, values those that fall at or below some threshold are censored.\n\ncensored vs truncated : There is sometimes confusion about the difference between truncated data and censored data.\nWith censored variables, all of the observations are in the dataset, but we don’t know the “true” values of some of them.\nWith truncation some of the observations are not included in the analysis because of the value of the variable.\nWhen a variable is censored, regression models for truncated data provide inconsistent estimates of the parameters. See Long (1997, chapter 7) for a more detailed discussion of problems of using regression models for truncated data to analyze censored data.\n\n\n\nCODE\n#set censoring values \nlo = 0\nhi = 8 \nrange(df$test_score)\n\n\n[1] 0 8\n\n\nCODE\nprint(\"Lo and Hi should equate to upper and lower bounds of the # Qs \")\n\n\n[1] \"Lo and Hi should equate to upper and lower bounds of the # Qs \"\n\n\nCODE\nlibrary(VGAM)\n\n#FIT MODEL\nm1<- vglm(test_score ~ condition, tobit(Lower = lo, Upper =hi ), data = df)\nsummary(m1)\n\n\n\nCall:\nvglm(formula = test_score ~ condition, family = tobit(Lower = lo, \n    Upper = hi), data = df)\n\nCoefficients: \n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept):1      -4.185      1.483   -2.82   0.0048 ** \n(Intercept):2       2.203      0.127   17.38   <2e-16 ***\nconditionimpasse    5.577      1.982    2.81   0.0049 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: mu, loglink(sd)\n\nLog-likelihood: -196 on 249 degrees of freedom\n\nNumber of Fisher scoring iterations: 11 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nCODE\n#CONFIDENCE INTERVALS\nb <- coef(m1)\nse <- sqrt(diag(vcov(m1)))\ncbind(LL = b - qnorm(0.975) * se, UL = b + qnorm(0.975) * se)\n\n\n                    LL    UL\n(Intercept):1    -7.09 -1.28\n(Intercept):2     1.95  2.45\nconditionimpasse  1.69  9.46\n\n\nCODE\n#TEST FIT\n#We can test the significance of condition overall by fitting an empty model and using a likelihood ratio test.\nm0 <- vglm(test_score ~ 1, tobit(Lower = lo, Upper = hi), data = df)\n(p <- pchisq(2 * (logLik(m1) - logLik(m0)), df = 2, lower.tail = FALSE))\n\n\n[1] 0.0134\n\n\nCODE\npaste(\"P value of likelihood ratio test less than alpha = 0.05? \", p <0.05)\n\n\n[1] \"P value of likelihood ratio test less than alpha = 0.05?  TRUE\"\n\n\nCODE\ncompare_performance(m0,m1)\n\n\n# Comparison of Model Performance Indices\n\nName | Model |     AIC | AIC weights |     BIC | BIC weights |  RMSE | Sigma\n----------------------------------------------------------------------------\nm0   |  vglm | 404.682 |       0.035 | 410.355 |       0.131 | 7.529 | 7.560\nm1   |  vglm | 398.060 |       0.965 | 406.569 |       0.869 | 7.101 | 7.144\n\n\nCODE\n#DIAGNOSTICS\nplot(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCODE\ndf$yhat <- fitted(m1)[,1]\ndf$rr <- resid(m1, type = \"response\")\ndf$rp <- resid(m1, type = \"pearson\")[,1]\n\npar(mfcol = c(2, 3))\n\nwith(df, {\n  plot(yhat, rr, main = \"Fitted vs Residuals\")\n  qqnorm(rr)\n  plot(yhat, rp, main = \"Fitted vs Pearson Residuals\")\n  qqnorm(rp)\n  plot(test_score, rp, main = \"Actual vs Pearson Residuals\")\n  plot(test_score, yhat, main = \"Actual vs Fitted\")\n})\n\n\n\n\n\nCODE\n#VARIANCE ACCOUNTED FOR\nprint(\"VARIANCE ACCOUNTED FOR\")\n\n\n[1] \"VARIANCE ACCOUNTED FOR\"\n\n\nCODE\n# correlation\n(r <- with(df, cor(yhat, test_score)))\n\n\n[1] 0.245\n\n\nCODE\n# variance accounted for\nr^2\n\n\n[1] 0.0598\n\n\nCODE\nperformance(m1)\n\n\n# Indices of model performance\n\nAIC     |     BIC |  RMSE | Sigma\n---------------------------------\n398.060 | 406.569 | 7.101 | 7.144\n\n\nCODE\n#NOTE: censReg package also does Tobit regression [including mixed models]\n\n\n\nThe coefficient labeled “(Intercept):1” is the intercept or constant for the model.\nThe coefficient labeled “(Intercept):2” is an ancillary statistic. If we exponentiate this value, we get a statistic that is analogous to the square root of the residual variance in OLS regression. logSigma is the variance of the model (logarithmized) (same as the second intercept in the VGAM output)\nThe predicted value of test_phase_score is 5.75 points higher for students in the impasse condition than for students in the control condition. (72% improvement in score!)\n\nUsing censReg package - https://cran.r-project.org/web/packages/censReg/vignettes/censReg.pdf\n\n\nCODE\nlibrary(censReg) #censored regression\n\n\nLoading required package: maxLik\n\n\nLoading required package: miscTools\n\n\n\nPlease cite the 'maxLik' package as:\nHenningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.\n\nIf you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:\nhttps://r-forge.r-project.org/projects/maxlik/\n\n\n\nPlease cite the 'censReg' package as:\nHenningsen, Arne (2017). censReg: Censored Regression (Tobit) Models. R package version 0.5. http://CRAN.R-Project.org/package=censReg.\n\nIf you have questions, suggestions, or comments regarding the 'censReg' package, please use a forum or 'tracker' at the R-Forge site of the 'sampleSelection' project:\nhttps://r-forge.r-project.org/projects/sampleselection/\n\n\nCODE\n#FIT MODEL\nc1 <- censReg( test_score ~ condition, left=lo, right=hi, data = df )\nsummary(c1)\n\n\n\nCall:\ncensReg(formula = test_score ~ condition, left = lo, right = hi, \n    data = df)\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n           126             71             37             18 \n\nCoefficients:\n                 Estimate Std. error t value Pr(> t)    \n(Intercept)       -4.1854     1.7149  -2.441 0.01466 *  \nconditionimpasse   5.5775     1.9995   2.789 0.00528 ** \nlogSigma           2.2033     0.1424  15.471 < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNewton-Raphson maximisation, 5 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-likelihood: -196 on 3 Df\n\n\nCODE\n#CONFIDENCE INTERVALS\nb <- coef(c1)\nse <- sqrt(diag(vcov(c1)))\ncbind(LL = b - qnorm(0.975) * se, UL = b + qnorm(0.975) * se)\n\n\n                    LL     UL\n(Intercept)      -7.55 -0.824\nconditionimpasse  1.66  9.496\nlogSigma          1.92  2.482\n\n\nCODE\n#TEST FIT\n#We can test the significance of condition overall by fitting an empty model and using a likelihood ratio test.\n\nc0 <- censReg( test_score ~ 1, left=lo, right=hi, data = df )\n(p <- pchisq(2 * (logLik(c1) - logLik(c0)), df = 2, lower.tail = FALSE))\n\n\n'log Lik.' 0.0134 (df=3)\n\n\nCODE\npaste(\"P value of likelihood ratio test less than alpha = 0.05? \", p <0.05)\n\n\n[1] \"P value of likelihood ratio test less than alpha = 0.05?  TRUE\"\n\n\nCODE\nperformance(c1)\n\n\nWarning in get_residuals.default(model, verbose = verbose, type = \"response\", :\nCan't extract residuals from model.\n\n\nWarning: Response residuals not available to calculate mean square error. (R)MSE\n  is probably not reliable.\n\n\nWarning: Models of class 'censReg' are not yet supported.\n\n\nNULL\n\n\n\nlogSigma is the variance of the model (logarithmized) (same as the second intercept in the VGAM output)\noutput should match that of VGAM"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#count-outcome",
    "href": "analysis/utils/modelling_ref.html#count-outcome",
    "title": "Modelling Reference",
    "section": "COUNT OUTCOME",
    "text": "COUNT OUTCOME\nDoes CONDITION have an effect on TEST PHASE ABSOLUTE SCORE?\n(# questions correct on test phase of task, [in lab] participants)\nThe outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can try considering it a type of count, (i.e. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function). Note that the process of answering questions on a test do not seem to strictly match the assumptions of a Poisson process.\n\n\nCODE\n#::::::::::::SETUP DATA\ndf = df_subjects %>% filter(mode == \"lab-synch\")\n\n#::::::::::::DESCRIPTIVES\nmosaic::favstats(test_score ~ condition, data = df)\n\n\n  condition min Q1 median Q3 max mean   sd  n missing\n1   control   0  0      0  1   8 1.71 3.05 62       0\n2   impasse   0  0      2  7   8 3.33 3.40 64       0\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\n\n# #GGFORMULA | FACETED HISTOGRAM\n# stats = df %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))\n# gf_props(~item_test_NABS, \n#          fill = ~pretty_condition, data = df) %>% \n#   gf_facet_grid(~pretty_condition) %>% \n#   gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n#   labs(x = \"# Correct\",\n#        y = \"proportion of subjects\",\n#        title = \"Test Phase Absolute Score (# Correct)\",\n#        subtitle = \"\") + theme(legend.position = \"blank\")\n\n##GGPUBR | HIST+DENSITY SCORE \np <- gghistogram(df, x = \"test_score\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"condition\")) +\n  labs( title = \"Distribution of TEST Absolute Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"Total Absolute Score (Test Phase)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = condition, y = test_score,\n                        fill = condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=condition, y = test_score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=condition, y = test_score, color = condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\nPoisson Regression\nhttps://stats.oarc.ucla.edu/r/dae/poisson-regression/\nGeneral Linear model using the Poisson distribution\n\n\nCODE\n#SCORE predicted by CONDITION --> POISSON DISTRIBUTION\np.1 <- glm(test_score ~ condition, data = df, family = \"poisson\")\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsumm(p.1)\n\n\n\n\n  \n    Observations \n    126 \n  \n  \n    Dependent variable \n    test_score \n  \n  \n    Type \n    Generalized linear model \n  \n  \n    Family \n    poisson \n  \n  \n    Link \n    log \n  \n\n \n\n  \n    𝛘²(1) \n    33.28 \n  \n  \n    Pseudo-R² (Cragg-Uhler) \n    0.23 \n  \n  \n    Pseudo-R² (McFadden) \n    0.04 \n  \n  \n    AIC \n    764.95 \n  \n  \n    BIC \n    770.62 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    z val. \n    p \n  \n \n\n  \n    (Intercept) \n    0.54 \n    0.10 \n    5.52 \n    0.00 \n  \n  \n    conditionimpasse \n    0.67 \n    0.12 \n    5.60 \n    0.00 \n  \n\n\n Standard errors: MLE\n\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(p.1)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: test_score\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev\nNULL                        125        604\ncondition  1     33.3       124        570\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(p.1)\n\n\nWaiting for profiling to be done...\n\n\n                 2.5 % 97.5 %\n(Intercept)      0.340  0.721\nconditionimpasse 0.436  0.902\n\n\nCODE\nreport(p.1) #sanity check\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a poisson model (estimated using ML) to predict test_score with condition (formula: test_score ~ condition). The model's explanatory power is moderate (Nagelkerke's R2 = 0.23). The model's intercept, corresponding to condition = control, is at 0.54 (95% CI [0.34, 0.72], p < .001). Within this model:\n\n  - The effect of condition [impasse] is statistically significant and positive (beta = 0.67, 95% CI [0.44, 0.90], p < .001; Std. beta = 0.67, 95% CI [0.44, 0.90])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nCODE\nplot_model(p.1)\n\n\n\n\n\nCODE\ncheck_model(p.1)\n\n\n\n\n\nThe variable condition has a coefficient of 0.67, (p < 0.005). This means that for the impasse condition, the expected log count # of questions increases by 0.67. By exponentiating the estimate we see that # question correct rate for the impasse condition is 1.95x that of the control condition. However, model diagnostics suggest the residuals are not normally distributed.\n\n\nNegative Binomial Regression\nhttps://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/ -overdispersed count data (variance much greater than mean)\n\nsimilar to Poisson regression, but using the negative binomial distribution, which can better account for ‘overdispersed’ data where variance is much greater than the mean\n\n\n\nCODE\n#NEGATIVE BIONOMIAL REGRESSION\n\nlibrary(MASS)\n\n#fit model \nnb.1 <- glm.nb(test_score ~ condition, data = df)\n\n#check overdispersion need \n#assumes conditional means are not equal to conditional variances\n#conduct likelihood ration test to compare and test [need poisson]\nm.t <- glm(test_score ~ condition, family = \"poisson\", data = df)\n#pchisq(2 * (logLik(nb.1) - logLik(m.t)), df = 1, lower.tail = FALSE)\n#A large (+) log likelihood suggests that the negative binomial is more appropriate than the Poisson model\ntest_lrt(m.t, nb.1)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName |  Model | df | df_diff |   Chi2 |      p\n----------------------------------------------\nm.t  |    glm |  2 |         |        |       \nnb.1 | negbin |  3 |       1 | 277.71 | < .001\n\n\nCODE\n#EXPONENTIATE PARAMETER ESTIMATES\nest <- cbind(Estimate = coef(nb.1), confint(nb.1))\n\n\nWaiting for profiling to be done...\n\n\nCODE\n#exponentiate parameter estimates\nprint(\"Exponentiated Estimates\")\n\n\n[1] \"Exponentiated Estimates\"\n\n\nCODE\nexp(est)\n\n\n                 Estimate 2.5 % 97.5 %\n(Intercept)          1.71  1.07   2.89\nconditionimpasse     1.95  0.98   3.86\n\n\nCODE\nsummary(nb.1)\n\n\n\nCall:\nglm.nb(formula = test_score ~ condition, data = df, init.theta = 0.2977734703, \n    link = log)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.220  -1.066  -1.066   0.447   1.067  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)  \n(Intercept)         0.536      0.252    2.13    0.033 *\nconditionimpasse    0.666      0.348    1.92    0.055 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.298) family taken to be 1)\n\n    Null deviance: 114.32  on 125  degrees of freedom\nResidual deviance: 110.70  on 124  degrees of freedom\nAIC: 489.2\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2978 \n          Std. Err.:  0.0586 \n\n 2 x log-likelihood:  -483.2380 \n\n\nCODE\nreport(nb.1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a negative-binomial model (estimated using ML) to predict test_score with condition (formula: test_score ~ condition). The model's explanatory power is weak (Nagelkerke's R2 = 0.05). The model's intercept, corresponding to condition = control, is at 0.54 (95% CI [0.07, 1.06], p = 0.033). Within this model:\n\n  - The effect of condition [impasse] is statistically non-significant and positive (beta = 0.67, 95% CI [-0.02, 1.35], p = 0.055; Std. beta = 0.67, 95% CI [-0.02, 1.35])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nCODE\nplot_model(nb.1)\n\n\n\n\n\nCODE\ncheck_model(nb.1)\n\n\n\n\n\nThe variable condition has a coefficient of 0.67, (p < 0.005). This means that for the impasse condition, the expected log count # of questions increases by 0.67. By exponentiating the estimate we see that # question correct rate for the impasse condition is nearly 1.95x that of the control condition. However, model diagnostics suggest the residuals are not normally distributed.\n\n\nCODE\n#COMPARE POISSON AND NEGATIVE BINOMIAL\ncompare_performance(p.1, nb.1)\n\n\n# Comparison of Model Performance Indices\n\nName |  Model |     AIC | AIC weights |     BIC | BIC weights | Nagelkerke's R2 |  RMSE | Sigma | Score_log | Score_spherical\n-----------------------------------------------------------------------------------------------------------------------------\np.1  |    glm | 764.945 |     < 0.001 | 770.618 |     < 0.001 |           0.234 | 3.208 | 2.145 |    -3.020 |           0.069\nnb.1 | negbin | 489.238 |        1.00 | 497.747 |        1.00 |           0.048 | 3.208 | 0.945 |    -2.213 |           0.074\n\n\nCODE\ntest_lrt(p.1, nb.1)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName |  Model | df | df_diff |   Chi2 |      p\n----------------------------------------------\np.1  |    glm |  2 |         |        |       \nnb.1 | negbin |  3 |       1 | 277.71 | < .001\n\n\nAIC, Pseudo-R2 and a likelihood ratio test indicate that the negative binomial regression model are a better fit for the distribution of test-phase scores."
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#mixture-models",
    "href": "analysis/utils/modelling_ref.html#mixture-models",
    "title": "Modelling Reference",
    "section": "MIXTURE MODELS",
    "text": "MIXTURE MODELS\n\nZero Inflated Poisson\nhttps://stats.oarc.ucla.edu/r/dae/zip/\nA Poisson count process with excess zeros\n\nThe Zero-Inflated model allows specification of two models (a mixture) where some of the zeros are included in the DGP for the Poisson model, while only the ‘excess’ zeros are included in the DGP for the Binomial model [the zero-inflated part]\nThe model includes:\n\nA logistic model to model which of the two processes the zero outcome is associated with\nA poisson model to model the count process\n\nCan specify different predictors for each part of the model\npredictors after the | are for the binomial zero-inflated part of the model, while those infront are for the poisson process\n\n\n\nCODE\n#ZERO INFLATED POISSON\n\nzinfp.1 <- zeroinfl(test_score ~  condition| condition , data = df)\nsummary(zinfp.1)\n\n\n\nCall:\nzeroinfl(formula = test_score ~ condition | condition, data = df)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.988 -0.575 -0.575  1.090  2.117 \n\nCount model coefficients (poisson with log link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        1.7702     0.0979   18.07   <2e-16 ***\nconditionimpasse  -0.0231     0.1199   -0.19     0.85    \n\nZero-inflation model coefficients (binomial with logit link):\n                 Estimate Std. Error z value Pr(>|z|)   \n(Intercept)         0.890      0.280    3.18   0.0015 **\nconditionimpasse   -1.213      0.378   -3.21   0.0013 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -213 on 4 Df\n\n\nCODE\nreport(zinfp.1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a zero-inflated poisson model to predict test_score with condition (formula: test_score ~ condition). The model's explanatory power is substantial (R2 = 0.39, adj. R2 = 0.38). The model's intercept, corresponding to condition = control, is at 1.77 (95% CI [1.58, 1.96], p < .001). Within this model:\n\n  - The effect of condition [impasse] is statistically non-significant and negative (beta = -0.02, 95% CI [-0.26, 0.21], p = 0.847; Std. beta = -0.02, 95% CI [-0.26, 0.21])\n  - The effect of condition [impasse] is statistically significant and negative (beta = -1.21, 95% CI [-1.95, -0.47], p = 0.001; Std. beta = -1.21, 95% CI [-1.95, -0.47])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\n\nCODE\nperformance(zinfp.1)\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-----------------------------------------------------------------------------------\n434.837 | 446.183 | 0.391 |     0.381 | 3.208 | 3.260 |    -1.694 |           0.069\n\n\nCODE\n#check_model(zinfp.1)\n\n\nIn the count model, the coefficient for the condition is not significant.\nIn the zero-inflation model, the coefficient for the condition variable is -1.23 and statistically significant. This suggests that the log odds of being an excessive zero decrease by 1.23 if you are in the impasse condition\n\n\nZero Inflated Negative Binomial Regression\nhttps://stats.oarc.ucla.edu/r/dae/zinb/\ncount data that are overdispersed and have excess zeros\nZero-inflated negative binomial regression is for modelling count variables with excessive zeros, and especially when the count data are overdispersed (mean is much larger than variance). It can help account for situations where theory suggests that excess zeros are generated by 2 separate processes, one that includes the other count values, and the other that is just the zeros, and thus that the excess zeros can be modelled independently.\nTotal Absolute Score (# items correct) may fit this situation, as the data are overdispersed (variance much greater than the mean) and there are are very large number of zeros. It is theoretically plausible that these excess zeros (no answers correct) are the result of a different ‘process’ … (i.e) little understanding and/or resistance to restructuring understanding of the coordinate system. However, I am not certain if it is plausible to suggest that the zeros themselves are the result of two different processes: (ie. perhaps trying to understand, and not trying to understand?) <- this could maybe be disentangled by first question latency?\nThe model includes:\n\nA logistic model to model which of the two processes the zero outcome is associated with\nA negative binomial model to model the count process\n\n\n\nCODE\nlibrary(pscl) #  for zeroinfl negbinomial\n\n#ZERO INFLATED NEGATIVE BINOMIAL\nzinb.1 <- zeroinfl(test_score ~ condition | condition , \n                   data = df, dist = \"negbin\")\n#before the | is the count part, after the | is the logit model\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(zinb.1)\n\n\n\nCall:\nzeroinfl(formula = test_score ~ condition | condition, data = df, dist = \"negbin\")\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.970 -0.568 -0.568  1.070  2.091 \n\nCount model coefficients (negbin with log link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        1.7689     0.1048   16.88   <2e-16 ***\nconditionimpasse  -0.0232     0.1282   -0.18    0.856    \nLog(theta)         3.7504     2.1488    1.75    0.081 .  \n\nZero-inflation model coefficients (binomial with logit link):\n                 Estimate Std. Error z value Pr(>|z|)   \n(Intercept)         0.888      0.280    3.17   0.0015 **\nconditionimpasse   -1.214      0.379   -3.21   0.0013 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 42.54 \nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -213 on 5 Df\n\n\nCODE\nreport(zinb.1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a zero-inflated negative-binomial model to predict test_score with condition (formula: test_score ~ condition). The model's explanatory power is substantial (R2 = 0.40, adj. R2 = 0.39). The model's intercept, corresponding to condition = control, is at 1.77 (95% CI [1.56, 1.97], p < .001). Within this model:\n\n  - The effect of condition [impasse] is statistically non-significant and negative (beta = -0.02, 95% CI [-0.27, 0.23], p = 0.856; Std. beta = -0.02, 95% CI [-0.27, 0.23])\n  - The effect of condition [impasse] is statistically significant and negative (beta = -1.21, 95% CI [-1.96, -0.47], p = 0.001; Std. beta = -1.21, 95% CI [-1.96, -0.47])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\n\nCODE\nperformance(zinb.1)\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-----------------------------------------------------------------------------------\n436.585 | 450.766 | 0.398 |     0.388 | 3.208 | 3.274 |    -1.740 |           0.068\n\n\nCODE\n#   rootogram(zinb.1)\n\n\n\n# #EXPONENTIATE PARAMETER ESTIMATES\n# est <- cbind(Estimate = coef(zinb.1), confint(zinb.1))\n# #exponentiate parameter estimates\n# print(\"Exponentiated Estimates\")\n# exp(est)\n\n\nIn the count model, the coefficient for the condition is very small, and not significant (suggesting it does not contribute to the count yielding process?).\nIn the zero-inflation model, the coefficient for the condition variable is -1.056 and statistically significant. This suggests that the log odds of being an excessive zero decrease by 1.06 if you are in the impasse condition\n\n\nCODE\ncompare_performance(zinfp.1, zinb.1)\n\n\nSome of the nested models seem to be identical\n\n\n# Comparison of Model Performance Indices\n\nName    |    Model |     AIC | AIC weights |     BIC | BIC weights |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n------------------------------------------------------------------------------------------------------------------------------------\nzinfp.1 | zeroinfl | 434.837 |       0.705 | 446.183 |       0.908 | 0.391 |     0.381 | 3.208 | 3.260 |    -1.694 |           0.069\nzinb.1  | zeroinfl | 436.585 |       0.295 | 450.766 |       0.092 | 0.398 |     0.388 | 3.208 | 3.274 |    -1.740 |           0.068\n\n\nCODE\ntest_likelihoodratio(zinfp.1, zinb.1)\n\n\nSome of the nested models seem to be identical\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName    |    Model | df | df_diff | Chi2 |     p\n------------------------------------------------\nzinfp.1 | zeroinfl |  4 |         |      |      \nzinb.1  | zeroinfl |  5 |       1 | 0.25 | 0.615\n\n\nTODO come back to this and discuss further\n\n\n\nHurdle Model\n\nhttps://data.library.virginia.edu/getting-started-with-hurdle-models/\nhttps://en.wikipedia.org/wiki/Hurdle_model#:~:text=A%20hurdle%20model%20is%20a,of%20the%20non%2Dzero%20values.\n\nclass of models for count data with both overdispersion and excess zeros;\ndifferent from zero-inflated models where the excess zeros are theorized to arise from two different processes; in the hurdle model, there is a separate model for P(x=0) and a separate model for P(x!=0)\nThe model includes:\n\nA binary logit model to model whether the observation takes a positive count or not. (1) Does the student get any questions right?\na truncated Poisson or Negative binomial model that only fits positive counts (2) How many questions does the student get right?\n\n\n\nCODE\nlibrary(pscl) #zero-inf and hurdle models \nlibrary(countreg) #rootogram\n\n\nRegistered S3 methods overwritten by 'countreg':\n  method                 from\n  print.zeroinfl         pscl\n  print.summary.zeroinfl pscl\n  summary.zeroinfl       pscl\n  coef.zeroinfl          pscl\n  vcov.zeroinfl          pscl\n  logLik.zeroinfl        pscl\n  predict.zeroinfl       pscl\n  residuals.zeroinfl     pscl\n  fitted.zeroinfl        pscl\n  terms.zeroinfl         pscl\n  model.matrix.zeroinfl  pscl\n  extractAIC.zeroinfl    pscl\n  print.hurdle           pscl\n  print.summary.hurdle   pscl\n  summary.hurdle         pscl\n  coef.hurdle            pscl\n  vcov.hurdle            pscl\n  logLik.hurdle          pscl\n  predict.hurdle         pscl\n  residuals.hurdle       pscl\n  fitted.hurdle          pscl\n  terms.hurdle           pscl\n  model.matrix.hurdle    pscl\n  extractAIC.hurdle      pscl\n\n\n\nAttaching package: 'countreg'\n\n\nThe following objects are masked from 'package:VGAM':\n\n    dzipois, pzipois, qzipois, rzipois\n\n\nThe following objects are masked from 'package:pscl':\n\n    hurdle, hurdle.control, hurdletest, zeroinfl, zeroinfl.control\n\n\nThe following object is masked from 'package:vcd':\n\n    rootogram\n\n\nCODE\n#install.packages(\"countreg\", repos=\"http://R-Forge.R-project.org\")\n\n#SYNTAX OUTCOME ~ count model predictor | hurdle predictor\n\nh.1 <- pscl::hurdle(test_score ~ condition | condition , data = df,\n              zero.dist = \"binomial\", dist = \"poisson\", size = 8)\n\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\n\nWarning in optim(fn = zeroDist, gr = zeroGrad, par = c(start$zero, if (zero.dist\n== : unknown names in control: size\n\n\nCODE\nh.2 <- pscl::hurdle(test_score ~ condition | condition , data = df,\n              zero.dist = \"binomial\", dist = \"negbin\", size = 8)\n\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\n\nCODE\nsummary(h.1)\n\n\n\nCall:\npscl::hurdle(formula = test_score ~ condition | condition, data = df, \n    dist = \"poisson\", zero.dist = \"binomial\", size = 8)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.988 -0.575 -0.575  1.090  2.117 \n\nCount model coefficients (truncated poisson with log link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        1.7702     0.0979   18.07   <2e-16 ***\nconditionimpasse  -0.0231     0.1199   -0.19     0.85    \nZero hurdle model coefficients (binomial with logit link):\n                 Estimate Std. Error z value Pr(>|z|)   \n(Intercept)        -0.894      0.280   -3.19   0.0014 **\nconditionimpasse    1.209      0.377    3.20   0.0014 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -213 on 4 Df\n\n\nCODE\nsummary(h.2)\n\n\n\nCall:\npscl::hurdle(formula = test_score ~ condition | condition, data = df, \n    dist = \"negbin\", zero.dist = \"binomial\", size = 8)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.970 -0.568 -0.568  1.070  2.091 \n\nCount model coefficients (truncated negbin with log link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        1.7689     0.1048   16.88   <2e-16 ***\nconditionimpasse  -0.0232     0.1282   -0.18    0.856    \nLog(theta)         3.7506     2.1492    1.75    0.081 .  \nZero hurdle model coefficients (binomial with logit link):\n                 Estimate Std. Error z value Pr(>|z|)   \n(Intercept)        -0.894      0.280   -3.19   0.0014 **\nconditionimpasse    1.209      0.377    3.20   0.0014 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 42.546\nNumber of iterations in BFGS optimization: 19 \nLog-likelihood: -213 on 5 Df\n\n\nCODE\nrootogram(h.1)\n\n\n\n\n\nCODE\nrootogram(h.2)\n\n\n\n\n\nCODE\nplot(compare_performance(h.1,h.2))\n\n\nSome of the nested models seem to be identical\n\n\n\n\n\nCODE\ntest_lrt(h.1, h.2)\n\n\nSome of the nested models seem to be identical\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName |  Model | df | df_diff | Chi2 |     p\n-------------------------------------------\nh.1  | hurdle |  4 |         |      |      \nh.2  | hurdle |  5 |       1 | 0.25 | 0.615"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#beta-regression",
    "href": "analysis/utils/modelling_ref.html#beta-regression",
    "title": "Modelling Reference",
    "section": "BETA REGRESSION",
    "text": "BETA REGRESSION\n\nBETA Distribution\nBeta regression on % correct (with standard transformation for including [0,1])\nhttps://stats.stackexchange.com/questions/63350/how-to-interpret-the-coefficients-from-a-beta-regression\nhttps://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#2-fractional-logistic-regression\nIf a test score is conceptualized as correct (a) and b (incorrect) score = correct / (correct + incorrect) = a/ (a + b)\nIn the beta distribution, a and b are the shape parameters!\n\n\nCODE\ntheme_clean <- function() {\n  theme_minimal(base_family = \"Barlow Semi Condensed\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.title = element_text(family = \"BarlowSemiCondensed-Bold\"),\n          axis.title = element_text(family = \"BarlowSemiCondensed-Medium\"),\n          strip.text = element_text(family = \"BarlowSemiCondensed-Bold\",\n                                    size = rel(1), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA))\n}\n\n\nggplot() +\n  geom_function(fun = dbeta, args = list(shape1 = 6, shape2 = 4),\n                aes(color = \"Beta(shape1 = 6, shape2 = 4)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", name = \"\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")+ labs(\n    title = \"Beta a = 6, b = 4\"\n  )\n\n\n\n\n\nCODE\nggplot() +\n  geom_function(fun = dbeta, args = list(shape1 = 8, shape2 = 0.001),\n                aes(color = \"Beta(shape1 = 8, shape2 = 0.001)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", name = \"\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\") + labs(\n    \"ALL CORRECT\"\n  )\n\n\n\n\n\nCODE\nggplot() +\n  geom_function(fun = dbeta, args = list(shape1 = 0.001, shape2 = 8),\n                aes(color = \"Beta(shape1 = 0.001, shape2 = 8)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", name = \"\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\") + labs(\n    \"ALL WRONG\"\n  )\n\n\n\n\n\nCODE\nggplot() +\n  geom_function(fun = dbeta, args = list(shape1 = 4, shape2 = 4),\n                aes(color = \"Beta(shape1 = 4, shape2 = 4)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", name = \"\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\") + labs(\n    \"FIGURED IT OUT HALFWAY \"\n  )\n\n\n\n\n\nALTERNATIVE conceptualization of a and b is as mean MU and precision PHI (same idea as variance).\na = mu * phi b = (1 - mu) * phi\nMU = a / (a+b) [the score] PHI = a + b [ the total # items ]\nThe extraDistr::dprop function can also handle mu and phi parameters, named as mean and size\n\n\nCODE\nlibrary(extraDistr)\n\n\n\nAttaching package: 'extraDistr'\n\n\nThe following object is masked from 'package:miscTools':\n\n    ddnorm\n\n\nThe following object is masked from 'package:purrr':\n\n    rdunif\n\n\nThe following objects are masked from 'package:VGAM':\n\n    dfrechet, dgev, dgompertz, dgpd, dgumbel, dhuber, dkumar, dlaplace,\n    dlomax, dpareto, drayleigh, dskellam, dslash, pfrechet, pgev,\n    pgompertz, pgpd, pgumbel, phuber, pkumar, plaplace, plomax,\n    ppareto, prayleigh, pslash, qfrechet, qgev, qgompertz, qgpd,\n    qgumbel, qhuber, qkumar, qlaplace, qlomax, qpareto, qrayleigh,\n    rfrechet, rgev, rgompertz, rgpd, rgumbel, rhuber, rkumar, rlaplace,\n    rlomax, rpareto, rrayleigh, rskellam, rslash\n\n\nThe following objects are masked from 'package:ordinal':\n\n    dgumbel, pgumbel, qgumbel, rgumbel\n\n\nCODE\nggplot() +\n  geom_function(fun = dprop, args = list(mean = 0.999, size = 8),\n                aes(color = \"Beta(mean = 0.99, size = 8)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", name = \"\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\") + labs(\n    \"ALL CORRECT\"\n  )\n\n\n\n\n\nCODE\nggplot() +\n  geom_function(fun = dprop, args = list(mean = 0.001, size = 8),\n                aes(color = \"Beta(mean = 0.001, size = 8)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", name = \"\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\") + labs(\n    \"ALL WRONG\"\n  )\n\n\n\n\n\nCODE\nggplot() +\n  geom_function(fun = dprop, args = list(mean = 0.5, size = 8),\n                aes(color = \"Beta(mean = 0.5, size = 8)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", name = \"\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\") + labs(\n    \"FIGURED IT OUT HALFWAY \"\n  )\n\n\n\n\n\nCODE\nggplot() +\n  geom_function(fun = dbeta, args = list(shape1 = 0.001, shape2 = 1),\n                aes(color = \"Beta(shape1 = 6, shape2 = 4)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", name = \"\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nNote that in all cases, we have to slightly adjust the parameters because the distribution can’t handle values at 0 or 1. So we scrunch the data in just a bit.\n\n\nBETA Regression\nhttps://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#2-fractional-logistic-regression\nThe syntax is just like all other formula-based regression functions, but with an added bit: the first part of the equation (x ~ y) models the mean, or MU, while anything that comes after a | in the formula will explain variation in the precision, or PHI.\nThe link parameter determines the link function for the mu estimate?.\n\nSetup\n\n\nCODE\n# \nlibrary(betareg)\n\n#RESCLAE VARIABLE\n#beta reg can't handle 0s and 1s \nsub <- df_subjects %>% dplyr::select(condition, DV_percent_NABS)\nn = nrow(sub) %>% unlist()\nsub$dv_transformed = (sub$DV_percent_NABS * (n-1) + 0.5)/n\n \n#VISUALIZE VARIABLES\npaste(\"Slightly squished data\")\n\n\n[1] \"Slightly squished data\"\n\n\nCODE\nhistogram(sub$dv_transformed)\n\n\n\n\n\nCODE\n#FIT MODEL\nmb1 <- betareg(dv_transformed ~ condition | condition, \n              data = sub)\n\nsummary(mb1)\n\n\n\nCall:\nbetareg(formula = dv_transformed ~ condition | condition, data = sub)\n\nStandardized weighted residuals 2:\n   Min     1Q Median     3Q    Max \n-1.065 -0.450 -0.217  0.537  1.678 \n\nCoefficients (mean model with logit link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        -0.961      0.119   -8.06  7.6e-16 ***\nconditionimpasse    0.545      0.158    3.44  0.00057 ***\n\nPhi coefficients (precision model with log link):\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -0.4273     0.1001   -4.27    2e-05 ***\nconditionimpasse   0.0212     0.1307    0.16     0.87    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  506 on 4 Df\nPseudo R-squared: 0.0725\nNumber of iterations: 16 (BFGS) + 2 (Fisher scoring) \n\n\nCODE\nplot(mb1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich link function should be used for each? We now have two sets of coefficients, one set for each parameter (the mean and precision). The parameters for the mean are measured on the logit scale, just like with logistic regression previously, and we can calculate the marginal effect of having a quota by using plogis() and piecing together the coefficient and the intercept:\n\n\n\nInterpret\n\n\nCODE\nbeta_mu_intercept <- mb1 %>% \n  tidy() %>% \n  filter(component == \"mean\", term == \"(Intercept)\") %>% \n  pull(estimate)\n\nbeta_mu_condition <- mb1 %>% \n  tidy() %>% \n  filter(component == \"mean\", term == \"conditionimpasse\") %>% \n  pull(estimate)\n\n\nprint(\"Coefficients —- PROBABILITIES\")\n\n\n[1] \"Coefficients —- PROBABILITIES\"\n\n\nCODE\nprint(\"[these should math pred plots]\")\n\n\n[1] \"[these should math pred plots]\"\n\n\nCODE\nprint(\"Probability at reference level\")\n\n\n[1] \"Probability at reference level\"\n\n\nCODE\nplogis(beta_mu_intercept)\n\n\n[1] 0.277\n\n\nCODE\nprint(\"Probability at coefficient level\")\n\n\n[1] \"Probability at coefficient level\"\n\n\nCODE\nplogis(beta_mu_intercept + beta_mu_condition)\n\n\n[1] 0.397\n\n\nCODE\nprint(\"CHANGE in probability by coefficient\")\n\n\n[1] \"CHANGE in probability by coefficient\"\n\n\nCODE\nplogis(beta_mu_intercept + beta_mu_condition) - plogis(beta_mu_intercept)\n\n\n[1] 0.121\n\n\nCODE\nplot_model(mb1, type = \"pred\")\n\n\n$condition\n\n\n\n\n\nImpasse increases the average of the distribution of accuracy by 12% percentage points, on average. This should match the value we get with fractional regression. (logistic regression with count variable instead of binomial)\n\nThe intercept of MU indicates the average of the distribution of score proportion in the control condition\nThe coefficient of MU indicates the change in the average distribution of score proportion in the impasse condition\n\nThe PHI parameter estimates are not measured on a logit scale. Instead, they’re log values. We can invert them by exponentiating them with exp(). - The phi intercept indicates the precision of the distribution of score proportion in the control condition - The phi coefficient indicates the change in the precision of the distribution of score proportion in the impasse condition\n\nCONDITION is a significant predictor of MEAN (location) but not PHI (precision). Put another way, the shape of both distributions is similar, but the impasse condition distribution is shift to the right (higher) than the control distribution.\n\n\n\nCompare\n\n\nCODE\n# COMPARE MODELS\n\n\n#PREP DATA \ndf <- df_subjects %>% filter(mode == \"lab-synch\")\nn = nrow(df) %>% unlist()\ndf <- df %>%  mutate(\n    accuracy = item_test_NABS/8,\n    test_trans = (accuracy * (n-1) + 0.5)/n,\n    all_trans = (DV_percent_NABS * (n-1) + 0.5)/n\n  )\n\n# LINEAR REGRESSION\nlm1 <- lm( test_trans ~ pretty_condition    , data = df)\n# summary(lm1)\n\n# LINEAR REGRESSION on LOGTRANSFORMED\nqlm1 <- lm( qlogis(test_trans) ~ pretty_condition    , data = df)\n# summary(qlm1)\n\n# FRACTIONAL REGRESSION\n#uses quasibinomial [like binomial] on proportion \n#instead of binomial data. coeffs on logit scale\nflm1 <- glm( test_trans ~ pretty_condition, data = df, family = quasibinomial())\nsummary(flm1)\n\n\n\nCall:\nglm(formula = test_trans ~ pretty_condition, family = quasibinomial(), \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.014  -0.667  -0.667   0.949   1.733  \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               -1.289      0.274   -4.71  6.6e-06 ***\npretty_conditionimpasse    0.953      0.354    2.69   0.0082 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.788)\n\n    Null deviance: 117.11  on 125  degrees of freedom\nResidual deviance: 111.18  on 124  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\n# BETA REGRESSION predictor phi\nbm1 <- betareg(test_trans ~ pretty_condition | pretty_condition, \n              data = df, link = \"logit\")\n\n\nWarning in betareg.fit(X, Y, Z, weights, offset, link, link.phi, type, control):\nno valid starting value for precision parameter found, using 1 instead\n\n\nCODE\nsummary(bm1)\n\n\n\nCall:\nbetareg(formula = test_trans ~ pretty_condition | pretty_condition, data = df, \n    link = \"logit\")\n\nStandardized weighted residuals 2:\n   Min     1Q Median     3Q    Max \n-0.867 -0.397 -0.397  0.619  1.525 \n\nCoefficients (mean model with logit link):\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -0.851      0.186   -4.58  4.7e-06 ***\npretty_conditionimpasse    0.530      0.252    2.10    0.036 *  \n\nPhi coefficients (precision model with log link):\n                        Estimate Std. Error z value Pr(>|z|)   \n(Intercept)              -0.4319     0.1545   -2.80   0.0052 **\npretty_conditionimpasse  -0.0629     0.2052   -0.31   0.7593   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  183 on 4 Df\nPseudo R-squared: 0.0608\nNumber of iterations: 17 (BFGS) + 2 (Fisher scoring) \n\n\nCODE\nmodels = list(\"LM\" = lm1, \n              \"LM log transform\" = qlm1, \n              \"GLM Quasibinomial\" = flm1, \n              \"beta\" = bm1)\nmodelsummary(models)\n\n\n\n \n  \n      \n    LM \n    LM log transform \n    GLM Quasibinomial \n    beta \n  \n \n\n  \n    (Intercept) \n    0.216 \n    −3.241 \n    −1.289 \n    −0.851 \n  \n  \n     \n    (0.051) \n    (0.518) \n    (0.274) \n    (0.186) \n  \n  \n    pretty_conditionimpasse \n    0.201 \n    2.059 \n    0.953 \n    0.530 \n  \n  \n     \n    (0.071) \n    (0.727) \n    (0.354) \n    (0.252) \n  \n  \n    Num.Obs. \n    126 \n    126 \n    126 \n    126 \n  \n  \n    R2 \n    0.060 \n    0.061 \n     \n    0.061 \n  \n  \n    R2 Adj. \n    0.052 \n    0.053 \n     \n     \n  \n  \n    AIC \n    131.3 \n    715.9 \n     \n    −357.1 \n  \n  \n    BIC \n    139.8 \n    724.4 \n     \n    −345.7 \n  \n  \n    Log.Lik. \n    −62.653 \n    −354.946 \n     \n     \n  \n  \n    F \n    7.887 \n    8.022 \n    7.225 \n     \n  \n  \n    RMSE \n    0.40 \n    4.05 \n    0.40 \n    0.40 \n  \n\n\n\n\n\n\n\nCODE\n#GET ESTIMATES\nmodel_beta <- bm1\n\nbeta_mu_intercept <- model_beta %>% \n  tidy() %>% \n  filter(component == \"mean\", term == \"(Intercept)\") %>% \n  pull(estimate)\n\nbeta_mu_condition <- model_beta %>% \n  tidy() %>% \n  filter(component == \"mean\", term == \"pretty_conditionimpasse\") %>% \n  pull(estimate)\n\nbeta_phi_intercept <- model_beta %>% \n  tidy() %>% \n  filter(component == \"precision\", term == \"(Intercept)\") %>% \n  pull(estimate)\n\nbeta_phi_condition <- model_beta %>% \n  tidy() %>% \n  filter(component == \"precision\", term == \"pretty_conditionimpasse\") %>% \n  pull(estimate)\n\n\nmu_control = plogis(beta_mu_intercept)\nphi_control = plogis(beta_phi_intercept)\n#when you don’t explicitly model the precision, the resulting coefficient in the table isn’t on the log scale—it’s a regular non-logged number, so there’s no need to exponentiate\n\n#PLOT ESTIMATES\n\ncontrol_title <- paste0(\"dprop(mean = plogis(\", round(beta_mu_intercept, 2),\n                         \"), size = exp(\", round(beta_phi_intercept, 2), \"))\")\n\nimpasse_title <- paste0(\"dprop(mean = plogis(\", round(beta_mu_intercept, 2),\n                      \" + \", round(beta_mu_condition, 2), \n                      \"), size = exp(\", round(beta_phi_intercept, 2),\n                      \" + \", round(beta_phi_condition, 2), \"))\")\n\nggplot(data = tibble(x = 0:1), aes(x = x)) +\n  stat_function(fun = dprop, size = 1,\n                args = list(size = exp(beta_phi_intercept), \n                            mean = plogis(beta_mu_intercept)),\n                aes(color = control_title)) + \n  stat_function(fun = dprop, size = 1,\n                args = list(size = exp(beta_phi_intercept + beta_phi_condition), \n                            mean = plogis(beta_mu_intercept + beta_mu_condition)),\n                aes(color = impasse_title)) +\n  geom_density(data = df, \n               aes(x = test_trans, fill = pretty_condition), \n               alpha = 0.5, color = NA) +\n  labs(x = \"Beta Regression Predictions\") +\n  theme_clean() +\n  theme(legend.position = \"right\")\n\n\n\n\n\nWe can also convert the parameter estimates back to A and B. for the impasse condition, MU = 0.397 SIZE = 0.666\na = mu * phi = 0.397 * 0.66 = 0.264 b = (1 - mu) * phi = (1 - 0.397) * 0.666 = 0.402\nwe should expect it to look more like … (for 50% ) a = correct = 3 b = incorrect = 10 a + b = total = 13\nMU = a / (a+b) [the score] = 3 / 13 = 0.23 PHI = a + b [ the total # items ] = 13\nmu = 0.277 = a / (a + b) phi = 0.395 = a + b let b = 13\n\n\nCODE\n#RECOVER A AND B FROM MU AND PHI\nmuphi_to_shapes <- function(mu, phi) {\n  shape1 <- mu * phi\n  shape2 <- (1 - mu) * phi\n  return(list(shape1 = shape1, shape2 = shape2))\n}\n\nshapes_to_muphi <- function(shape1, shape2) {\n  mu <- shape1 / (shape1 + shape2)\n  phi <- shape1 + shape2\n  return(list(mu = mu, phi = phi))\n}\n\npaste(\"model predicts control A and B \")\n\n\n[1] \"model predicts control A and B \"\n\n\nCODE\nmuphi_to_shapes(mu_control, phi_control)\n\n\n$shape1\n[1] 0.118\n\n$shape2\n[1] 0.276\n\n\nCODE\npaste(\"should be something like \")\n\n\n[1] \"should be something like \"\n\n\nCODE\nshapes_to_muphi(5,13)\n\n\n$mu\n[1] 0.278\n\n$phi\n[1] 18\n\n\nIt seems like the although the (shrinkled) beta regression model is working, it is really messing up the estimate of the PHI parameter, such that we can’t even recover the A and B. I expect whole numbers, and instead I’m geting values less than 1. I think this might be because the phi value is so low (note that phi = precision, so like inverse of variance. Lower values are more less precise (more variance)). Since I have so many values near 0 and near 1 (even though I’ve shrunk the estimates to be in the open interval (0,1) rather than [0,1], I think it causing problems with the estimation. )\n\n\nbayesian version\nhttps://www.bayesrulesbook.com/\n\n\nCODE\nlibrary(brms)\n\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.17.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following objects are masked from 'package:extraDistr':\n\n    ddirichlet, dfrechet, pfrechet, qfrechet, rdirichlet, rfrechet\n\n\nThe following objects are masked from 'package:VGAM':\n\n    acat, cratio, cumulative, dfrechet, dirichlet, exponential,\n    frechet, geometric, lognormal, multinomial, negbinomial, pfrechet,\n    qfrechet, rfrechet, s, sratio\n\n\nThe following objects are masked from 'package:ordinal':\n\n    ranef, VarCorr\n\n\nThe following object is masked from 'package:lme4':\n\n    ngrps\n\n\nThe following objects are masked from 'package:ggdist':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\n\nThe following object is masked from 'package:survival':\n\n    kidney\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\nCODE\nmodel_beta_bayes <- brm(\n  bf(test_trans ~ pretty_condition,\n     phi ~ pretty_condition),\n  data = df,\n  family = Beta(),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234,\n  # Use the cmdstanr backend for Stan because it's faster and more modern than\n  # the default rstan You need to install the cmdstanr package first\n  # (https://mc-stan.org/cmdstanr/) and then run cmdstanr::install_cmdstan() to\n  # install cmdstan on your computer.\n  backend = \"cmdstanr\"\n)\n\n\nStart sampling\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.5 seconds.\n\n\n\n\nCODE\nsummary(model_beta_bayes)\n\n\n Family: beta \n  Links: mu = logit; phi = log \nFormula: test_trans ~ pretty_condition \n         phi ~ pretty_condition\n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                      -0.85      0.19    -1.22    -0.47 1.00     2740\nphi_Intercept                  -0.45      0.16    -0.77    -0.16 1.00     2846\npretty_conditionimpasse         0.53      0.25     0.02     1.01 1.00     3056\nphi_pretty_conditionimpasse    -0.06      0.20    -0.45     0.34 1.00     3452\n                            Tail_ESS\nIntercept                       2893\nphi_Intercept                   2889\npretty_conditionimpasse         3205\nphi_pretty_conditionimpasse     2805\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\npaste(\"Cool! parameters are almost identical to betareg\")\n\n\n[1] \"Cool! parameters are almost identical to betareg\"\n\n\n\n\nCODE\nlibrary(tidybayes)\n\n\n\nAttaching package: 'tidybayes'\n\n\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\n\nThe following object is masked from 'package:bayestestR':\n\n    hdi\n\n\nThe following object is masked from 'package:parameters':\n\n    parameters\n\n\nThe following object is masked from 'package:distributional':\n\n    parameters\n\n\nThe following object is masked from 'package:gnm':\n\n    parameters\n\n\nThe following objects are masked from 'package:ggridges':\n\n    scale_point_color_continuous, scale_point_color_discrete,\n    scale_point_colour_continuous, scale_point_colour_discrete,\n    scale_point_fill_continuous, scale_point_fill_discrete,\n    scale_point_size_continuous\n\n\nCODE\nposterior_beta <- model_beta_bayes %>% \n  gather_draws(`b_.*`, regex = TRUE) %>% \n  mutate(component = ifelse(str_detect(.variable, \"phi_\"), \"Precision\", \"Mean\"),\n         intercept = str_detect(.variable, \"Intercept\"))\n\nggplot(posterior_beta, aes(x = .value, y = fct_rev(.variable), fill = component)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(aes(slab_alpha = intercept), \n               .width = c(0.8, 0.95), point_interval = \"median_hdi\") +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.6) +\n  scale_slab_alpha_discrete(range = c(1, 0.4)) +\n  guides(fill = \"none\", slab_alpha = \"none\") +\n  labs(x = \"Coefficient\", y = \"Variable\",\n       caption = \"80% and 95% credible intervals shown in black; RAW COEFF\") +\n  facet_wrap(vars(component), ncol = 1, scales = \"free_y\") +\n  theme_clean()\n\n\n\n\n\nCODE\n##GET USEFUL ESTIMATES\npaste(\"TRANSFORMED ESTIMATES\")\n\n\n[1] \"TRANSFORMED ESTIMATES\"\n\n\nCODE\nmodel_beta_bayes %>% \n  spread_draws(`b_.*`, regex = TRUE) %>% \n  mutate(across(starts_with(\"b_phi\"), ~exp(.))) %>%\n  mutate(across((!starts_with(\".\") & !starts_with(\"b_phi\")), ~plogis(.))) %>%\n  gather_variables() %>% \n  median_hdi()\n\n\n# A tibble: 4 × 7\n  .variable                     .value .lower .upper .width .point .interval\n  <chr>                          <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 b_Intercept                    0.300  0.223  0.377   0.95 median hdi      \n2 b_phi_Intercept                0.643  0.451  0.843   0.95 median hdi      \n3 b_phi_pretty_conditionimpasse  0.944  0.613  1.36    0.95 median hdi      \n4 b_pretty_conditionimpasse      0.628  0.511  0.737   0.95 median hdi      \n\n\nCODE\n# now phi is a raw value indicating precision\n# now mu is an odds ratio for median? of the distribution\n\n\n\n\nBayesian predictions\n\n\nCODE\n# Plug a dataset where quota is FALSE and TRUE into the model\nbeta_bayes_pred <- model_beta_bayes %>% \n  epred_draws(newdata = tibble(pretty_condition = c(\"control\", \"impasse\")))\n\nggplot(beta_bayes_pred, \n       aes(x = .epred, y = pretty_condition, fill = pretty_condition)) +\n  stat_halfeye(.width = c(0.8, 0.95), point_interval = \"median_hdi\") +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.8) +\n  guides(fill = \"none\") +\n  labs(x = \"Predicted Comprehension Task Score Percentage Correct\", y = NULL,\n       caption = \"80% and 95% credible intervals shown in black\") +\n  theme_clean()\n\n\n\n\n\n\n\nCODE\n#TODO POSTERIOR PREDICTIVE CHECKS \n\n# describe_posterior(model_beta_bayes, test = c(\"p_direction\", \"rope\", \"bayesfactor\"))\n\n# library(rstanarm)\n# pp_check(model_beta_bayes, plotfun = \"boxplot\", nreps = 10, notch = FALSE)\n# pp_check(model_beta_bayes, plotfun = \"stat_grouped\", stat = \"median\", group = \"pretty_condition\")\n\n\n\n\n\nZERO INFLATED BETA\n\nlogistic regression for zero inflated \\(\\alpha\\) (logit scale)\nbeta regression for proportion \\(\\mu\\) (logit scale) and \\(\\phi\\) (log scale)\n\n\n\nCODE\ndf <- df_subjects %>% dplyr::select(pretty_condition, DV_percent_NABS) %>% \n  mutate(\n    condition = pretty_condition,\n    accuracy = DV_percent_NABS\n  )\n#shrink accuracy just a tiny bit from 1 for testing purposes \nlibrary(tidyfst)\n\n\n\nLife's short, use R.\n\n\n\nAttaching package: 'tidyfst'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, cummean, nth\n\n\nThe following object is masked from 'package:scales':\n\n    percent\n\n\nCODE\ndf <- df %>% mutate_when(accuracy == 1, accuracy = 0.99)\n\n#INTERCEPT ONLY model for alpha\nzinb <- brm(\n  bf(accuracy ~ condition, #mean of (0,1) values, mu\n     phi ~ condition, #precision of (0,1) values, phi\n     zi ~ condition), #zero inflated is extreme? alpha\n  data = df,\n  family = zero_inflated_beta(),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234,\n  backend = \"cmdstanr\",\n  file =\"analysis/utils/models/zinb.rds\"\n)\n\n\n\n\nCODE\nsummary(zinb)\n\n\n Family: zero_inflated_beta \n  Links: mu = logit; phi = log; zi = logit \nFormula: accuracy ~ condition \n         phi ~ condition\n         zi ~ condition\n   Data: df (Number of observations: 330) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                0.18      0.16    -0.14     0.51 1.00     4388\nphi_Intercept            0.04      0.15    -0.26     0.32 1.00     4447\nzi_Intercept             0.52      0.16     0.21     0.86 1.00     5364\nconditionimpasse         0.04      0.20    -0.36     0.43 1.00     4433\nphi_conditionimpasse     0.17      0.18    -0.17     0.53 1.00     4856\nzi_conditionimpasse     -1.36      0.24    -1.83    -0.90 1.00     4822\n                     Tail_ESS\nIntercept                2768\nphi_Intercept            3118\nzi_Intercept             3269\nconditionimpasse         3138\nphi_conditionimpasse     3267\nzi_conditionimpasse      2933\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nperformance(zinb)\n\n\nWarning: Response residuals not available to calculate mean square error. (R)MSE\n  is probably not reliable.\n\n\n# Indices of model performance\n\nELPD     | ELPD_SE |   LOOIC | LOOIC_SE |    WAIC |    R2 |  RMSE | Sigma\n-------------------------------------------------------------------------\n-185.435 |   9.411 | 370.869 |   18.823 | 370.856 | 0.058 | 0.368 | 1.000\n\n\nCODE\n# plot(rope(zinb))\n\n\n\n\nCODE\ntidy(zinb, effects = \"fixed\")\n\n\nWarning in tidy.brmsfit(zinb, effects = \"fixed\"): some parameter names contain\nunderscores: term naming may be unreliable!\n\n\n# A tibble: 6 × 7\n  effect component term                 estimate std.error conf.low conf.high\n  <chr>  <chr>     <chr>                   <dbl>     <dbl>    <dbl>     <dbl>\n1 fixed  cond      (Intercept)            0.182      0.164   -0.142     0.506\n2 fixed  cond      phi_(Intercept)        0.0379     0.146   -0.256     0.317\n3 fixed  zi        (Intercept)            0.524      0.165    0.208     0.858\n4 fixed  cond      conditionimpasse       0.0375     0.202   -0.356     0.427\n5 fixed  cond      phi_conditionimpasse   0.173      0.178   -0.169     0.528\n6 fixed  zi        conditionimpasse      -1.36       0.236   -1.83     -0.899\n\n\nCODE\npaste(\"INTERPRET ALPHA\")\n\n\n[1] \"INTERPRET ALPHA\"\n\n\nCODE\nzi_intercept <- tidy(zinb, effects = \"fixed\") %>% \n  filter(component == \"zi\", term == \"(Intercept)\") %>% \n  pull(estimate)\n\n\nWarning in tidy.brmsfit(zinb, effects = \"fixed\"): some parameter names contain\nunderscores: term naming may be unreliable!\n\n\nCODE\n# Logit scale intercept\nzi_intercept\n\n\nb_zi_Intercept \n         0.524 \n\n\nCODE\n## b_zi_Intercept \n##          0.525\n\n# Transformed to a probability/proportion\nplogis(zi_intercept)\n\n\nb_zi_Intercept \n         0.628 \n\n\nCODE\n## b_zi_Intercept \n##         0.628\n\nnrow(df %>% filter(accuracy == 0 & condition == \"control\"))/nrow(df %>% filter(condition == \"control\"))\n\n\n[1] 0.627\n\n\nThe value of the z_intercept should be the same as the proportion of 0s in the control condition.\nVISUALIZE ZERO INFLATED COMPONENT ::: {.cell}\n\nCODE\nbeta_zi_pred_int <- zinb %>% \n  predicted_draws(newdata = tibble(condition = c(\"control\", \"impasse\"))) %>% \n  mutate(is_zero = .prediction == 0,\n         .prediction = ifelse(is_zero, .prediction - 0.01, .prediction))\n\nggplot(beta_zi_pred_int, aes(x = .prediction)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 0.025, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.5,\n                       guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"Predicted proportion of Correct Responses\", \n       y = \"Count\", fill = \"Is zero?\") +\n  facet_wrap(vars(condition), ncol = 2,\n             labeller = labeller(condition = c(`control` = \"Control\", \n                                           `impasse` = \"Impasse\"))) + \n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n:::\n\n\nZERO-ONE INFLATED BETA Regression\nhttps://sometimesir.com/posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/#zoib-regression https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#2-fractional-logistic-regression\nTODO - explore priors - explore random effects - explore non-beta distribution; once the 1s and zeros are gone, do we need the beta?\nA zero-inflated beta regression allows us to model a separate data generating process for the 0s and 1s and the rest.\nA ZOINB is a mixture of:\n\nA logistic regression model that predicts if an outcome is either 0 or 1 or not 0 or 1, defined by (or alternatively, a model that predicts if outcomes are extreme (0 or 1) or not (between 0 and 1); “IS EXTREME” model; defined by \\(\\alpha\\)\nA logistic regression model that predicts if any of the 0 or 1 outcomes are actually 1s, defined by (or alternatively, a model that predicts if the extreme values are 1) “WHICH EXTREME” model defined by \\(\\gamma\\)\nA beta regression model that predicts if an outcome is between 0 and 1 if it’s not zero or not one, defined by and (or alternatively, a model that predicts the non-extreme (0 or 1) values) “THE REST” model \\(\\mu\\) and \\(\\phi\\)\n\nCan also do mixed! (but no reason do this here, as the item level data is not beta reg )\n\n\nCODE\n#DATA SETUP\n# sub <- df_subjects %>% dplyr::select(condition, DV_percent_NABS) \n# don't need to transform anymore\ndf <- df_subjects %>% filter(mode == \"lab-synch\") %>% \n  mutate(\n    condition = pretty_condition,\n    scaled = (s_SCALED + 13)/26,  #transform scaled onto 0-1 scale\n    accuracy = item_test_NABS/8\n  ) %>% dplyr::select(condition, scaled, accuracy)\n\n\n#INTERCEPT ONLY model for alpha\nzoinb <- brm(\n  bf(accuracy ~ condition, #mean of (0,1) values, mu\n     phi ~ condition, #precision of (0,1) values, phi\n     zoi ~ condition, #is extreme? alpha\n     coi ~ condition), #is 1? gamma\n  data = df,\n  family = zero_one_inflated_beta(),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234,\n  backend = \"cmdstanr\",\n  file =\"analysis/utils/models/zoinb.rds\"\n)\n\n\nzoinb_scaled <- brm(\n  bf(scaled ~ condition, #mean of (0,1) values, mu\n     phi ~ condition, #precision of (0,1) values, phi\n     zoi ~ condition, #is extreme? alpha\n     coi ~ condition), #is 1? gamma\n  data = df,\n  family = zero_one_inflated_beta(),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234,\n  backend = \"cmdstanr\",\n  file =\"analysis/utils/models/zoinb_scaled.rds\"\n)\n\n\n\n\nCODE\nsummary(zoinb)\n\n\n Family: zero_one_inflated_beta \n  Links: mu = logit; phi = log; zoi = logit; coi = logit \nFormula: accuracy ~ condition \n         phi ~ condition\n         zoi ~ condition\n         coi ~ condition\n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                0.18      0.34    -0.50     0.83 1.00     4686\nphi_Intercept            0.64      0.37    -0.14     1.30 1.00     5921\nzoi_Intercept            1.54      0.33     0.90     2.21 1.00     6065\ncoi_Intercept           -1.86      0.42    -2.72    -1.10 1.00     6592\nconditionimpasse         0.15      0.40    -0.61     0.94 1.00     4866\nphi_conditionimpasse     0.28      0.44    -0.57     1.16 1.00     6320\nzoi_conditionimpasse    -1.18      0.43    -2.02    -0.36 1.00     5970\ncoi_conditionimpasse     0.98      0.55    -0.10     2.06 1.00     7590\n                     Tail_ESS\nIntercept                3068\nphi_Intercept            3140\nzoi_Intercept            3053\ncoi_Intercept            3007\nconditionimpasse         3428\nphi_conditionimpasse     3219\nzoi_conditionimpasse     3058\ncoi_conditionimpasse     2865\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nplot(rope(zoinb))\n\n\nPossible multicollinearity between b_phi_conditionimpasse and b_phi_Intercept (r = 0.83), b_zoi_conditionimpasse and b_zoi_Intercept (r = 0.8), b_coi_conditionimpasse and b_coi_Intercept (r = 0.76). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\n\n\n\nCODE\nplot_model(zoinb)\n\n\n\n\n\nCODE\nplot(rope(zoinb_scaled))\n\n\nPossible multicollinearity between b_phi_conditionimpasse and b_phi_Intercept (r = 0.81). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\n\n\n\nCODE\nplot_model(zoinb_scaled)\n\n\n\n\n\nMore plots\n\n\nCODE\ntidy(zoinb, effects = \"fixed\")\n\n\nWarning in tidy.brmsfit(zoinb, effects = \"fixed\"): some parameter names contain\nunderscores: term naming may be unreliable!\n\n\n# A tibble: 8 × 7\n  effect component term                 estimate std.error conf.low conf.high\n  <chr>  <chr>     <chr>                   <dbl>     <dbl>    <dbl>     <dbl>\n1 fixed  cond      (Intercept)             0.184     0.337   -0.495     0.826\n2 fixed  cond      phi_(Intercept)         0.636     0.366   -0.143     1.30 \n3 fixed  cond      zoi_(Intercept)         1.54      0.332    0.904     2.21 \n4 fixed  cond      coi_(Intercept)        -1.86      0.418   -2.72     -1.10 \n5 fixed  cond      conditionimpasse        0.153     0.397   -0.609     0.940\n6 fixed  cond      phi_conditionimpasse    0.281     0.441   -0.573     1.16 \n7 fixed  cond      zoi_conditionimpasse   -1.18      0.425   -2.02     -0.358\n8 fixed  cond      coi_conditionimpasse    0.976     0.553   -0.103     2.06 \n\n\nCODE\npaste(\"INTERPRET ALPHA\")\n\n\n[1] \"INTERPRET ALPHA\"\n\n\nCODE\nzoi_intercept <- tidy(zoinb, effects = \"fixed\") %>% \n  filter(component == \"cond\", term == \"zoi_(Intercept)\") %>% \n  pull(estimate)\n\n\nWarning in tidy.brmsfit(zoinb, effects = \"fixed\"): some parameter names contain\nunderscores: term naming may be unreliable!\n\n\nCODE\n# Logit scale intercept\nzoi_intercept\n\n\nb_zoi_Intercept \n           1.54 \n\n\nCODE\n# Transformed to a probability/proportion\nplogis(zoi_intercept)\n\n\nb_zoi_Intercept \n          0.824 \n\n\nCODE\nnrow(df %>% filter(accuracy %in% c(0,1) & condition == \"control\"))/nrow(df %>% filter(condition == \"control\"))\n\n\n[1] 0.823\n\n\nThe value of the zoi_intercept should be the same as the proportion of 0s and 1s (vs non 0s and 1s) in the control condition… and it does!\nVISUALIZE ZERO INFLATED COMPONENT ::: {.cell}\n\nCODE\nbeta_zoi_pred_int <- zoinb %>% \n  predicted_draws(newdata = tibble(condition = c(\"control\", \"impasse\"))) %>% \n  mutate(is_extreme = .prediction %in% c(0,1),\n         is_one = .prediction ==1, \n         .prediction = ifelse(is_extreme & !is_one, .prediction - 0.01, .prediction),\n         .prediction = ifelse(is_one, .prediction + 0.01, .prediction))\n\nggplot(beta_zoi_pred_int, aes(x = .prediction)) +\n  geom_histogram(aes(fill = is_extreme), binwidth = 0.05, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) +\n  geom_vline(xintercept = 1) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.5,\n                       guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"Predicted proportion of Correct Responses\", \n       y = \"Count\", fill = \"Is extreme?\") +\n  facet_wrap(vars(condition), ncol = 2,\n             labeller = labeller(condition = c(`control` = \"Control\", \n                                           `impasse` = \"Impasse\"))) + \n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n::: ::: {.cell}\n\nCODE\nbeta_zoi_pred_int <- zoinb_scaled %>% \n  predicted_draws(newdata = tibble(condition = c(\"control\", \"impasse\"))) %>% \n  mutate(is_extreme = .prediction %in% c(0,1),\n         is_one = .prediction ==1, \n         .prediction = ifelse(is_extreme & !is_one, .prediction - 0.01, .prediction),\n         .prediction = ifelse(is_one, .prediction + 0.01, .prediction))\n\nggplot(beta_zoi_pred_int, aes(x = .prediction)) +\n  geom_histogram(aes(fill = is_extreme), binwidth = 0.05, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) +\n  geom_vline(xintercept = 1) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.5,\n                       guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"Predicted proportion of Correct Responses\", \n       y = \"Count\", fill = \"Is extreme?\") +\n  facet_wrap(vars(condition), ncol = 2,\n             labeller = labeller(condition = c(`control` = \"Control\", \n                                           `impasse` = \"Impasse\"))) + \n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n:::\n\n\nCODE\npp_check(zoinb, draws = 8000 )\n\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\nWarning: The following arguments were unrecognized and ignored: draws\n\n\n\n\n\n\n\nHURDLE + NEG BINOM (bayesian)\nRather than transform score to a proportion correct, keep it in native count form, and model as negative binomial. hurdle model :\n\nA logistic regression model that predicts if an outcome is 0 or not (this is the hurdle part)\nA lognormal (hurdle_lognormal()), gamma (hurdle_gamma()), Poisson (hurdle_poisson()), or negative binomial (hurdle_negbinomial()) model for outcomes that are not zero\n\n\n\nCODE\n#SETUP DATA \ndf <- df_subjects %>% filter(mode == \"lab-synch\") %>% \n  mutate(\n    score = item_test_NABS,\n    condition = pretty_condition\n  )\n\n#PLOT\ngf_histogram(~score, fill = ~ condition, data = df) %>% gf_facet_wrap(~condition)\n\n\n\n\n\nCODE\n#FIT MODEL\nhln <- brm(\n  bf(score ~ condition,\n     hu ~ condition),\n  data = df,\n  family = hurdle_lognormal(),\n  chains = 4, iter = 2000, warmup = 1000, seed = 1234,\n  silent = 2, cores = 4, \n  backend = \"cmdstanr\",\n  file =\"analysis/utils/models/hln.rds\"\n)\n\nhnb <- brm(\n  bf(score ~ condition,\n     hu ~ condition),\n  data = df,\n  family = hurdle_negbinomial(),\n  chains = 4, iter = 2000, warmup = 1000, seed = 1234,\n  silent = 2, cores = 4, \n  backend = \"cmdstanr\",\n  file =\"analysis/utils/models/hnb.rds\"\n)\n\nhpoi <- brm(\n  bf(score ~ condition,\n     hu ~ condition),\n  data = df,\n  family = hurdle_poisson(),\n  chains = 4, iter = 2000, warmup = 1000, seed = 1234,\n  silent = 2, cores = 4, \n  backend = \"cmdstanr\",\n  file =\"analysis/utils/models/hpoi.rds\"\n)\n\n\nsummary(hln)\n\n\n Family: hurdle_lognormal \n  Links: mu = identity; sigma = identity; hu = logit \nFormula: score ~ condition \n         hu ~ condition\n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               1.56      0.17     1.23     1.90 1.00     4165     2973\nhu_Intercept            0.90      0.28     0.38     1.46 1.00     3768     2802\nconditionimpasse        0.02      0.21    -0.39     0.42 1.00     4336     3019\nhu_conditionimpasse    -1.23      0.38    -1.99    -0.48 1.00     4061     2334\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.73      0.07     0.61     0.88 1.00     4271     2950\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nsummary(hnb)\n\n\n Family: hurdle_negbinomial \n  Links: mu = log; shape = identity; hu = logit \nFormula: score ~ condition \n         hu ~ condition\n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               1.76      0.11     1.54     1.97 1.00     3785     2780\nhu_Intercept            0.90      0.29     0.34     1.49 1.00     4271     2678\nconditionimpasse       -0.02      0.14    -0.29     0.25 1.00     3901     2831\nhu_conditionimpasse    -1.23      0.38    -2.00    -0.48 1.00     4206     2768\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape    42.90     51.27     5.69   184.18 1.00     3524     2681\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nsummary(hpoi)\n\n\n Family: hurdle_poisson \n  Links: mu = log; hu = logit \nFormula: score ~ condition \n         hu ~ condition\n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               1.76      0.10     1.57     1.95 1.00     3312     2732\nhu_Intercept            0.91      0.28     0.37     1.49 1.00     3524     2752\nconditionimpasse       -0.02      0.12    -0.25     0.22 1.00     3995     3309\nhu_conditionimpasse    -1.24      0.38    -2.00    -0.53 1.00     4350     3112\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nplot_model(hln)\n\n\n\n\n\nCODE\nplot_model(hnb)\n\n\n\n\n\nCODE\nplot_model(hpoi)\n\n\n\n\n\nCODE\ncompare_performance(hln, hnb, hpoi)\n\n\nSome of the nested models seem to be identical\n\n\nWarning: Type 'pearson' is deprecated and will be removed in the future.\n\nWarning: Type 'pearson' is deprecated and will be removed in the future.\n\n\n# Comparison of Model Performance Indices\n\nName |   Model |     ELPD | ELPD_SE |   LOOIC | LOOIC weights | LOOIC_SE |    WAIC | WAIC weights |    R2 |  RMSE | Sigma\n-------------------------------------------------------------------------------------------------------------------------\nhln  | brmsfit | -231.964 |  17.047 | 463.928 |       < 0.001 |   34.094 | 463.888 |      < 0.001 | 0.078 | 3.224 | 0.660\nhnb  | brmsfit | -218.127 |  15.993 | 436.255 |         0.256 |   31.987 | 436.257 |        0.393 | 0.061 | 3.208 | 1.000\nhpoi | brmsfit | -217.708 |  16.373 | 435.416 |         0.744 |   32.746 | 435.383 |        0.607 | 0.060 | 3.208 | 1.000\n\n\nCODE\n# Exponential\np1 <- pp_check(hpoi) + labs(title=\"Poisson Hurdle\")\n\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\nCODE\np2 <- pp_check(hnb) + labs(title=\"NegBinom Hurdle\")\n\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\nCODE\np3 <- pp_check(hln) + labs(title=\"LogNorm Hurdle\")\n\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\nCODE\ncowplot::plot_grid( p1 + p2 + p3)\n\n\n\n\n\nCODE\n# Logged\npred <- posterior_predict(hpoi)\nbayesplot::ppc_dens_overlay(y = log1p(df$score), \n                            yrep = log1p(pred[1:10,])) + \n  labs(title = \"Posterior Predictive Checks\")\n\n\n\n\n\n\n\nCODE\npred_gdp_hurdle <- hpoi |> \n  predicted_draws(newdata = tibble(condition = c(\"control\",\"impasse\"))) |>\n  mutate(is_zero = .prediction == 0,\n         .prediction = ifelse(is_zero, .prediction - 0.1, .prediction))\n\nggplot(pred_gdp_hurdle, aes(x = .prediction)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 1, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) +\n  labs(x = \"test phase correct\", y = \"Count\", fill = \"Is zero?\",\n       title = \"Predicted number correct from hurdle model\") +\n  # coord_cartesian(xlim = c(-2500, 75000)) +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nThis model is making predictions outside the bounds of the data (beyond 8) TODO https://jsdajournal.springeropen.com/articles/10.1186/s40488-021-00121-4 consider motivation of hurdle vs zero inflated\n\n\nMIXED ZERO-ONE-INFLATED BETA Regression\nNext we’ll explore doing a beta regression on a proportion transformed scaled score at the item level, as a zero one inflated mixed beta reg ::: {.cell}\n\nCODE\ndf <- df_items %>% filter(mode == \"lab-synch\") %>% \n  filter(q %nin% c(6,9)) %>% \n  mutate(scaled = (score_SCALED + 1)/2,\n         condition = pretty_condition) %>% \n  dplyr::select(subject, scaled, condition)\n\n\nzoinb_scaled <- brm(\n  bf(scaled ~ condition + (1|subject) , #mean of (0,1) values, mu\n     phi ~ condition , #precision of (0,1) values, phi\n     zoi ~ condition , #is extreme? alpha\n     coi ~ condition ), #is 1? gamma\n  data = df,\n  family = zero_one_inflated_beta(),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234,\n  backend = \"cmdstanr\",\n  file =\"analysis/utils/models/zoinb_scaled_2.rds\"\n)\n\nsummary(zoinb_scaled)\n\n\n Family: zero_one_inflated_beta \n  Links: mu = logit; phi = log; zoi = logit; coi = logit \nFormula: scaled ~ condition + (1 | subject) \n         phi ~ condition\n         zoi ~ condition\n         coi ~ condition\n   Data: df (Number of observations: 1638) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~subject (Number of levels: 126) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.21      0.09     0.02     0.38 1.01      686      999\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept               -0.56      0.10    -0.75    -0.37 1.00     3621\nphi_Intercept            2.09      0.18     1.74     2.43 1.00     3764\nzoi_Intercept            2.41      0.13     2.17     2.66 1.00     4732\ncoi_Intercept           -1.16      0.09    -1.33    -0.99 1.00     6594\nconditionimpasse         0.25      0.12     0.02     0.47 1.00     3722\nphi_conditionimpasse    -0.09      0.20    -0.49     0.30 1.00     4326\nzoi_conditionimpasse    -1.20      0.15    -1.50    -0.92 1.00     5779\ncoi_conditionimpasse     1.46      0.12     1.23     1.69 1.00     6619\n                     Tail_ESS\nIntercept                2831\nphi_Intercept            3016\nzoi_Intercept            2525\ncoi_Intercept            2578\nconditionimpasse         2872\nphi_conditionimpasse     3305\nzoi_conditionimpasse     3425\ncoi_conditionimpasse     2759\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nplot_model(zoinb_scaled)\n\n\n\n\n:::\n\n\nBETA HURDLE Regression (Frequentist)\nhttps://github.com/markhwhiteii/beta_hurdle/blob/master/manuscript/beta_hurdle.pdf\n\nMU tells if mean is different by condition\nSIGMA tells if variance is different by condition\nNU coefficient tells if condition yields different probability at floor\nTAU coefficient tells if condition yields different probability at ceiling\n\n\n\nCODE\n#BETA HURDLE REGRESSION EXAMPLE\nlibrary(gamlss)\n\n\nLoading required package: gamlss.data\n\n\n\nAttaching package: 'gamlss.data'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nLoading required package: gamlss.dist\n\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThe following objects are masked from 'package:ordinal':\n\n    ranef, VarCorr\n\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\n\nLoading required package: parallel\n\n\n **********   GAMLSS Version 5.4-3  ********** \n\n\nFor more on GAMLSS look at https://www.gamlss.com/\n\n\nType gamlssNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'gamlss'\n\n\nThe following object is masked from 'package:brms':\n\n    cs\n\n\nThe following object is masked from 'package:extraDistr':\n\n    pcat\n\n\nThe following object is masked from 'package:lme4':\n\n    refit\n\n\nCODE\n#CREATE SAMPLE DATA \nn <- 5000 \nmu <- 0.40 \nsigma <- 0.60 \np0 <- 0.13 \np1 <- 0.17 \np2 <- 1- p0- p1\na <- mu * (1- sigma ^ 2) / (sigma ^ 2) \nb <- a * (1- mu) / mu\n\n#CREATE DIST\nset.seed(1839) \ny <- rbeta(n, a, b) \ncat <- sample(1:3, n, prob = c(p0, p2, p1), replace = TRUE) \ny[cat == 1] <- 0 \ny[cat == 3] <- 1\n\n#VISUALIZE DISTRIBUTION\nx <- as.data.frame(y)\ngf_histogram(~x$y)\n\n\n\n\n\nCODE\n#this looks not unlike my distribution! \n\n#CREATE AN EMPTY MODEL\nfit <- gamlss( formula = y ~ 1, # formula for mu \n               formula.sigma = ~ 1, # formula for sigma \n               formula.nu = ~ 1, # formula for nu \n               formula.tau = ~ 1, # formula for tau \n               family = BEINF() )\n\n\nGAMLSS-RS iteration 1: Global Deviance = 7799 \nGAMLSS-RS iteration 2: Global Deviance = 7778 \nGAMLSS-RS iteration 3: Global Deviance = 7778 \nGAMLSS-RS iteration 4: Global Deviance = 7778 \n\n\nCODE\nsummary(fit)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = y ~ 1, family = BEINF(), formula.sigma = ~1,  \n    formula.nu = ~1, formula.tau = ~1) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.3796     0.0196   -19.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.3951     0.0162    24.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.632      0.042   -38.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.4014     0.0382   -36.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  5000 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  4996 \n                      at cycle:  4 \n \nGlobal Deviance:     7778 \n            AIC:     7786 \n            SBC:     7812 \n******************************************************************\n\n\nCODE\nplot(fit)\n\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  0.000571 \n                       variance   =  1 \n               coef. of skewness  =  0.0294 \n               coef. of kurtosis  =  2.95 \nFilliben correlation coefficient  =  1 \n******************************************************************\n\n\nCODE\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nfit_mu <- inv_logit(fit$mu.coefficients) \npaste(\"MU: \",fit_mu)\n\n\n[1] \"MU:  0.406229902102452\"\n\n\nCODE\nfit_sigma <- inv_logit(fit$sigma.coefficients) \npaste(\"SIGMA: \",fit_sigma)\n\n\n[1] \"SIGMA:  0.597499259410111\"\n\n\nCODE\nfit_nu <- exp(fit$nu.coefficients) \nfit_tau <- exp(fit$tau.coefficients) \nfit_p0 <- fit_nu / (1 + fit_nu + fit_tau) \npaste(\"P0: \",fit_p0)\n\n\n[1] \"P0:  0.135600165493784\"\n\n\nCODE\nfit_p1 <- fit_tau / (1 + fit_nu + fit_tau)\npaste(\"P1: \",fit_p1)\n\n\n[1] \"P1:  0.170800000002391\"\n\n\nBETA HURDLE INTERPRETATION - beta component\n- MU “location” (mean)\n- SIGMA “scale” (positively related to variance; variance = sigma.squared mean (1-mean)\n- Rigby, Stasinopoulos, Heller, and De Bastiani (2017) “reparameterized” the beta distribution so that the two parameters determining the shape of the distribution would be more useful in a regression framework (see Ferrari & Cribari-Neto, 2004 for a different parameterization)\nZERO-ONE HURDLE COMPONENT\n- The two additional parameters, ν NU and τTAU , are related to p0 and p1, respectively.\n- p0 is the probability that a case equals 0,\n- p1 is the probability that a case equals 1,\n- p2 (i.e., 1 −p0 −p1) is the probability that the case comes from the beta distribution\n\n\nCODE\n#MY DATA\n#SETUP DATA \n\nmin = 0 #min possible value of scale\nmax = 13 #max possible value of scale\n\nlibrary(mosaic) #for shuffling\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following object is masked from 'package:brms':\n\n    mm\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following objects are masked from 'package:VGAM':\n\n    chisq, logit\n\n\nThe following object is masked from 'package:lmerTest':\n\n    rand\n\n\nThe following object is masked from 'package:lme4':\n\n    factorize\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following objects are masked from 'package:rstatix':\n\n    cor_test, prop_test, t_test\n\n\nThe following object is masked from 'package:modelr':\n\n    resample\n\n\nThe following object is masked from 'package:modelsummary':\n\n    msummary\n\n\nThe following object is masked from 'package:vcd':\n\n    mplot\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nThe following object is masked from 'package:cowplot':\n\n    theme_map\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\n\nCODE\n#1. Rescale accuracy using \n# recommended adjustment \n#rescaled = value-min/(max-min)\ndf <- df_subjects %>% mutate(\n  accuracy = s_NABS,\n  R_acc = (accuracy-min)/(max-min), #as %\n  T_acc = (accuracy * (nrow(df)-1) + 0.5)/nrow(df)/max, #transform for no 0 and 1\n  # perm = shuffle(condition),\n  scaffold_rt = item_scaffold_rt\n) %>% dplyr::select(accuracy,R_acc, T_acc, condition,scaffold_rt)\n\n#VISUALIZE DISTRIBUTION\ngf_histogram(~R_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = \"Histogram of accuracy\")\n\n\n\n\n\nCODE\n#VISUALIZE DISTRIBUTION\ngf_histogram(~T_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = \"Histogram of [rescaled] accuracy\")\n\n\n\n\n\nCODE\n# gf_histogram(~R_acc, fill = ~perm, data = df) %>% gf_facet_wrap(~perm) + labs(title = \"Histogram of shuffled accuracy\")\n\n#SUMMARIZE SAMPLE\npaste(\"Grand mean\", mean(df$R_acc))\n\n\n[1] \"Grand mean 0.288344988344988\"\n\n\nCODE\nlibrary(mosaic)\nstats = favstats(df$R_acc ~ df$condition)\nstats$mean <- mean(df$R_acc ~ df$condition)\nstats$var <- var(df$R_acc ~ df$condition)\nprint(\"Grand stats\")\n\n\n[1] \"Grand stats\"\n\n\nCODE\nstats \n\n\n  df$condition min Q1 median    Q3 max  mean    sd   n missing   var\n1      control   0  0  0.000 0.154   1 0.190 0.343 158       0 0.118\n2      impasse   0  0  0.154 0.788   1 0.379 0.395 172       0 0.156\n\n\nCODE\nprint(\"P0\")\n\n\n[1] \"P0\"\n\n\nCODE\nnrow(df %>% filter(R_acc ==0))/nrow(df)\n\n\n[1] 0.458\n\n\nCODE\nprint(\"P1\")\n\n\n[1] \"P1\"\n\n\nCODE\nnrow(df %>% filter(R_acc ==1))/nrow(df)\n\n\n[1] 0.0939\n\n\nCODE\n#CREATE MODEL\n\n#CREATE AN EMPTY MODEL\nm0 <- gamlss( formula = R_acc ~ 1, # formula for mu \n              formula.sigma =  ~ 1, # formula for sigma \n              formula.nu =  ~ 1, # formula for nu \n              formula.tau =  ~ 1, # formula for tau \n              family = BEINF(), data = df )\n\n\nGAMLSS-RS iteration 1: Global Deviance = 611 \nGAMLSS-RS iteration 2: Global Deviance = 610 \nGAMLSS-RS iteration 3: Global Deviance = 610 \nGAMLSS-RS iteration 4: Global Deviance = 610 \n\n\nCODE\nm0 <- gamlss(R_acc ~ 1, ~ 1, ~ 1, ~ 1, \n            data = df, family = BEINF())\n\n\nGAMLSS-RS iteration 1: Global Deviance = 611 \nGAMLSS-RS iteration 2: Global Deviance = 610 \nGAMLSS-RS iteration 3: Global Deviance = 610 \nGAMLSS-RS iteration 4: Global Deviance = 610 \n\n\nCODE\npaste(\"THE EMPTY MODEL\")\n\n\n[1] \"THE EMPTY MODEL\"\n\n\nCODE\nsummary(m0)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ 1, sigma.formula = ~1, nu.formula = ~1,  \n    tau.formula = ~1, family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  -0.2304     0.0947   -2.43    0.015 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.4262     0.0775     5.5  7.7e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.0201     0.1157    0.17     0.86\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.563      0.198   -7.91  3.9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  326 \n                      at cycle:  4 \n \nGlobal Deviance:     610 \n            AIC:     618 \n            SBC:     634 \n******************************************************************\n\n\nCODE\nplot(m0)\n\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  -0.00445 \n                       variance   =  1.03 \n               coef. of skewness  =  0.0264 \n               coef. of kurtosis  =  3.03 \nFilliben correlation coefficient  =  0.996 \n******************************************************************\n\n\nCODE\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nm0_mu <- inv_logit(m0$mu.coefficients) \npaste(\"MU: \",m0_mu)\n\n\n[1] \"MU:  0.442655135079248\"\n\n\nCODE\nm0_sigma <- inv_logit(m0$sigma.coefficients) \npaste(\"SIGMA: \",m0_sigma)\n\n\n[1] \"SIGMA:  0.604970105601303\"\n\n\nCODE\nm0_nu <- exp(m0$nu.coefficients) \npaste(\"NU: \",m0_nu)\n\n\n[1] \"NU:  1.02032203257766\"\n\n\nCODE\nm0_tau <- exp(m0$tau.coefficients) \npaste(\"TAU: \",m0_tau)\n\n\n[1] \"TAU:  0.209464832192797\"\n\n\nCODE\nm0_p0 <- fit_nu / (1 + fit_nu + fit_tau) \npaste(\"P0: \",m0_p0)\n\n\n[1] \"P0:  0.135600165493784\"\n\n\nCODE\nm0_p1 <- fit_tau / (1 + fit_nu + fit_tau)\npaste(\"P1: \",m0_p1)\n\n\n[1] \"P1:  0.170800000002391\"\n\n\nCODE\n#CREATE PREDICTOR MODEL\nm1 <- gamlss(R_acc ~ condition, \n             ~ condition, \n             ~ condition, \n             ~ condition, \n            data = df, family = BEINF())\n\n\nGAMLSS-RS iteration 1: Global Deviance = 573 \nGAMLSS-RS iteration 2: Global Deviance = 572 \nGAMLSS-RS iteration 3: Global Deviance = 572 \nGAMLSS-RS iteration 4: Global Deviance = 572 \n\n\nCODE\npaste(\"THE PREDICTOR MODEL\")\n\n\n[1] \"THE PREDICTOR MODEL\"\n\n\nCODE\nsummary(m1)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)  \n(Intercept)        -0.421      0.171   -2.46    0.014 *\nconditionimpasse    0.274      0.205    1.33    0.183  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)   \n(Intercept)        0.3949     0.1407    2.81   0.0053 **\nconditionimpasse   0.0309     0.1687    0.18   0.8549   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.767      0.178    4.30  2.3e-05 ***\nconditionimpasse   -1.440      0.247   -5.84  1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -1.264      0.314   -4.02 0.000072 ***\nconditionimpasse   -0.471      0.405   -1.16     0.25    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  4 \n \nGlobal Deviance:     572 \n            AIC:     588 \n            SBC:     618 \n******************************************************************\n\n\nCODE\n#LOOKING PREDICTOR MODEL\nm <- gamlss(R_acc ~ condition , \n            ~ condition , \n            ~ condition , \n            ~ condition , \n            data = df, family = BEINF())\n\n\nGAMLSS-RS iteration 1: Global Deviance = 573 \nGAMLSS-RS iteration 2: Global Deviance = 572 \nGAMLSS-RS iteration 3: Global Deviance = 572 \nGAMLSS-RS iteration 4: Global Deviance = 572 \n\n\nCODE\npaste(\"THE PREDICTOR MODEL\")\n\n\n[1] \"THE PREDICTOR MODEL\"\n\n\nCODE\nsummary(m)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)  \n(Intercept)        -0.421      0.171   -2.46    0.014 *\nconditionimpasse    0.274      0.205    1.33    0.183  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)   \n(Intercept)        0.3949     0.1407    2.81   0.0053 **\nconditionimpasse   0.0309     0.1687    0.18   0.8549   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.767      0.178    4.30  2.3e-05 ***\nconditionimpasse   -1.440      0.247   -5.84  1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -1.264      0.314   -4.02 0.000072 ***\nconditionimpasse   -0.471      0.405   -1.16     0.25    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  4 \n \nGlobal Deviance:     572 \n            AIC:     588 \n            SBC:     618 \n******************************************************************\n\n\nCODE\n#CREATE PREDICTOR MODEL ON SHUFFLED [PERMUTATION TEST]\n# mperm <- gamlss(R_acc ~ perm, ~ perm, ~ perm, ~ perm, \n#             data = df, family = BEINF())\n# summary(mperm)\n\n#sanity check with scaled outcome, no zeros ones\n# m3 <- gamlss(T_acc ~ condition, ~ condition, ~ condition, ~ condition, \n#             data = df, family = BEINF())\n# summary(m3)\n#m3 shouldn't show condition as significant for nu and tau, because T_acc was scaled to not include any 0s and 1s\n\n#investigate beta negative binomial distribution\n#https://en.wikipedia.org/wiki/Beta_negative_binomial_distribution\n\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nm1_mu <- inv_logit(m1$mu.coefficients) \npaste(\"MU: \",m1_mu)\n\n\n[1] \"MU:  0.396369901111619\" \"MU:  0.568036956954873\"\n\n\nCODE\nm1_sigma <- inv_logit(m0$sigma.coefficients) \npaste(\"SIGMA: \",m1_sigma)\n\n\n[1] \"SIGMA:  0.604970105601303\"\n\n\nCODE\nm1_nu <- exp(m1$nu.coefficients) \npaste(\"NU: \",m1_nu)\n\n\n[1] \"NU:  2.15227350452983\"  \"NU:  0.236870904856183\"\n\n\nCODE\nm1_tau <- exp(m1$tau.coefficients) \npaste(\"TAU: \",m1_tau)\n\n\n[1] \"TAU:  0.282617628302748\" \"TAU:  0.624417570712948\"\n\n\nCODE\nsummary(m)\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)  \n(Intercept)        -0.421      0.171   -2.46    0.014 *\nconditionimpasse    0.274      0.205    1.33    0.183  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)   \n(Intercept)        0.3949     0.1407    2.81   0.0053 **\nconditionimpasse   0.0309     0.1687    0.18   0.8549   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.767      0.178    4.30  2.3e-05 ***\nconditionimpasse   -1.440      0.247   -5.84  1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        -1.264      0.314   -4.02 0.000072 ***\nconditionimpasse   -0.471      0.405   -1.16     0.25    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  4 \n \nGlobal Deviance:     572 \n            AIC:     588 \n            SBC:     618 \n******************************************************************\n\n\nCODE\nplot(m)\n\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  0.0127 \n                       variance   =  0.938 \n               coef. of skewness  =  0.11 \n               coef. of kurtosis  =  2.76 \nFilliben correlation coefficient  =  0.997 \n******************************************************************\n\n\n\nMU tells if mean is different by condition\nSIGMA tells if variance is different by condition\nNU coefficient tells if condition yields different probability at floor\nTAU coefficient tells if condition yields different probability at ceiling"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#binomial-outcome",
    "href": "analysis/utils/modelling_ref.html#binomial-outcome",
    "title": "Modelling Reference",
    "section": "BINOMIAL OUTCOME",
    "text": "BINOMIAL OUTCOME\n\n\nCODE\n#PREPARE DATA \ndf <- df_items %>% filter(q ==1) %>% filter(mode == \"lab-synch\")\n# %>% mutate(\n#   accuracy = recode_factor(score_niceABS, \"0\" =\"incorrect\",\"1\"=\"correct\"),\n#   scaled = as.ordered(score_SCALED),\n#   q = as.factor(q),\n#   high_interpretation = as.factor(high_interpretation)\n# )\n\n#GROUPED PROPORTIONAL BAR CHART\ngf_props(~accuracy, fill = ~pretty_condition, x =~pretty_condition,\n       position = position_dodge(), data = df) %>%\n  gf_facet_grid(~pretty_mode) +\n   labs(x = \"Question 1 Accuracy\",\n       title = \"Accuracy on Q1\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n   labs(#y = \"\",\n       title = \"Accuracy on Test Phase\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\n\nCHI SQUARE\n\n\nCODE\n#::::::::::::CROSSTABLE\n# CrossTable( x = df$condition, y = df$accuracy, \n#              fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n#::::::::::::MOSAIC PLOT\n# note: blue indicates cell count higher than expected, \n# red indicates cell count less than expected; under null hypothesis\n# mosaicplot(main=\"Accuracy on First Question by Condition\",\n#             data = df, pretty_condition ~ accuracy, \n#             shade = T)\n\n#::::::::::::TABLE\ndf %>% sjtab( fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=F, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = c(\"auto\"))\n\n\n\n \n accuracy\n pretty_condition\n Total\n \n \n\n control\n impasse\n \n \n \n0\n524883.9 %\n454970.3 %\n979777 % \n\n \n \n1\n101416.1 %\n191529.7 %\n292923 % \n\n \n \nTotal\n6262100 %\n6464100 %\n126126100 % \n\nχ2=2.547 · df=1 · φ=0.161 · p=0.111 \n\n \n \n observed values\n expected values\n % within pretty_condition\n \n\n\n\nCODE\n#::::::::::::BAR PLOT\nggbarstats(data = df, x = accuracy, y = condition,\n           type = \"nonparametric\")\n\n\n\n\n\nCODE\n#::::::::::::CHISQR TEST\n(x <- stats::chisq.test(x = df$accuracy, y = df$condition, simulate.p.value = T))\n\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  df$accuracy and df$condition\nX-squared = 3, df = NA, p-value = 0.09\n\n\nCODE\n#::::::::::::POWER ANALYSIS\n(po <- pwr.chisq.test( w = 0.1, df=(2-1), N = nrow(df), sig.level = 0.05))\n\n\n\n     Chi squared power calculation \n\n              w = 0.1\n              N = 126\n             df = 1\n      sig.level = 0.05\n          power = 0.202\n\nNOTE: N is the number of observations\n\n\nA Chi-Square test of independence of Q1 accuracy [correct,incorrect] by condition indicates the question accuracy is not dependent on condition. However, this test may be underpowered, as with the given sample size it has only 20% power to detect a small effect (w = 0.1)\n\n\nLOGISTIC REGRESSION\n\nhttp://post8000.svmiller.com/lab-scripts/logistic-regression-lab.html\n\nFit a logistic regression (at the subject-item level), predicting Q1 accuracy (absolute score) by condition.\nnote: this example uses the combined dataset rather than lab-only, as learning notes were done with the combined and I don’t want to recalcualte all the marginal probabilities by hand for learning purposes.\n\nFit a logistic regression predicting accuracy (absolute score) (n = r nrow(df)) by condition. (k = 2).\nParameter estimate: \\(\\beta_{0}\\) = Log Odds of (correct) responses in CONTROL condition\n\n\\(e^{\\beta_{0}}\\) = ODDS of correct response in CONTROL condition\n\nParameter estimate: \\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for correct response in impasse (vs) control [log scale])\n\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of correct response in IMPASSE (vs) CONTROL\n\nNull hypothesis:\\(\\beta_{impasse} \\le 0\\) the odds for a correct response does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\gt 0\\) the odds of a correct response increases\n\n\nFit Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\n\nCODE\n#combined dataset, not lab only\ndf <- df_items %>% filter(q==1) %>% mutate(\n  accuracy = as.factor(score_niceABS)\n)\n\n# FREQUENCY TABLE\n# my.table <- table(df$accuracy, df$pretty_condition)\n# addmargins(my.table) #counts\n# addmargins(prop.table(my.table)) #props\n\n# MODEL FITTING:::::::::::::::::::::::::::::::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\nprint(\"EMPTY MODEL\")\n\n\n[1] \"EMPTY MODEL\"\n\n\nCODE\nsummary(m0)\n\n\n\nCall:\nglm(formula = accuracy ~ 1, family = \"binomial\", data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.696  -0.696  -0.696  -0.696   1.753  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.294      0.134   -9.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 343.66  on 329  degrees of freedom\nAIC: 345.7\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\n#: 2 CONDITION model\nm1 <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.819  -0.819  -0.548  -0.548   1.986  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.822      0.230   -7.93  2.2e-15 ***\npretty_conditionimpasse    0.901      0.285    3.16   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 333.07  on 328  degrees of freedom\nAIC: 337.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\n#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > m1$aic)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff |  Chi2 |     p\n-------------------------------------------\nm0   |   glm |  1 |         |       |      \nm1   |   glm |  2 |       1 | 10.59 | 0.001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.00113745235691825\"\n\n\nThe Condition predictor significantly improves model fit.\n\n\nLearning Notes\n\n\nCODE\n# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.819  -0.819  -0.548  -0.548   1.986  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.822      0.230   -7.93  2.2e-15 ***\npretty_conditionimpasse    0.901      0.285    3.16   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 333.07  on 328  degrees of freedom\nAIC: 337.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\n#: INTERPRET COEFFICIENTS\nprint(\"Coefficients —- LOG ODDS\")\n\n\n[1] \"Coefficients —- LOG ODDS\"\n\n\nCODE\nconfint(m1)\n\n\nWaiting for profiling to be done...\n\n\n                         2.5 % 97.5 %\n(Intercept)             -2.299  -1.39\npretty_conditionimpasse  0.353   1.48\n\n\nCODE\nprint(\"Coefficients —- ODDS RATIOS\")\n\n\n[1] \"Coefficients —- ODDS RATIOS\"\n\n\nCODE\n(e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiate)\n\n\nWaiting for profiling to be done...\n\n\n                              2.5 % 97.5 %\n(Intercept)             0.162  0.10  0.248\npretty_conditionimpasse 2.463  1.42  4.374\n\n\nCODE\nprint(\"Coefficients —- PROBABILITIES\")\n\n\n[1] \"Coefficients —- PROBABILITIES\"\n\n\nCODE\nprint(\"[these should math pred plots]\")\n\n\n[1] \"[these should math pred plots]\"\n\n\nCODE\nprint(\"Probability at reference level\")\n\n\n[1] \"Probability at reference level\"\n\n\nCODE\nplogis(m1$coefficients[[1]])\n\n\n[1] 0.139\n\n\nCODE\nprint(\"Probability at coefficient level\")\n\n\n[1] \"Probability at coefficient level\"\n\n\nCODE\nplogis(m1$coefficients[[1]] + m1$coefficients[[2]])\n\n\n[1] 0.285\n\n\nCODE\nprint(\"CHANGE in probability by coefficient\")\n\n\n[1] \"CHANGE in probability by coefficient\"\n\n\nCODE\nplogis(m1$coefficients[[1]] + m1$coefficients[[2]]) - plogis(m1$coefficients[[1]])\n\n\n[1] 0.146\n\n\nCODE\nprint(\"MODEL PREDICTIONS\")\n\n\n[1] \"MODEL PREDICTIONS\"\n\n\nCODE\n# Retrieve predictions as probabilities \n# (for each level of the predictor)\npred.control <- predict(m1,data.frame(pretty_condition=\"control\"),type=\"response\")\n#this should match : plogis(intercept coefficient)\npaste(\"Probability of success in control,\", pred.control)\n\n\n[1] \"Probability of success in control, 0.139240506329147\"\n\n\nCODE\npred.impasse <- predict(m1,data.frame(pretty_condition=\"impasse\"),type=\"response\")\n#this should match : plogis(intercept coefficient + predictor coeff)\npaste(\"Probability of success in impasse,\", pred.impasse)\n\n\n[1] \"Probability of success in impasse, 0.284883720930631\"\n\n\nCODE\n# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: \n\n# one-sided (right tail) z test for B COEFFICIENT\n#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients\n#https://www.ibm.com/support/pages/can-one-get-one-tailed-tests-logistic-regression-dividing-significance-levels-half\n\n#SANITY CHECK 2-tailed test should match the model output\ntt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"p value for two-tailed test, null B = 0 : \",round(tt,3))\n\n\n[1] \"p value for two-tailed test, null B = 0 :  0.002\"\n\n\nCODE\not <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"BUT we want a one tailed directional, null: B <= 0: \",round(ot,3))\n\n\n[1] \"BUT we want a one tailed directional, null: B <= 0:  0.001\"\n\n\nCODE\npaste(\"adjusted confint for directional hypothesis\")\n\n\n[1] \"adjusted confint for directional hypothesis\"\n\n\nCODE\n(dcint <- confint(m1, level = 0.90)) # get 90% for right side))\n\n\nWaiting for profiling to be done...\n\n\n                          5 %  95 %\n(Intercept)             -2.22 -1.46\npretty_conditionimpasse  0.44  1.38\n\n\nCODE\n# https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte\n\n\n#:::::::: INTERPRET COEFFICIENTS [directional]\n\n# print(\"Confidence Interval —- LOG ODDS\")\n# confint(m1) #not adjusted for 1-tailed\n# print(\"Coefficients —- ODDS RATIOS\")\n# (e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiated, not adjusted\n# (e <- cbind( exp(coef(m1)), exp(dcint))) #exponentiated, adjusted\n\n\nUnderstanding the logistic regression model\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\ntable(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n     \n      control impasse   Sum\n  0     0.861   0.715 0.785\n  1     0.139   0.285 0.215\n  Sum   1.000   1.000 1.000\n\n\nCODE\ntable(df$accuracy, df$condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1) #sanity check sum of columns\n\n\n     \n      control impasse Sum\n  0       136     123 259\n  1        22      49  71\n  Sum     158     172 330\n\n\nThe logistic regression intercept gives the log odds of the outcome for the reference level of the predictor variable\nThe logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.\n[the empty model\n\nThe intercept of an empty model (glm(accuracy ~ 1) is equal to log(p/(1-p)), where p = the overall probability of a correct response (df$accuracy ==1 ).\nIn SGC3A Q1 accuracy this = 71 correct / 330 = 0.215 -> log(0.215 / (1-0.215)) = -1.29.\nIn other words, the intercept from the model with no predictor variables is the estimated log odds of a correct response for the whole sample.\nWe can also transform the log of the odds back to a probability: p = ODDS/ (1+ODDS) = exp(-1.29)/(1+exp(-1.29)) = 0.215. This should matched the prediction of the empty model\n\n[a dichotomous predictor]\nnatural log (odds of +) = -1.822 + 0.901(x1) ; x1 = 0 for control, 1 for impasse - b0 intercept is ODDS OF CORRECT RESPONSE IN REFERENCE (control) - b1 intercept is ODDS RATIO (difference in odds) FOR CORRECT RESPONSE IN IMPASSE\n\nINTERCEPT: log odds of (+ response) in control condition\n\nlog odds of (+) in control : -1.822 + 0.9(0) = -1.822\nconvert to odds by exponentiating the coefficients\nlog odds of (+) in control = exp(-1.822) = 0.162 odds\nconvert to probability by formula =>\np(+) = odds / (1+odds) = 0.162 / (1 + 0.162) = 0.139\nprobability of (+) in control = ~14%\n\nB1 COEFFICIENT: DIFFERENCE in log odds of (+) in impasse vs. control\n\nlog odds of (+) in impasse: -1.822 + 0.901 = -0.921\nconvert to odds by exponentiating log odds\nlog odds (+) in impasse = exp(-0.921) = 0.398\nconvert to probability by formula =>\np(+) = odds / (1 + odds) = 0.398 / (1+0.398) = 0.285\nprobaility of (+) in impasse = ~ 29%\n\nODDS RATIO : exponentiated B1 COEFFICIENT\n\nB1 = (slope of logit model = difference in log odds = log odds ratio\nB1 = 0.901 is log odds ratio of (+) in impasse vs control\nexp(b1) = exp(0.901) = 2.46\nRatio of odds in impasse are 2.46 times higher than in control. Bein in the impasse condition yields odds athat are 2.46 X higher than in control.\n\n\n\n\n\n\n\n\nMARGINAL\ntotal = 330 success : 71, failure : 259\np(+) = 71 / 330 = 0.215 = 22%\nodds(+) = 71 / 259 = 0.274\n\n\nCONTROL total = 158 success = 22; failure = 136\np(+) = 22/158 = 0.139 = 14%\nodds(+) = 22/136 = 0.162\n\n\nIMPASSE total = 172 success = 49; failure = 123\np(+) = 49/172 = 0.285 = 29%\nodds(+) = 49/123 = 0.398\n\n\n\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \nggcoefstats(m1, output = \"plot\", \n              conf.level = 0.90) + \n  labs(x = \"Log Odds Estimate\", \n       subtitle = \"p is for two tailed test\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m1, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) + #manually adjusted for directional test   \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m1, type=\"eff\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\n# tab_model(m1)\n\n#MODELSUMMARY | MODEL | PROBABILITIES\n# modelsummary(m1, exponentiate = TRUE)\n\n#MARGINALEFFECTS | MODEL | PROBABILITIES\nlibrary(marginaleffects)\n\n\n\nAttaching package: 'marginaleffects'\n\n\nThe following object is masked from 'package:modelr':\n\n    typical\n\n\nCODE\n#plot adjusted predictions, conditional on the CONDITION variable using the plot_cap\n#interactions plot_cap(mod, condition = c(\"length\", \"style\"))\nplot_cap(m1, condition = \"pretty_condition\")\n\n\n\n\n\nCODE\n# The plot_cme function can be used to draw “Conditional Marginal Effects.” \n#use for ixns \n# plot_cme(m1, effect = \"var1\", condition = \"pretty_condition\")\n\n\n\n\nDiagnostics\n\n\nCODE\nprint(\"SANITY CHECK REPORTING\")\n\n\n[1] \"SANITY CHECK REPORTING\"\n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic model (estimated using ML) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model's explanatory power is weak (Tjur's R2 = 0.03). The model's intercept, corresponding to pretty_condition = control, is at -1.82 (95% CI [-2.30, -1.39], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.90, 95% CI [0.35, 1.48], p = 0.002; Std. beta = 0.90, 95% CI [0.35, 1.48])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nCODE\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(m1)\n\n\n# Indices of model performance\n\nAIC     |     BIC | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n----------------------------------------------------------------------------------------------\n337.074 | 344.673 |     0.031 | 0.404 | 1.008 |    0.505 |   -16.847 |           0.021 | 0.673\n\n\nCODE\nprint(\"MODEL DIAGNOSTICS\")\n\n\n[1] \"MODEL DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m1)\n\n\n\n\n\nCODE\nbinned_residuals(m1)\n\n\nOk: About 100% of the residuals are inside the error bounds.\n\n\nCODE\n# logitgof(df$accuracy, m1$fitted.values, ord=FALSE)\n# hoslem.test(x = df$accuracy, y = fitted(m1), g =  2)\n# p should be non significant\n\n\n\n\nInference\nWe fit a logistic regression model to analyze the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 3.16, p = 0.0016). The model predicts that the odds of a correct response on the first question in the impasse condition increase by 146% (\\(e^{beta_1}\\) = 2.46, 95% CI [1.42, 4.37]) over the control condition.\nEquivalent statements:\n\nbeing in impasse condition increases log odds of correct response by 0.901 (over control)\nbeing in impasse increases odds of correct response in impasse over control increases by factor of 2.46\nprobability of correct response in control predicted as 28.5%, vs only 14% in control condition\n\n\n\nCODE\n#PRETTY TABLE SJPLOT\ntab_model(m1)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.16\n0.10 – 0.25\n<0.001\n\n\npretty condition[impasse]\n2.46\n1.42 – 4.37\n0.002\n\n\nObservations\n330\n\n\nR2 Tjur\n0.031"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#ordinal-outcome",
    "href": "analysis/utils/modelling_ref.html#ordinal-outcome",
    "title": "Modelling Reference",
    "section": "ORDINAL OUTCOME",
    "text": "ORDINAL OUTCOME\nDoes CONDITION affect the Q1 [ordered] type of response given?\n\n\nCODE\n#SETUP DATA\ndf <- df_items %>% filter(q==1)  %>% filter(mode == \"lab-synch\") \n#scaled has already been set as an ordered factor of score_SCALED\n\n#::::::::::::DESCRIPTIVES\nprop.table(table(df$scaled, df$condition))\n\n\n           \n            control impasse\n  orth      0.39683 0.15873\n  unknown   0.00794 0.00000\n  uncertain 0.00000 0.14286\n  lines     0.00794 0.05556\n  tri       0.07937 0.15079\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\ngf_props(~scaled, fill= ~condition, data = df) %>% \n  gf_facet_grid(condition ~ .) + easy_remove_legend()\n\n\n\n\n\nCODE\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = condition,\n                       fill = scaled)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"\",\n       title = \"Q1 Interpretation\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nDoes CONDITION affect the Q1 [ordered] state of understanding?\n\n\nCODE\n#SETUP DATA\ndf <- df_items %>% filter(q==1)  %>% filter(mode == \"lab-synch\") \n#scaled has already been set as an ordered factor of score_SCALED\n\n#::::::::::::DESCRIPTIVES\nprop.table(table(df$state, df$condition))\n\n\n           \n            control impasse\n  orth-like 0.39683 0.15873\n  unknown   0.00794 0.14286\n  tri-like  0.08730 0.20635\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\ngf_props(~state, fill= ~condition, data = df) %>% \n  gf_facet_grid(condition ~ .) + easy_remove_legend()\n\n\n\n\n\nCODE\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"\",\n       title = \"Q1 State\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\n\nORDINAL REGRESSION — Cumulative Link; Proportional Odds\nFit an ordinal logistic regression (at the subject level), predicting Q1 interpretation by condition.\n\nhttps://stats.oarc.ucla.edu/r/faq/ologit-coefficients/\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\ntodo see ordinal regression video: https://www.youtube.com/watch?v=rPcMcW25PPA&ab_channel=NCRMUK\nhttps://peopleanalytics-regression-book.org/ord-reg.html\nhttps://medium.com/evangelinelee/brant-test-for-proportional-odds-in-r-b0b373a93aa2\nhttps://github.com/runehaubo/ordinal/blob/master/old_vignettes/clm_tutorial.pdf\n\nLearning Notes\n\nproportional odds regression models effectively act as a series of stratified binomial models under the assumption that the ‘slope’ of the logistic function of each stratified model is the same.\nthus need to verify proportional odds assumption - ordinal regression requires an proportional odds assumption (the same slope holds for each equation)\nthis is required because the model simultaneously estimates k-1 equations, but each equation has the same slope, with different intercepts.\nconversely, a multinomial (categorical) model will have different slopes as well as intercepts - the intercepts are always ordered in size alpha 1 < alpha 2 < alpha k-1…\nTODO - see difference between the three types of ordinal models (adjacent category (vs) cumulative proportions; check Agresti book\n\n\nFit Model\n\n\nCODE\n#::::::::::::ORDINAL REGRESSION MODELS\n\n#EMPTY MODEL\npaste(\"EMPTY Ordinal regression of q1 SCALED score (ordered interpretation)\")\n\n\n[1] \"EMPTY Ordinal regression of q1 SCALED score (ordered interpretation)\"\n\n\nCODE\nom.0 <- clm(state ~ 1 , data = df)\n# summary(om.0)\n\n#PREDICTOR MODEL\npaste(\"Ordinal regression of q1 SCALED score (ordered interpretation)\")\n\n\n[1] \"Ordinal regression of q1 SCALED score (ordered interpretation)\"\n\n\nCODE\nom <- clm(state ~ condition, data = df)\n# summary(om)\n\n#COMPARE EMPTY AND PREDICTOR\ntest_lrt(om.0, om)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff |  Chi2 |      p\n--------------------------------------------\nom.0 |   clm |  2 |         |       |       \nom   |   clm |  3 |       1 | 25.77 | < .001\n\n\nCODE\n#::::::::: EQUIVALENT APPROACH USING POLYR \n\n# #MODEL\nm <- polr(state ~ condition , data = df, Hess=TRUE)\n# summary(m)\n\n#exponentiate coefficients and CIs\n# (ctable <- coef(summary(m)))\n# (p <- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2)\n# (ctable <- cbind(ctable, \"p value\" = p))\n# (ci <- confint(m))\n# (e <- coef(m))\n\n\nLikelihood ratio test suggests the predictor model is a better fit than the empty (intercept only) model.\n\n\nInference\n\n\nCODE\npaste(\"SUMMARY\")\n\n\n[1] \"SUMMARY\"\n\n\nCODE\nsummary(om)\n\n\nformula: state ~ condition\ndata:    df\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  126  -109.55 225.09 6(0)  2.18e-10 2.5e+01\n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \nconditionimpasse      1.9        0.4    4.74  2.1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n                  Estimate Std. Error z value\north-like|unknown    1.326      0.323    4.10\nunknown|tri-like     2.109      0.355    5.94\n\n\nCODE\n#LOG ODDS\npaste(\"IN LOG ODDS\")\n\n\n[1] \"IN LOG ODDS\"\n\n\nCODE\n(ctable <- coef(summary(om)))\n\n\n                  Estimate Std. Error z value Pr(>|z|)\north-like|unknown     1.33      0.323    4.10 4.07e-05\nunknown|tri-like      2.11      0.355    5.94 2.80e-09\nconditionimpasse      1.90      0.400    4.74 2.10e-06\n\n\nCODE\n(ci <- confint(om)) \n\n\n                 2.5 % 97.5 %\nconditionimpasse  1.14   2.72\n\n\nCODE\npaste(\"IN ODDS RATIOS\")\n\n\n[1] \"IN ODDS RATIOS\"\n\n\nCODE\n#ODDS RATIOS\nexp(coef(om))\n\n\north-like|unknown  unknown|tri-like  conditionimpasse \n             3.77              8.24              6.68 \n\n\nCODE\nexp(ci)\n\n\n                 2.5 % 97.5 %\nconditionimpasse  3.12   15.1\n\n\nOverall, participants in the impasse condition had higher odds (6.68 X as likely) to offer more correct interpretations than those in the control condition (z = 4.74, p < 0.001).\n\nwe see the estimates for the 2 intercepts, which are sometimes called cutpoints.\nThe intercepts indicate where the latent variable is cut to make the three groups that we observe in our data.\nNote that this latent variable is continuous. In general, these are not used in the interpretation of the results.\nThe cutpoints are closely related to thresholds, which are reported by other statistical packages.\nfor k groups there will be k-1 intercepts (cutpoints)\nconfirm that the CI does not include 0 (the units are ordered logits [ordered log odds])\nas with logistic regression we exponentiate the coefficients and confints to get odds ratio\n\n\n\nVisualize Model\n\n\nCODE\n# sjPlot::tab_model(om)\nsjPlot::plot_model(om)\n\n\n\n\n\nCODE\nsjPlot::plot_model(om, type = \"eff\")\n\n\n$condition\n\n\n\n\n\nCODE\n                   # show.data = TRUE, jitter = TRUE)\n\n\n\n\nDiagnostics\n\n\nCODE\n#:: ASSESS FIT\nperformance(om)\n\n\nCan't calculate log-loss.\nCan't calculate proper scoring rules for ordinal, multinomial or cumulative link models.\n\n\n# Indices of model performance\n\nAIC     |     BIC | Nagelkerke's R2 |  RMSE | Sigma\n---------------------------------------------------\n225.090 | 233.599 |           0.216 | 1.629 | 1.648\n\n\nCODE\n# #test proporitional odds assumption \nbrant(m) #only works for polyr type model not clm type model\n\n\n---------------------------------------------------- \nTest for        X2  df  probability \n---------------------------------------------------- \nOmnibus         15.6    1   0\nconditionimpasse    15.6    1   0\n---------------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nCODE\n# # A p-value of less than 0.05 on this test—particularly on the Omnibus plus at least one of the variables—should be interpreted as a failure of the proportional odds assumption.\n\n#test proportional odds assumption\nlibrary(pomcheckr)\n# https://cran.r-project.org/web/packages/pomcheckr/pomcheckr.pdf\n(p <- pomcheck( scaled ~ condition , data = df))\n\n\n[[1]]\n# A tibble: 2 × 6\n# Groups:   condition [2]\n  condition `scaled_>=1` `scaled_>=2` `scaled_>=4` `scaled_>=5` `scaled_>=3`\n  <fct>            <dbl>        <dbl>        <dbl>        <dbl>        <dbl>\n1 control            Inf        -1.43       -1.53        -1.65        NA    \n2 impasse            Inf        NA          -0.379       -0.862        0.788\n\nattr(,\"class\")\n[1] \"pomcheck\" \"list\"    \n\n\nCODE\nplot(p)\n\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\nThe output of the graphical test for proportional odds assumption suggests that the proportional odds assumption may be unreasonable for this dataset. Also, inspecting the output table, we see the coefficients for each level of the scaled variable are quite different.\nThus, an alternative approach may be more appropriate:\n\nBaseline logistic model. This model is the same as the multinomial regression model covered in the previous chapter, using the lowest ordinal value as the reference.\nAdjacent-category logistic model. This model compares each level of the ordinal variable to the next highest level, and it is a constrained version of the baseline logistic model. The brglm2 package in R offers a function bracl() for calculating an adjacent category logistic model.\nContinuation-ratio logistic model. This model compares each level of the ordinal variable to all lower levels. This can be modeled using binary logistic regression techniques, but new variables need to be constructed from the data set to allow this. The R package rms has a function cr.setup() which is a utility for preparing an outcome variable for a continuation ratio model.\n\n*Note: for multiple regression, the ordinal package offers a parameter (nominal = ~predictors) that allow you to designate some predictors as nominal rather than ordinal. But this is not appropriate for this use case._"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#categorical-outcome",
    "href": "analysis/utils/modelling_ref.html#categorical-outcome",
    "title": "Modelling Reference",
    "section": "CATEGORICAL OUTCOME",
    "text": "CATEGORICAL OUTCOME\nDoes CONDITION affect the Q1 [ordered] type of response given?\n\n\nCODE\n#SETUP DATA\ndf <- df_items %>% filter(q==1)  %>% filter(mode == \"lab-synch\") \n#scaled has already been set as an ordered factor of score_SCALED\n\n#::::::::::::DESCRIPTIVES\nprop.table(table(df$scaled, df$condition))\n\n\n           \n            control impasse\n  orth      0.39683 0.15873\n  unknown   0.00794 0.00000\n  uncertain 0.00000 0.14286\n  lines     0.00794 0.05556\n  tri       0.07937 0.15079\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\ngf_props(~scaled, fill= ~condition, data = df) %>% \n  gf_facet_grid(condition ~ .) + easy_remove_legend()\n\n\n\n\n\nCODE\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = condition,\n                       fill = scaled)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"\",\n       title = \"Q1 Interpretation\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nDoes CONDITION affect the Q1 [ordered] state of understanding?\n\n\nCODE\n#SETUP DATA\ndf <- df_items %>% filter(q==1)  %>% filter(mode == \"lab-synch\") \n#scaled has already been set as an ordered factor of score_SCALED\n\n#::::::::::::DESCRIPTIVES\ntable(df$state, df$condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse    Sum\n  orth-like  0.8065  0.3125 0.5556\n  unknown    0.0161  0.2812 0.1508\n  tri-like   0.1774  0.4062 0.2937\n  Sum        1.0000  1.0000 1.0000\n\n\nCODE\n#::::::::::::VISUALIZE DISTRIBUTIONS\ngf_props(~state, fill= ~condition, data = df) %>% \n  gf_facet_grid(condition ~ .) + easy_remove_legend()\n\n\n\n\n\nCODE\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"\",\n       title = \"Q1 State\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#GGSTATSPLOT\nggbarstats(df, state, condition)\n\n\n\n\n\n\nCHI SQUARE\nDoes CONDITION affect the Q1 [categorical] type of response given?\n\n\nCODE\nCrossTable( x = df$condition, y = df$scaled, \n             fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\nWarning in chisq.test(t, correct = FALSE, ...): Chi-squared approximation may be\nincorrect\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  126 \n\n \n             | df$scaled \ndf$condition |      orth |   unknown | uncertain |     lines |       tri | Row Total | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n     control |        50 |         1 |         0 |         1 |        10 |        62 | \n             |    34.444 |     0.492 |     8.857 |     3.937 |    14.270 |           | \n             |     7.025 |     0.524 |     8.857 |     2.191 |     1.278 |           | \n             |     0.806 |     0.016 |     0.000 |     0.016 |     0.161 |     0.492 | \n             |     0.714 |     1.000 |     0.000 |     0.125 |     0.345 |           | \n             |     0.397 |     0.008 |     0.000 |     0.008 |     0.079 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n     impasse |        20 |         0 |        18 |         7 |        19 |        64 | \n             |    35.556 |     0.508 |     9.143 |     4.063 |    14.730 |           | \n             |     6.806 |     0.508 |     8.580 |     2.122 |     1.238 |           | \n             |     0.312 |     0.000 |     0.281 |     0.109 |     0.297 |     0.508 | \n             |     0.286 |     0.000 |     1.000 |     0.875 |     0.655 |           | \n             |     0.159 |     0.000 |     0.143 |     0.056 |     0.151 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\nColumn Total |        70 |         1 |        18 |         8 |        29 |       126 | \n             |     0.556 |     0.008 |     0.143 |     0.063 |     0.230 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  39.1     d.f. =  4     p =  6.55e-08 \n\n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nAlternative hypothesis: two.sided\np =  1.01e-09 \n\n \n\n\n\n\nCODE\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Interpretation on First Question by Condition\",\n            data = df, condition ~ scaled, \n            shade = T)\n\n\n\n\n\nCODE\n#::::::::::::WITH STATISTICS\nggbarstats(data = df, x = condition, y = scaled,\n           type = \"nonparametric\") \n\n\n\n\n\n\n\nCODE\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Response Type on First Question by Condition\",\n            data = df, condition ~ scaled, \n            shade = T)\n\n\n\n\n\nCODE\nCrossTable( x = df$condition, y = df$scaled, \n            fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\nWarning in chisq.test(t, correct = FALSE, ...): Chi-squared approximation may be\nincorrect\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  126 \n\n \n             | df$scaled \ndf$condition |      orth |   unknown | uncertain |     lines |       tri | Row Total | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n     control |        50 |         1 |         0 |         1 |        10 |        62 | \n             |    34.444 |     0.492 |     8.857 |     3.937 |    14.270 |           | \n             |     7.025 |     0.524 |     8.857 |     2.191 |     1.278 |           | \n             |     0.806 |     0.016 |     0.000 |     0.016 |     0.161 |     0.492 | \n             |     0.714 |     1.000 |     0.000 |     0.125 |     0.345 |           | \n             |     0.397 |     0.008 |     0.000 |     0.008 |     0.079 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n     impasse |        20 |         0 |        18 |         7 |        19 |        64 | \n             |    35.556 |     0.508 |     9.143 |     4.063 |    14.730 |           | \n             |     6.806 |     0.508 |     8.580 |     2.122 |     1.238 |           | \n             |     0.312 |     0.000 |     0.281 |     0.109 |     0.297 |     0.508 | \n             |     0.286 |     0.000 |     1.000 |     0.875 |     0.655 |           | \n             |     0.159 |     0.000 |     0.143 |     0.056 |     0.151 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\nColumn Total |        70 |         1 |        18 |         8 |        29 |       126 | \n             |     0.556 |     0.008 |     0.143 |     0.063 |     0.230 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  39.1     d.f. =  4     p =  6.55e-08 \n\n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nAlternative hypothesis: two.sided\np =  1.01e-09 \n\n \n\n\nCODE\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"scaled\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n\n\n\n \n scaled\n pretty_condition\n Total\n \n \n\n control\n impasse\n \n \n \north\n503471.4 %80.6 %\n203628.6 %31.2 %\n7070100 %55.6 % \n\n \n \nunknown\n10100 %1.6 %\n010 %0 %\n11100 %0.8 % \n\n \n \nuncertain\n090 %0 %\n189100 %28.1 %\n1818100 %14.3 % \n\n \n \nlines\n1412.5 %1.6 %\n7487.5 %10.9 %\n88100 %6.3 % \n\n \n \ntri\n101434.5 %16.1 %\n191565.5 %29.7 %\n2929100 %23 % \n\n \n \nTotal\n626249.2 %100 %\n646450.8 %100 %\n126126100 %100 % \n\nχ2=39.128 · df=4 · Cramer's V=0.557 · Fisher's p=0.000 \n\n \n \n observed values\n expected values\n % within scaled\n % within pretty_condition\n \n\n\n\nTODO INFERENCE\n\n\nMULTINOMIAL REGRESSION\non interpreting coefficients (marginal probability) - https://data.princeton.edu/wws509/stata/mlogit\nTODO RECONCILE actual predictions with coefficients and visualization of predictions\nDoes condition affect the response state of Q1?\n\nhttps://peopleanalytics-regression-book.org/multinomial-logistic-regression-for-nominal-category-outcomes.html#running-a-multinomial-regression-model\nhttps://bookdown.org/chua/ber642_advanced_regression/multinomial-logistic-regression.html\nhttps://www.youtube.com/watch?v=JcCBIPqcwFo&list=PLzv58M2GAfm50X_Twskr1aXaV5qMuIszx&ab_channel=NCRMUK\nCan use nnet package multinom() or mclogit package mblogit() [“baseline logit model”] or brms with family = “categorical”\n\nFit a logistic regression predicting interpretation (k=#response categories) by condition. (k = 2).\n\n2 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer)\n(from experience, seems) Each cell must have at least one observation (if one cell is blank, then it seems to be incorrectly estimated, see predicting high_interpretation vs. state. suggests nonsignificant OR for ‘unknown’ category, when infact that difference drives the effect\nFor each equation:\n\n\\(\\beta_{0}\\) = Log Odds of [this category type vs. reference category type) response in CONTROL condition\n\n\\(e^{\\beta_{0}}\\) = ODDS of [this category type vs. reference category type] response in CONTROL condition\n\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for [this category] type response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of [this. vs reference category type] response in IMPASSE (vs) CONTROL\nNull hypothesis: \\(\\beta_{impasse} \\le 0\\) the odds for [this category of response vs. reference] does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\gt 0\\) the odds of [this category of response vs. reference] increases\n\n\n\n\n\nCODE\n#::::::::::::DESCRIPTIVES RESPONSE STATE\n\ntable(df$state, df$condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse    Sum\n  orth-like  0.8065  0.3125 0.5556\n  unknown    0.0161  0.2812 0.1508\n  tri-like   0.1774  0.4062 0.2937\n  Sum        1.0000  1.0000 1.0000\n\n\nCODE\n(t <- table(df$state, df$condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n           \n            control impasse Sum\n  orth-like      50      20  70\n  unknown         1      18  19\n  tri-like       11      26  37\n  Sum            62      64 126\n\n\n\nFit Model\n\n\nCODE\nlibrary(nnet)\n\n#check reference level \nlevels(df$state)\n\n\n[1] \"orth-like\" \"unknown\"   \"tri-like\" \n\n\nCODE\n#FIT EMPTY MODEL\ncatm.0 <- multinom(state ~ 1, data = df)\n\n\n# weights:  6 (2 variable)\ninitial  value 138.425148 \nfinal  value 122.428550 \nconverged\n\n\nCODE\n# summary(catm.0)\n\n#FIT PREDICTOR MODEL\ncatm <- multinom(formula = state ~ condition, data = df, model = TRUE)\n\n\n# weights:  9 (4 variable)\ninitial  value 138.425148 \niter  10 value 103.421004\niter  10 value 103.421004\nfinal  value 103.421004 \nconverged\n\n\nCODE\n# summary(catm)\n\n#COMPARE MODEL FIT\ntest_lrt(catm.0, catm)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |      p\n-------------------------------------------------\ncatm.0 | multinom |  2 |         |       |       \ncatm   | multinom |  4 |       2 | 38.02 | < .001\n\n\nCODE\n##compare bayesian version\n#library(brms)\n# bm1 <- brm( state ~ pretty_condition, data = df, family = \"categorical\")\n# summary(bm1)\n# plot_model(bm1)\n# report(bm1)\n# coefficient estimates are very simliar to catm. super cool!\n\n##compare mclogit version\n#\"baseline-category logit model\n# https://www.elff.eu/software/mclogit/manual/mblogit/\n# blm1 <- mblogit(state ~ pretty_condition , data = df)\n# summary(blm1)\n#identical to catm. super cool!\n\n\nLikelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.\n\n\nInterpretation\n\n\nCODE\n#::::::::INTERPRETATION\npaste(\"MODEL SUMMARY\")\n\n\n[1] \"MODEL SUMMARY\"\n\n\nCODE\nsummary(catm)\n\n\nCall:\nmultinom(formula = state ~ condition, data = df, model = TRUE)\n\nCoefficients:\n         (Intercept) conditionimpasse\nunknown        -3.91             3.81\ntri-like       -1.51             1.78\n\nStd. Errors:\n         (Intercept) conditionimpasse\nunknown        1.010            1.061\ntri-like       0.333            0.447\n\nResidual Deviance: 207 \nAIC: 215 \n\n\nCODE\n# calculate z-statistics of coefficients\n(z_stats <- summary(catm)$coefficients/summary(catm)$standard.errors)\n\n\n         (Intercept) conditionimpasse\nunknown        -3.87             3.59\ntri-like       -4.55             3.98\n\n\nCODE\n# convert to p-values\np_values <- (1 - pnorm(abs(z_stats)))*2\n# display p-values in transposed data frame\np_values <- data.frame(p = (p_values))\n# display odds ratios in transposed data frame\n\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\nodds_ratios <- data.frame(OR = exp(summary(catm)$coefficients))\n\n# options(scipen = 3)\n(results <- cbind(odds_ratios, p_values))\n\n\n         OR..Intercept. OR.conditionimpasse p..Intercept. p.conditionimpasse\nunknown            0.02               44.99      1.07e-04          0.0003330\ntri-like           0.22                5.91      5.45e-06          0.0000692\n\n\nLearning Notes\n\nModel estimates encompass two equations:\neffect of predictor on log odds of being in [unknown] instead of reference category [orth-like]\neffect of predictor on log odds of being in [tri-like] instead of reference category [orth-like]\n[need to to double check interpretation, but I think that the OR intercepts converted to probabilities equate to the marginal probability of being in each state in the [reference] control condition. which makes sense. I think]\nIF I change reference category for condition… then the intercepts should no longer be significant. The b1 coefficients should still be significant, but with changed sign (much less likely) [Yup! this works!]\n\n\n\nInference\n\nBeing in the IMPASSE condition increases the odds of giving ‘unknown/uncertain’ response rather than an orthogonal (or satisficing) response by a factor of 50 (z = 3.59, p < 0.001 ).\nBeing in the IMPASSE condition increases the odds of giving an ‘triangular or line-driven’ response rather than an orthogonal (or satisficing) response by a factor of 6 (z = 3.98, p <0.001 )\n\n\n\nVisualize\n\n\nCODE\nplot_model(catm, vline.color = 'red')\n\n\n\n\n\nCODE\nplot_model(catm, type = \"eff\")\n\n\n$condition\n\n\n\n\n\n\n\nDiagnostics\n\n\nCODE\n#EXAMINE PREDICTIONS\n#create sample data frame\ntest <- data.frame(condition = c(\"control\", \"impasse\"))\npred <- predict(catm, newdata = test, \"probs\")\npaste(\"Predicted Probability of Being in Each State\")\n\n\n[1] \"Predicted Probability of Being in Each State\"\n\n\nCODE\n(cbind(test, pred))\n\n\n  condition orth-like unknown tri-like\n1   control     0.806  0.0161    0.177\n2   impasse     0.312  0.2812    0.406\n\n\nCODE\n#performance\nperformance(catm)\n\n\nCan't calculate log-loss.\nCan't calculate proper scoring rules for ordinal, multinomial or cumulative link models.\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n-----------------------------------------------------\n214.842 | 226.187 | 0.155 |     0.147 | 0.404 | 1.302\n\n\nCODE\nDescTools::PseudoR2(catm, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n\n\nRegistered S3 method overwritten by 'DescTools':\n  method         from \n  reorder.factor gdata\n\n\n  McFadden   CoxSnell Nagelkerke \n     0.155      0.260      0.304 \n\n\nCODE\n#General Goodness of Fit\n# library(generalhoslem)\n# logitgof(df$state, catm$fitted.values, g = 3)\n# hoslem.test(x = df$state, y = catm$fitted.values, g =  10)\n#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).\n#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#mixed-continuous",
    "href": "analysis/utils/modelling_ref.html#mixed-continuous",
    "title": "Modelling Reference",
    "section": "MIXED — CONTINUOUS",
    "text": "MIXED — CONTINUOUS"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#mixed-binomial",
    "href": "analysis/utils/modelling_ref.html#mixed-binomial",
    "title": "Modelling Reference",
    "section": "MIXED — BINOMIAL",
    "text": "MIXED — BINOMIAL\n\nMixed Logistic Regression\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on test phase questions by condition; accounting for random effects of subject.\n\nFit Model\n\n\nCODE\n#SETUP DATA \nn_items = 8 #number of items in test\n\n#item level\ndf = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  accuracy = as.factor(score_niceABS),\n  q = as.factor(q)\n) %>% filter(mode ==\"lab-synch\")\n\nlibrary(lmerTest) #for CIs in glmer \n\n## 1 | SETUP RANDOM EFFECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \n\n#:: RANDOM INTERCEPT SUBJECT\nmm.rS <- glmer(accuracy ~ (1|subject), data = df,family = \"binomial\")\n\n#:: RANDOM INTERCEPT SUBJECT ITEM\nmm.rS <- glmer(accuracy ~ (1|subject) + (1|q), data = df,family = \"binomial\")\n\n# :: TEST random effect\npaste(\"AIC with random effect is lower than glm empty model?\", m0$aic > AIC(logLik(mm.rS)))\n\n\n[1] \"AIC with random effect is lower than glm empty model? TRUE\"\n\n\nCODE\ntest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName  |    Model | df | df_diff |   Chi2 |      p\n-------------------------------------------------\nm0    |      glm |  1 |         |        |       \nmm.rS | glmerMod |  3 |       2 | 691.72 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  6.23684317028986e-151\"\n\n\nCODE\n## 2 | ADD FIXED EFFECT\n\n# SUBJECT INTERCEPT | FIXED CONDITION \nmm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject), \n                data = df,family = \"binomial\")\n\n# SUBJECT INTERCEPT | FIXED CONDITION \nmm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q), \n                data = df,family = \"binomial\")\n\n# :: TEST fixed factor \npaste(\"AIC with fixed effect is lower than random intercept only model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )\n\n\n[1] \"AIC with fixed effect is lower than random intercept only model? FALSE\"\n\n\nCODE\ntest_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff | Chi2 |      p\n------------------------------------------------\nmm.rS  | glmerMod |  3 |         |      |       \nmm.CrS | glmerMod |  3 |       0 | 9.40 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS,mm.CrS))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0\"\n\n\n\n\nVisualize\n\n\nCODE\n#: PRINT MODEL \nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsumm(mm.CrS)\n\n\n\n\n  \n    Observations \n    1008 \n  \n  \n    Dependent variable \n    accuracy \n  \n  \n    Type \n    Mixed effects generalized linear model \n  \n  \n    Family \n    binomial \n  \n  \n    Link \n    logit \n  \n\n \n\n  \n    AIC \n    582.02 \n  \n  \n    BIC \n    596.77 \n  \n  \n    Pseudo-R² (fixed effects) \n    0.07 \n  \n  \n    Pseudo-R² (total) \n    0.95 \n  \n\n \n \nFixed Effects\n  \n      \n    Est. \n    S.E. \n    z val. \n    p \n  \n \n\n  \n    (Intercept) \n    -7.46 \n    1.20 \n    -6.21 \n    0.00 \n  \n  \n    pretty_conditionimpasse \n    4.03 \n    1.67 \n    2.42 \n    0.02 \n  \n\n \n \nRandom Effects\n  \n    Group \n    Parameter \n    Std. Dev. \n  \n \n\n  \n    subject \n    (Intercept) \n    7.36 \n  \n\n \n \nGrouping Variables\n  \n    Group \n    # groups \n    ICC \n  \n \n\n  \n    subject \n    126 \n    0.94 \n  \n\n\n\n\nCODE\n#: INTERPRET COEFFICIENTS\n\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(mm.CrS)\n\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n----------------------------------------------------------------------------------------------------------------------\n582.024 | 582.048 | 596.771 |      0.946 |      0.066 | 0.943 | 0.213 | 1.000 |    0.145 |      -Inf |           0.021\n\n\nCODE\nprint(\"SANITY CHECK REPORTING\")\n\n\n[1] \"SANITY CHECK REPORTING\"\n\n\nCODE\nreport(mm.CrS)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model included subject as random effect (formula: ~1 | subject). The model's total explanatory power is substantial (conditional R2 = 0.95) and the part related to the fixed effects alone (marginal R2) is of 0.07. The model's intercept, corresponding to pretty_condition = control, is at -7.46 (95% CI [-9.81, -5.11], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 4.03, 95% CI [0.76, 7.29], p = 0.016; Std. beta = 4.03, 95% CI [0.76, 7.29])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\n# se <- sqrt(diag(stats::vcov(m1)))\n# # table of estimates with 95% CI\n# (tab <- cbind(Est = fixef(m1), LL = fixef(m1) - 1.96 * se, UL = fixef(m1) + 1.96 *\n#     se))\n# (e <- exp(tab))\n\n#: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(mm.CrS, type=\"std2\", vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(mm.CrS, type=\"pred\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\n# ggeffect(model = mm.CrS) %>% plot()\n\n#SANITY CHECK SJPLOT\n# library(effects)\n# plot(allEffects(mm.CrS))\n\n\n\n\nDiagnostics\n\n\nCODE\ncheck_model(mm.CrS)\n\n\n\n\n\nCODE\nbinned_residuals(mm.CrS)\n\n\nWarning: Probably bad model fit. Only about 64% of the residuals are inside the error bounds.\n\n\n\n\nInference\nWe fit a mixed-effect binomial logistic regression model with random intercepts for subjects to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(3): 4.98, p < 0.05). Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 5 over the control condition \\(e^{\\beta_1}\\) = 5.11, 95% CI [1.17,22,36], p < 0.05.\n\n\nCODE\n# PRETTY TABLE SJPLOT\ntab_model(mm.CrS)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.01\n<0.001\n\n\npretty condition[impasse]\n56.09\n2.14 – 1472.88\n0.016\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 subject\n54.11\n\n\nICC\n0.94\n\n\nN subject\n126\n\nObservations\n1008\n\n\nMarginal R2 / Conditional R2\n0.066 / 0.946\n\n\n\n\n\n\n\n\n\nBAYESIAN Mixed Lostistic Regression\nSanity Check Compare frequentist model with bayesian alternative\n\n\nCODE\ndf_i = df_items %>% filter(q %nin% c(6,9)) \n\n\n##:::::::: FLAT PRIORS\n#fit matching bayesian model [flat priors]\nFLAT_B.mm.CrSQ <-  brm(\n  bf(accuracy ~ pretty_condition + (1|subject) + (1|q)),\n  data = df_i,\n  family = bernoulli(link = \"logit\"),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234,\n  backend = \"cmdstanr\",\n  file = \"analysis/utils/models/FLAT_B.mm.CrSQ.rds\"\n)\n\n#describe model\nsummary(FLAT_B.mm.CrSQ)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: accuracy ~ pretty_condition + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 4290) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.93      0.23     0.58     1.47 1.00     1163     1843\n\n~subject (Number of levels: 330) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     4.68      0.35     4.05     5.39 1.00      852     1424\n\nPopulation-Level Effects: \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                  -4.63      0.58    -5.80    -3.52 1.01      495\npretty_conditionimpasse     3.11      0.61     1.98     4.34 1.00      331\n                        Tail_ESS\nIntercept                   1081\npretty_conditionimpasse      786\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n#compare to frequentist model\npaste(\"COMPARE TO FREQUENTIST\")\n\n\n[1] \"COMPARE TO FREQUENTIST\"\n\n\nCODE\ncompare_models(mm.CrSQ, FLAT_B.mm.CrSQ)\n\n\nParameter                  |               mm.CrSQ |       FLAT_B.mm.CrSQ\n-------------------------------------------------------------------------\n(Intercept)                | -9.35 (-11.68, -7.03) | -4.62 (-5.80, -3.52)\npretty condition (impasse) |  2.18 ( -0.84,  5.19) |                     \npretty_conditionimpasse    |                       |  3.09 ( 1.98,  4.34)\n-------------------------------------------------------------------------\nObservations               |                  1008 |                 4290\n\n\nCODE\n#_Here we see that the model with default priors yields estimates comparable to the frequentist mixed model, though slightly smaller for fixed effects condition, with tighter credible intervals._\n\n\n##:::::::: INFORMATIVE PRIORS\n\n#describe priors that were used\npaste(\"DEFAULT PRIORS\")\n\n\n[1] \"DEFAULT PRIORS\"\n\n\nCODE\nprior_summary(FLAT_B.mm.CrSQ)\n\n\n                prior     class                    coef   group resp dpar nlpar\n               (flat)         b                                                \n               (flat)         b pretty_conditionimpasse                        \n student_t(3, 0, 2.5) Intercept                                                \n student_t(3, 0, 2.5)        sd                                                \n student_t(3, 0, 2.5)        sd                               q                \n student_t(3, 0, 2.5)        sd               Intercept       q                \n student_t(3, 0, 2.5)        sd                         subject                \n student_t(3, 0, 2.5)        sd               Intercept subject                \n lb ub       source\n            default\n       (vectorized)\n            default\n  0         default\n  0    (vectorized)\n  0    (vectorized)\n  0    (vectorized)\n  0    (vectorized)\n\n\nCODE\n#set informative priors for fixed effects\n#parameters normally distributed around the mean of the expected log coefficient\n#we expect low probability of accuracy in control [intercept] and in impasse, but higher variance in impasse\npriors <- c(\n  prior(normal(-1, 2), class = b),\n  prior(normal(-1, 4), class = b, coef=\"pretty_conditionimpasse\")\n)\n\n#fit model with informative priors\nB.mm.CrSQ <-  brm(\n  accuracy ~ pretty_condition + (1|subject) + (1|q),\n  prior = priors,\n  data = df_i,\n  family = bernoulli(link = \"logit\"),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234,\n  backend = \"cmdstanr\",\n  file = \"analysis/utils/models/B.mm.CrSQ.rds\"\n)\n\n#describe model\nsummary(B.mm.CrSQ)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: accuracy ~ pretty_condition + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 4290) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.93      0.23     0.59     1.46 1.00     1270     2257\n\n~subject (Number of levels: 330) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     4.64      0.33     4.06     5.32 1.01     1010     1894\n\nPopulation-Level Effects: \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                  -4.48      0.57    -5.61    -3.38 1.01      485\npretty_conditionimpasse     2.93      0.60     1.76     4.11 1.01      379\n                        Tail_ESS\nIntercept                   1292\npretty_conditionimpasse      832\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n#compare to frequentist model\npaste(\"COMPARE TO UNFINFORMATIVE PRIOR\")\n\n\n[1] \"COMPARE TO UNFINFORMATIVE PRIOR\"\n\n\nCODE\ncompare_models(FLAT_B.mm.CrSQ, B.mm.CrSQ)\n\n\nParameter               |       FLAT_B.mm.CrSQ |            B.mm.CrSQ\n---------------------------------------------------------------------\n(Intercept)             | -4.62 (-5.80, -3.52) | -4.48 (-5.61, -3.38)\npretty_conditionimpasse |  3.09 ( 1.98,  4.34) |  2.92 ( 1.76,  4.11)\n---------------------------------------------------------------------\nObservations            |                 4290 |                 4290\n\n\nHere we see tha the model with the informative prior makes predictions similar to the uninformative prior model.\n\n\nCODE\n#DIAGNOSTICS\n\n#set model \nm <- B.mm.CrSQ\n\nsummary(m)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: accuracy ~ pretty_condition + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 4290) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.93      0.23     0.59     1.46 1.00     1270     2257\n\n~subject (Number of levels: 330) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     4.64      0.33     4.06     5.32 1.01     1010     1894\n\nPopulation-Level Effects: \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                  -4.48      0.57    -5.61    -3.38 1.01      485\npretty_conditionimpasse     2.93      0.60     1.76     4.11 1.01      379\n                        Tail_ESS\nIntercept                   1292\npretty_conditionimpasse      832\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n#POSTERIOR CHECKS\nlibrary(bayestestR)\ndiagnostic_posterior(m)\n\n\n                  Parameter Rhat ESS   MCSE\n1               b_Intercept 1.01 484 0.0257\n2 b_pretty_conditionimpasse 1.01 382 0.0308\n\n\nCODE\npp_check(m, ndraws = 500)\n\n\n\n\n\nCODE\nplot(m)\n\n\n\n\n\nCODE\n#INTERPRET\ndescribe_posterior(m, test = c(\"p_direction\", \"rope\", \"bayesfactor\"))\n\n\nSampling priors, please wait...\n\n\nWarning: Bayes factors might not be precise.\nFor precise Bayes factors, sampling at least 40,000 posterior samples is recommended.\n\n\nSummary of Posterior Distribution\n\nParameter               | Median |         95% CI |   pd |          ROPE | % in ROPE |  Rhat |    ESS |     BF\n--------------------------------------------------------------------------------------------------------------\n(Intercept)             |  -4.48 | [-5.61, -3.38] | 100% | [-0.18, 0.18] |        0% | 1.006 | 484.00 | > 1000\npretty_conditionimpasse |   2.92 | [ 1.76,  4.11] | 100% | [-0.18, 0.18] |        0% | 1.010 | 382.00 | > 1000\n\n\nCODE\n#set working model\nm <- B.mm.CrSQ\n\n\n#equivalence test \nresult <- equivalence_test(m, rule = \"cet\")\nplot(result)\n\n\nPicking joint bandwidth of 0.0912\n\n\nWarning: Removed 200 rows containing non-finite values (stat_density_ridges).\n\n\n\n\n\nCODE\n#rope \nresult <- rope(m)\nplot(result)\n\n\n\n\n\nCODE\n#direction of effect \nresult <- pd(m)\nplot(result)\n\n\n\n\n\nCODE\n# all parameters\nm %>% \n  posterior_samples() %>% \n  select(-c(lp__, contains(\"[\"))) %>%\n  pivot_longer(cols = everything(),\n               names_to = \"variable\",\n               values_to = \"value\") %>% \n  ggplot(data = .,\n         mapping = aes(x = value)) +\n  stat_halfeye(point_interval = mode_hdi,\n               fill = \"lightblue\") + \n  facet_wrap(~ variable,\n             ncol = 2,\n             scales = \"free\") +\n  theme(text = element_text(size = 12))\n\n\nWarning: Method 'posterior_samples' is deprecated. Please see ?as_draws for\nrecommended alternatives.\n\n\n\n\n\nCODE\n#plot parameters\nstanplot(m, \n         type = \"areas\",\n         prob = 0.95) \n\n\nWarning: Method 'stanplot' is deprecated. Please use 'mcmc_plot' instead.\n\n\n\n\n\nCODE\n#plot model\nplot_model(m)\n\n\n\n\n\nCODE\n#PLOT PARAMETERS\nresult <- model_parameters(m, exponentiate = TRUE, \n                           component = \"all\")\nplot(result) + \n  labs(\n  title = \"Accuracy ~ Condition + (1 | subject) + (1 | question)\",\n  subtitle = \"Logistic Mixed Effects Model\"\n)  + theme_clean() +  theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n#PLOT MODEL PREDICTION\nplot_model(m, type = \"pred\")[[1]] + \n  ylim(0,1) + labs(\n    title = \"Model Prediction | Probability of Accurate Response\",\n    subtitle = \"Impasse increases Probability of Correct Response\"\n  )\n\n\nNote: uncertainty of error terms are not taken into account. You may want to use `rstantools::posterior_predict()`.\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\n\n\nCODE\n#report(m)"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#mixed-ordinal",
    "href": "analysis/utils/modelling_ref.html#mixed-ordinal",
    "title": "Modelling Reference",
    "section": "MIXED — ORDINAL",
    "text": "MIXED — ORDINAL"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#mixed-categorical",
    "href": "analysis/utils/modelling_ref.html#mixed-categorical",
    "title": "Modelling Reference",
    "section": "MIXED — CATEGORICAL",
    "text": "MIXED — CATEGORICAL\n\nMixed Multinomial Regression\nDoes condition affect the response state of of items across the task?\nFit a MIXED logistic regression predicting interpretation state (k=3) by condition(k = 2).\n\nCan use mclogit mblogit() with random effect or bayesian brms package b/c nlme, lme4 don’t support random effects on multinomial (ie no categorical family on glmer())\nAlternative would be to manually run [k-1] X binomial mixed models [should compare outcomes]\n[k-1] equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) [essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing [reference category] vs [this category])\nFor each equation:\n\n\\(\\beta_{0}\\) = Log Odds of [this category type vs. reference category type) response in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of [this category type vs. reference category type] response in CONTROL condition\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for [this category] type response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of [this. vs reference category type] response in IMPASSE (vs) CONTROL\nTwo-tailed NHST Null hypothesis: \\(\\beta_{impasse} = 0\\) the odds for [this category of response vs. reference] are not different for IMPASSE condition\nAlternative hypothesis: \\(\\beta_{impasse} \\ne 0\\) the odds of [this category of response vs. reference] increases or decreases for IMPASSE condition\n\n\n\nFit Model [MANUAL INDIVIDUAL BINOMIALS]\n\n\nCODE\n#VERIFY RESULTS BELOW VIA MULTIPLE INDIVIDUAL MODELS\n\npaste(\"ORTH vs. UNKNOWN\")\n\n\n[1] \"ORTH vs. UNKNOWN\"\n\n\nCODE\nd1 <- df_i %>% filter(state %nin% c(\"tri-like\")) %>% droplevels()\ntable(d1$pretty_condition, d1$state)\n\n\n         \n          orth-like unknown\n  control      1358     237\n  impasse       719     510\n\n\nCODE\nplot(d1$pretty_condition, d1$state)\n\n\n\n\n\nCODE\nm.unknown <- glmer(state ~ pretty_condition + ( 1 | subject) + (1 | q), family = \"binomial\", data = d1)\nsummary(m.unknown)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: state ~ pretty_condition + (1 | subject) + (1 | q)\n   Data: d1\n\n     AIC      BIC   logLik deviance df.resid \n    2478     2502    -1235     2470     2820 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.840 -0.464 -0.212  0.361  4.430 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 1.67     1.29    \n q       (Intercept) 2.16     1.47    \nNumber of obs: 2824, groups:  subject, 292; q, 13\n\nFixed effects:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -2.554      0.435   -5.87  4.2e-09 ***\npretty_conditionimpasse    2.218      0.204   10.89  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nprtty_cndtn -0.257\n\n\nBeing in the IMPASSE condition increases the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 19 ( logodds = 2.92, z = 6.89, p < 0.001) . Participants in the impasse condition were 19x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control. (Conditional R2 = 0.67, marginal R2 = 0.21; SD(subject) = 1.55, SD(question) = 2.22)\n\n\nCODE\npaste(\"ORTH vs. TRIANGULAR\")\n\n\n[1] \"ORTH vs. TRIANGULAR\"\n\n\nCODE\nd2 <- df_i %>% filter(state %nin% c(\"unknown\")) %>% droplevels()\ntable(d2$pretty_condition, d2$state)\n\n\n         \n          orth-like tri-like\n  control      1358      459\n  impasse       719     1007\n\n\nCODE\nplot(d2$pretty_condition, d2$state)\n\n\n\n\n\nCODE\nm.tri <- glmer(state ~ pretty_condition + ( 1 | subject) + (1 | q), family = \"binomial\", data = d2)\nsummary(m.tri)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: state ~ pretty_condition + (1 | subject) + (1 | q)\n   Data: d2\n\n     AIC      BIC   logLik deviance df.resid \n    2113     2138    -1052     2105     3539 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-10.465  -0.189  -0.051   0.152   6.603 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 19.99    4.47    \n q       (Intercept)  1.28    1.13    \nNumber of obs: 3543, groups:  subject, 330; q, 13\n\nFixed effects:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -3.571      0.573   -6.24  4.5e-10 ***\npretty_conditionimpasse    4.350      0.631    6.90  5.4e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nprtty_cndtn -0.661\n\n\nBeing in the IMPASSE condition increases the odds of giving an triangle-like response by a factor of 305 ( logodds = 5.72, z = 4.14, p < 0.001) . Participants in the impasse condition were 305 times as likely to give an triangle-like response rather than an orthogonal response compared to participants in control. (Conditional R2 = 0.91, marginal R2 = 0.21; SD(subject) = 5.05, SD(q) = 1.09)\n\n\nFit Model [mblogit]\n\n\nCODE\n#https://www.elff.eu/software/mclogit/manual/mblogit/\n#\"baseline category logit\" model matches multinom()\n\n#check reference level \nprint(\"Categories (first is reference)\")\n\n\n[1] \"Categories (first is reference)\"\n\n\nCODE\nlevels(df_i$state)\n\n\n[1] \"orth-like\" \"unknown\"   \"tri-like\" \n\n\nCODE\n#FIT EMPTY MODEL\n# print(\"EMPTY MODEL\")\nm.mbl0 <- mblogit(state ~ pretty_condition ,  #no random intercepts; fixed only model \n                  data = df_i)\n\n\n\nIteration 1 - deviance = 8288 - criterion = 0.471\nIteration 2 - deviance = 8269 - criterion = 0.00228\nIteration 3 - deviance = 8269 - criterion = 6.61e-06\nIteration 4 - deviance = 8269 - criterion = 7.44e-11\nconverged\n\n\nCODE\n#summary(m.mbl0)\n\n#FIT PREDICTOR MODEL\n# print(\"PREDICTOR MODEL\")\nm.mbl1 <- mblogit(state ~ pretty_condition , \n                  random = list( ~ 1|subject, ~1|q), \n                  data = df_i)\n\n\n\nIteration 1 - deviance = 5845 - criterion = 0.772\nIteration 2 - deviance = 5237 - criterion = 0.0926\nIteration 3 - deviance = 5038 - criterion = 0.0186\nIteration 4 - deviance = 4957 - criterion = 0.00278\nIteration 5 - deviance = 4907 - criterion = 0.000674\nIteration 6 - deviance = 4884 - criterion = 0.00015\nIteration 7 - deviance = 4874 - criterion = 0.0000316\nIteration 8 - deviance = 4871 - criterion = 6.67e-06\nIteration 9 - deviance = 4869 - criterion = 1.44e-06\nIteration 10 - deviance = 4868 - criterion = 3.16e-07\nIteration 11 - deviance = 4868 - criterion = 6.99e-08\nIteration 12 - deviance = 4868 - criterion = 1.55e-08\nIteration 13 - deviance = 4868 - criterion = 3.38e-09\nconverged\n\n\nCODE\n# summary(m.mbl1)\n\n#COMPARE MODEL FIT\npaste(\"AIC wth predictor is lower than empty model?\", AIC(m.mbl0) > AIC(m.mbl1))\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(m.mbl0, m.mbl1)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |    Chi2 |      p\n---------------------------------------------------\nm.mbl0 |  mblogit |  4 |         |         |       \nm.mbl1 | mmblogit | 10 |       6 | 4981.16 | < .001\n\n\nCODE\n#DESCRIBE MODEL\nsummary(m.mbl1)\n\n\n\nCall:\nmblogit(formula = state ~ pretty_condition, data = df_i, random = list(~1 | \n    subject, ~1 | q))\n\nEquation for unknown vs orth-like:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -2.068      0.484   -4.27 0.000019 ***\npretty_conditionimpasse    2.080      0.212    9.83  < 2e-16 ***\n\nEquation for tri-like vs orth-like:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -2.387      0.538   -4.44  9.0e-06 ***\npretty_conditionimpasse    2.989      0.423    7.06  1.7e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Co-)Variances:\nGrouping level: subject \n           Estimate      Std.Err.   \nunknown~1   2.36         0.187      \ntri-like~1  3.90 12.15   1.553 4.802\n\nGrouping level: q \n           Estimate    Std.Err. \nunknown~1  2.72        25.8     \ntri-like~1 1.90 2.47   24.1 22.5\n\nNull Deviance:     9430 \nResidual Deviance: 4870 \nNumber of Fisher Scoring iterations:  13\nNumber of observations\n  Groups by subject: 330\n  Groups by q: 13\n  Individual observations:  4290\n\n\nCODE\n#INTERPRET COEFFICIENTS\ncint <- confint(m.mbl1, level = 0.95)\nprint(\"ODDS RATIO\")\n\n\n[1] \"ODDS RATIO\"\n\n\nCODE\n(e <- cbind( exp(coef(m.mbl1)), exp(cint))) #exponentiated, adjusted\n\n\n                                         2.5 % 97.5 %\nunknown~(Intercept)               0.1264 0.049  0.326\ntri-like~(Intercept)              0.0919 0.032  0.264\nunknown~pretty_conditionimpasse   8.0025 5.285 12.118\ntri-like~pretty_conditionimpasse 19.8582 8.660 45.538\n\n\nCODE\n#PERFORMANCE\nperformance(m.mbl1)\n\n\n# Indices of model performance\n\nAIC       |       BIC |  RMSE | Sigma\n-------------------------------------\n36798.535 | 36862.176 | 0.293 | 1.066\n\n\nCODE\n#TABLE\ntab_model(m.mbl1, transform = \"exp\", title = \"Model Predicted Odds Ratio\")\n\n\n\nModel Predicted Odds Ratio\n\n \nstate\n\n\nPredictors\nEstimates\nCI\np\n\n\nunknown~(Intercept)\n0.13\n0.05 – 0.33\n<0.001\n\n\ntri-like~(Intercept)\n0.09\n0.03 – 0.26\n<0.001\n\n\nunknown~pretty_conditionimpasse\n8.00\n5.28 – 12.12\n<0.001\n\n\ntri-like~pretty_conditionimpasse\n19.86\n8.66 – 45.54\n<0.001\n\n\n\nN subject\n330\n\n\nN q\n13\n\nObservations\n4290\n\n\n\n\n\n\n\n\nInference\n\nBeing in the IMPASSE condition increases the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 12 (z = 6.48, p < 0.001) . Participants in the impasse condition were 12x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 29 (z = 4.63, p < 0.001 ). Participants in the impasse condition were more than 29x as likely to give an a triangular response rather than an orthogonal response compared to participants in control.\nAs with the (binary) logistic regression on accuracy ~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 in the control condition. (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)\n\n\n\nVisualize\n\n\nCODE\n#set model \nm <- m.mbl1\n\n#:::::::: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m, vline.color = \"red\", \n           transform = \"exp\", #for some reason have to manually add for mixed?\n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.05, #can manually adjust to account for directional test\n           ci.lvl = 0.95) +  #can manually adjusted for directional test   \n  labs(title = \"ODDS RATIO | State ~ condition\",\n       subtitle = \"(p for  two-tailed test)\")\n\n\n\n\n\nCODE\nlibrary(parameters)\n\n#PLOT PARAMETER ESTIMATES\nresult <- simulate_parameters(m)\nplot(result, stack = FALSE)\n\n\n\n\n\nCODE\nresult <- equivalence_test(m, rule = \"cet\")\n\n\nWarning: Could not estimate a good default ROPE range. Using 'c(-0.1, 0.1)'.\n\n\nCODE\nplot(result)\n\n\n\n\n\nCODE\n#raw coefficients\nplot_model(m)\n\n\n\n\n\nCODE\n#PLOT PARAMETERS\nresult <- model_parameters(m, exponentiate = TRUE, \n                           component = \"all\")\nplot(result) + \n  labs(\n  title = \"Accuracy ~ Condition + (1 | subject) + (1 | question)\",\n  subtitle = \"Logistic Mixed Effects Model\"\n)  + theme_clean() +  theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\ns\n\n\nfunction (...) \n{\n    mgcv::s(...)\n}\n<bytecode: 0x7f849707a900>\n<environment: namespace:brms>\n\n\nCODE\n#TODO CAN'T SEEM TO COMPUTE MARGINAL EFFECTS PLOT FOR THE MIXED MODEL\n# pr <- ggpredict(m, \"pretty_condition\", type = \"fixed\")\n# plot(pr)\n#PLOT MODEL PREDICTION\n# plot_model(m, type = \"pred\")\n# plot_model(m, type = \"eff\")\n\n# TERNARY PLOT\n# library(plot3logit)\n# field3logit(ggpredict(m))\n#https://cran.r-project.org/web/packages/plot3logit/vignettes/plot3logit-overview.html\n\n\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \nstate\n\n\nPredictors\nEstimates\nCI\np\n\n\nunknown~(Intercept)\n-2.07\n-3.02 – -1.12\n<0.001\n\n\ntri-like~(Intercept)\n-2.39\n-3.44 – -1.33\n<0.001\n\n\nunknown~pretty_conditionimpasse\n2.08\n1.66 – 2.49\n<0.001\n\n\ntri-like~pretty_conditionimpasse\n2.99\n2.16 – 3.82\n<0.001\n\n\n\nN subject\n330\n\n\nN q\n13\n\nObservations\n4290\n\n\n\n\n\n\n\n\nDiagnostics\nCOMPARE TO MANUAL MODELS ::: {.cell}\n\nCODE\ncompare_models(m, m.unknown, m.tri)\n\n\nParameter                        |                    m |            m.unknown |                m.tri\n-----------------------------------------------------------------------------------------------------\n(Intercept)                      |                      | -2.55 (-3.41, -1.70) | -3.57 (-4.69, -2.45)\nunknown~(Intercept)              | -2.07 (-3.02, -1.12) |                      |                     \ntri-like~(Intercept)             | -2.39 (-3.44, -1.33) |                      |                     \nunknown~pretty conditionimpasse  |  2.08 ( 1.66,  2.49) |                      |                     \ntri-like~pretty conditionimpasse |  2.99 ( 2.16,  3.82) |                      |                     \npretty condition (impasse)       |                      |  2.22 ( 1.82,  2.62) |  4.35 ( 3.11,  5.59)\n-----------------------------------------------------------------------------------------------------\nObservations                     |                 4290 |                 2824 |                 3543\n\n:::\n\n\nFit Model [brms]\n\n\nCODE\n#BAYESIAN MIXED VERSION\nmixcat.1 <- brm( state ~ pretty_condition + (1|subject) + (1|q), \n                 data = df_i, \n                 family = \"categorical\",\n                 chains = 4, iter = 2000, warmup = 1000,\n                 cores = 4, seed = 1234,\n                 backend = \"cmdstanr\",\n                 file =\"analysis/utils/models/mixcat1.rds\")\n\n\n\n\nCODE\nm <- mixcat.1\n\n#DESCRIBE MODEL\nsummary(m)\n\n\n Family: categorical \n  Links: muunknown = logit; mutrilike = logit \nFormula: state ~ pretty_condition + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 4290) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muunknown_Intercept)     1.54      0.35     1.01     2.36 1.00     1571\nsd(mutrilike_Intercept)     1.22      0.29     0.80     1.93 1.00     1588\n                        Tail_ESS\nsd(muunknown_Intercept)     2624\nsd(mutrilike_Intercept)     2353\n\n~subject (Number of levels: 330) \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muunknown_Intercept)     1.20      0.10     1.02     1.40 1.00     1570\nsd(mutrilike_Intercept)     4.09      0.27     3.59     4.67 1.00     1271\n                        Tail_ESS\nsd(muunknown_Intercept)     2708\nsd(mutrilike_Intercept)     2159\n\nPopulation-Level Effects: \n                                  Estimate Est.Error l-95% CI u-95% CI Rhat\nmuunknown_Intercept                  -2.43      0.45    -3.29    -1.52 1.00\nmutrilike_Intercept                  -3.16      0.52    -4.19    -2.12 1.00\nmuunknown_pretty_conditionimpasse     2.04      0.19     1.67     2.41 1.00\nmutrilike_pretty_conditionimpasse     3.52      0.51     2.56     4.54 1.00\n                                  Bulk_ESS Tail_ESS\nmuunknown_Intercept                   1166     1903\nmutrilike_Intercept                   1006     1533\nmuunknown_pretty_conditionimpasse     2775     3303\nmutrilike_pretty_conditionimpasse      772     1479\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n# report(mixcat.1)\n\n#TABLE\n# tab_model(m,\n          # show.r2 = FALSE) #, transform = \"exp\", title = \"Model Predicted Odds Ratio\")\n\n\n#TODO OUTPUT TABLE \n#https://arelbundock.com/posts/modelsummary_multinomial_logit/\n# modelsummary(m)\n\n\n\n\nInference\n\nBeing in the IMPASSE condition increases the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 43.86.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 6.64.\n\n\n\nVisualize\n\n\nCODE\n# \n# #:::::::: PLOT\n# \n# #SJPLOT | MODEL | ODDS RATIO\n# #library(sjPlot)\n# plot_model(m, vline.color = \"red\", \n#            show.intercept = TRUE, \n#            show.values = TRUE,\n#            p.threshold = 0.05, #can manually adjust to account for directional test\n#            ci.lvl = 0.95 ) +  #can manually adjusted for directional test   \n#   labs(title = \"ODDS RATIO | State ~ condition\",\n#        subtitle = \"(p for one two test)\")\n# \n# \n# result <- estimate_density(m)\n# plot(result, stack = FALSE, priors = TRUE)\n# \n# result <- describe_posterior(m)\n# plot(result, stack = FALSE, priors = TRUE)\n# \n# result <- p_direction(m)\n# plot(result, stack = FALSE)\n# \n# #ROPE\n# result <- rope(m, ci = 0.89)\n# plot(result) + theme_clean()\n# \n# result <- equivalence_test(m)\n# plot(result)\n# \n# #ERROR incompatible arguments to calculate multivariate normal distribution\n# # result <- simulate_parameters(m)\n# # plot(result)\n# \n# #check posterior\n# pp_check(m, ndraws=1000)\n# \n# #PERFORMANCE\n# # performance(m)\n# \n# #Plot parameters\n# plot(model_parameters(m, exponentiate = TRUE))\n# \n# #plot marginal predictions of model\n# plot_model(m, type = \"pred\")[[1]] + \n#   labs(\n#     title = \"Model Predicted Marginal Probabilities\",\n#     x = \"Condition\", \n#     y = \"probability of being in each state\"\n#   ) + theme_clean() \n\n\n\n\nCOMPARE\n\n\nCODE\n# compare_models(m.unknown, m.tri, m.mbl1, mixcat.1)\n\n\nThe predictions of the manual, frequentist mixed multinomial and bayesian mixed multinomial models are comparable.\nDECISION: Report bayesian model, because for some reason the visualization of the marginal predictions is actually working?\n\n\nTODO\n\npriors? used default flat priors… ok?\nposterior predictive checks\ndiagnostics on random effects\nreconcilliation of mblogit() vs brms versions of the model; seems like they should yield similar estimates\n\n\n\nCODE\n#BAYESIAN MIXED VERSION\n#df <- df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% filter(term==\"spring18\")\n# mm1 <- brm( state ~ condition + (1|subject), data = df, family = \"categorical\", \n# file = \"analysis/models/sgc3a_brms_mixedcat_teststate.rds\" # cache model (can be removed)))\n# summary(mm1)\n# performance(mm1)\n# plot(mm1)\n# #report(mm1)\n# #check_posterior_predictions(mm1, draws=100)\n# # library(bayesplot)\n# library(bayestestR)\n# plot(rope(mm1))"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#wip-unknown",
    "href": "analysis/utils/modelling_ref.html#wip-unknown",
    "title": "Modelling Reference",
    "section": "WIP UNKNOWN",
    "text": "WIP UNKNOWN\n\nCummulative Ordinal (Bayesian) — Equal Variance\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\n\n\nCODE\nlibrary(brms)\n\n\n#DEFINE DATA \ndf <- df_items %>% filter(q==1) %>% filter(mode ==\"lab-synch\")\n  \n#TODO: why is this probit instead of logit?\ncumord <- brm( formula = state ~ condition,\n               data = df,\n               family = cumulative(\"probit\"),\n               file = \"analysis/utils/models/sgc3a_brms_cumord_q1state.rds\" # cache model (can be removed)\n)\n\nsummary(cumord)\n\n\n Family: cumulative \n  Links: mu = probit; disc = identity \nFormula: state ~ condition \n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]         0.76      0.19     0.40     1.14 1.00     2783     2309\nIntercept[2]         1.22      0.20     0.84     1.62 1.00     3000     2548\nconditionimpasse     1.11      0.24     0.66     1.58 1.00     2746     2646\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nconditional_effects(cumord, \"condition\", categorical = TRUE)\n\n\n\n\n\nCODE\n#SJPLOT\nplot_model(cumord)\n\n\n\n\n\nCODE\nplot(cumord)\n\n\n\n\n\nCODE\nplot(rope(cumord))\n\n\nPossible multicollinearity between b_conditionimpasse and b_Intercept[2] (r = 0.77). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\n\n\n\nCODE\n#REPORT\nreport(cumord)\n\n\nPossible multicollinearity between b_conditionimpasse and b_Intercept[2] (r = 0.77). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nPossible multicollinearity between b_conditionimpasse and b_Intercept[2] (r = 0.77). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL '3a68d5685b54180e73711b90f350310a' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.49 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.104655 seconds (Warm-up)\nChain 1:                0.116506 seconds (Sampling)\nChain 1:                0.221161 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '3a68d5685b54180e73711b90f350310a' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.110988 seconds (Warm-up)\nChain 2:                0.116263 seconds (Sampling)\nChain 2:                0.227251 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '3a68d5685b54180e73711b90f350310a' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.105778 seconds (Warm-up)\nChain 3:                0.099159 seconds (Sampling)\nChain 3:                0.204937 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '3a68d5685b54180e73711b90f350310a' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.105963 seconds (Warm-up)\nChain 4:                0.110911 seconds (Sampling)\nChain 4:                0.216874 seconds (Total)\nChain 4: \n\n\nPossible multicollinearity between b_conditionimpasse and b_Intercept[2] (r = 0.76). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is likely invalid for ordinal families.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a Bayesian probit model (estimated using MCMC sampling with 4 chains of 2000 iterations and a warmup of 1000) to predict state with condition (formula: state ~ condition). Priors over parameters were set as uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50) and uniform (location = , scale = ) distributions. The model's explanatory power is moderate (R2 = 0.18, 95% CI [0.07, 0.28]).  Within this model:\n\n  - The effect of b Intercept[1] (Median = 0.75, 95% CI [0.40, 1.14]) has a 100.00% probability of being positive (> 0), 99.98% of being significant (> 0.05), and 99.35% of being large (> 0.30). The estimation successfully converged (Rhat = 0.999) and the indices are reliable (ESS = 2725)\n  - The effect of b Intercept[2] (Median = 1.22, 95% CI [0.84, 1.62]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30). The estimation successfully converged (Rhat = 1.000) and the indices are reliable (ESS = 2723)\n  - The effect of b conditionimpasse (Median = 1.11, 95% CI [0.66, 1.58]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 99.92% of being large (> 0.30). The estimation successfully converged (Rhat = 1.000) and the indices are reliable (ESS = 3047)\n\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT) framework, we report the median of the posterior distribution and its 95% CI (Highest Density Interval), along the probability of direction (pd), the probability of significance and the probability of being large. The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.05| and |0.30|. Convergence and stability of the Bayesian sampling has been assessed using R-hat, which should be below 1.01 (Vehtari et al., 2019), and Effective Sample Size (ESS), which should be greater than 1000 (Burkner, 2017).\n\n\nCODE\n# ord_cum %>%\n#   spread_draws(b_Intercept, r_condition[condition,]) %>%\n#   mutate(condition_mean = b_Intercept + r_condition) %>%\n#   ggplot(aes(y = condition, x = condition_mean)) +\n#   stat_halfeye()\n\n# performance(cumord)\n# plot(cumord)\n\n\n\n\nCummulative Ordinal (Bayesian) — Unequal Variance\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\n\n\nCODE\nlibrary(brms)\n\n\n#DEFINE DATA \ndf <- df_items %>% filter(q==1) %>% filter(mode ==\"lab-synch\")\n  \n#TODO: why is this probit instead of logit?\nu.cumord <- brm( \n  formula = bf(state ~ condition) +\n               lf(disc ~ 0 + condition, cmc = FALSE),\n               data = df,\n               family = cumulative(\"probit\"),\n               file = \"analysis/utils/models/sgc3a_brms_ucumord_q1state.rds\" # cache model (can be removed)\n)\n\nsummary(u.cumord)\n\n\nWarning: There were 76 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See http://mc-stan.org/misc/\nwarnings.html#divergent-transitions-after-warmup\n\n\n Family: cumulative \n  Links: mu = probit; disc = log \nFormula: state ~ condition \n         disc ~ 0 + condition\n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept[1]              0.83      0.18     0.48     1.19 1.00     1294\nIntercept[2]              1.00      0.19     0.66     1.38 1.01     1236\nconditionimpasse          0.95      0.19     0.60     1.32 1.01     1325\ndisc_conditionimpasse     1.48      0.56     0.41     2.64 1.01      302\n                      Tail_ESS\nIntercept[1]              1484\nIntercept[2]              1540\nconditionimpasse          1717\ndisc_conditionimpasse      142\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nconditional_effects(u.cumord, \"condition\", categorical = TRUE)\n\n\n\n\n\nCODE\n#SJPLOT\nplot_model(u.cumord)\n\n\n\n\n\nCODE\nplot(u.cumord)\n\n\n\n\n\nCODE\nplot(rope(u.cumord))\n\n\n\n\n\nCODE\n#REPORT\nreport(u.cumord)\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL '8051b427c5dbb5a865c588f34a9e215a' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.52 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.453928 seconds (Warm-up)\nChain 1:                0.262774 seconds (Sampling)\nChain 1:                0.716702 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '8051b427c5dbb5a865c588f34a9e215a' NOW (CHAIN 2).\nChain 2: Rejecting initial value:\nChain 2:   Log probability evaluates to log(0), i.e. negative infinity.\nChain 2:   Stan can't start sampling from this initial value.\nChain 2: \nChain 2: Gradient evaluation took 2.8e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.443154 seconds (Warm-up)\nChain 2:                0.386187 seconds (Sampling)\nChain 2:                0.829341 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '8051b427c5dbb5a865c588f34a9e215a' NOW (CHAIN 3).\nChain 3: Rejecting initial value:\nChain 3:   Log probability evaluates to log(0), i.e. negative infinity.\nChain 3:   Stan can't start sampling from this initial value.\nChain 3: Rejecting initial value:\nChain 3:   Log probability evaluates to log(0), i.e. negative infinity.\nChain 3:   Stan can't start sampling from this initial value.\nChain 3: \nChain 3: Gradient evaluation took 2.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.385624 seconds (Warm-up)\nChain 3:                0.474365 seconds (Sampling)\nChain 3:                0.859989 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '8051b427c5dbb5a865c588f34a9e215a' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.357993 seconds (Warm-up)\nChain 4:                0.305731 seconds (Sampling)\nChain 4:                0.663724 seconds (Total)\nChain 4: \n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is likely invalid for ordinal families.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a Bayesian probit model (estimated using MCMC sampling with 4 chains of 2000 iterations and a warmup of 1000) to predict state with condition (formula: state ~ condition). Priors over parameters were set as uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), uniform (location = , scale = ) and uniform (location = , scale = ) distributions. The model's explanatory power is moderate (R2 = 0.17, 95% CI [0.06, 0.26]).  Within this model:\n\n  - The effect of b Intercept[1] (Median = 0.83, 95% CI [0.48, 1.19]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 99.92% of being large (> 0.30). The estimation successfully converged (Rhat = 1.006) and the indices are reliable (ESS = 1320)\n  - The effect of b Intercept[2] (Median = 1.00, 95% CI [0.66, 1.38]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30). The estimation successfully converged (Rhat = 1.006) but the indices are unreliable (ESS = 312)\n  - The effect of b conditionimpasse (Median = 0.94, 95% CI [0.60, 1.32]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30). The estimation successfully converged (Rhat = 1.003) and the indices are reliable (ESS = 1284)\n  - The effect of b disc conditionimpasse (Median = 1.45, 95% CI [0.41, 2.64]) has a 99.78% probability of being positive (> 0), 99.72% of being significant (> 0.05), and 98.45% of being large (> 0.30). The estimation successfully converged (Rhat = 1.006) and the indices are reliable (ESS = 1238)\n\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT) framework, we report the median of the posterior distribution and its 95% CI (Highest Density Interval), along the probability of direction (pd), the probability of significance and the probability of being large. The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.05| and |0.30|. Convergence and stability of the Bayesian sampling has been assessed using R-hat, which should be below 1.01 (Vehtari et al., 2019), and Effective Sample Size (ESS), which should be greater than 1000 (Burkner, 2017).\n\n\nCODE\n# ord_cum %>%\n#   spread_draws(b_Intercept, r_condition[condition,]) %>%\n#   mutate(condition_mean = b_Intercept + r_condition) %>%\n#   ggplot(aes(y = condition, x = condition_mean)) +\n#   stat_halfeye()\n\n# performance(cumord)\n# plot(cumord)\n\n\nIn brms, the parameter related to latent variances is called disc (short for “discrimination”), following conventions in item response theory. Note that disc is not the variance itself, but the inverse of the standard deviation, s. That is, s = 1/disc. Further, because disc must be strictly positive, it is by default modeled on the log scale.\n\n\nAdjacent-Category Ordinal (Bayesian)\n\n\nCODE\ndf <- df_items %>% filter(q==1) %>% filter(mode ==\"lab-synch\")\n\n\n# # To specify an adjacent-category model, we use family = acat() instead of family = cumulative() as an argument to the brm() function. Then, to model condition with possible category-specific effects, we wrap this variable in cs() in the model’s formula:\n\nadjcat <- brm( formula = state ~ cs(condition),\n               data = df,\n               family = acat(\"probit\"),\n               file = \"analysis/utils/models/sgc3a_brms_adjcat_q1state.rds\" # cache model (can be removed)\n)\n \nsummary(adjcat)\n\n\n Family: acat \n  Links: mu = probit; disc = identity \nFormula: state ~ cs(condition) \n   Data: df (Number of observations: 126) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]            2.01      0.38     1.36     2.88 1.00      639      551\nIntercept[2]           -1.29      0.49    -2.36    -0.41 1.00      614      579\nconditionimpasse[1]     1.95      0.43     1.16     2.90 1.00      674      589\nconditionimpasse[2]    -1.06      0.53    -2.20    -0.11 1.00      586      604\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nconditional_effects(cumord, \"condition\", categorical = TRUE)\n\n\n\n\n\nCODE\nconditional_effects(adjcat, \"condition\", categorical = TRUE)\n\n\n\n\n\nCODE\n#WHICH IS BETTER? cumulative or adjacent?\nplot(compare_performance(cumord, adjcat))\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\nWarning: A `range` must be provided for data with only one unique value.\n\n\n\n\n\nCODE\ncompare_performance(cumord, adjcat)\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\n# Comparison of Model Performance Indices\n\nName   |   Model |     ELPD | ELPD_SE |   LOOIC | LOOIC weights | LOOIC_SE |    WAIC | WAIC weights |    R2 | Sigma\n-------------------------------------------------------------------------------------------------------------------\ncumord | brmsfit | -113.736 |   7.542 | 227.471 |         0.071 |   15.085 | 227.450 |        0.002 | 0.178 | 1.000\nadjcat | brmsfit | -107.521 |   7.199 | 215.042 |         0.929 |   14.397 | 214.897 |        0.998 | 0.171 | 1.000\n\n\nCODE\n# #TIDYBAYES VISUALIZATION\n# library(tidybayes)\n# adjcat %>%\n#   spread_draws(b_Intercept, r_condition[condition,]) %>%\n#   mutate(condition_mean = b_Intercept + r_condition) %>%\n#   ggplot(aes(y = condition, x = condition_mean)) +\n#   stat_halfeye()\n\nplot(cumord)\n\n\n\n\n\nCODE\nplot(adjcat)\n\n\n\n\n\nCODE\nreport(adjcat)\n\n\nPossible multicollinearity between bcs_conditionimpasse[1] and b_Intercept[2] (r = 0.83), bcs_conditionimpasse[2] and bcs_conditionimpasse[1] (r = 0.86). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is\nlikely invalid for ordinal families.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nPossible multicollinearity between bcs_conditionimpasse[1] and b_Intercept[2] (r = 0.83), bcs_conditionimpasse[2] and bcs_conditionimpasse[1] (r = 0.86). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'ebf0c4c0e51ade2b00ca4594e215d3ad' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000125 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.25 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.35813 seconds (Warm-up)\nChain 1:                1.74949 seconds (Sampling)\nChain 1:                3.10762 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'ebf0c4c0e51ade2b00ca4594e215d3ad' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000101 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.48609 seconds (Warm-up)\nChain 2:                1.56339 seconds (Sampling)\nChain 2:                3.04949 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'ebf0c4c0e51ade2b00ca4594e215d3ad' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000106 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.46677 seconds (Warm-up)\nChain 3:                1.43753 seconds (Sampling)\nChain 3:                2.9043 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'ebf0c4c0e51ade2b00ca4594e215d3ad' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000102 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.41558 seconds (Warm-up)\nChain 4:                1.46047 seconds (Sampling)\nChain 4:                2.87605 seconds (Total)\nChain 4: \n\n\nPossible multicollinearity between bcs_conditionimpasse[1] and b_Intercept[2] (r = 0.82), bcs_conditionimpasse[2] and bcs_conditionimpasse[1] (r = 0.86). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nWarning: Predictions are treated as continuous variables in 'bayes_R2' which is likely invalid for ordinal families.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a Bayesian probit model (estimated using MCMC sampling with 4 chains of 2000 iterations and a warmup of 1000) to predict state with condition (formula: state ~ cs(condition)). Priors over parameters were set as uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), uniform (location = , scale = ), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), student_t (location = 0.00, scale = 2.50), NA (NA) and NA (NA) distributions. The model's explanatory power is moderate (R2 = 0.17, 95% CI [0.07, 0.27]).  Within this model:\n\n  - The effect of b Intercept[1] (Median = 1.97, 95% CI [1.36, 2.88]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30). The estimation successfully converged (Rhat = 1.005) but the indices are unreliable (ESS = 580)\n  - The effect of b Intercept[2] (Median = -1.25, 95% CI [-2.36, -0.41]) has a 99.85% probability of being negative (< 0), 99.83% of being significant (< -0.05), and 98.78% of being large (< -0.30). The estimation successfully converged (Rhat = 1.005) but the indices are unreliable (ESS = 577)\n  - The effect of bcs conditionimpasse[1] (Median = 1.92, 95% CI [1.16, 2.90]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30). The estimation successfully converged (Rhat = 1.004) but the indices are unreliable (ESS = 629)\n  - The effect of bcs conditionimpasse[2] (Median = -1.02, 95% CI [-2.20, -0.11]) has a 98.40% probability of being negative (< 0), 98.05% of being significant (< -0.05), and 93.62% of being large (< -0.30). The estimation successfully converged (Rhat = 1.005) but the indices are unreliable (ESS = 560)\n\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT) framework, we report the median of the posterior distribution and its 95% CI (Highest Density Interval), along the probability of direction (pd), the probability of significance and the probability of being large. The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.05| and |0.30|. Convergence and stability of the Bayesian sampling has been assessed using R-hat, which should be below 1.01 (Vehtari et al., 2019), and Effective Sample Size (ESS), which should be greater than 1000 (Burkner, 2017).\n\n\nConverges, but estimates are unreliable?"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#helpers",
    "href": "analysis/utils/modelling_ref.html#helpers",
    "title": "Modelling Reference",
    "section": "HELPERS",
    "text": "HELPERS\n\nOdds and Probabilities\n\n\nCODE\n# t <- table( df$pretty_condition, df$accuracy)\n# \n# print(\"AS FREQUENCY\")\n# t %>% addmargins()\n# \n# print(\"AS PROBABILITY\")\n# prop.table(t) %>% addmargins()\n# \n# print(\"GRAND PROBABILITY\")\n# (p_grand <-  sum(t[,2]) / sum(t))\n# print(\"GRAND ODDS\")\n# (odds_grand <- sum(t[,2]) / sum(t[,1]))\n# \n# print(\"P_control\")\n# (p_control <- t[1,2]/sum(t[1,]))\n# print(\"ODDS_control\") # correct:incorrect\n# (odds_control <- t[1,2] / t[1,1])\n# \n# print(\"P_impasse\")\n# (p_impasse <- t[2,2] / sum(t[2,]))\n# print(\"ODDS_impasse\") # correct:incorrect\n# (odds_impasse <- t[2,2] / t[2,1])\n# # \n# paste(\"ODDS RATIO FOR IMPASSE\", odds_impasse-odds_control)\n\n# to calculate odds from odds ratio ... ODDS = OR * [denominator of ratio]\n#eg.  ODDS [impasse] = OR[impasse] * ODDS[control] = [b_impasse] * [intercept] #for dummy coding"
  },
  {
    "objectID": "analysis/utils/modelling_ref.html#ggdist-bayesian-posterior-vis",
    "href": "analysis/utils/modelling_ref.html#ggdist-bayesian-posterior-vis",
    "title": "Modelling Reference",
    "section": "GGDist bayesian posterior vis",
    "text": "GGDist bayesian posterior vis\n\n\nCODE\n# df <- df_i %>% filter(q %nin% c(6,9)) %>% droplevels()\n# \n# #WORKING visualize post dist of prediction\n# #create df of 'draws' from the \n# draws <- df %>%\n#   data_grid(pretty_condition, subject, q) %>% \n#   add_predicted_draws(Bmm.CrSQ) \n# \n# #visualize the predictions\n# x <- draws %>% \n#   ggplot(aes(x = .prediction, y = pretty_condition)) +\n#   stat_slab() + labs(title = \"Posterior Predictions??\")\n\n# \n# #WORKING visualize parameter distribution?\n# #create df of 'draws' from the \n# draws2 <- df %>%\n#   data_grid(pretty_condition, subject, q) %>% \n#   add_linpred_draws(Bmm.CrSQ,\n#                       re_formula = NA) %>%     # zero out random effects (condition on average subj avg q)) %>% \n#   ungroup() \n#   # filter(!duplicated(.)) \n#   # mutate(probs = exp(.prediction)/(1+ exp(.prediction)))\n# \n# #visualize the predictions\n# draws2 %>% \n#   ggplot(aes(x = .linpred, y = pretty_condition)) +\n#   stat_slab()\n# \n# \n# #WORKING visualize posterior distribution as probabilities?\n# #create df of 'draws' from the \n# draws2 <- df %>%\n#   data_grid(pretty_condition, subject, q) %>% \n#   add_epred_draws(Bmm.CrSQ,\n#                    re_formula = NA)\n#                 \n# #visualize the predictions\n# draws2 %>% \n#   ggplot(aes(x = .epred, y = pretty_condition)) +\n#   stat_slab()\n# \n# \n# \n# #WORKING visualize posterior distribution as probabilities?\n# #create df of 'draws' from the \n# draws3 <- df %>%\n#   data_grid(pretty_condition, subject, q) %>% \n#   add_fitted_draws(Bmm.CrSQ,\n#                    # scale = \"linear\", without scale linear, .value is probability of %\n#                    re_formula = NA) %>% \n#   ungroup() \n#   # filter(!duplicated(.)) \n#   # mutate(probs = exp(.value)/(1+ exp(.value))) #same as scale = linear and doing the transformation manually\n# \n# #visualize the predictions\n# draws3 %>% \n#   ggplot(aes(x = .value, y = pretty_condition)) +\n#   stat_slab() \n# #x = .value plots the distribution of the ODDS ratios?\n# #without the scale = linear... gives probabilities of correct response?\n# \n# \n# THE GOOD ONE\n\n##WORKING\n## VIS probability of correct response\n# draws <- df %>%\n#   data_grid(pretty_condition, subject, q) %>% \n#   add_linpred_draws(Bmm.CrSQ,\n#                    transform = TRUE, #gives prob%, otherwise OR\n#                    re_formula = NA) \n# \n# #visualize the predictions\n# #TODO adjust bandwidth/smoothing? + put on same line + \n# draws %>%  \n#   ggplot(aes(x = .linpred,  y = 0, fill = pretty_condition)) +\n#   stat_halfeye(width = c(.95), alpha = 0.5) + \n#   xlim(0,1) + labs(\n#     title = \"Model Predicted Probability of Correct Response\",\n#     x = \"probability of correct response\",\n#     y = \"condition\"\n#   ) +  theme_clean() + ggeasy::easy_remove_legend() + ggeasy::easy_remove_y_axis()\n# #TO PLOT ON THE SAME LINE, INCLUDE Y = 0 in aes and ggeasy::remove_y_axis()\n# \n# #equivalent to...this! \n# # plot_model(Bmm.CrSQ, type  = \"pred\")[[1]]+ylim(0,1) + coord_flip()"
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html",
    "href": "analysis/SGC2/1_sgc2_introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In Study Two we examine if scaffolding is effective in aiding untrained students to understand the Triangular Model (TM) graph. We know that students are unlikely to construct the correct interpretation of the TM without assistance. Guided by the results of the Study One Design Task, we created four scaffolds. We test the effectiveness of these scaffolds by seeking to replicate the Qiang et.al (2014) finding that after 20 minutes of video training, students perform faster and more accurately with the unconventional TM than the conventional Linear Model (LM). Will our participants show similar performance on the TM with scaffolds rather than formal instruction? Further, will engagement with the TM in a reading task be sufficient for students to reproduce the graph in a subsequent drawing task?\nTo try the study yourself: http://morning-gorge-17056.herokuapp.com/\nEnter “github” as your session code, and number of the condition you wish to test\n0 = control (no-scaffold), 1 = “what-text”, 2 = “how-text”, 3 = “static-image”, 4 = “interactive-image”"
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html#methods",
    "href": "analysis/SGC2/1_sgc2_introduction.html#methods",
    "title": "1  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a 5 (scaffold: none-control, what-text, how-text, static image, interactive image) x 2 (graph: LM, TM) mixed design, with scaffold as a between-subjects variable and graph as a within-subject variable. To test our hypothesis that exposure to the conventional LM acts as a scaffold for the TM, we counterbalanced the order of graph-reading tasks (order: LM-first, TM-first). For each task we measured response accuracy and time. For the follow-up graph-drawing task, a team of raters coded the type of graph produced by each participant.\n\n\n\nMaterials\n\nScaffolds\nFor the first five questions of each graph-reading task, participants saw their assigned scaffold along with the designated graph. On the following ten questions, the scaffold was not present. Examples of each scaffold-condition for the TM and LM graphs are shown in the table above.\n\n\nThe Graph Drawing Task\nIn the  graph drawing task participants were given a sheet of isometric dot paper and a table containing a set of 10 time intervals. Isometric dot paper equally supports the construction of lines at 0, 45 and 90 degrees, thus minimizing any biasing effects of the paper on the type of graph the participants chose to draw. Participants were directed to draw a triangular graph of the data (“like the triangle graph you saw in the previous task”), using the pencil, eraser and ruler provided.\n \n\n\n\nProcedure\nParticipants completed the study individually in a computer lab. Each participant was randomly assigned to one of five conditions which determined what additional information (scaffold) they received while solving the first five problems with each graph: no-scaffold (control), ‘what’ text, ‘how’-text, static-image, and interactive-image. After a short introduction they continued to the first of two graph reading tasks (graph order counterbalanced). After completing the first graph reading task, they were introduced to the second scenario, and completed the second graph reading task with the remaining graph. Finally, participants completed the graph drawing task. They finished the study by completing a short demographic survey, and reading the debriefing text. The runtime of the entire study ranged from 20 to 60 minutes.\n\n\nSample\nData was collected by convenience sample of a university subject pool. Data were collected in the Spring of 2017 with, in-person, with large groups of students simultaneously completing the study (independently) in a computer lab."
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html#analysis",
    "href": "analysis/SGC2/1_sgc2_introduction.html#analysis",
    "title": "1  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\nnote: Unlike studies SGC3 and onwards, scoring for SGC2 is already included in the raw data files.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(mbp)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC2/data/2-scored-data/sgc2_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC2/data/2-scored-data/sgc2_items.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \nrio::export(df_subjects, \"analysis/SGC2/data/2-scored-data/sgc2_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC2/data/2-scored-data/sgc2_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html#resources",
    "href": "analysis/SGC2/1_sgc2_introduction.html#resources",
    "title": "1  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     \n [5] readr_2.1.2      tidyr_1.2.0      tibble_3.1.7     tidyverse_1.3.1 \n [9] Hmisc_4.7-0      ggplot2_3.3.6    Formula_1.2-4    survival_3.3-1  \n[13] lattice_0.20-45  kableExtra_1.3.4 codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            lubridate_1.8.0     bit64_4.0.5        \n [4] webshot_0.5.3       RColorBrewer_1.1-3  httr_1.4.3         \n [7] tools_4.2.1         backports_1.4.1     utf8_1.2.2         \n[10] R6_2.5.1            rpart_4.1.16        DBI_1.1.3          \n[13] colorspace_2.0-3    nnet_7.3-17         withr_2.5.0        \n[16] tidyselect_1.1.2    gridExtra_2.3       bit_4.0.4          \n[19] curl_4.3.2          compiler_4.2.1      cli_3.3.0          \n[22] rvest_1.0.2         htmlTable_2.4.0     xml2_1.3.3         \n[25] scales_1.2.0        checkmate_2.1.0     systemfonts_1.0.4  \n[28] digest_0.6.29       foreign_0.8-82      rmarkdown_2.14     \n[31] svglite_2.1.0       rio_0.5.29          base64enc_0.1-3    \n[34] jpeg_0.1-9          pkgconfig_2.0.3     htmltools_0.5.2    \n[37] labelled_2.9.1      dbplyr_2.2.1        fastmap_1.1.0      \n[40] highr_0.9           htmlwidgets_1.5.4   rlang_1.0.3        \n[43] readxl_1.4.0        rstudioapi_0.13     generics_0.1.2     \n[46] jsonlite_1.8.0      vroom_1.5.7         zip_2.2.0          \n[49] magrittr_2.0.3      Matrix_1.4-1        Rcpp_1.0.8.3       \n[52] munsell_0.5.0       fansi_1.0.3         lifecycle_1.0.1    \n[55] stringi_1.7.6       yaml_2.3.5          grid_4.2.1         \n[58] parallel_4.2.1      crayon_1.5.1        haven_2.5.0        \n[61] splines_4.2.1       hms_1.1.1           knitr_1.39         \n[64] pillar_1.7.0        reprex_2.0.1        glue_1.6.2         \n[67] evaluate_0.15       latticeExtra_0.6-29 data.table_1.14.2  \n[70] modelr_0.1.8        png_0.1-7           vctrs_0.4.1        \n[73] tzdb_0.3.0          cellranger_1.1.0    gtable_0.3.0       \n[76] assertthat_0.2.1    xfun_0.31           openxlsx_4.2.5     \n[79] broom_0.8.0         viridisLite_0.4.0   cluster_2.1.3      \n[82] ellipsis_0.3.2"
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html",
    "href": "analysis/SGC2/3_sgc2_description.html",
    "title": "2  Description",
    "section": "",
    "text": "The purpose of this notebook is describe the distributions of dependent variables for Study SGC2."
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#sample",
    "href": "analysis/SGC2/3_sgc2_description.html#sample",
    "title": "2  Description",
    "section": "SAMPLE",
    "text": "SAMPLE\n\nData Collection\nData was initially collected (in person, SONA groups in computer lab) in Spring 2017.\n\n\nCODE\ntitle = \"Participants by Condition and (counterbalanced) Task-order\"\ncols = c(\"Control\",\"Text[what]\",\"Text[how]\",\"Image[static]\", \"Image[ixv]\",\"Total\")\ncont <- table(df_subjects$order, df_subjects$pretty_condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and (counterbalanced) Task-order\n \n  \n      \n    Control \n    Text[what] \n    Text[how] \n    Image[static] \n    Image[ixv] \n    Total \n  \n \n\n  \n    LM-First \n    29 \n    31 \n    30 \n    30 \n    34 \n    154 \n  \n  \n    TM-First \n    32 \n    28 \n    36 \n    32 \n    34 \n    162 \n  \n  \n    Sum \n    61 \n    59 \n    66 \n    62 \n    68 \n    316 \n  \n\n\n\n\n\n\n\nParticipants\n\n\nCODE\n#Describe participants\nsubject.stats <- df_subjects %>% dplyr::select(AGE) %>% unlist() %>% favstats() \n\nsubject.stats$percent.female <- df_subjects %>% filter(GENDER==\"Female\") %>% count() %>% pull()/nrow(df_subjects)\n\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()%>% \n  footnote(general = \"Age in Years\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    percent.female \n  \n \n\n  \n     \n    17 \n    19 \n    20 \n    21 \n    33 \n    20.5 \n    2.2 \n    316 \n    0 \n    0.69 \n  \n\n\nNote:   Age in Years\n\n\n\n\nFor in person data collection 316 participants (69 % female ) undergraduate STEM majors at a public American University participated in exchange for course credit (age: 17 - 33 years)."
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#response-accuracy",
    "href": "analysis/SGC2/3_sgc2_description.html#response-accuracy",
    "title": "2  Description",
    "section": "RESPONSE ACCURACY",
    "text": "RESPONSE ACCURACY\n\nBlock Scores\nSubject level scores summarize the the response accuracy by a particular participant across all blocks of the two graph comprehension tasks. The task score refers to the number of questions correct (absolute scoring) in each block (linear graph, triangular graph) of the graph comprehension task.\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy by Block (Total Absolute Score)\"\nabs.stats <- rbind(\n  \"linear.block\"= df_subjects %>% dplyr::select(linear_score) %>% unlist() %>% favstats(),\n  \"triangular.block\" = df_subjects %>% dplyr::select(triangular_score) %>% unlist() %>% favstats(),\n  \"block.differences\" = df_subjects %>% dplyr::select(score_diff) %>% unlist() %>% favstats(),\n  \"total\" = df_subjects %>% dplyr::select(totalScore) %>% unlist() %>% favstats()\n)\n\nabs.stats %>% kbl (caption = title) %>% kable_classic() %>%\n  footnote(general = \"block # questions correct [0,15]; DIFF = triangular - linear\",\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nDescriptive Statistics of Response Accuracy by Block (Total Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    linear.block \n    2 \n    10 \n    11 \n    13 \n    15 \n    10.95 \n    2.13 \n    316 \n    0 \n  \n  \n    triangular.block \n    0 \n    5 \n    10 \n    12 \n    15 \n    8.77 \n    4.45 \n    316 \n    0 \n  \n  \n    block.differences \n    -13 \n    -5 \n    -1 \n    1 \n    5 \n    -2.18 \n    4.11 \n    316 \n    0 \n  \n  \n    total \n    6 \n    16 \n    22 \n    26 \n    31 \n    20.98 \n    6.00 \n    316 \n    0 \n  \n\n\nNote:   block # questions correct [0,15]; DIFF = triangular - linear\n\n\n\n\nTotal absolute scores for the LINEAR graph (n = 316) range from 2 to 15 with a mean score of (M = 10.95, SD = 2.13).\nTotal absolute scores for the TRIANGULAR graph (n = 316) range from 0 to 15 with a mean score of (M = 8.77, SD = 4.45).\nTotal absolute scores across the ENTIRE TASK (n = 316) range from 6 to 31 with a mean score of (M = 20.98, SD = 6).\nDifference scores (difference between TRIANGULAR and LINEAR) scores for each participant (n = 316) range from -13 to 5 with a mean score of (M = -2.18, SD = 4.11). (note: negative difference scores indicate the participant performed better on the linear block than the triangular block.)\n\nBy Block\n\n\nCODE\n#DATA SETUP\nlong_scores <- df_subjects %>% dplyr::select(subject,pretty_condition,order,lm_scenarios,tm_scenarios,linear_score, triangular_score) %>% pivot_longer(\n  cols = ends_with(\"score\"),\n  names_to = \"graph\",\n  values_to = \"score\"\n)\n\n#DISTRIBUTION OF SCORES BY GRAPH\nstats <- favstats(score~graph, data = long_scores)\ngf_dhistogram(~score, fill = ~graph, binwidth = 0.5,data = long_scores) %>%\n  gf_vline(xintercept = ~mean, color = ~graph, data = stats) %>%\n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(~ graph) +\n  labs( title = \"Distribution of Score (by Block)\",\n        subtitle =\"Performance on Linear Graph is better than Triangular\",\n        x = \"Block Score (# correct)\", y = \"proportion of subjects\") +\n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_scores, aes(x = graph, y = score,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Score (by Block) \",\n    x = \"Condition\", y = \"Score (# correct)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nTODO explore interactions\n\n\nCODE\n# \n# \n# library(interactions)\nm = lm(score ~ graph + pretty_condition + order + tm_scenarios, data = long_scores)\nm2 = lm(score ~ graph * pretty_condition * order * tm_scenarios, data = long_scores)\n# \n# cat_plot(model = m, pred = graph, modx = order, mod2 = pretty_condition,\n#          INT.TYPE = \"confidence\", int.width = 0.95,\n#          rug = TRUE)\n# \n# cat_plot(model = m2, pred = graph, modx = tm_scenarios, mod2 = pretty_condition,\n#          INT.TYPE = \"confidence\", int.width = 0.95,\n#          rug = TRUE)\n# \n# # cat_plot(model = m, pred = graph, modx = tm_scenarios, mod2 = pretty_condition,\n# #          INT.TYPE = \"confidence\", int.width = 0.95,\n# #          rug = TRUE)\n\n\n\n\nBy Condition\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\nstats <- favstats(score ~ pretty_condition+graph, data = long_scores)\ngf_dhistogram(~score, fill = ~pretty_condition, binwidth = 0.5,data = long_scores) %>% \n  gf_dens(color = ~pretty_condition) %>%  \n  # gf_vline(xintercept = ~mean, data = stats) %>% \n  gf_facet_grid(pretty_condition ~ graph) +\n  labs( title = \"Distribution of Score (by Condition)\",\n        subtitle =\"\",\n        x = \"Block Score (# correct)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_scores, aes(x = graph, y = score,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~pretty_condition) + labs( \n    title = \"Distribution of Score (by Condition) \",\n    x = \"Condition\", y = \"Score (# correct)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nBy Order\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~score, fill = ~graph, binwidth = 0.5,data = long_scores) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~order) +\n  labs( title = \"Distribution of Score (by Order)\",\n        subtitle =\"\",\n        x = \"Block Score (# correct)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_scores, aes(x = graph, y = score,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~order) + labs( \n    title = \"Distribution of Score (by Order)\",\n    x = \"Graph\", y = \"Score (# correct)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nBy Scenario\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~score, fill = ~graph, binwidth = 0.5,data = long_scores) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~tm_scenarios) +\n  labs( title = \"Distribution of Score (by Scenario)\",\n        subtitle =\"\",\n        x = \"Block Score (# correct)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_scores, aes(x = graph, y = score,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~tm_scenarios) + labs( \n    title = \"Distribution of Score (by Scenario) \",\n    x = \"Graph\", y = \"Score (# correct)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\n\nBlock Difference Scores\n\n\nCODE\n#DIFFERENCE SCORE BY SUBJECT\ngf_line(score~graph, group=~subject, color = ~order, data = long_scores) %>% \n  gf_facet_grid(order~pretty_condition) + \n  labs(title = \"Block Scores by Condition\") + easy_remove_legend()\n\n\n\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~score_diff, fill = ~pretty_condition, binwidth = 0.5,data = df_subjects) %>% \n  # gf_dens(color = ~graph) %>%  \n  gf_facet_grid(order~pretty_condition) +\n  labs( title = \"Block Difference Score (by Condition)\",\n        subtitle =\"\",\n        x = \"Difference Score (Triangular - Linear)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\n\n\nItem Scores\nTask Accuracy summarized over items rather than subjects\n\n\nCODE\ndf <- df_items %>% filter(graph %in% c(\"linear\",\"triangular\"))\n\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM by Condition\n# gf_props(~correct, data = df) %>% \n#   gf_facet_grid(pretty_condition~graph, labeller = label_both) +\n#   labs(x = \"Item Accuracy\",\n#        title = \"Item Accuracy by Graph and Condition\",\n#        subtitle=\"\")\n\n#STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = graph,\n                       fill = score)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_condition) +\n   labs(y = \"Proportion of Items\",\n       title = \"Item Accuracy by Graph and Condition\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#HISTOGRAM\n# gf_props(~correct, data = df) %>% \n#   gf_facet_grid(scenario~graph, labeller = label_both) +\n#   labs(x = \"Item Accuracy\",\n#        title = \"Item Accuracy by Graph and (TM Graph) Scenario\",\n#        subtitle=\"\")\n\n#STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = graph,\n                       fill = score)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~scenario) +\n   labs(y = \"Proportion of Items\",\n       title = \"Item Accuracy by Graph and (TM Graph) Scenario\",\n       x = \"TM Graph Scenario\",\n       fill = \"\",\n       subtitle=\"If the scenarios are of equal difficulty, these should be the same\")\n\n\n\n\n\nCODE\n#HISTOGRAM\n# gf_props(~correct, data = df) %>% \n#   gf_facet_grid(order~graph, labeller = label_both) +\n#   labs(x = \"Item Accuracy\",\n#        title = \"Item Accuracy by Graph and Block Order\",\n#        subtitle=\"\")\n\n#STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = graph,\n                       fill = score)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~order) +\n   labs(y = \"Proportion of Items\",\n       title = \"Item Accuracy by Graph and Block Order\",\n       x = \"Block Order\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = graph,\n                       fill = score)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~q) +\n   labs( \n     #y = \"Proportion of Items\",\n       title = \"Item Accuracy by Graph and Question Number\",\n       x = \"Question Number\",\n       fill = \"\",\n       subtitle=\"\")"
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#response-latency",
    "href": "analysis/SGC2/3_sgc2_description.html#response-latency",
    "title": "2  Description",
    "section": "RESPONSE LATENCY",
    "text": "RESPONSE LATENCY\n\nTime on Block\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Time by Block\"\ntime.stats <- rbind(\n  \"linear.block\"= df_subjects %>% dplyr::select(LM_T_M) %>% unlist() %>% favstats(),\n  \"triangular.block\" = df_subjects %>% dplyr::select(TM_T_M) %>% unlist() %>% favstats(),\n  \"block.differences\" = df_subjects %>% dplyr::select(DIFF_T_M) %>% unlist() %>% favstats(),\n  \"total\" = df_subjects %>% dplyr::select(TOTAL_T_M) %>% unlist() %>% favstats()\n)\n\ntime.stats %>% kbl (caption = title) %>% kable_classic() %>%\n  footnote(general = \"time in minutes; DIFF = triangular - linear\",\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nDescriptive Statistics of Response Time by Block\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    linear.block \n    3.92 \n    7.63 \n    8.87 \n    10.440 \n    23.1 \n    9.20 \n    2.53 \n    316 \n    0 \n  \n  \n    triangular.block \n    3.14 \n    8.82 \n    10.40 \n    12.502 \n    26.8 \n    10.85 \n    3.28 \n    316 \n    0 \n  \n  \n    block.differences \n    -12.66 \n    -3.15 \n    -1.42 \n    0.472 \n    12.9 \n    -1.64 \n    3.09 \n    316 \n    0 \n  \n  \n    total \n    21.91 \n    34.66 \n    39.66 \n    45.853 \n    66.1 \n    40.42 \n    8.54 \n    316 \n    0 \n  \n\n\nNote:   time in minutes; DIFF = triangular - linear\n\n\n\n\nResponse time (in minutes) for the LINEAR graph (n = 316) range from 3.92 to 23.06 with a mean time of (M = 9.2, SD = 2.53).\nResponse time (in minutes) for the TRIANGULAR graph (n = 316) range from 3.14 to 26.82 with a mean time of (M = 10.85, SD = 3.28).\nResponse time (in minutes) across the ENTIRE TASK (n = 316) range from 21.91 to 66.12 with a mean time of (M = 40.42, SD = 8.54).\nDifference in response time (in minutes) (difference between TRIANGULAR - LINEAR) for each participant (n = 316) range from -12.66 to 12.95 with a mean difference in time of (M = -1.64, SD = 3.09). (note: negative difference scores indicate the participant performed faster on the linear block than the triangular block.)\n\n\nCODE\n#DATA SETUP\nlong_times <- df_subjects %>% dplyr::select(subject,pretty_condition,order,lm_scenarios,tm_scenarios, LM_T_M, TM_T_M) %>% pivot_longer(\n  cols = ends_with(\"M\"),\n  names_to = \"graph\",\n  values_to = \"time\") %>% mutate(\n    graph = recode(graph, \"LM_T_M\" = \"Linear Graph\", \"TM_T_M\" = \"Triangular Graph\")\n  )\n\n#DISTRIBUTION OF SCORES BY GRAPH\nstats <- favstats(time~graph, data = long_times)\ngf_dhistogram(~time, fill = ~graph, binwidth = 0.5,data = long_times) %>%\n  gf_vline(xintercept = ~mean, color = ~graph, data = stats) %>%\n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(~ graph) +\n  labs( title = \"Distribution of Response Time\",\n        subtitle =\"Performance on Linear Graph is faster than Triangular\",\n        x = \"Block Score (# correct)\", y = \"number of subjects\") +\n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_times, aes(x = graph, y = time,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Response Time by Graph \",\n    x = \"Condition\", y = \"Response Time (minutes)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF TIME BY GRAPH\nstats <- favstats(time ~ pretty_condition+graph, data = long_times)\ngf_dhistogram(~time, fill = ~pretty_condition, binwidth = 0.5,data = long_times) %>% \n  gf_dens(color = ~pretty_condition) %>%  \n  gf_facet_grid(pretty_condition ~ graph) +\n  labs( title = \"Distribution of Response Time (by Condition)\",\n        subtitle =\"\",\n        x = \"Response Time (minutes)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_times, aes(x = graph, y = time,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~pretty_condition) + labs( \n    title = \"Distribution of Response Time by Graph \",\n    x = \"Condition\", y = \"Response Time (minutes)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF TIMES BY GRAPH\ngf_dhistogram(~time, fill = ~graph, binwidth = 0.5,data = long_times) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~order) +\n  labs( title = \"Distribution of Response Time (by Order)\",\n        subtitle =\"\",\n        x = \"Response Time (minutes)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_times, aes(x = graph, y = time,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~order) + labs( \n    title = \"Distribution of Response Time (by Order)\",\n    x = \"\", y = \"Response Time (minutes)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~time, fill = ~graph, binwidth = 0.5,data = long_times) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~tm_scenarios) +\n  labs( title = \"Distribution of Response Time (by Scenario)\",\n        subtitle =\"\",\n        x = \"Response Time (minutes)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(long_times, aes(x = graph, y = time,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = time, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~tm_scenarios) + labs( \n    title = \"Distribution of Response Time (by Scenario) \",\n    x = \"TM Scenario\", y = \"Respone Time (minutes)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DIFFERENCE TIME BY SUBJECT\ngf_line(time~graph, group=~subject, color = ~order, data = long_times) %>% \n  gf_facet_grid(order~pretty_condition) + \n  labs(title = \"Response Times by Condition\") + easy_remove_legend()\n\n\n\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~DIFF_T_M, fill = ~pretty_condition, binwidth = 0.5,data = df_subjects) %>% \n  # gf_dens(color = ~graph) %>%  \n  gf_facet_grid(order~pretty_condition) +\n  labs( title = \"Block Time Difference (by Condition)\",\n        subtitle =\"\",\n        x = \"Difference Time (Triangular - Linear)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\n\n\nTime on Item\n\n\nCODE\ntitle = \"Descriptive Statistics of Item Response Time by Block\"\ntime.stats <- rbind(\n  \"linear.block\"= df_items %>% filter(graph == \"linear\") %>% dplyr::select(rt_sec) %>% unlist() %>% favstats(),\n  \"triangular.block\" = df_items %>%  filter(graph == \"triangular\") %>% dplyr::select(rt_sec) %>% unlist() %>% favstats()\n)\n\ntime.stats %>% kbl (caption = title) %>% kable_classic() %>%\n  footnote(general = \"time in minutes; DIFF = triangular - linear\",\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nDescriptive Statistics of Item Response Time by Block\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    linear.block \n    4 \n    21 \n    31 \n    45 \n    302 \n    36.8 \n    24.2 \n    4740 \n    0 \n  \n  \n    triangular.block \n    2 \n    23 \n    36 \n    55 \n    401 \n    43.4 \n    30.8 \n    4739 \n    0 \n  \n\n\nNote:   time in minutes; DIFF = triangular - linear\n\n\n\n\nItem Response time (in seconds) for the LINEAR graph (n = 4740) range from 4 to 302 with a mean time of (M = 36.81, SD = 24.21).\nItem Response time (in seconds) for the TRIANGULAR graph (n = 4739) range from 2 to 401 with a mean time of (M = 43.38, SD = 30.83).\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\nstats <- favstats(rt_sec~graph, data = df)\ngf_dhistogram(~rt_sec, fill = ~graph, binwidth = 0.5,data = df) %>%\n  gf_vline(xintercept = ~mean, color = ~graph, data = stats) %>%\n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(~ graph) +\n  labs( title = \"Distribution of Response Time\",\n        subtitle =\"Performance on Linear Graph is faster than Triangular\",\n        x = \"Block Score (# correct)\", y = \"number of subjects\") +\n  easy_remove_legend()\n\n\nWarning: Removed 1 rows containing missing values (geom_vline).\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = graph, y = rt_sec,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec, color = graph),\n    size = 1.3,\n    alpha = .05,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Item Response Time by Graph \",\n    x = \"Condition\", y = \"Response Time (seconds)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF TIME BY GRAPH\nstats <- favstats(rt_sec ~ pretty_condition+graph, data = df)\ngf_dhistogram(~rt_sec, fill = ~pretty_condition, binwidth = 0.5,data = df) %>% \n  gf_dens(color = ~pretty_condition) %>%  \n  gf_facet_grid(pretty_condition ~ graph) +\n  labs( title = \"Distribution of Item Response Time (by Condition)\",\n        subtitle =\"\",\n        x = \"Response Time (seconds)\", y = \"proportion of items\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = graph, y = rt_sec,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~pretty_condition) + labs( \n    title = \"Distribution of Item Response Time by Graph \",\n    x = \"Condition\", y = \"Response Time (seconds)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF TIMES BY GRAPH\ngf_dhistogram(~rt_sec, fill = ~graph, binwidth = 0.5,data = df) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~order) +\n  labs( title = \"Distribution of Item Response Time (by Order)\",\n        subtitle =\"\",\n        x = \"Item Response Time (sec)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = graph, y = rt_sec,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~order) + labs( \n    title = \"Distribution of Item Response Time (by Order)\",\n    x = \"\", y = \"Response Time (seconds)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\n\n\nCODE\n#DISTRIBUTION OF SCORES BY GRAPH\ngf_dhistogram(~rt_sec, fill = ~graph, binwidth = 0.5,data = df) %>% \n  gf_dens(color = ~graph) %>%  \n  gf_facet_grid(graph~scenario) +\n  labs( title = \"Distribution of Item Response Time (by Scenario)\",\n        subtitle =\"\",\n        x = \"Response Time (seconds)\", y = \"number of subjects\") + \n  easy_remove_legend()\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df, aes(x = graph, y = rt_sec,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = rt_sec, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + \n  facet_wrap(~scenario) + labs( \n    title = \"Distribution of Item Response Time (by Scenario) \",\n    x = \"TM Scenario\", y = \"Item Response Time (seconds)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")"
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#drawing-task",
    "href": "analysis/SGC2/3_sgc2_description.html#drawing-task",
    "title": "2  Description",
    "section": "DRAWING TASK",
    "text": "DRAWING TASK\nFinally, we explore the distribution of graph types produced by participants during the graph drawing task.\n\n\nCODE\ngf_props(~draw_type, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Type of Graph drawn by Participant\"\n  )"
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#exploring-relationships",
    "href": "analysis/SGC2/3_sgc2_description.html#exploring-relationships",
    "title": "2  Description",
    "section": "EXPLORING RELATIONSHIPS",
    "text": "EXPLORING RELATIONSHIPS\n\nBlock Accuracy\n\n\nCODE\n#SCATTERPLOT \ngf_jitter( linear_score ~ triangular_score, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Relationship between Linear and Triangular Block Scores\",\n    subtitle = \"\", \n    x = \"Linear Score\", y = \"Triangular Score\"\n  ) + easy_remove_legend()\n\n\n\n\n\n\n\nTime + Accuracy\n\n\nCODE\nq.stats <- df %>%  dplyr::group_by(graph, q, pretty_condition, score) %>% dplyr::summarise(\n  m = mean(rt_sec),\n  sd = sd(rt_sec),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score)\n)\n\ngf_line( m ~ q, group = ~group,  color = ~score, data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~q) %>% \n  gf_facet_grid(graph~pretty_condition) + scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Absolute Score\",\n       subtitle = \"\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\ndf %>%\n  ggplot(aes(y = rt_sec, x = q,  fill = pretty_condition)) +\n  stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + facet_grid(graph~pretty_condition)+labs(title = \"Average Item Response Time by Absolute Score\",\n       subtitle = \"\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")"
  },
  {
    "objectID": "analysis/SGC2/3_sgc2_description.html#resources",
    "href": "analysis/SGC2/3_sgc2_description.html#resources",
    "title": "2  Description",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] forcats_0.5.1      stringr_1.4.0      purrr_0.3.4        readr_2.1.2       \n [5] tidyr_1.2.0        tibble_3.1.6       tidyverse_1.3.1    performance_0.8.0 \n [9] fitdistrplus_1.1-8 MASS_7.3-55        multimode_1.5      ggeasy_0.1.3      \n[13] ggdist_3.1.1       ggpubr_0.4.0       vcd_1.4-9          kableExtra_1.3.4  \n[17] mosaic_1.8.3       ggridges_0.5.3     mosaicData_0.20.2  ggformula_0.10.1  \n[21] ggstance_0.3.5     dplyr_1.0.8        Matrix_1.4-0       Hmisc_4.6-0       \n[25] ggplot2_3.3.5      Formula_1.2-4      survival_3.3-1     lattice_0.20-45   \n\nloaded via a namespace (and not attached):\n  [1] colorspace_2.0-3     ggsignif_0.6.3       ellipsis_0.3.2      \n  [4] mclust_5.4.10        leaflet_2.1.0        htmlTable_2.4.0     \n  [7] fs_1.5.2             base64enc_0.1-3      ggdendro_0.1.23     \n [10] rstudioapi_0.13      farver_2.1.0         ggrepel_0.9.1       \n [13] lubridate_1.8.0      mvtnorm_1.1-3        fansi_1.0.2         \n [16] xml2_1.3.3           codetools_0.2-18     splines_4.1.1       \n [19] rootSolve_1.8.2.3    knitr_1.38           polyclip_1.10-0     \n [22] jsonlite_1.8.0       broom_0.7.12         dbplyr_2.1.1        \n [25] cluster_2.1.2        png_0.1-7            ggforce_0.3.3       \n [28] compiler_4.1.1       httr_1.4.2           backports_1.4.1     \n [31] assertthat_0.2.1     fastmap_1.1.0        cli_3.2.0           \n [34] tweenr_1.0.2         htmltools_0.5.2      tools_4.1.1         \n [37] gtable_0.3.0         glue_1.6.2           Rcpp_1.0.8.3        \n [40] carData_3.0-5        cellranger_1.1.0     vctrs_0.3.8         \n [43] svglite_2.1.0        crosstalk_1.2.0      insight_0.18.0      \n [46] lmtest_0.9-39        xfun_0.30            rvest_1.0.2         \n [49] lifecycle_1.0.1      mosaicCore_0.9.0     rstatix_0.7.0       \n [52] zoo_1.8-9            scales_1.1.1         hms_1.1.1           \n [55] RColorBrewer_1.1-2   yaml_2.3.5           gridExtra_2.3       \n [58] labelled_2.9.0       rpart_4.1.16         latticeExtra_0.6-29 \n [61] stringi_1.7.6        highr_0.9            checkmate_2.0.0     \n [64] rlang_1.0.2          pkgconfig_2.0.3      systemfonts_1.0.4   \n [67] distributional_0.3.0 pracma_2.3.8         evaluate_0.15       \n [70] labeling_0.4.2       ks_1.13.5            htmlwidgets_1.5.4   \n [73] tidyselect_1.1.2     plyr_1.8.6           magrittr_2.0.2      \n [76] R6_2.5.1             generics_0.1.2       DBI_1.1.2           \n [79] pillar_1.7.0         haven_2.4.3          foreign_0.8-82      \n [82] withr_2.5.0          abind_1.4-5          nnet_7.3-17         \n [85] modelr_0.1.8         crayon_1.5.0         car_3.0-12          \n [88] KernSmooth_2.23-20   utf8_1.2.2           tzdb_0.2.0          \n [91] rmarkdown_2.13       jpeg_0.1-9           readxl_1.3.1        \n [94] data.table_1.14.2    reprex_2.0.1         diptest_0.76-0      \n [97] digest_0.6.29        webshot_0.5.2        munsell_0.5.0       \n[100] viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_introduction.html",
    "href": "analysis/SGC3A/1_sgc3A_introduction.html",
    "title": "3  Introduction",
    "section": "",
    "text": "In Study 3A we explore the extent to which confronting a learner with an implicit obstacle (a mental impasse) influences their interpretation of the underlying coordinate system. This is a hypothesis that emerged from analysis of Study 2, leading us to suspect that presenting a learner with a situation that induces a state of impasse will increase the probability that learners experience a moment of insight, and in turn restructure their interpretation of the coordinate system.\nIn the context of Study 2, an impasse state was (unintentionally) induced when the combination of question + data set yielded no available answer in the incorrect (cartesian) interpretation of the graph. In Study 3A, we test this hypothesis by comparing performance between a (treatment) group receiving impasse-inducing questions followed by normal questions, and a non-impasse control."
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_introduction.html#methods",
    "href": "analysis/SGC3A/1_sgc3A_introduction.html#methods",
    "title": "3  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Scaffold: control,impasse)\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 19.1. The list of questions can be found here.\n\n\n\nFigure 3.2: Sample Question (Q=1) for Graph Comprehension Task\n\n\nNote that across both control and impasse conditions, both the question, response options and graph structure were identical. The experimental manipulation (posing a mental impasse) was accomplished by changing the position of datapoints in the impasse-condition graph, such that for any given question, there was no available response option if the reader were to interpret the graph as cartesian (making an orthogonal rather than diagonal projection from the x-axis.)\nThe green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. We hypothesize that this presents the reader with an obstacle, at which point they are forced to confront their interpretation of the coordinate system and (ideally) develop a new strategy.\n\n\n\nFigure 3.3: Sample Question (Q=1) graphs for each condition\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items.\n(3A) The first five items in the task are defined as the SCAFFOLDING block. In the IMPASSE condition, the first five questions included an IMPASSE problem state. For participants in the CONTROL condition, the dataset was structure such that there was always an available ‘orthogonal answer’ for the first 5 questions.\n(3B) The remaining 10 items are defined as the TESTING block. In both conditions, these questions were not structured as impasse (i.e. contained an available orthogonal answer)\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData was collected by convenience sample of a university subject pool. Initial data (Fall 2017, Spring 2018) were collected in-person, with large groups of students simultaneously completing the study (independently) in a computer lab. In Fall 2021 and Winter 2022 we collected additional data to replicate results in a remote format (students completing the study asynchronously on their own computers)."
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_introduction.html#analysis",
    "href": "analysis/SGC3A/1_sgc3A_introduction.html#analysis",
    "title": "3  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\nBefore analysis, data files from individual data collection periods are harmonized into a common data format.\n\n\n\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nspring17_clean_data.Rmd  spring18_clean_data.Rmd  fall21_clean_data.Rmd  winter2022_clean_sgc3a.Rmd\n2_sgc3A_scoring.qmd\n\n\n\nData for study SGC_3A were collected across four time periods, interrupted by the Covid-19 pandemic.\n\n\n\nPeriod\nModality\n\n\n\n\nFall 2017\nin person, SONA groups in computer lab\n\n\nSpring 2018\nin person, SONA groups in computer lab\n\n\nFall 2021\nasynchronous, online, SONA\n\n\nWinter 2022\nasynchronous, online, SONA\n\n\n\nData collected in Fall 2017 (in person pilot) were analyzed and published as a Cognitive Science Society conference paper (“When Graph Comprehension is an Insight Problem). Additional data were collected in person in Spring 2018. Combined, Fall 2017 and Spring 2018 constitute the original SGC_3A study, conducted in person. Data collected in Fall 2021, Winter 2022 constitute the web-based replication, conducted online (asynchronously). In all cases, the experiment was administered via a web application. The replication study was conducted to validate the use of remote, asynchronous data collection during the Covid-19 pandemic.\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3A/data/0-session-level/fall17_sgc3a_participants.csv\" #COGSCI18 data\nspring18 <- \"analysis/SGC3A/data/0-session-level/spring18_sgc3a_participants.csv\"\nfall21 <- \"analysis/SGC3A/data/0-session-level/fall21_sgc3a_participants.csv\"\nwinter22 <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_participants.rds\"\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_subjects_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% \n  dplyr::select(\n  subject,condition,term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m, \n  # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n  #exploratory factors\n  violations, browser, width, height\n)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\",\n    violations = \"NULL\",\n    browser = \"NULL\",\n    width = \"NULL\",\n    height = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, \n                # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                #exploratory factors\n                violations, browser, width, height)\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_winter22_q16 <- df_subjects_winter22 %>% \n  dplyr::select(subject, condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% dplyr::select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects_winter22 <- df_subjects_winter22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors \n                 violations, browser, width, height)\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_winter22, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\"),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#IMPORT OSPAN DATA \nospan <- read_csv(\"analysis/SGC3A/data/0-session-level/fall21_scored_ospan.csv\") %>% mutate(\n  subject = SUBJECTID\n) %>% dplyr::select(-SUBJECTID)\n\n#MERGE OSPAN DATA WITH SGC DATA \n#special dataframe with just ospan subjects\n#should be 133 subjects. Some of the 200 who completed the task failed the \n#attention check question. Others were allocated to SGC4A pilot. \n#note that rather than adding OSPAN data to main dataframe, the after-scored data\n#will be manually joined to df_ospan during exploratory analysis\ndf_ospan <- df_subjects %>% filter(\n  subject %in% ospan$subject\n) %>% merge(ospan)  \n\n\n#CLEANUP\nrm(df_subjects_fall17,df_subjects_fall21, df_subjects_spring18, df_subjects_winter22,df_subjects_before)\nrm(fall17,fall21,spring18,winter22)\n\n\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_subjects$term, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    fall17 \n    27 \n    27 \n    54 \n  \n  \n    spring18 \n    35 \n    37 \n    72 \n  \n  \n    fall21 \n    68 \n    71 \n    139 \n  \n  \n    winter22 \n    28 \n    37 \n    65 \n  \n  \n    Sum \n    158 \n    172 \n    330 \n  \n\n\n\n\n\nCODE\ntitle = \"Subset of Participants who completed OSPAN TASK [Fall 2021]\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_ospan$mode, df_ospan$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nSubset of Participants who completed OSPAN TASK [Fall 2021]\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    lab-synch \n    0 \n    0 \n    0 \n  \n  \n    asynch \n    65 \n    68 \n    133 \n  \n  \n    Sum \n    65 \n    68 \n    133 \n  \n\n\n\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3A/data/0-session-level/fall17_sgc3a_blocks.csv\"\nspring18 <- \"analysis/SGC3A/data/0-session-level/spring18_sgc3a_blocks.csv\"\nfall21 <- \"analysis/SGC3A/data/0-session-level/fall21_sgc3a_blocks.csv\"\nwinter22 <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_items.rds\"\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_items_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- df_items_winter22 %>% group_by(q) %>% dplyr::select(q,relation) %>% unique()\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- df_items_winter22 %>% filter(condition=='X') %>% dplyr::select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n  \n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18, df_items_fall21) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  dplyr::select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n  \n#reduce data collected using new webapp\ndf_items_winter22 <- df_items_winter22 %>% \n  dplyr::select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_winter22,df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% dplyr::select(-question,-correct,-rt_s,-num_o)\n#add data from wi22 [stored on subject data]\ndf_freeresponse <- rbind(df_freeresponse, df_winter22_q16)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n    question = \"Please describe how to determine what event(s) start at 12pm?\",\n    response = as.character(response) #doesn't need to be factor\n  ) \n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#add back pretty condition \ndf_items <- df_items %>% mutate(\n  pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n  pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_items_fall17,df_items_fall21, df_items_spring18, df_items_winter22, df_items_before, df_winter22_q16)\nrm(fall17,fall21,spring18,winter22, map_relations)\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3A/data/1-study-level/sgc3a_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3A/data/1-study-level/sgc3a_items.csv\", row.names = FALSE)\nwrite.csv(df_freeresponse,\"analysis/SGC3A/data/1-study-level/sgc3a_freeresponse.csv\", row.names = FALSE)\nwrite.csv(df_ospan,\"analysis/SGC3A/data/1-study-level/sgc3a_ospan.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3A/data/1-study-level/sgc3a_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3A/data/1-study-level/sgc3a_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html",
    "title": "4  Response Scoring",
    "section": "",
    "text": "TODO\nThe purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC3A study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#score-sgc-data",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#score-sgc-data",
    "title": "4  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#backup <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#summarize-by-subject",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#summarize-by-subject",
    "title": "4  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#explore-distributions",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#explore-distributions",
    "title": "4  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"Impasse Condition (blue) yields more correct responses across the entire task\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(pretty_condition~q) + \n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields more correct responses on each item\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>%\ngf_facet_wrap(~pretty_condition)+\n  labs( x = \"Total Absolute Score\",\n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher total absolute scores\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE TEST PHASE\ngf_props(~item_test_NABS, fill = ~pretty_condition, \n             data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Absolute Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#QUICK PEEK\nresult <- two_sample_test(data = df_subjects, x = pretty_condition, y = s_NABS,\n                          type = \"nonparametric\", var.equal = FALSE,alternative = \"less\",\n                          k = 2L, conf.level = 0.89, effsize.type = \"g\",\n                          bf.prior = 0.707, tr = 0.2, nboot = 100L)\n\nggbetweenstats( data = df_subjects,\n                x = pretty_condition, y = s_NABS,\n                type = \"non-parametric\",\n                title = \"Total Absolute Score [directional test]\",\n                results.subtitle = FALSE,\n                subtitle = result$expression[[1]])\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores across the entire task\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(q~pretty_condition) + \n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores on each item\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) + \n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>% \n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\", \n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher cumulative scaled scores\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE TEST PHASE\ngf_histogram(~item_test_SCALED, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Scaled Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Scaled Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#QUICK PEEK\nresult <- two_sample_test(data = df_subjects, x = pretty_condition, y = s_SCALED,\n                          type = \"nonparametric\", var.equal = FALSE,alternative = \"less\",\n                          k = 2L, conf.level = 0.89, effsize.type = \"g\",\n                          bf.prior = 0.707, tr = 0.2, nboot = 100L)\n\nggbetweenstats( data = df_subjects,\n                x = pretty_condition, y = s_SCALED,\n                type = \"non-parametric\",\n                title = \"Total Scaled Score [directional test]\",\n                results.subtitle = FALSE,\n                subtitle = result$expression[[1]])\n\n\n\n\n\n\nTODO: INVESTIGATE if some of the scores assigned to 0 should be assigned to -0.5 to balance\nTODO: INVESTIGATE DISTRIBUTIONS of each subscore type\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"Impasse condition (blue) yields fewer Orthogonal and more Triangular responses\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more Triangular responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more positive trending responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"Impasse shifts density toward higher Triagular scores\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"Impasse shifts density toward lower Orthogonal scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#explore-responses",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#explore-responses",
    "title": "4  Response Scoring",
    "section": "EXPLORE RESPONSES",
    "text": "EXPLORE RESPONSES\nIn this section we explore responses given by participants to each particular item in the graph comprehension task, indicate how each response was scored, and what interpretation of the graph is indicated by different responses.\n\nScaffold Phase\nThe first five questions constitute the ‘scaffold’ (or learning) phase, where participants see a different version of the stimulus (specifically a different dataset is visualized) invoking a different experimental condition.\n\nQuestion #1\n\nQ1. Control Condition\nWe start by exploring the range of response options checked by participants on Question 1, for those assigned to the control (non-impasse) condition (condition = 111).\n\n\n\nFigure 4.1: Question 1 — Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==1)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q1 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q1 Control Condition :  Which shift(s) start at 11 am?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n    Z \n  \n  \n    Orthgonal \n    A \n    OI \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    CF \n    Z \n  \n  \n    Tversky [start diagonal] \n    F \n    Z \n  \n  \n    Tversky [end diagonal] \n    C \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nHere we summarize the distinct response options given by participants on this item. Each letter in response indicates a checkbox selected by the participant (See Figure 4.1 ). n indicates the number of participants who gave this response, while interpretation indicates the graph interpretation most consistent with that response. At the right of this table are the Absolute, followed by Partial Credit subscores for each response. NA indicates that there is no score calculated (occurs when there is no subset of response options that accord with that interpretation for this question).\nNotice that for this Question, the Triangular answer is the same as the Tversky [start diagonal] answer. In fact, for most questions, one of the Tversky sub-types will match the correct response.\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #1 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 1 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 1) %>% \n  pack_rows(\"Lines-Connect\", 2, 2) %>% \n  pack_rows(\"Orthogonal\", 3, 3) %>% \n  pack_rows(\"Other\", 4, 4)  %>% \n  pack_rows(\"Unknown\", 5, 7)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #1 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    22 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.083 \n    1.0 \n  \n  Lines-Connect\n\n    CF \n    3 \n    Tversky \n    0 \n    0.923 \n    1.000 \n    NA \n    -0.167 \n    0.5 \n  \n  Orthogonal\n\n    A \n    129 \n    Orthogonal \n    0 \n    -0.077 \n    -0.071 \n    NA \n    1.000 \n    -1.0 \n  \n  Other\n\n    AF \n    1 \n    ? \n    0 \n    0.923 \n    0.923 \n    NA \n    0.917 \n    -0.5 \n  \n  Unknown\n\n    DIJ \n    1 \n    ? \n    0 \n    -0.231 \n    -0.214 \n    NA \n    -0.167 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.077 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    0.000 \n    0.000 \n    NA \n    -0.083 \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nWe see that nearly all of the subjects selected a response consistent with one of the identified interpretations. Responses that do not accord with any interpretation are indicated as ? .\n\n\n\n\n\n\n\nWhich shifts start at 11am?\n\n\n\n\n\n\nResponse: A\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, projecting an invisible orthogonal line upward, and locating data point A.\n\n\n\n\nResponse: F\n\nindicates the triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following the right-diagonal gridline, identifying data point F.\n\n\n\n\nResponse: C, F\n\nindicates a maximal-Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following both the right-diagonal and left-diagonal gridlines, identifying both datapoints F and C.\n\n\n\n\nResponse: A , F\n\nThe reader selects both triangular and orthogonal-consistent data points\nPossibly indicates uncertainty or confusion\n\n\n\n\nThree responses were given that were not consistent with any of the identified interpretations. Note that options highlighted in light grey are considered within the range of ‘visual error’, defined by 0.5hr offset from the interpretation-specific projection.\n\n\n\n\n\n\n\n\nD I J\nX\nZ\n[found this subject F86ZM, thought maybe this was a missed ‘F’, but they have a series of other unknown answers]\n\n\n\n\n\n\n\n\n\n\n\n\nQ1. Impasse Condition\nNext we explore the range of response options checked by participants on Question 1, for those assigned to the control (non-impasse) condition (condition = 111).\n\n\n\nFigure 4.2: Question 1 — Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==1)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q1 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q1 Impasse Condition :  Which shift(s) start at 11 am?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    O \n     \n  \n  \n    Satisficing [right] \n    AI \n     \n  \n  \n    Tversky [maximal] \n    CF \n     \n  \n  \n    Tversky [start diagonal] \n    F \n     \n  \n  \n    Tversky [end diagonal] \n    C \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nNotice that there is no orthogonal answer for this question. This is the purpose of the impasse condition, to remove the possibility of selecting the orthogonal answer, we expect learners will be more likely to restructure their understanding of the coordinate system, and arrive at a correct (triangular) interpretation.\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #1 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 1 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 1) %>% \n  pack_rows(\"Lines-Connect\", 2, 4) %>% \n  pack_rows(\"Satisfice\", 5, 9) %>% \n  pack_rows(\"Other\", 10, 10) %>% \n  pack_rows(\"Unknown\", 11, 12) %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #1 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    49 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.071 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    CF \n    14 \n    Tversky \n    0 \n    0.929 \n    1.000 \n    -0.143 \n    NA \n    0.5 \n  \n  \n    C \n    3 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    -0.071 \n    NA \n    0.5 \n  \n  \n    CO \n    1 \n    Tversky \n    0 \n    -0.143 \n    0.929 \n    0.929 \n    NA \n    0.5 \n  \n  Satisfice\n\n    O \n    28 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AI \n    9 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    A \n    4 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    AO \n    2 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    0.929 \n    NA \n    -1.0 \n  \n  \n    I \n    2 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  Other\n\n     \n    57 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  Unknown\n\n    E \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.071 \n    NA \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.071 \n    NA \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nWe see that nearly all of the subjects selected a response consistent with one of the identified interpretations. Responses that do not accord with any interpretation are indicated as ? .\nTODO ADJUST ‘both’ to select for both tri/satisfice or both tri/orth\n\n\n\n\n\n\n\nWhich shifts start at 11am?\n\n\n\n\n\n\nResponse: F\n\nindicates the triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following the right-diagonal gridline, identifying data point F.\n\n\n\n\nResponse: [C, F]\n\nindicates a maximal-Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following both the right-diagonal and left-diagonal gridlines, identifying both datapoints F and C gridline.\n\n\n\n\nResponses: [AOI]\n\nindicates a satisficing strategy\nConsistent with the reader identifying the datapoints nearest to the orthogonal projection from the reference point point\n\n\n\n\nTwo responses were given that were not consistent with any of the identified interpretations.\n\n\n\n\n\n\n[E],[X]\n\n\n\n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==1)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q1 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==1)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q1 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #2\n\nQ2. Control Condition\n\n\n\nFigure 4.3: Q2—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==2)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q2 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q2 Control Condition :  Which shift(s) start at the same time as D?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    K \n    Z \n  \n  \n    Orthgonal \n    E \n    G \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    AKJX \n    Z \n  \n  \n    Tversky [start diagonal] \n    AK \n    Z \n  \n  \n    Tversky [end diagonal] \n    X \n     \n  \n  \n    Tversky [duration line] \n    J \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #2 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 2 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 4) %>% \n  pack_rows(\"Orthogonal\", 5, 7) %>%\n  pack_rows(\"Other\", 8, 8)  %>% \n  pack_rows(\"Unknown\", 9, 10)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #2 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    K \n    24 \n    Triangular \n    1 \n    1.000 \n    0.500 \n    NA \n    -0.083 \n    1.0 \n  \n  \n    DK \n    1 \n    Triangular \n    1 \n    1.000 \n    0.500 \n    NA \n    -0.083 \n    1.0 \n  \n  Lines-Connect\n\n    J \n    4 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    NA \n    -0.083 \n    0.5 \n  \n  \n    AK \n    1 \n    Tversky \n    0 \n    0.917 \n    1.000 \n    NA \n    -0.167 \n    0.5 \n  \n  Orthogonal\n\n    E \n    121 \n    Orthogonal \n    0 \n    -0.083 \n    -0.077 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    DE \n    3 \n    Orthogonal \n    0 \n    -0.083 \n    -0.077 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EG \n    1 \n    Orthogonal \n    0 \n    -0.167 \n    -0.154 \n    NA \n    1.000 \n    -1.0 \n  \n  Other\n\n    D \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    B \n    1 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    NA \n    -0.083 \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nAgain, we see that most subjects selected a response consistent with one of the identified interpretations. (note, when the question stem includes a data point rather than time as reference, we do not penalize respondents for selecting the reference data point in addition to an interpretation consistent response. For example, in this question, we do not penalize respondents for selecting option D, the reference point in the question. )\n\n\n\n\n\n\n\nWhich shift(s) start at the same time as D?\n\n\n\n\nReponse: E (also EG, DE)\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (D) on the graph, projecting an invisible orthogonal line through it, and locating data point E.\n\n\n\n\nResponse: K (also KD)\n\nindicates an triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (D) on the graph, and following its descending-leftward diagonal gridline, and locating data point K.\n\n\n\n\nResponse: AK\n\nindicates an Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (D) on the graph, and following its descending-leftward diagonal gridline, and locating data point K then continuing along the connecting ascending leftward diagonal locating data point A.\n\n\n\n\nResponse: J\n\nindicates an Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (D) on the graph, and following its horizontal gridline to the y-axis, locating data point J.\n\n\n\n\nResponse: D\n\nthe reader selected only the reference point\nConsistent with the reader identifying the reference point (D) on the graph\nPossibly indicates uncertainty or confusion\n\n\n\n\n\n\n\n\n\n\n\nB\nC\n\n\n\n\n\n\n\n\n\n\n\nQ2. Impasse Condition\n\n\n\nFigure 4.4: Q2—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==2)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q2 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q2 Impasse Condition :  Which shift(s) start at the same time as D?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    K \n    Z \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n    G \n     \n  \n  \n    Tversky [maximal] \n    JKE \n    Z \n  \n  \n    Tversky [start diagonal] \n    K \n    Z \n  \n  \n    Tversky [end diagonal] \n    E \n     \n  \n  \n    Tversky [duration line] \n    J \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #2 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 2 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 10) %>% \n  pack_rows(\"Satisfice\", 11, 12) %>%\n  pack_rows(\"Other\", 13, 16)  %>% \n  pack_rows(\"Unknown\", 17, 18)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #2 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    K \n    69 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    DK \n    1 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    J \n    12 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    EK \n    3 \n    Tversky \n    0 \n    0.917 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    EX \n    2 \n    Tversky \n    0 \n    -0.167 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BEG \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.846 \n    0.846 \n    NA \n    0.5 \n  \n  \n    E \n    1 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    EKX \n    1 \n    Tversky \n    0 \n    0.833 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    HJZ \n    1 \n    Tversky \n    0 \n    -0.167 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    JK \n    1 \n    Tversky \n    0 \n    0.917 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  Satisfice\n\n    G \n    19 \n    Satisfice \n    0 \n    -0.083 \n    -0.077 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    BG \n    2 \n    Satisfice \n    0 \n    -0.167 \n    -0.154 \n    0.923 \n    NA \n    -1.0 \n  \n  Other\n\n    D \n    7 \n    reference \n    0 \n    0.000 \n    NA \n    0.000 \n    NA \n    0.0 \n  \n  \n     \n    43 \n    blank \n    0 \n    0.000 \n    NA \n    0.000 \n    NA \n    0.0 \n  \n  \n    ACDFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.250 \n    0.250 \n    -0.846 \n    NA \n    -0.5 \n  \n  \n    BEGKUZ \n    1 \n    frenzy \n    0 \n    0.667 \n    0.667 \n    0.615 \n    NA \n    -0.5 \n  \n  Unknown\n\n    C \n    6 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    FO \n    1 \n    ? \n    0 \n    -0.167 \n    -0.154 \n    -0.154 \n    NA \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==2)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q2 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==2)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q2 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #3\n\nQ3. Control Condition\n\n\n\nFigure 4.5: Q3—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==3)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q3 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q3 Control Condition :  Which shift(s) begin when C ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n    Z \n  \n  \n    Orthgonal \n    Z \n    FIO \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    AUBFOJ \n     \n  \n  \n    Tversky [start diagonal] \n    OJ \n     \n  \n  \n    Tversky [end diagonal] \n    F \n    Z \n  \n  \n    Tversky [duration line] \n    AUB \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #3 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 3 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 7) %>% \n  pack_rows(\"Orthogonal\", 8, 8) %>% \n  pack_rows(\"Other\", 9, 10) %>% \n  pack_rows(\"Unknown\", 11, 17)  \n\n\n\nFrequency of Selected Response Options for Question #3 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    24 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    0.0 \n    1.0 \n  \n  \n    EFK \n    1 \n    Triangular \n    0 \n    0.833 \n    0.833 \n    NA \n    -0.2 \n    1.0 \n  \n  Lines-Connect\n\n    ABU \n    4 \n    Tversky \n    0 \n    -0.250 \n    1.000 \n    NA \n    -0.3 \n    0.5 \n  \n  \n    O \n    3 \n    Tversky \n    0 \n    -0.083 \n    0.500 \n    NA \n    0.0 \n    0.5 \n  \n  \n    JO \n    2 \n    Tversky \n    0 \n    -0.167 \n    1.000 \n    NA \n    -0.1 \n    0.5 \n  \n  \n    DJO \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.917 \n    NA \n    -0.2 \n    0.5 \n  \n  \n    KO \n    1 \n    Tversky \n    0 \n    -0.167 \n    0.417 \n    NA \n    -0.1 \n    0.5 \n  \n  Orthogonal\n\n    Z \n    94 \n    Orthogonal \n    0 \n    0.000 \n    0.000 \n    NA \n    1.0 \n    -1.0 \n  \n  Other\n\n    C \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.0 \n    0.0 \n  \n  \n    ABDEFGHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.000 \n    NA \n    NA \n    0.0 \n    -0.5 \n  \n  Unknown\n\n    A \n    18 \n    ? \n    0 \n    -0.083 \n    0.333 \n    NA \n    -0.1 \n    -0.5 \n  \n  \n    K \n    3 \n    ? \n    0 \n    -0.083 \n    -0.083 \n    NA \n    -0.1 \n    -0.5 \n  \n  \n    AH \n    1 \n    ? \n    0 \n    -0.167 \n    0.242 \n    NA \n    -0.2 \n    -0.5 \n  \n  \n    DE \n    1 \n    ? \n    0 \n    -0.167 \n    -0.167 \n    NA \n    -0.2 \n    -0.5 \n  \n  \n    E \n    1 \n    ? \n    0 \n    -0.083 \n    -0.083 \n    NA \n    -0.1 \n    -0.5 \n  \n  \n    EU \n    1 \n    ? \n    0 \n    -0.167 \n    0.242 \n    NA \n    -0.2 \n    -0.5 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.083 \n    0.333 \n    NA \n    -0.1 \n    -0.5 \n  \n\n\n\n\n\nTODO\n\naddress RESPONSE FKE which is classified as Triangular but doesn’t seem to fit this interpretation?\nShould O,K be considered Tvresky ?\nconsider adding trapdoor on n_q, such that score is penalized (OR interpretation is not predicted?) if the Ss selects more than 1 extra options, or is missing more than 2 options?\nLEFT OFF HERE\n\n\n\n\n\n\n\n\nWhat shift(s) begin when C ends?\n\n\n\n\n\n\nResponse: Z\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (C) then using the duration encoded on the y-axis (2) , project along the horizontal gridline by two hours, and then project an invisible orthogonal line through that time (12PM) locating data point Z.\n\n\n\n\nResponse: F\n\nindicates a (correct) triangular interpretation of the coordinate system\nConsistent with the reader identifying the reference point (C) on the graph, and following the descending gridline to the x-axis to identify the end-time (11AM) and then following the ascending gridline to identify datapoints starting at 11AM and locating data point F.\n\n\n\n\nResponse: AUB (also A)\n\nindicates a Tversky strategy following connecting lines (duration)\nConsistent with the reader identifying the reference point (C) on the graph, and following the horizontal y-axis gridline and locating data points A U B.\n\n\n\n\nResponse: OJ\n\nindicates a Tversky strategy following connecting lines (start-time)\nConsistent with the reader identifying the reference point (C) on the graph, and following the ascending diagonal gridline and locating data points O J.\n\n\n\n\nResponse: C\n\nthe participant selected the point referenced in the question\npossibly indicates confusion or uncertainty\n\n\n\n\nResponse: AIOZFHJXKUDEGB\n\nthe participant selects all (or nearly all) the data points\npossibly indicates confusion or uncertainty\n\n\n\n\nSix responses (from 9 participants) appear inconsistent with any interpretation.\n\n\n\n\n\n\n\n\nK (n=3)\nAH (n=1)\nDE (n=1)\n\n\n\n\n\n\n\n\n\nUE (n=1)\nU (n=1)\nE (n=1)\n\n\n\n\n\n\n\n\n\n\nQ3. Impasse Condition\n\n\n\nFigure 4.6: Q3—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==3)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q3 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q3 Impasse Condition :  Which shift(s) begin when C ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    AI \n     \n  \n  \n    Satisficing [right] \n    F \n     \n  \n  \n    Tversky [maximal] \n    BJ \n     \n  \n  \n    Tversky [start diagonal] \n    J \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n    B \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nTODO investigate these responses 17 at O?\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #3 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 3 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 5) %>% \n  pack_rows(\"Lines-Connect\", 3, 5) %>% \n  pack_rows(\"Satisfice\", 6, 15) %>% \n  pack_rows(\"Other\", 16, 21) %>% \n  pack_rows(\"Unknown\", 22, 29) \n\n\n\nFrequency of Selected Response Options for Question #3 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    61 \n    Triangular \n    1 \n    1.000 \n    -0.077 \n    1.000 \n    NA \n    1.0 \n  \n  \n    AF \n    5 \n    Triangular \n    0 \n    0.923 \n    -0.154 \n    0.923 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    AFG \n    1 \n    Triangular \n    0 \n    0.846 \n    -0.231 \n    0.846 \n    NA \n    1.0 \n  \n  \n    B \n    8 \n    Tversky \n    0 \n    -0.077 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    J \n    3 \n    Tversky \n    0 \n    -0.077 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  Satisfice\n\n    BE \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BJ \n    1 \n    Tversky \n    0 \n    -0.154 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    HJZ \n    1 \n    Tversky \n    0 \n    -0.231 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    A \n    7 \n    Satisfice \n    0 \n    -0.077 \n    -0.077 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    AH \n    5 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    0.417 \n    NA \n    -1.0 \n  \n  \n    AI \n    3 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AOU \n    3 \n    Satisfice \n    0 \n    -0.231 \n    -0.231 \n    0.333 \n    NA \n    -1.0 \n  \n  \n    AFI \n    2 \n    Satisfice \n    0 \n    0.846 \n    -0.231 \n    0.917 \n    NA \n    -1.0 \n  \n  \n    AIO \n    2 \n    Satisfice \n    0 \n    -0.231 \n    -0.231 \n    0.917 \n    NA \n    -1.0 \n  \n  \n    AO \n    2 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    0.417 \n    NA \n    -1.0 \n  \n  Other\n\n    C \n    2 \n    reference \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  \n     \n    36 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  \n    ABDEFGHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    -0.5 \n  \n  \n    ABDEFGHJKUZ \n    1 \n    frenzy \n    0 \n    0.231 \n    0.250 \n    0.231 \n    NA \n    -0.5 \n  \n  \n    BDEFGHJKU \n    1 \n    frenzy \n    0 \n    0.385 \n    0.417 \n    0.385 \n    NA \n    -0.5 \n  \n  \n    BDEFGHJKUXZ \n    1 \n    frenzy \n    0 \n    0.231 \n    0.250 \n    0.231 \n    NA \n    -0.5 \n  \n  Unknown\n\n    O \n    17 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    DK \n    2 \n    ? \n    0 \n    -0.154 \n    -0.154 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    FJZ \n    1 \n    ? \n    0 \n    0.846 \n    0.846 \n    0.846 \n    NA \n    -0.5 \n  \n  \n    K \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    KO \n    1 \n    ? \n    0 \n    -0.154 \n    -0.154 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==3)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q3 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==3)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q3 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #4\n[PLACEHOLDER — NOT YET CONSIDERED THIS QUESTION]\n\nQ4. Control Condition\n\n\n\nFigure 4.7: Q4—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==4)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q4 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q4 Control Condition :  Which shift(s) end at 4 pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    H \n     \n  \n  \n    Orthgonal \n    U \n    OF \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    BH \n     \n  \n  \n    Tversky [start diagonal] \n    B \n     \n  \n  \n    Tversky [end diagonal] \n    H \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #4 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 4 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 3) %>% \n  pack_rows(\"Orthogonal\", 4, 8) %>% \n  pack_rows(\"Other\", 9, 10) %>% \n  pack_rows(\"Unknown\", 11, 16) \n\n\n\nFrequency of Selected Response Options for Question #4 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    H \n    29 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.083 \n    1.0 \n  \n  \n    AH \n    1 \n    Triangular \n    0 \n    0.929 \n    0.929 \n    NA \n    -0.167 \n    1.0 \n  \n  Lines-Connect\n\n    B \n    3 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    NA \n    -0.083 \n    0.5 \n  \n  Orthogonal\n\n    U \n    87 \n    Orthogonal \n    0 \n    -0.071 \n    -0.071 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    FU \n    2 \n    Orthogonal \n    0 \n    -0.143 \n    -0.143 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    DEOU \n    1 \n    Orthogonal \n    0 \n    -0.286 \n    -0.286 \n    NA \n    0.833 \n    -1.0 \n  \n  \n    DEU \n    1 \n    Orthogonal \n    0 \n    -0.214 \n    -0.214 \n    NA \n    0.833 \n    -1.0 \n  \n  \n    KU \n    1 \n    Orthogonal \n    0 \n    -0.143 \n    -0.143 \n    NA \n    0.917 \n    -1.0 \n  \n  Other\n\n     \n    6 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n    ACFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.286 \n    0.286 \n    NA \n    0.333 \n    -0.5 \n  \n  Unknown\n\n    DE \n    14 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    NA \n    -0.167 \n    -0.5 \n  \n  \n    E \n    6 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    O \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    0.000 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    G \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n\n\n\n\n\n\nTBL4 test\n\n\n\n\n\n\nOrthogonal\nOrthogonal-LinesConnecting\n\n\n\n\n |\n\n\n\nIf the subject calculates end time for each data point (using duration on the y axis), they find that an (incorrect) projection of point U ‘end time’ intersects with the (incorrect) orthogonal projection of 4:00PM.\nAlternatively, some subjects selected points E and D which intersect with an orthogonal projection from 4:00pm. We call this an ’orthogonal-lines connect” strategy, because it (incorrectly) adapts the orthogonal procedure for finding events that start at 4:00pm in order to find those that end at 4:00pm, thus selecting any data point with an orthogonal intersection with 4:00pm.\n\n\n\n\n\nQ4. Impasse Condition\n\n\n\nFigure 4.8: Q4—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==4)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q4 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q4 Impasse Condition :  Which shift(s) end at 4 pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    H \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    FO \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    BH \n     \n  \n  \n    Tversky [start diagonal] \n    B \n     \n  \n  \n    Tversky [end diagonal] \n    H \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nTODO investigate D? add to tversky or orth?\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #4 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 4 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 6) %>% \n  pack_rows(\"Satisfice\", 7, 10) %>% \n  pack_rows(\"Other\", 11, 12) %>% \n  pack_rows(\"Unknown\", 13, 19) \n\n\n\nFrequency of Selected Response Options for Question #4 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    H \n    64 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    DH \n    1 \n    Triangular \n    0 \n    0.929 \n    0.929 \n    -0.154 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    B \n    6 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    BD \n    2 \n    Tversky \n    0 \n    -0.143 \n    0.929 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BH \n    2 \n    Tversky \n    0 \n    0.929 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BDEG \n    1 \n    Tversky \n    0 \n    -0.286 \n    0.786 \n    -0.308 \n    NA \n    0.5 \n  \n  Satisfice\n\n    O \n    11 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    F \n    8 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    FO \n    7 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AFG \n    1 \n    Satisfice \n    0 \n    -0.214 \n    -0.214 \n    0.346 \n    NA \n    -1.0 \n  \n  Other\n\n     \n    20 \n    blank \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n    ACFHIJKOUZ \n    1 \n    frenzy \n    0 \n    0.357 \n    0.357 \n    0.385 \n    NA \n    -0.5 \n  \n  Unknown\n\n    D \n    35 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    A \n    5 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    K \n    3 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    G \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    AI \n    1 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    DK \n    1 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    J \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==4)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q4 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==4)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q4 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #5\n\nQ5. Control Condition\n\n\n\nFigure 4.9: Q5—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==5)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q5 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q5 Control Condition :  Coffee breaks happen halfway through a shift. Which shift(s) share a break with I?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    O \n    AZ \n  \n  \n    Orthgonal \n    U \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    UGX \n    AZKD \n  \n  \n    Tversky [start diagonal] \n    X \n     \n  \n  \n    Tversky [end diagonal] \n    UG \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #5 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 5 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>% \n  pack_rows(\"Lines-Connect\", 5, 7) %>% \n  pack_rows(\"Orthogonal\", 8, 9) %>% \n  pack_rows(\"Other\", 10, 11) %>% \n  pack_rows(\"Unknown\", 12, 22) \n\n\n\nFrequency of Selected Response Options for Question #5 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    O \n    50 \n    Triangular \n    1 \n    1.000 \n    -0.077 \n    NA \n    -0.077 \n    1.0 \n  \n  \n    FO \n    3 \n    Triangular \n    0 \n    0.909 \n    -0.154 \n    NA \n    -0.154 \n    1.0 \n  \n  \n    HO \n    1 \n    Triangular \n    0 \n    0.909 \n    -0.154 \n    NA \n    -0.154 \n    1.0 \n  \n  \n    KO \n    1 \n    Triangular \n    0 \n    0.909 \n    -0.143 \n    NA \n    -0.154 \n    1.0 \n  \n  Lines-Connect\n\n    FG \n    2 \n    Tversky \n    0 \n    -0.182 \n    0.417 \n    NA \n    -0.154 \n    0.5 \n  \n  \n    G \n    1 \n    Tversky \n    0 \n    -0.091 \n    0.500 \n    NA \n    -0.077 \n    0.5 \n  \n  \n    X \n    1 \n    Tversky \n    0 \n    -0.091 \n    1.000 \n    NA \n    -0.077 \n    0.5 \n  \n  Orthogonal\n\n    U \n    64 \n    Orthogonal \n    0 \n    -0.091 \n    0.500 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    HU \n    1 \n    Orthogonal \n    0 \n    -0.182 \n    0.417 \n    NA \n    0.923 \n    -1.0 \n  \n  Other\n\n    I \n    1 \n    reference \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n     \n    6 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    F \n    10 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    H \n    3 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    B \n    2 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    DJ \n    2 \n    ? \n    0 \n    -0.182 \n    -0.143 \n    NA \n    -0.154 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.091 \n    0.000 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    DEHJ \n    1 \n    ? \n    0 \n    -0.364 \n    -0.308 \n    NA \n    -0.308 \n    -0.5 \n  \n  \n    FK \n    1 \n    ? \n    0 \n    -0.182 \n    -0.143 \n    NA \n    -0.154 \n    -0.5 \n  \n  \n    HJ \n    1 \n    ? \n    0 \n    -0.182 \n    -0.154 \n    NA \n    -0.154 \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    0.000 \n    0.000 \n    NA \n    -0.077 \n    -0.5 \n  \n\n\n\n\n\nTODO note the compelling cases of internal inconsistency (HJDE)\n\n\nQ5. Impasse Condition\n\n\n\nFigure 4.10: Q5—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==5)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q5 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q5 Impasse Condition :  Coffee breaks happen halfway through a shift. Which shift(s) share a break with I?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    A \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    K \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    OX \n     \n  \n  \n    Tversky [start diagonal] \n    OX \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #5 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 5 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 7) %>% \n  pack_rows(\"Lines-Connect\", 8, 13) %>% \n  pack_rows(\"Orthogonal\", 14, 16) %>% \n  pack_rows(\"Other\", 17, 21) %>% \n  pack_rows(\"Unknown\", 22, 31) \n\n\n\nFrequency of Selected Response Options for Question #5 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    A \n    83 \n    Triangular \n    1 \n    1.000 \n    -0.083 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    AFG \n    5 \n    Triangular \n    0 \n    0.846 \n    -0.250 \n    -0.231 \n    NA \n    1.0 \n  \n  \n    AF \n    4 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AO \n    2 \n    Triangular \n    0 \n    0.923 \n    0.417 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AI \n    1 \n    Triangular \n    1 \n    1.000 \n    -0.083 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    AU \n    1 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AZ \n    1 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    O \n    6 \n    Tversky \n    0 \n    -0.077 \n    0.500 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    CO \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.417 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    JO \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.417 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    OX \n    1 \n    Tversky \n    0 \n    -0.154 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    UXZ \n    1 \n    Tversky \n    0 \n    -0.231 \n    0.333 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    X \n    1 \n    Tversky \n    0 \n    -0.077 \n    0.500 \n    -0.077 \n    NA \n    0.5 \n  \n  Orthogonal\n\n    K \n    5 \n    Satisfice \n    0 \n    -0.077 \n    -0.083 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    HK \n    3 \n    Satisfice \n    0 \n    -0.154 \n    -0.167 \n    0.923 \n    NA \n    -1.0 \n  \n  \n    HKUZ \n    1 \n    Satisfice \n    0 \n    -0.308 \n    -0.333 \n    0.769 \n    NA \n    -1.0 \n  \n  Other\n\n    I \n    2 \n    reference \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n     \n    24 \n    blank \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n    ABCFGUZ \n    1 \n    frenzy \n    0 \n    0.538 \n    -0.583 \n    -0.538 \n    NA \n    -0.5 \n  \n  \n    ACDEFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.154 \n    0.167 \n    0.154 \n    NA \n    -0.5 \n  \n  \n    FHJKX \n    1 \n    frenzy \n    0 \n    -0.385 \n    0.167 \n    0.692 \n    NA \n    -0.5 \n  \n  Unknown\n\n    H \n    11 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    C \n    2 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    DJ \n    2 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    F \n    2 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    FU \n    2 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    DG \n    1 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    FHZ \n    1 \n    ? \n    0 \n    -0.231 \n    -0.250 \n    -0.231 \n    NA \n    -0.5 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==5)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q5 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==5)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q5 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\n\nTesting Phase\nThe following 10 questions were the same for both conditions.\n\nQuestion #6 NONDISCRIM\n\n\n\nFigure 4.11: Q6-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==6)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) are six hours long?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    EG \n     \n  \n  \n    Orthgonal \n    EG \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\nTODO discuss non discriminant\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #6\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 6) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) \n\n\n\nFrequency of Selected Response Options for Question #6\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  \n    EG \n    330 \n    both tri + orth \n    1 \n    1 \n    NA \n    NA \n    1 \n    0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==6)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q6 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==6)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q6 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #7\n\n\n\nFigure 4.12: Q7-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==7)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which 2 shifts less than 5 hours long start at the same time?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    OX \n     \n  \n  \n    Orthgonal \n    FB \n    M \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    IJZNCHOX \n     \n  \n  \n    Tversky [start diagonal] \n    OX \n     \n  \n  \n    Tversky [end diagonal] \n    IJZN \n     \n  \n  \n    Tversky [duration line] \n    CH \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #7\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 7) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 5) %>%\n  pack_rows(\"Lines-Connect\", 6, 9) %>%\n  pack_rows(\"Orthogonal\", 10, 13) %>%\n  pack_rows(\"Other\", 14, 14) %>%\n  pack_rows(\"Unknown\", 15, 17)\n\n\n\nFrequency of Selected Response Options for Question #7\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    OX \n    93 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MO \n    2 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    AX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MOX \n    1 \n    Triangular \n    0 \n    0.938 \n    0.938 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.067 \n    1.0 \n  \n  Lines-Connect\n\n    IJ \n    3 \n    Tversky \n    0 \n    -0.125 \n    0.500 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    CH \n    1 \n    Tversky \n    0 \n    -0.125 \n    1.000 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    DJNX \n    1 \n    Tversky \n    0 \n    0.312 \n    0.357 \n    NA \n    -0.267 \n    0.5 \n  \n  \n    HK \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.438 \n    NA \n    -0.133 \n    0.5 \n  \n  Orthogonal\n\n    BF \n    203 \n    Orthogonal \n    0 \n    -0.125 \n    -0.125 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    FZ \n    16 \n    Orthogonal \n    0 \n    -0.125 \n    0.179 \n    NA \n    0.433 \n    -1.0 \n  \n  \n    B \n    1 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    1 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    0.500 \n    -1.0 \n  \n  Other\n\n     \n    2 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    GK \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.125 \n    0.179 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    KM \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 7)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q7 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 7)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q7 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #8\n\n\n\nFigure 4.13: Q8-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==8)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q: \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q:  Which shift(s) under 7 hours long starts before B starts, and ends after X ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    G \n     \n  \n  \n    Orthgonal \n    E \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #8\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 8) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 10) %>%\n  pack_rows(\"Orthogonal\", 11, 16) %>%\n  pack_rows(\"Other\", 17, 21) %>%\n  pack_rows(\"Unknown\", 22, 45)\n\n\n\nFrequency of Selected Response Options for Question #8\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    G \n    64 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    -0.067 \n    1.0 \n  \n  \n    AGK \n    4 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1.0 \n  \n  \n    CG \n    3 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    FG \n    3 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    AG \n    2 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    CFGO \n    2 \n    Triangular \n    0 \n    0.800 \n    NA \n    NA \n    -0.267 \n    1.0 \n  \n  \n    ACGP \n    1 \n    Triangular \n    0 \n    0.800 \n    NA \n    NA \n    -0.267 \n    1.0 \n  \n  \n    CFG \n    1 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1.0 \n  \n  \n    CGM \n    1 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1.0 \n  \n  \n    GM \n    1 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  Orthogonal\n\n    E \n    157 \n    Orthogonal \n    0 \n    -0.067 \n    NA \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EIJ \n    5 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.867 \n    -1.0 \n  \n  \n    EFIJ \n    3 \n    Orthogonal \n    0 \n    -0.267 \n    NA \n    NA \n    0.800 \n    -1.0 \n  \n  \n    EF \n    2 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.933 \n    -1.0 \n  \n  \n    EI \n    2 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.933 \n    -1.0 \n  \n  \n    EFI \n    1 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.867 \n    -1.0 \n  \n  Other\n\n     \n    12 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    DEHIJNOZ \n    2 \n    frenzy \n    0 \n    -0.533 \n    NA \n    NA \n    0.533 \n    -0.5 \n  \n  \n    EFGIJ \n    2 \n    frenzy \n    0 \n    0.733 \n    NA \n    NA \n    0.733 \n    -0.5 \n  \n  \n    CDGHLNOXZ \n    1 \n    frenzy \n    0 \n    0.533 \n    NA \n    NA \n    -0.533 \n    -0.5 \n  \n  \n    DEIJN \n    1 \n    frenzy \n    0 \n    -0.333 \n    NA \n    NA \n    0.733 \n    -0.5 \n  \n  Unknown\n\n    IJ \n    17 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    I \n    7 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    EFG \n    3 \n    ? \n    0 \n    0.867 \n    NA \n    NA \n    0.867 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    O \n    3 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    A \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    AK \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    C \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DN \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    F \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    IJM \n    2 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    L \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    M \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CM \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    CX \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DHNZ \n    1 \n    ? \n    0 \n    -0.267 \n    NA \n    NA \n    -0.267 \n    -0.5 \n  \n  \n    DIJN \n    1 \n    ? \n    0 \n    -0.267 \n    NA \n    NA \n    -0.267 \n    -0.5 \n  \n  \n    EFGI \n    1 \n    ? \n    0 \n    0.800 \n    NA \n    NA \n    0.800 \n    -0.5 \n  \n  \n    HLO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    IO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    KL \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 8)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q8 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 8)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q8 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #9 NONDISCRIM\n\n\n\nFigure 4.14: Q9-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==9)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) begins before J begins and ends during B?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    I \n     \n  \n  \n    Orthgonal \n    I \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #9\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q ==9) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Other\", 1, 2) %>%\n  pack_rows(\"Unknown\", 3, 19)\n\n\n\nFrequency of Selected Response Options for Question #9\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Other\n\n    I \n    247 \n    both tri + orth \n    1 \n    1.000 \n    NA \n    NA \n    1.000 \n    0.5 \n  \n  \n    IJ \n    1 \n    both tri + orth \n    1 \n    1.000 \n    NA \n    NA \n    1.000 \n    0.5 \n  \n  Unknown\n\n     \n    23 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    E \n    29 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    F \n    6 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    M \n    4 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    EI \n    3 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    FI \n    3 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    AGN \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    B \n    1 \n    ? \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CHO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DK \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    IM \n    1 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    IO \n    1 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 9)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q9 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 9)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q9 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #10\n\n\n\nFigure 4.15: Q10-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==10)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) end at the same time as F?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    E \n     \n  \n  \n    Orthgonal \n    X \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    EGZ \n     \n  \n  \n    Tversky [start diagonal] \n    G \n     \n  \n  \n    Tversky [end diagonal] \n    E \n     \n  \n  \n    Tversky [duration line] \n    Z \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #10\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 10) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 7) %>%\n  pack_rows(\"Orthogonal\", 8, 11) %>%\n  pack_rows(\"Other\", 12, 14) %>%\n  pack_rows(\"Unknown\", 15, 27)\n\n\n\nFrequency of Selected Response Options for Question #10\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    E \n    103 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    EF \n    1 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  Lines-Connect\n\n    Z \n    23 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    XZ \n    2 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    0.938 \n    0.5 \n  \n  \n    CG \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    -0.125 \n    0.5 \n  \n  \n    G \n    1 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    HLPZ \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.812 \n    NA \n    -0.250 \n    0.5 \n  \n  Orthogonal\n\n    X \n    139 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BX \n    2 \n    Orthogonal \n    0 \n    -0.125 \n    -0.125 \n    NA \n    0.938 \n    -1.0 \n  \n  \n    FX \n    2 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    AMX \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    -0.188 \n    NA \n    0.875 \n    -1.0 \n  \n  Other\n\n    F \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n     \n    6 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    CEGIO \n    1 \n    frenzy \n    0 \n    0.750 \n    0.750 \n    NA \n    -0.312 \n    -0.5 \n  \n  Unknown\n\n    B \n    27 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    J \n    6 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    IJ \n    2 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    P \n    2 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    BO \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    HLP \n    1 \n    ? \n    0 \n    -0.188 \n    -0.188 \n    NA \n    -0.188 \n    -0.5 \n  \n  \n    I \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    K \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    L \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    O \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 10)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q10 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 10)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q10 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #11\n\n\n\nFigure 4.16: Q11-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==11)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) start at 12pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    ML \n     \n  \n  \n    Orthgonal \n    FB \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #11\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 11) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>%\n  pack_rows(\"Orthogonal\", 5, 9) %>%\n  pack_rows(\"Other\", 10, 12) %>%\n  pack_rows(\"Unknown\", 13, 17)\n\n\n\nFrequency of Selected Response Options for Question #11\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    LM \n    99 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    -0.125 \n    1.0 \n  \n  \n    M \n    7 \n    Triangular \n    0 \n    0.500 \n    NA \n    NA \n    -0.062 \n    1.0 \n  \n  \n    BLM \n    2 \n    Triangular \n    0 \n    0.938 \n    NA \n    NA \n    0.375 \n    1.0 \n  \n  \n    EKM \n    1 \n    Triangular \n    0 \n    0.375 \n    NA \n    NA \n    -0.188 \n    1.0 \n  \n  Orthogonal\n\n    BF \n    201 \n    Orthogonal \n    0 \n    -0.125 \n    NA \n    NA \n    1.000 \n    -1.0 \n  \n  \n    B \n    4 \n    Orthogonal \n    0 \n    -0.062 \n    NA \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    2 \n    Orthogonal \n    0 \n    -0.062 \n    NA \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BFXZ \n    1 \n    Orthogonal \n    0 \n    -0.250 \n    NA \n    NA \n    0.875 \n    -1.0 \n  \n  \n    BH \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    NA \n    NA \n    0.438 \n    -1.0 \n  \n  Other\n\n     \n    4 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    ACDGHKLMNOPXZ \n    1 \n    frenzy \n    0 \n    0.312 \n    NA \n    NA \n    -0.812 \n    -0.5 \n  \n  \n    DHLMNOXZ \n    1 \n    frenzy \n    0 \n    0.625 \n    NA \n    NA \n    -0.500 \n    -0.5 \n  \n  Unknown\n\n    J \n    2 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    CX \n    1 \n    ? \n    0 \n    -0.125 \n    NA \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    XZ \n    1 \n    ? \n    0 \n    -0.125 \n    NA \n    NA \n    -0.125 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 11)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q11 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 11)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q11 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #12\n\n\n\nFigure 4.17: Q12-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==12)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) start at the same time as F?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    G \n     \n  \n  \n    Orthgonal \n    B \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    GZ \n     \n  \n  \n    Tversky [start diagonal] \n    G \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n    Z \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #12\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 12) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 3) %>%\n  pack_rows(\"Lines-Connect\", 4, 6) %>%\n  pack_rows(\"Orthogonal\", 7, 8) %>%\n  pack_rows(\"Other\", 9, 10) %>%\n  pack_rows(\"Unknown\", 11, 14)\n\n\n\nFrequency of Selected Response Options for Question #12\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    G \n    98 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    FG \n    3 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    GP \n    1 \n    Triangular \n    0 \n    0.938 \n    0.938 \n    NA \n    -0.125 \n    1.0 \n  \n  Lines-Connect\n\n    Z \n    4 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    BZ \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    0.938 \n    0.5 \n  \n  \n    B \n    206 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  Orthogonal\n\n    BF \n    5 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n     \n    3 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Other\n\n    CEGIO \n    1 \n    frenzy \n    0 \n    0.750 \n    0.750 \n    NA \n    -0.312 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  Unknown\n\n    E \n    2 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    FM \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 12)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q12 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 12)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q12 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #13\n\n\n\nFigure 4.18: Q13-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==13)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which 2 shifts end when Z begins?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    EF \n     \n  \n  \n    Orthgonal \n    FX \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #13\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 13) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 3) %>%\n  pack_rows(\"Orthogonal\", 4, 13) %>%\n  pack_rows(\"Other\", 14, 14) %>%\n  pack_rows(\"Unknown\", 15, 36)\n\n\n\nFrequency of Selected Response Options for Question #13\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    EF \n    91 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    0.433 \n    1.0 \n  \n  \n    CE \n    1 \n    Triangular \n    0 \n    0.433 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    E \n    1 \n    Triangular \n    0 \n    0.500 \n    NA \n    NA \n    -0.067 \n    1.0 \n  \n  Orthogonal\n\n    FX \n    141 \n    Orthogonal \n    0 \n    0.433 \n    NA \n    NA \n    1.000 \n    -1.0 \n  \n  \n    X \n    9 \n    Orthogonal \n    0 \n    -0.067 \n    NA \n    NA \n    0.500 \n    -1.0 \n  \n  \n    OX \n    4 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    KX \n    3 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    ACX \n    1 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.367 \n    -1.0 \n  \n  \n    BX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    CX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    DJNX \n    1 \n    Orthogonal \n    0 \n    -0.267 \n    NA \n    NA \n    0.300 \n    -1.0 \n  \n  \n    GX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    JX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  Other\n\n     \n    5 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    HN \n    13 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    BF \n    11 \n    ? \n    0 \n    0.433 \n    NA \n    NA \n    0.433 \n    -0.5 \n  \n  \n    F \n    10 \n    ? \n    0 \n    0.500 \n    NA \n    NA \n    0.500 \n    -0.5 \n  \n  \n    EX \n    6 \n    ? \n    0 \n    0.433 \n    NA \n    NA \n    0.433 \n    -0.5 \n  \n  \n    HL \n    5 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    HLP \n    5 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    BM \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    CO \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    DN \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    AG \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    BO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CG \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    CGO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    CH \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DKM \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    HZ \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    LP \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    NO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    NZ \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 13)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q13 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 13)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q13 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #14\n\n\n\nFigure 4.19: Q14-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==14)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) end at 3pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    X \n     \n  \n  \n    Orthgonal \n    B \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    XJND \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n    X \n     \n  \n  \n    Tversky [duration line] \n    JND \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #14\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 14) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>%\n  pack_rows(\"Orthogonal\", 5, 7) %>%\n  pack_rows(\"Other\", 8, 9) %>%\n  pack_rows(\"Unknown\", 10, 22)\n\n\n\nFrequency of Selected Response Options for Question #14\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    X \n    107 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.059 \n    1.0 \n  \n  \n    FX \n    2 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  \n    EX \n    1 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  \n    OX \n    1 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  Orthogonal\n\n    B \n    150 \n    Orthogonal \n    0 \n    -0.059 \n    -0.059 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BF \n    12 \n    Orthogonal \n    0 \n    -0.118 \n    -0.118 \n    NA \n    0.941 \n    -1.0 \n  \n  \n    BIO \n    2 \n    Orthogonal \n    0 \n    -0.176 \n    -0.176 \n    NA \n    0.882 \n    -1.0 \n  \n  Other\n\n     \n    29 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n    O \n    5 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  Unknown\n\n    F \n    3 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    G \n    3 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    A \n    2 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    BX \n    2 \n    ? \n    0 \n    0.941 \n    0.941 \n    NA \n    0.941 \n    -0.5 \n  \n  \n    HLP \n    2 \n    ? \n    0 \n    -0.176 \n    -0.176 \n    NA \n    -0.176 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    AH \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    DHO \n    1 \n    ? \n    0 \n    -0.176 \n    0.200 \n    NA \n    -0.176 \n    -0.5 \n  \n  \n    FG \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    HL \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    IJ \n    1 \n    ? \n    0 \n    -0.118 \n    0.267 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    M \n    1 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    P \n    1 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 14)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q14 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 14)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q14 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #15\n\n\n\nFigure 4.20: Q15-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==15)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Coffee breaks happen halfway through a shift. Which shifts share a break at 2pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    XK \n     \n  \n  \n    Orthgonal \n    EF \n    B \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    XKZ \n     \n  \n  \n    Tversky [start diagonal] \n    Z \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #15\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 15) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 10) %>%\n  pack_rows(\"Lines-Connect\", 11, 13) %>%\n  pack_rows(\"Orthogonal\", 14, 22) %>%\n  pack_rows(\"Other\", 23, 23) %>%\n  pack_rows(\"Unknown\", 24, 44)\n\n\n\nFrequency of Selected Response Options for Question #15\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    KX \n    100 \n    Triangular \n    1 \n    1.000 \n    0.667 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    X \n    6 \n    Triangular \n    0 \n    0.500 \n    0.333 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    CX \n    2 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    DJNX \n    2 \n    Triangular \n    0 \n    0.312 \n    0.133 \n    NA \n    -0.267 \n    1.0 \n  \n  \n    AKPX \n    1 \n    Triangular \n    0 \n    0.875 \n    0.533 \n    NA \n    -0.267 \n    1.0 \n  \n  \n    CK \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    GK \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    JX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    K \n    1 \n    Triangular \n    0 \n    0.500 \n    0.333 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    LX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  Lines-Connect\n\n    FZ \n    3 \n    Tversky \n    0 \n    -0.125 \n    0.941 \n    NA \n    0.433 \n    0.5 \n  \n  \n    OZ \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.941 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    Z \n    1 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.067 \n    0.5 \n  \n  Orthogonal\n\n    EF \n    118 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BF \n    17 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    13 \n    Orthogonal \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    E \n    8 \n    Orthogonal \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BE \n    4 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BEF \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    -0.176 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EFZ \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    0.882 \n    NA \n    0.933 \n    -1.0 \n  \n  \n    EI \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.433 \n    -1.0 \n  \n  \n    FI \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.433 \n    -1.0 \n  \n  Other\n\n     \n    11 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    G \n    4 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    B \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.000 \n    -0.5 \n  \n  \n    C \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    O \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    AG \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    BM \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CG \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    M \n    2 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    A \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    BG \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DN \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    FK \n    1 \n    ? \n    0 \n    0.438 \n    0.267 \n    NA \n    0.433 \n    -0.5 \n  \n  \n    FX \n    1 \n    ? \n    0 \n    0.438 \n    0.267 \n    NA \n    0.433 \n    -0.5 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    HN \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    HO \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    I \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    IJ \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    J \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    L \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 15)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q15 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 15)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q15 \") + \n  theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#export",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#export",
    "title": "4  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC3A/data/2-scored-data/sgc3a_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC3A/data/2-scored-data/sgc3a_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures\n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#resources",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#resources",
    "title": "4  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\nset operations\nhttps://stat.ethz.ch/R-manual/R-devel/library/base/html/sets.html\nkableExtra tables\nhttps://haozhu233.github.io/kableExtra/awesome_table_in_html.html#grouped_columns__rows\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1          stringr_1.4.0          dplyr_1.0.9           \n [4] purrr_0.3.4            readr_2.1.2            tidyr_1.2.0           \n [7] tibble_3.1.7           tidyverse_1.3.1        statsExpressions_1.3.2\n[10] ggstatsplot_0.9.3      Hmisc_4.7-0            Formula_1.2-4         \n[13] survival_3.3-1         lattice_0.20-45        pbapply_1.5-0         \n[16] ggformula_0.10.1       ggridges_0.5.3         scales_1.2.0          \n[19] ggstance_0.3.5         ggplot2_3.3.6          kableExtra_1.3.4      \n\nloaded via a namespace (and not attached):\n  [1] TH.data_1.1-1       colorspace_2.0-3    rio_0.5.29         \n  [4] ellipsis_0.3.2      estimability_1.4    htmlTable_2.4.0    \n  [7] fs_1.5.2            parameters_0.18.1   base64enc_0.1-3    \n [10] rstudioapi_0.13     farver_2.1.0        ggrepel_0.9.1      \n [13] bit64_4.0.5         fansi_1.0.3         mvtnorm_1.1-3      \n [16] lubridate_1.8.0     xml2_1.3.3          codetools_0.2-18   \n [19] splines_4.2.1       knitr_1.39          polyclip_1.10-0    \n [22] zeallot_0.1.0       jsonlite_1.8.0      broom_0.8.0        \n [25] cluster_2.1.3       dbplyr_2.2.1        png_0.1-7          \n [28] effectsize_0.7.0    ggforce_0.3.3       compiler_4.2.1     \n [31] httr_1.4.3          emmeans_1.7.5       backports_1.4.1    \n [34] assertthat_0.2.1    Matrix_1.4-1        fastmap_1.1.0      \n [37] cli_3.3.0           tweenr_1.0.2        htmltools_0.5.2    \n [40] tools_4.2.1         coda_0.19-4         gtable_0.3.0       \n [43] glue_1.6.2          Rcpp_1.0.8.3        cellranger_1.1.0   \n [46] vctrs_0.4.1         svglite_2.1.0       insight_0.18.0     \n [49] xfun_0.31           openxlsx_4.2.5      rvest_1.0.2        \n [52] lifecycle_1.0.1     mosaicCore_0.9.0    MASS_7.3-57        \n [55] zoo_1.8-10          vroom_1.5.7         hms_1.1.1          \n [58] parallel_4.2.1      sandwich_3.0-2      rematch2_2.1.2     \n [61] RColorBrewer_1.1-3  prismatic_1.1.0     curl_4.3.2         \n [64] yaml_2.3.5          gridExtra_2.3       labelled_2.9.1     \n [67] rpart_4.1.16        latticeExtra_0.6-29 stringi_1.7.6      \n [70] highr_0.9           paletteer_1.4.0     bayestestR_0.12.1  \n [73] checkmate_2.1.0     zip_2.2.0           boot_1.3-28        \n [76] rlang_1.0.3         pkgconfig_2.0.3     systemfonts_1.0.4  \n [79] evaluate_0.15       labeling_0.4.2      patchwork_1.1.1    \n [82] htmlwidgets_1.5.4   bit_4.0.4           tidyselect_1.1.2   \n [85] plyr_1.8.7          magrittr_2.0.3      R6_2.5.1           \n [88] generics_0.1.2      multcomp_1.4-19     DBI_1.1.3          \n [91] pillar_1.7.0        haven_2.5.0         foreign_0.8-82     \n [94] withr_2.5.0         datawizard_0.4.1    nnet_7.3-17        \n [97] performance_0.9.1   modelr_0.1.8        crayon_1.5.1       \n[100] utf8_1.2.2          correlation_0.8.1   tzdb_0.3.0         \n[103] rmarkdown_2.14      jpeg_0.1-9          readxl_1.4.0       \n[106] grid_4.2.1          data.table_1.14.2   reprex_2.0.1       \n[109] digest_0.6.29       webshot_0.5.3       xtable_1.8-4       \n[112] munsell_0.5.0       viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#archive",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#archive",
    "title": "4  Response Scoring",
    "section": "ARCHIVE",
    "text": "ARCHIVE\nPrior versions of functions for for-loop version of scoring, not optimized to use mapply\n\n\nCODE\n# #CALCULATE THE TRIANGULAR, ORTHOGONAL OR TVERSKIAN SUBSCORES FROM KEYFRAME\n# calc_sub_score <- function(question, cond, response,keyframe){\n# \n#   #STEP 1 GET KEY\n#   if (question < 6) #for q1 - q5 find key for question by condition\n#   {\n#     # print(keyframe)\n#     #GET KEY FOR THIS SCORE TYPE, QUESTION AND CONDITION\n#     p =  keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#     q =  keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% unlist()\n#     pn = keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(n_p)\n#     qn = keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(n_q)\n# \n#   } else {\n#     #GET KEY FOR THIS SCORE TYPE, QUESTION\n#     p =  keyframe %>% filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#     q =  keyframe %>% filter(Q == question) %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% unlist()\n#     pn = keyframe %>% filter(Q == question) %>% select(n_p)\n#     qn = keyframe %>% filter(Q == question) %>% select(n_q)\n#   }\n# \n#   #STEP 2 CALC INTERSECTIONS BETWEEN RESPONSE AND KEY\n#   \n#   #if response is not empty, split apart response for set comparison\n#     if(response != \"\")\n#     { response = response %>% str_split(\"\") %>% unlist()}\n#     \n#   ps = length(intersect(response,p))\n#   qs = length(intersect(response,q))\n#   # df_items[x,'tri_ps'] = tri_ps\n#   # df_items[x,'tri_qs'] = tri_qs\n# \n#   #STEP 3 CALC f_partialP schema SCORE FOR THIS INTERSECTION\n#   x = f_partialP(ps,pn,qs,qn) %>% unlist() %>% as.numeric()\n#   \n#   #cleanup\n#   rm(p,q,pn,qn,ps,qs)\n#   return(x)\n# \n# }\n# \n# #CALCULATE THE REFERENCE SCORES\n# calc_ref_score <- function(question, cond, response){\n#   \n#     #1. GET reference point from REF_POINT column in raw keys\n#     ref_p = keys_raw %>% filter(Q == question) %>% filter(condition == cond) %>% select(REF_POINT) %>% pull(REF_POINT) %>% str_split(\"\") %>% unlist()\n#      \n#     #2. if response has more than one character, it can't be correct\n#     #there is only ever 1 reference character\n#     n = nchar(response)\n#     if (n == 0) {x = 0}\n#     else if(n>1) {x = 0}\n#     else {\n#       #3 is the response PRECISELY the REFERENCE POINT?\n#       x = ref_p == response\n#       x = as.numeric(x)  \n#     }\n#     \n#     #cleanup\n#     rm(ref_p, response, question, cond)   \n#     return(x) #1 = match, 0 = not match\n# }\n# \n# \n# #CALCULATE SCORE BASED ON UNION OF ORTH & TRI (SUBJECT SELECTS BOTH ANSWERS )\n# calc_both_score <- function(question, cond, response){\n#   \n#TRAPDOOR \n#   #since no orth responses exist for impasse condition q1 - q5, set to 0\n#   if (question < 6 & cond == 121) {x = NA}\n#   \n#   #ELSE \n#   #calculate union of ORTH and TRI\n#   else {\n#     if (question < 6 & cond == 111) #for q1 - q5 find key for question by condition\n#   {\n#      #grab the tri and orth keys for this question as well as N option set\n#      tri_p =  keys_tri %>%  filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      orth_p = keys_orth %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      set_n =  keys_tri %>%  filter(Q == question) %>% filter(condition == cond) %>% select(set_n) %>% pull(set_n) %>% str_split(\"\") %>% unlist() \n#      #1. calc answer that is both tri and orth and only these --> union of tri_p and orth_p\n#      both_p = union(tri_p, orth_p) #the selection of tri and p\n#      #2. calc answers that should't be selected as diffrence between N [same for all keys] and both_p\n#      both_q = setdiff(set_n,both_p)\n#      both_pn = length(both_p)\n#      both_qn = length(both_q)\n#   } else{\n#     \n#      #grab the tri and orth keys for this question as well as N option set\n#      tri_p =  keys_tri %>%  filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      orth_p = keys_orth %>% filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      set_n =  keys_tri %>%  filter(Q == question) %>% select(set_n) %>% pull(set_n) %>% str_split(\"\") %>% unlist() \n#      #1. calc answer that is both tri and orth and only these --> union of tri_p and orth_p\n#      both_p = union(tri_p, orth_p) #the selection of tri and p\n#      #2. calc answers that shouldn't be selected as difference between N [same for all keys] and both_p\n#      both_q = setdiff(set_n,both_p)\n#      both_pn = length(both_p)\n#      both_qn = length(both_q)\n#   }\n#     \n#   #STEP 2 CALC INTERSECTIONS BETWEEN RESPONSE AND KEY\n#   \n#   #if response is not empty, split apart response for set comparison\n#     if(response != \"\")\n#     { response = response %>% str_split(\"\") %>% unlist()}\n#   \n#     both_ps = length(intersect(response,both_p))\n#     both_qs = length(intersect(response,both_q))\n#   \n#  \n#   #STEP 3 CALC f_partialP schema SCORE FOR THIS INTERSECTION \n#   x = f_partialP(both_ps,both_pn,both_qs,both_qn)%>% unlist() %>% as.numeric()\n#   \n#   #cleanup\n#   rm(both_p,both_q,both_pn,both_qn,both_ps,both_qs, question, cond, response )   \n#   }\n#   \n#   return(x) #true correct, trues, false correct, falses\n# }\n\n\nLooping to do the scoring (not using MAPPLY)\n\n\nCODE\n#RUN THIS OR THE CALCULATE-SCORES-MAPPLY\n# df_items = trad \n# \n# pb <- timerProgressBar() \n# on.exit(close(pb)) \n#  \n# #CALCULATE SUBSCORES (in loop)\n# \n# for (x in 1:nrow(df_items)) {\n#   \n#   #show progress bar \n#   setTimerProgressBar(pb, x) \n#   \n#   #PREPARE ITEMS FOR SCORING\n#   #sort response vectors alphabetically\n#   #doesn't impact scoring, but does impact response display tables\n#    df_items[x,'response'] <-  df_items[x,'response'] %>% str_split(\"\") %>% unlist() %>% sort() %>% str_c(collapse=\"\")\n# \n#   #get properties of the RESPONSE ITEM\n#   qu = df_items[x,'q'] %>% as.numeric()\n#   cond = as.character(df_items[x,'condition']) %>% as.numeric()\n#   r = df_items[x,'response'] \n# \n#   #calculate the main subscores\n#   df_items[x,'score_TRI'] = calc_sub_score(qu, cond, r,keys_tri)\n#   df_items[x,'score_ORTH'] = calc_sub_score(qu, cond, r,keys_orth)\n#   df_items[x,'score_SATISFICE'] = calc_sub_score(qu, cond, r,keys_satisfice)\n#   df_items[x,'score_TV_max'] = calc_sub_score(qu, cond, r,keys_tversky_max)\n#   df_items[x,'score_TV_start'] = calc_sub_score(qu, cond, r,keys_tversky_start)\n#   df_items[x,'score_TV_end'] = calc_sub_score(qu, cond, r,keys_tversky_end)\n#   df_items[x,'score_TV_duration'] = calc_sub_score(qu, cond, r, keys_tversky_duration)\n#   \n#   #calculate special subscores\n#   df_items[x,'score_REF'] = calc_ref_score(qu, cond, r)\n#   df_items[x,'score_BOTH'] = calc_both_score(qu, cond, r)\n# }\n# \n# #CALCULATE ABSOLUTE SCORES\n# #calculate absolute scores dichotomous\n# df_items$score_ABS = as.integer(df_items$correct)\n# #niceABS indicates if the response is correct without penalizing the allowable triangular options(ie. the ref point)\n# df_items$score_niceABS  <- as.integer((df_items$score_TRI == 1))\n#  \n# #cleanup\n# rm(qu,cond,r, x)\n\n# trad_scored = df_items\n\n\nsanity check equivalence of for-loop and mapply scoring\n\n\nCODE\n#CHECK EQUIVALENCE OF LOOP AND MAPPLY SCORING \n# tests = data.frame (\n#   alt_tri = alt_scored$score_TRI,\n#   trad_tri = trad_scored$score_TRI,\n#   alt_orth = alt_scored$score_ORTH,\n#   trad_orth = trad_scored$score_ORTH,\n#   alt_ref = alt_scored$score_REF,\n#   trad_ref = trad_scored$score_REF,\n#   alt_tv_max = alt_scored$score_TV_max,\n#   trad_tv_max = trad_scored$score_TV_max,\n#   alt_tv_dur = alt_scored$score_TV_duration,\n#   trad_tv_dur = trad_scored$score_TV_duration,\n#   alt_tv_start = alt_scored$score_TV_start,\n#   trad_tv_start = trad_scored$score_TV_start,\n#   alt_tv_end = alt_scored$score_TV_end,\n#   trad_tv_end = trad_scored$score_TV_end,\n#   alt_both = alt_scored$score_BOTH,\n#   trad_both = trad_scored$score_BOTH,\n#   trad_response = trad_scored$response,\n#   alt_response = alt_scored$response,\n#   q_match = trad_scored$q == alt_scored$q,\n#   q = trad_scored$q,\n#   c_match = trad_scored$condition == alt_scored$condition,\n#   condition = trad_scored$condition\n# )\n# \n# tests$tri = tests$alt_tri == tests$trad_tri\n# tests$orth = tests$alt_orth == tests$trad_orth\n# tests$ref = tests$alt_ref == tests$trad_ref\n# tests$tvdur = tests$alt_tv_dur == tests$trad_tv_dur\n# tests$tvstart = tests$alt_tv_start == tests$trad_tv_start\n# tests$tvend = tests$alt_tv_end == tests$trad_tv_end\n# tests$both = tests$alt_both == tests$trad_both\n# \n# #CHECKS \n# unique(tests$tri)\n# unique(tests$orth)\n# unique(tests$ref)\n# unique(tests$tvdur)\n# unique(tests$tvstart)\n# unique(tests$tvend)\n# unique(tests$both)\n# \n# unique(alt_scored$score_ABS == trad_scored$score_ABS)\n# unique(alt_scored$score_niceABS == trad_scored$score_niceABS)\n\n\nPrior inline version of derive interpretation, before externalizing to a function in the scoring script.\n\n\nCODE\n# threshold_range = 0.5 #set required variance in subscores to be discriminant\n# threshold_frenzy = 4\n# \n# for (x in 1:nrow(df_items)) {\n#   \n#   #CALCULATE MAX TVERSKY SUBSCORE\n#   t = df_items[x,] %>% select(score_TV_max, score_TV_start, score_TV_end, score_TV_duration) #reshape\n#   t.long = gather(t,score, value, 1:4)\n#   t.long[t.long == \"\"] = NA #replace empty scores with NA so we can ignore them\n#   if(length(unique(t.long$value)) == 1 ){\n#     if(is.na(unique(t.long$value))){\n#       df_items[x,'score_TVERSKY'] = NA\n#       df_items[x,'tv_type'] = NA   \n#     }\n#   } else {\n#     df_items[x,'score_TVERSKY'] = as.numeric(max(t.long$value,na.rm = TRUE))\n#     df_items[x,'tv_type'] = t.long[which.max(t.long$value),'score']\n#   }\n#   \n#   #CALCULATE MAX SATISFICING SUBSCORE\n#   t = df_items[x,] %>% select(score_SAT_left, score_SAT_right)\n#   t.long = gather(t,score, value, 1:2)\n#   t.long[t.long == \"\"] = NA #replace empty scores\n#   if(length(unique(t.long$value)) == 1 ){\n#     if(is.na(unique(t.long$value))){\n#       df_items[x,'score_SATISFICE'] = NA\n#       df_items[x,'sat_type'] = NA   \n#     }\n#   } else {\n#     df_items[x,'score_SATISFICE'] = as.numeric(max(t.long$value,na.rm = TRUE))\n#     df_items[x,'sat_type'] = t.long[which.max(t.long$value),'score']  \n#   }\n#   \n#   #NOW CALCULATE RANGE AMONG SUBSCORES\n#   #order of this selection matters in breaking ties! \n#   t = df_items[x,] %>% select(score_TRI, score_TVERSKY, score_SATISFICE, score_ORTH)\n#   t.long = gather(t,score, value, 1:4)\n#   t.long[t.long == \"\"] = NA\n#   \n#   df_items[x,'top_score'] = as.numeric(max(t.long$value,na.rm = TRUE))\n#   df_items[x,'top_type'] = t.long[which.max(t.long$value),'score']\n#   \n#   #calculate the range between highest and lowest scores \n#   r = as.numeric(range(t.long$value,na.rm = TRUE))\n#   r = diff(r)\n#   df_items[x,'range'] = r\n#   \n#   #DISCRIMINANT BETWEEN SUBSCORES TO PREDICT BEST FIT INTERPRETATION\n#   \n#   if (r < threshold_range) {\n#       #then we can't predict the interpretation, leave it as \"?\"\n#     df_items[x,'best'] = \"?\"\n#   } else {\n#       p =  df_items[x,'top_type']\n#       if (p == \"score_TRI\") {df_items[x,'best'] = \"Triangular\"\n#       } else if(p == \"score_ORTH\") {df_items[x,'best'] = \"Orthogonal\"\n#       } else if(p == \"score_TVERSKY\") {df_items[x,'best'] = \"Tversky\"\n#       } else if(p == \"score_SATISFICE\") {df_items[x,'best'] = \"Satisfice\"}\n#   }\n#   \n#   #CHECK SPECIAL SITUATIONS\n# \n#   #BOTH TRI AND ORTH?  \n#   if (!is.na(df_items[x,'score_BOTH'])) { #only check if both is not null\n#       if( df_items[x,'score_BOTH'] == 1) {\n#         df_items[x,'best'] = \"both tri + orth\"}\n#   }\n#   \n#   #IS BLANK?\n#   if( df_items[x,'num_o'] == 0) {  \n#     df_items[x,'best'] = \"blank\"\n#   }\n#   \n#   #IS FRENZY?\n#   if( df_items[x,'num_o'] > threshold_frenzy) { \n#       df_items[x,'best'] = \"frenzy\"\n#   }\n# \n#   #IS REF POINT?\n#   if (!is.na(df_items[x,'score_REF'])) { #only check if the score is NOT null\n#       if( df_items[x,'score_REF'] == 1) {\n#           df_items[x,'best'] = \"reference\"\n#       }\n#   }\n# \n# }#end loop\n# \n# #cleanup \n# rm(t, t.long, x, r,p)\n# rm(threshold_frenzy, threshold_range)\n# \n# #set order of levels for response exploration table\n# df_items$int2 <- factor(df_items$best,\n#                                   levels = c(\"Triangular\", \"Tversky\",\n#                                              \"Satisfice\", \"Orthogonal\", \"reference\", \"both tri + orth\", \"blank\",\"frenzy\",\"?\"))\n# \n# #set order of levels\n# df_items$interpretation <- factor(df_items$best,\n#                                   levels = c(\"Orthogonal\",\"Satisfice\", \"frenzy\",\"?\",\"reference\",\"blank\",\n#                                                \"both tri + orth\", \"Tversky\",\"Triangular\"))\n# \n# #collapsed representation of scale of interpretations\n# df_items$high_interpretation <- fct_collapse(df_items$interpretation,\n#   orthogonal = c(\"Satisfice\", \"Orthogonal\"),\n#   neg.trans = c(\"frenzy\",\"?\"),\n#   neutral = c(\"reference\",\"blank\"),\n#   pos.trans = c(\"Tversky\",\"both tri + orth\"),\n#   triangular = \"Triangular\"\n# ) \n# \n# #reorder levels\n# df_items$high_interpretation = factor(df_items$high_interpretation, levels= c(\"orthogonal\", \"neg.trans\",\"neutral\",\"pos.trans\",\"triangular\"))\n# \n# #cleanup \n# df_items <- df_items %>% dplyr::select(-best)\n# \n# #recode as numeric inase they are char \n# # df_items$score_TV_duration <- df_items$score_TV_duration %>% as.numeric()\n# # df_items$score_SATISFICE <- df_items$score_SATISFICE %>% as.numeric()\n\n\nOld inline calculation of score_SCALED before externalizing as function\n\n\nCODE\n# df_items$score_SCALED <- recode(df_items$interpretation,\n#                           \"Orthogonal\" = -1,\n#                           \"Satisfice\" = -1,\n#                           \"frenzy\" = -0.5,\n#                           \"?\" = -0.5,\n#                           \"reference\" = 0,\n#                           \"blank\" = 0, \n#                           \"both tri + orth\" = 0.5,\n#                           \"Tversky\" = 0.5,\n#                           \"Triangular\" = 1)\n\n\nOriginal summary by subject before externalizing as function\n\n\nCODE\n# #prep items\n# df_items <- df_items %>% mutate(\n#   tv_type = as.factor(tv_type),\n#   top_type = as.factor(top_type)\n# )\n# \n# #summarize SCORES and TIME by subject\n# subjects_summary <- df_items %>% filter(q %nin% c(6,9)) %>% group_by(subject) %>% dplyr::summarise (\n#   subject = as.character(subject),\n#   pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n#   s_TRI = sum(score_TRI,na.rm=TRUE),\n#   s_ORTH = sum(score_ORTH,na.rm=TRUE),\n#   s_TVERSKY = sum(score_TVERSKY,na.rm=TRUE),\n#   s_SATISFICE = sum(score_SATISFICE, na.rm=TRUE),\n#   s_REF = sum(score_REF,na.rm=TRUE),\n#   s_ABS = sum(score_ABS,na.rm=TRUE),\n#   s_NABS = sum(score_niceABS,na.rm=TRUE),\n#   s_SCALED = sum(score_SCALED,na.rm=TRUE),\n#   DV_percent_NABS = s_NABS/13,\n#   rt_m = sum(rt_s)/60,\n#   item_avg_rt = mean(rt_s),\n#   item_min_rt = min(rt_s),\n#   item_max_rt = max(rt_s),\n#   item_n_TRI = sum(interpretation == \"Triangular\"),\n#   item_n_ORTH = sum(interpretation == \"Orthogonal\"),\n#   item_n_TV = sum(interpretation == \"Tversky\"),\n#   item_n_SAT = sum(interpretation == \"Satisfice\"),\n#   item_n_OTHER = sum(interpretation %nin% c(\"Triangular\",\"Orthogonal\",\"Tversky\",\"Satisfice\")),\n#   item_n_POS = sum(high_interpretation == \"pos.trans\"),\n#   item_n_NEG = sum(high_interpretation == \"neg.trans\"),\n#   item_n_NEUTRAL = sum(high_interpretation == \"neutral\")\n# ) %>% arrange(subject) %>% slice(1L)\n# \n# #summarize first scaffold item of interest by subject\n# subjects_q1 <- df_items %>% filter(q == 1) %>% mutate(\n#   item_q1_NABS = score_niceABS,\n#   item_q1_SCALED = score_SCALED,\n#   item_q1_interpretation = interpretation,\n#   item_q1_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q1_NABS, item_q1_SCALED, item_q1_interpretation,item_q1_rt) %>% arrange(subject)\n# \n# #summarize last test item of interest by subject\n# subjects_q5 <- df_items %>% filter(q == 5) %>% mutate(\n#   item_q5_NABS = score_niceABS,\n#   item_q5_SCALED = score_SCALED,\n#   item_q5_interpretation = interpretation,\n#   item_q5_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q5_NABS, item_q5_SCALED, item_q5_interpretation,item_q5_rt) %>% arrange(subject)\n# \n# #summarize first test item of interest by subject\n# subjects_q7 <- df_items %>% filter(q == 7) %>% mutate(\n#   item_q7_NABS = score_niceABS,\n#   item_q7_interpretation = interpretation,\n#   item_q7_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q7_NABS, item_q7_interpretation,item_q7_rt) %>% arrange(subject)\n# \n# #summarize last test item of interest by subject\n# subjects_q15 <- df_items %>% filter(q == 15) %>% mutate(\n#   item_q15_NABS = score_niceABS,\n#   item_q15_interpretation = interpretation,\n#   item_q15_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q15_NABS, item_q15_interpretation,item_q15_rt) %>% arrange(subject)\n# \n# #summarize scaffold phase performance\n# subjects_scaffold <- df_items %>% filter(q<6)  %>% group_by(subject) %>% dplyr::summarise (\n#   item_scaffold_NABS = sum(score_niceABS),\n#   item_scaffold_SCALED = sum(score_SCALED),\n#   item_scaffold_rt = sum(rt_s)\n# )%>% dplyr::select(subject, item_scaffold_NABS, item_scaffold_SCALED, item_scaffold_rt) %>% arrange(subject)\n# \n# #summarize test phase performance\n# subjects_test <- df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% group_by(subject) %>% dplyr::summarise (\n#   item_test_NABS = sum(score_niceABS),\n#   item_test_SCALED = sum(score_SCALED),\n#   item_test_rt = sum(rt_s)\n# )%>% dplyr::select(subject, item_test_NABS, item_test_SCALED, item_test_rt) %>% arrange(subject)\n# \n# #import subjects\n# df_subjects <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n# \n# #SANITY CHECK SUBJECT ORDER BEFORE MERGE; BOTH SHOULD BE TRUE\n# unique(subjects_summary$subject == df_subjects$subject)\n# unique(subjects_summary$subject == subjects_q1$subject)\n# unique(subjects_summary$subject == subjects_q5$subject)\n# unique(subjects_summary$subject == subjects_q7$subject)\n# unique(subjects_summary$subject == subjects_q15$subject)\n# unique(subjects_summary$subject == subjects_scaffold$subject)\n# unique(subjects_summary$subject == subjects_test$subject)\n# \n# #CAREFULLY CHECK THIS — RELIES ON \n# x = merge(df_subjects, subjects_summary)\n# x = merge(x, subjects_q1)\n# x = merge(x, subjects_q5)\n# x = merge(x, subjects_q7)\n# x = merge(x, subjects_q15)\n# x = merge(x, subjects_scaffold)\n# x = merge(x, subjects_test)\n# df_subjects <- x %>% dplyr::select(-absolute_score) #drop absolute score from webapp that includes Q6 and Q9\n# \n# #cleanup\n# rm(subjects_q1, subjects_q5, subjects_q7, subjects_q15, subjects_scaffold, subjects_test, subjects_summary, x)\n\n\nSummarize Cummulative Progress versions before functionize\n\n\nCODE\n# #SUMMARIZE-CUMULATIVE ABSOLUTE PROGRESS\n# \n# \n# #filter for valid items\n# x <- df_items %>% filter(q %nin% c(6,9)) %>% dplyr::select(subject,mode, pretty_condition, q,score_niceABS) \n# \n# #pivot wider\n# wide <- x %>% pivot_wider(names_from=q, names_glue = \"q_{q}\", values_from = score_niceABS)\n# \n# #calc stepwise cumulative score\n# wide$c1 = wide$q_1\n# wide$c2 = wide$c1 + wide$q_2\n# wide$c3 = wide$c2 + wide$q_3\n# wide$c4 = wide$c3 + wide$q_4\n# wide$c5 = wide$c4 + wide$q_5\n# wide$c6 = wide$c5 + wide$q_7\n# wide$c7 = wide$c6 + wide$q_8\n# wide$c8 = wide$c7 + wide$q_10\n# wide$c9 = wide$c8 + wide$q_11\n# wide$c10 = wide$c9 + wide$q_12\n# wide$c11 = wide$c10 + wide$q_13\n# wide$c12 = wide$c11 + wide$q_14\n# wide$c13 = wide$c12 + wide$q_15\n# wide <- wide %>% dplyr::select(subject,mode, pretty_condition,c1,c2,c3,c4,c5,c6, c7,c8,c9, c10,c11,c12,c13)\n# \n# #lengthen \n# df_absolute_progress <- wide %>% pivot_longer(cols= c1:c13, names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n# df_absolute_progress$question <- as.integer(df_absolute_progress$question)\n# \n# \n# #cleanup \n# rm(x,wide)\n#   \n# # SUMMARIZE-CUMULATIVE SCALED PROGRESS\n# \n# #filter for valid items\n# x <- df_items %>% filter(q %nin% c(6,9)) %>% select(subject,mode, pretty_condition, q,score_SCALED)\n# \n# #pivot wider\n# wide <- x %>% pivot_wider(names_from=q, names_glue = \"q_{q}\", values_from = score_SCALED)\n# \n# #calc stepwise cumulative score\n# wide$c1 = wide$q_1\n# wide$c2 = wide$c1 + wide$q_2\n# wide$c3 = wide$c2 + wide$q_3\n# wide$c4 = wide$c3 + wide$q_4\n# wide$c5 = wide$c4 + wide$q_5\n# wide$c6 = wide$c5 + wide$q_7\n# wide$c7 = wide$c6 + wide$q_8\n# wide$c8 = wide$c7 + wide$q_10\n# wide$c9 = wide$c8 + wide$q_11\n# wide$c10 = wide$c9 + wide$q_12\n# wide$c11 = wide$c10 + wide$q_13\n# wide$c12 = wide$c11 + wide$q_14\n# wide$c13 = wide$c12 + wide$q_15\n# wide <- wide %>% select(subject,mode, pretty_condition,c1,c2,c3,c4,c5,c6, c7,c8,c9, c10,c11,c12,c13)\n# \n# #lengthen \n# df_scaled_progress <- wide %>% pivot_longer(cols= c1:c13, names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n# df_scaled_progress$question <- as.integer(df_scaled_progress$question)\n# \n# #cleanup \n# rm(x,wide)"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html",
    "href": "analysis/SGC3A/3_sgc3A_description.html",
    "title": "5  Description",
    "section": "",
    "text": "The purpose of this notebook is describe the distributions of dependent variables for Study SGC3A."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#sample",
    "href": "analysis/SGC3A/3_sgc3A_description.html#sample",
    "title": "5  Description",
    "section": "SAMPLE",
    "text": "SAMPLE\n\nData Collection\nData was initially collected (in person, SONA groups in computer lab) in Fall 2017. In Spring 2018, additional data were collected after small modifications were made to the experimental platform to increase the size of multiple-choice input buttons, and to add an additional free-response question following the main task block. In Fall 2021, the study was replicated using asynchronous, online SONA pool, with additional participants collected in Winter 2022.\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_subjects$pretty_mode, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    laboratory \n    62 \n    64 \n    126 \n  \n  \n    online-replication \n    96 \n    108 \n    204 \n  \n  \n    Sum \n    158 \n    172 \n    330 \n  \n\n\n\n\n\n\n\nParticipants\n\n\nCODE\n#Describe participants\nsubject.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(age) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% filter(mode == \"asynch\") %>% dplyr::select(age) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(age) %>% unlist() %>% favstats()\n) \n\nsubject.stats$percent.male <- c(\n  (df_lab %>%  filter(gender==\"Male\") %>% count())$n/count(df_lab) %>% unlist(),\n  (df_online %>% filter(gender==\"Male\") %>% count())$n/count(df_online) %>% unlist(),\n  (df_subjects %>% filter(gender==\"Male\") %>% count())$n/count(df_subjects) %>% unlist()\n)\n\nsubject.stats$percent.female <- c(\n  (df_lab %>%  filter(gender==\"Female\") %>% count())$n/count(df_lab) %>% unlist(),\n  (df_online %>% filter(gender==\"Female\") %>% count())$n/count(df_online) %>% unlist(),\n  (df_subjects %>% filter(gender==\"Female\") %>% count())$n/count(df_subjects) %>% unlist()\n)\n\nsubject.stats$percent.other <- c(\n  (df_lab %>%  filter(gender %nin% c(\"Female\",\"Male\")) %>% count())$n/count(df_lab) %>% unlist(),\n  (df_online %>% filter(gender %nin% c(\"Female\",\"Male\")) %>% count())$n/count(df_online) %>% unlist(),\n  (df_subjects %>% filter(gender %nin% c(\"Female\",\"Male\")) %>% count())$n/count(df_subjects) %>% unlist()\n)\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()%>% \n  footnote(general = \"Age in Years\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    percent.male \n    percent.female \n    percent.other \n  \n \n\n  \n    lab \n    18 \n    19 \n    20 \n    21 \n    33 \n    20.4 \n    2.12 \n    126 \n    0 \n    0.373 \n    0.619 \n    0.008 \n  \n  \n    online \n    18 \n    20 \n    20 \n    21 \n    31 \n    20.6 \n    2.00 \n    204 \n    0 \n    0.304 \n    0.672 \n    0.025 \n  \n  \n    combined \n    18 \n    19 \n    20 \n    21 \n    33 \n    20.5 \n    2.05 \n    330 \n    0 \n    0.330 \n    0.652 \n    0.018 \n  \n\n\nNote:   Age in Years\n\n\n\n\nFor in-person collection, 126 participants (37 % male, 62 % female, 1 % other) undergraduate STEM majors at a public American University participated in person in exchange for course credit (age: 18 - 33 years). Participants were randomly assigned to one of two experimental groups.\nFor online replication 204 participants (30 % male, 67 % female, 2 % other) undergraduate STEM majors at a public American University participated online, asynchronously in exchange for course credit (age: 18 - 31 years). Participants were randomly assigned to one of two experimental groups.\nCombined overall 330 participants (33 % male, 65 % female, 2 % other) undergraduate STEM majors at a public American University participated in exchange for course credit (age: 18 - 33 years)."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#response-accuracy",
    "href": "analysis/SGC3A/3_sgc3A_description.html#response-accuracy",
    "title": "5  Description",
    "section": "RESPONSE ACCURACY",
    "text": "RESPONSE ACCURACY\n\nSubject Level Scores\nSubject level scores summarize the the response accuracy by a particular participant across all discriminant items in the graph comprehension task.\n\nTest Phase Absolute Score\nRecall from Section 3.1.2.1 that the absolute score (following the dichotomous scoring approach) s_NABS indicates if the subject’s response for a particular item was perfectly correct: whether they selected all correct answer options and no others (excluding certain allowed exceptions, such as also selecting the data point referenced in the question). The absolute score for an individual item is either 0 or 1. When summarized across the entire set of discriminant items, the total absolute score for an individual subject ranges from [0,13]. When summarized across just the test phase (final items following scaffolding phase) scores for an individual subject range from [0,8]. First we examine performance on the test phase (final 8 questions, appears after scaffolding phase). This tells us how the participants perform after exposure to the 5 scaffolding questions (in the impasse condition).\n\n\nCODE\ntitle = \"Descriptive Statistics of TEST PHASE Response Accuracy (Total Absolute Score)\"\nabs.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_test_NABS) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(item_test_NABS) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(item_test_NABS) %>% unlist() %>% favstats()\n) \nabs.stats %>% kbl (caption = title) %>% kable_classic() %>% \n  footnote(general = \"# questions correct [0,8]\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of TEST PHASE Response Accuracy (Total Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    0 \n    0 \n    0 \n    6 \n    8 \n    2.53 \n    3.32 \n    126 \n    0 \n  \n  \n    online \n    0 \n    0 \n    0 \n    6 \n    8 \n    2.16 \n    3.19 \n    204 \n    0 \n  \n  \n    combined \n    0 \n    0 \n    0 \n    6 \n    8 \n    2.30 \n    3.24 \n    330 \n    0 \n  \n\n\nNote:   # questions correct [0,8]\n\n\n\n\nFor in person collection, total absolute scores in the TEST phase (n = 126) range from 0 to 8 with a mean score of (M = 2.53, SD = 3.32).\nFor online replication, (online) total absolute scores in the TEST phase (n = 204) range from 0 to 8 with a slightly lower mean score of (M = 2.16, SD = 3.19).\nWhen combined overall, total absolute accuracy scores in the TEST phase (n = 330) range from 0 to 8 with a slightly lower mean score of (M = 2.3, SD = 3.24).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL ABSOLUTE\n  gf_props(~item_test_NABS, data = df_subjects) + \n  labs(x = \"number of correct responses (test phase)\",\n       y = \"% of subjects\",\n       title = \"Distribution of TEST Absolute Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_test_NABS\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of TEST Absolute Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"Total Absolute Score (Test Phase)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_test_NABS,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_NABS),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_NABS, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbetweenstats(y = item_test_NABS, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(item_test_NABS)) + \n  stat_ecdf(geom = \"step\") +\n  facet_grid(pretty_condition~pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — TEST Absolute Score \",\n        x = \"Total Absolute Score (Test Phase) [0,8]\", \n        y = \"Cumulative Probability\")\n\n\n\n\n\nCODE\n#NOTE this is clobbered by the shift function imports; so I load those later\n\n\nVisual inspection of this distribution suggests it is not normal, and likely bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018). TODO REFERENCE\n\n\nCODE\nmultimode::modetest(df_subjects$item_test_NABS)\n\n\nWarning in multimode::modetest(df_subjects$item_test_NABS): A modification of\nthe data was made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$item_test_NABS\nExcess mass = 0.1, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$item_test_NABS, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$item_test_NABS,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$item_test_NABS, mod0 = n_modes, : If\nthe density function has an unbounded support, artificial modes may have been\ncreated in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is infact multimodal (m = 0.1, p < 0.001), with two identifiable modes at 0.013 and 7.894, and an antimode at 2.867.\n\n\n\n\n\n\nNote\n\n\n\nCondition appears (through visual inspection) to yield a positive influence on Total Absolute Score in the TEST Phase, across data collection modalities.\n\n\n\n\nTest Phase Absolute Percentage\nTest Phase Score converted to percentage.\n\n\nCODE\ntitle = \"Descriptive Statistics of TEST PHASE Response Accuracy (% Accuracy Absolute Score)\"\nabs.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(DV_ptest_NABS) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(DV_ptest_NABS) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(DV_ptest_NABS) %>% unlist() %>% favstats()\n) \nabs.stats %>% kbl (caption = title) %>% kable_classic() %>% \n  footnote(general = \"% questions correct [0 - 100%]\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of TEST PHASE Response Accuracy (% Accuracy Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    0 \n    0 \n    0 \n    0.75 \n    1 \n    0.316 \n    0.415 \n    126 \n    0 \n  \n  \n    online \n    0 \n    0 \n    0 \n    0.75 \n    1 \n    0.270 \n    0.399 \n    204 \n    0 \n  \n  \n    combined \n    0 \n    0 \n    0 \n    0.75 \n    1 \n    0.288 \n    0.405 \n    330 \n    0 \n  \n\n\nNote:   % questions correct [0 - 100%]\n\n\n\n\nFor in person collection, total absolute scores in the TEST phase (n = 126) range from 0 to 1 with a mean score of (M = 0.32, SD = 0.42).\nFor online replication, (online) total absolute scores in the TEST phase (n = 204) range from 0 to 1 with a slightly lower mean score of (M = 0.27, SD = 0.4).\nWhen combined overall, total absolute accuracy scores in the TEST phase (n = 330) range from 0 to 1 with a slightly lower mean score of (M = 0.29, SD = 0.41).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL ABSOLUTE\n  gf_props(~DV_ptest_NABS, data = df_subjects) + \n  labs(x = \"number of correct responses (test phase)\",\n       y = \"% of subjects\",\n       title = \"Distribution of TEST Absolute Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"DV_ptest_NABS\",\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\n\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\nCODE\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of TEST Absolute Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"Total Absolute Score (Test Phase)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = DV_ptest_NABS,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = DV_ptest_NABS),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = DV_ptest_NABS, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbetweenstats(y = DV_ptest_NABS, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(DV_ptest_NABS)) + \n  stat_ecdf(geom = \"step\") +\n  facet_grid(pretty_condition~pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — TEST Absolute Score \",\n        x = \"Total Absolute Score (Test Phase) [0,8]\", \n        y = \"Cumulative Probability\")\n\n\n\n\n\nCODE\n#NOTE this is clobbered by the shift function imports; so I load those later\n\n\n\n\nTest Phase Scaled Scores\nThe test phase scaled score s_SCALED summarizes the scaled score on the 8 strategy-discriminant questions in the test phase, for each subject. This score ranges from from -8 (all orthogonal) to 8 (all triangular). Recall that the s_SCALED score for an item is a numeric representation of the strategy-consistent response, scaled from -1 to +1 (see Section 4.1.4)\nMost importantly, the Scaled score gives us a way of quantitatively examining how correctly a participant interpreted the coordinate system across the entire set of items. It offers a more nuanced look into performance than absolute score.\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total Scaled Score)\"\nscaled.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_test_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(item_test_SCALED) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(item_test_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -8 \n    -8.0 \n    -6.00 \n    6 \n    8 \n    -2.11 \n    6.69 \n    126 \n    0 \n  \n  \n    online \n    -8 \n    -7.5 \n    -5.75 \n    5 \n    8 \n    -2.32 \n    6.29 \n    204 \n    0 \n  \n  \n    combined \n    -8 \n    -8.0 \n    -6.00 \n    6 \n    8 \n    -2.24 \n    6.43 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, TEST phase scaled scores (n = 126) range from -8 to 1 with a mean score of (M = -2.11, SD = 6.69).\nFor online replication, TEST phase scaled scores (n = 204) range from -8 to 8 with a slightly lower mean score of (M = -2.32, SD = 6.29).\nWhen combined overall, TEST phase scaled scores (n = 330) range from -8 to 8 with a slightly lower mean score of (M = -2.24, SD = 6.44).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL SCALED\ngf_props(~item_test_SCALED, data = df_subjects) +\n  labs(x = \"total scaled score (test phase)\",\n       y = \"% of subjects\",\n       title = \"Distribution of TEST Scaled Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_test_SCALED\",binwidth=1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) + \n  labs( title = \"Distribution of TEST Scaled Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"total scaled score (test phase)\", y = \"number of participants\") + \n theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_test_SCALED,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_SCALED),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_SCALED, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Scaled Score \",\n    x = \"Condition\", y = \"Total Scaled Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n#EASY STATS\nggbetweenstats(y = item_test_SCALED, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(item_test_SCALED)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Test Phase Scaled Score\",\n        x = \"Test Phase Scaled Score [-8,8]\", \n        y = \"Cumulative Probability\") \n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and perhaps perhaps bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018).\n\n\nCODE\nmultimode::modetest(df_subjects$item_test_SCALED)\n\n\nWarning in multimode::modetest(df_subjects$item_test_SCALED): A modification of\nthe data was made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$item_test_SCALED\nExcess mass = 0.2, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$item_test_SCALED, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$item_test_SCALED,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$item_test_SCALED, mod0 = n_modes, :\nIf the density function has an unbounded support, artificial modes may have been\ncreated in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is in fact multimodal (m = 0.1, p < 0.001), with two identifiable modes at -7.721 and 7.822, and an antimode at 1.93.\n\n\n\nFirst Item Scores\nNext we consider the response accuracy on just the first question of the graph comprehension task: a subject’s first exposure to the TM graph.\n\nFirst Item Absolute Score\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Lab)\"\nitem.contingency <- df_lab %>%  dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>%  addmargins(2) %>% prop.table(margin=2) %>% addmargins(1)\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Lab)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    0 \n    0.839 \n    0.703 \n    0.77 \n  \n  \n    1 \n    0.161 \n    0.297 \n    0.23 \n  \n  \n    Sum \n    1.000 \n    1.000 \n    1.00 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Online)\"\nitem.contingency <- df_online %>%  dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>%  addmargins(2) %>% prop.table(margin=2) %>% addmargins(1)\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Online)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    0 \n    0.875 \n    0.722 \n    0.794 \n  \n  \n    1 \n    0.125 \n    0.278 \n    0.206 \n  \n  \n    Sum \n    1.000 \n    1.000 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Combined)\"\nitem.contingency <- df_subjects %>%  dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>%  addmargins(2) %>% prop.table(margin=2) %>% addmargins(1)\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Combined)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    0 \n    0.861 \n    0.715 \n    0.785 \n  \n  \n    1 \n    0.139 \n    0.285 \n    0.215 \n  \n  \n    Sum \n    1.000 \n    1.000 \n    1.000 \n  \n\n\n\n\n\nAcross data collection sessions, first-item accuracy is consistent across experimental conditions. Incorrect answers are far more frequent (78%) than correct answers (22%). Accuracy is somewhat improved in the IMPASSE condition, with roughly 28% of all IMPASSE-condition questions answered correctly, compared to only 14% in the CONTROL condition.\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_props(~item_q1_NABS, data = df_subjects) +\n  labs(x = \"response accuracy\",\n       y = \"% subjects\",\n       title = \"Proportion of Correct Responses on First Item\",\n       subtitle=\"\")+\n  theme(legend.position = \"none\")+theme_ggdist()\n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_subjects %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(item_q1_NABS))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n  labs(x = \"response accuracy\",\n       title = \"Proportion of Correct Responses on First Item (by Modality and Condition)\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Proportion of Correct Responses on First Item\",\n            data = df_subjects, item_q1_NABS ~ pretty_condition, rot_labels=c(0,90,0,0), \n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\"))) \n\n\n\n\n\nCODE\n#STATSPLOT\nggbarstats(\n  x = item_q1_NABS,\n  y = pretty_condition, \n  data = df_subjects\n)\n\n\n\n\n\n\n\nFirst Item Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1. (note: we evaluate scaled_score on the first item rather than interpretation, because no orthogonal interpretation is available in the impasse condition)\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (First Item Scaled Score)\"\nfirstscaled.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats()\n) \nfirstscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (First Item Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -1 \n    -1 \n    -1 \n    0.5 \n    1 \n    -0.298 \n    0.849 \n    126 \n    0 \n  \n  \n    online \n    -1 \n    -1 \n    -1 \n    0.5 \n    1 \n    -0.287 \n    0.812 \n    204 \n    0 \n  \n  \n    combined \n    -1 \n    -1 \n    -1 \n    0.5 \n    1 \n    -0.291 \n    0.825 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, first item scaled scores (n = 126) range from -1 to 1 with a mean score of (M = -0.3, SD = 0.85).\nFor online replication, (online) first item scaled scores (n = 204) range from -1 to 1 with a slightly lower mean score of (M = -0.29, SD = 0.81).\nWhen combined overall, first item scaled scores (n = 330) range from -1 to 1 with a slightly lower mean score of (M = -0.29, SD = 0.83).\n\n\nCODE\n#GGFORMULA | PROPORTIONAL HISTOGRAM SUBJECT FIRST SCALED\ngf_props(~item_q1_SCALED, data = df_subjects) +\n  labs(x = \"scaled score (first item)\",\n       y = \"% of subjects\",\n       title = \"Distribution of First Item Scaled Score\",\n       subtitle = \"\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_q1_SCALED\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) + \n  labs( title = \"Distribution of First Item Scaled Score (by Mode and Condition)\",\n        subtitle =\"Impasse condition yields more intermediate scores (indicating uncertainty)\",\n        x = \"scaled score (firt item) \", y = \"number of participants\") + \n  theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_subjects %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(item_q1_SCALED))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n  labs(x = \"response accuracy\",\n       title = \"Type of Responses on First Item (by Modality and Condition)\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbarstats(\n  x = item_q1_SCALED,\n  y = pretty_condition, \n  data = df_subjects\n)\n\n\n\n\n\n\n\n\nInterpretation Scores\nNext we consider the the interpretations assigned to each response. For each response given by a participant to a question, we assign an interpretation label based on the interpretation the response most closely matches (see Section 3.2.3).\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Lab)\"\nitem.contingency <- df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(interpretation, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Lab)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.297 \n    0.116 \n    0.414 \n  \n  \n    Satisfice \n    0.000 \n    0.028 \n    0.028 \n  \n  \n    frenzy \n    0.002 \n    0.005 \n    0.007 \n  \n  \n    ? \n    0.026 \n    0.053 \n    0.079 \n  \n  \n    reference \n    0.001 \n    0.004 \n    0.005 \n  \n  \n    blank \n    0.008 \n    0.034 \n    0.042 \n  \n  \n    both tri + orth \n    0.060 \n    0.056 \n    0.116 \n  \n  \n    Tversky \n    0.004 \n    0.017 \n    0.021 \n  \n  \n    Triangular \n    0.094 \n    0.195 \n    0.288 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Online)\"\nitem.contingency <- df_items %>% filter(mode == \"asynch\") %>% dplyr::select(interpretation, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Online)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.260 \n    0.122 \n    0.382 \n  \n  \n    Satisfice \n    0.000 \n    0.024 \n    0.024 \n  \n  \n    frenzy \n    0.002 \n    0.001 \n    0.003 \n  \n  \n    ? \n    0.050 \n    0.066 \n    0.116 \n  \n  \n    reference \n    0.000 \n    0.002 \n    0.002 \n  \n  \n    blank \n    0.013 \n    0.055 \n    0.068 \n  \n  \n    both tri + orth \n    0.056 \n    0.061 \n    0.117 \n  \n  \n    Tversky \n    0.011 \n    0.023 \n    0.035 \n  \n  \n    Triangular \n    0.078 \n    0.175 \n    0.253 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Combined)\"\nitem.contingency <- df_items %>%  dplyr::select(interpretation, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Combined)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.274 \n    0.120 \n    0.394 \n  \n  \n    Satisfice \n    0.000 \n    0.025 \n    0.025 \n  \n  \n    frenzy \n    0.002 \n    0.003 \n    0.004 \n  \n  \n    ? \n    0.041 \n    0.061 \n    0.102 \n  \n  \n    reference \n    0.001 \n    0.002 \n    0.003 \n  \n  \n    blank \n    0.011 \n    0.047 \n    0.058 \n  \n  \n    both tri + orth \n    0.058 \n    0.059 \n    0.117 \n  \n  \n    Tversky \n    0.009 \n    0.021 \n    0.029 \n  \n  \n    Triangular \n    0.084 \n    0.183 \n    0.267 \n  \n  \n    Sum \n    0.479 \n    0.521 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_propsh(~interpretation, data = df_items, fill = ~pretty_condition) %>% \n  gf_facet_grid(pretty_condition~pretty_mode) +\n  labs(x = \"% of items\",\n       title = \"Proportion of Interpretations Across Items\",\n       subtitle=\"Impasse Condition yields shift from Orthogonal to alternative interpretations\")+\n  theme(legend.position = \"none\")\n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_items %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(interpretation))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n  labs(x = \"response accuracy\",\n       title = \"Response Types on All Items (by Modality and Condition)\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Proportion of Interpretations across Conditions\",\n            data = df_items, pretty_condition ~ interpretation, rot_labels=c(0,90,0,0),\n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\")))\n\n\n\n\n\nCODE\n#STATSPLOT\nggbarstats(\n  x = high_interpretation,\n  y = pretty_condition, \n  data = df_items\n)\n\n\n\n\n\n\n\nCumulative Task Performance\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#response-latency",
    "href": "analysis/SGC3A/3_sgc3A_description.html#response-latency",
    "title": "5  Description",
    "section": "RESPONSE LATENCY",
    "text": "RESPONSE LATENCY\n\nTime on Task\nHere we consider the time spent on the graph comprehension task portion of the study.\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_lab%>% dplyr::select(totaltime_m) %>% unlist()  %>%  favstats(),\n  \"online\"= df_online %>% dplyr::select(totaltime_m) %>% unlist() %>% favstats(),\n  \"combined\"= df_subjects %>% dplyr::select(totaltime_m) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of First Response Time (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of First Response Time (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    6.01 \n    10.50 \n    12.2 \n    14.4 \n    23.9 \n    12.8 \n    3.37 \n    126 \n    0 \n  \n  \n    online \n    2.91 \n    9.18 \n    11.5 \n    15.0 \n    111.0 \n    13.4 \n    9.21 \n    204 \n    0 \n  \n  \n    combined \n    2.91 \n    9.55 \n    12.0 \n    14.7 \n    111.0 \n    13.2 \n    7.53 \n    330 \n    0 \n  \n\n\n\n\n\nResponse time on the graph comprehension task for in person subjects (n = 126) ranged from 6.01 to 23.86 minutes with a mean duration of (M = 12.8, SD = 3.37).\nResponse time on the graph comprehension task for online replication subjects (n = 204) ranged from 2.91 to 111.02 minutes with a mean duration of (M = 13.37, SD = 9.21).\nResponse time on the graph comprehension task for combined subjects (n = 330) ranged from 2.91 to 111.02 minutes with a mean duration of (M = 13.16, SD = 7.53).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~totaltime_m, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of Task Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Task Time (minutes)\", y = \"% items\")\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"totaltime_m\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n   labs(title=\"Distribution of Task Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Task Time (minutes)\", y = \"% items\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nTime on First Item\nHere we consider the time spent on just the first individual item (first exposure to graph).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_lab%>% dplyr::select(item_q1_rt) %>% unlist()  %>%  favstats(),\n  \"online\"= df_online %>% dplyr::select(item_q1_rt) %>% unlist() %>% favstats(),\n  \"combined\"= df_subjects %>% dplyr::select(item_q1_rt) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of First Response Time (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of First Response Time (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    7.22 \n    26.6 \n    39.3 \n    52.2 \n    161 \n    44.5 \n    26.2 \n    126 \n    0 \n  \n  \n    online \n    4.84 \n    19.9 \n    31.0 \n    48.9 \n    306 \n    43.3 \n    41.3 \n    204 \n    0 \n  \n  \n    combined \n    4.84 \n    22.3 \n    34.0 \n    50.7 \n    306 \n    43.8 \n    36.2 \n    330 \n    0 \n  \n\n\n\n\n\nResponse time on the first item for in person subjects (n = 126) ranged from 7.22 to 161.36 minutes with a mean duration of (M = 44.53, SD = 26.22).\nResponse time on the first item for online replication subjects (n = 204) ranged from 4.84 to 305.94 minutes with a mean duration of (M = 43.32, SD = 41.27).\nResponse time on the first item for combined subjects (n = 330) ranged from 4.84 to 305.94 minutes with a mean duration of (M = 43.78, SD = 36.23).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_q1_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of First Item Response Time (seconds)\", subtitle = \"fit by gamma distribution\", x = \"First Item Response Time (seconds)\", y = \"% items\")\n\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_q1_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"First Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#recode as boolean correct\ndf_subjects <- df_subjects %>% mutate(\n  item_q1_NABS = as.logical(item_q1_NABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_q1_rt, color = item_q1_NABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .1\n  )) + \n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"First Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\n\n\nCODE\n# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n\n\nTime on Item\nHere we consider the time spent on an individual item (across all items).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(rt_s) %>% unlist()  %>%  favstats(),\n  \"online\"= df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(rt_s) %>% unlist() %>% favstats(),\n  \"combined\"= df_items %>%   dplyr::select(rt_s) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of Item Response Latency (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Item Response Latency (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    1.264 \n    13.8 \n    24.9 \n    46.1 \n    336 \n    35.5 \n    33.1 \n    1890 \n    0 \n  \n  \n    online \n    1.264 \n    13.8 \n    24.9 \n    46.1 \n    336 \n    35.5 \n    33.1 \n    1890 \n    0 \n  \n  \n    combined \n    0.003 \n    12.5 \n    23.7 \n    43.9 \n    532 \n    35.2 \n    37.2 \n    4950 \n    0 \n  \n\n\n\n\n\nTime on an individual item for in person subjects (n = 1890) ranged from 1.26 to 336.03 minutes with a mean duration of (M = 35.47, SD = 33.12).\nTime on an individual item for online replication subjects (n = 1890) ranged from 1.26 to 336.03 minutes with a mean duration of (M = 35.47, SD = 33.12).\nTime on an individual item for combined subjects (n = 4950) ranged from 0 to 531.52 minutes with a mean duration of (M = 35.24, SD = 37.21).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~rt_s, data = df_items) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of Item Response Time (seconds)\", \n       subtitle = \"fit by gamma distribution\", x = \"Item Response Time (seconds)\", y = \"% items\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_items, x = \"rt_s\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#recode as boolean correct\ndf_items <- df_items %>% mutate(\n  score_niceABS = as.logical(score_niceABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_items, aes(x = pretty_condition, y = rt_s, color = score_niceABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    # position = position_dodgejust(),\n    justification = 1.5, \n    # adjust = .5, \n    width = .5, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA,\n    position = position_dodge2()\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitterdodge(\n      # seed = 1,\n      dodge.width = 0.5,\n      jitter.width = 0.075\n  )) +\n  labs( title = \"Distribution of Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\n\n\nCODE\n# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n\n\nTime on SCAFFOLD Phase\nHere we consider just the time spent on the first five items of the task (the scaffold phase).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_scaffold_rt) %>% unlist()  %>%  favstats(),\n  \"online\"= df_online %>% dplyr::select(item_scaffold_rt) %>% unlist() %>% favstats(),\n  \"combined\"= df_subjects %>% dplyr::select(item_scaffold_rt) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of SCAFFOLD Phase Response Latency (minutes)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of SCAFFOLD Phase Response Latency (minutes)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    1.235 \n    2.66 \n    3.71 \n    4.92 \n    11.1 \n    4.03 \n    1.88 \n    126 \n    0 \n  \n  \n    online \n    0.614 \n    2.10 \n    2.92 \n    4.31 \n    15.4 \n    3.52 \n    2.26 \n    204 \n    0 \n  \n  \n    combined \n    0.614 \n    2.29 \n    3.25 \n    4.58 \n    15.4 \n    3.72 \n    2.13 \n    330 \n    0 \n  \n\n\n\n\n\nTotal time on SCAFFOLD phase for in person subjects (n = 126) ranged from 1.24 to 11.1 minutes with a mean duration of (M = 4.03, SD = 1.88).\nTotal time on SCAFFOLD phase for online replication subjects (n = 204) ranged from 0.61 to 15.39 minutes with a mean duration of (M = 3.52, SD = 2.26).\nTotal time on SCAFFOLD phase for combined subjects (n = 330) ranged from 0.61 to 15.39 minutes with a mean duration of (M = 3.72, SD = 2.13).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_scaffold_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of SCAFFOLD Phase Response Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Scaffold Phase Time (minutes)\", y = \"% subjects\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_scaffold_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of SCAFFOLD Phase Response Time (minutes)\",\n        subtitle =\"\",\n        x = \"Scaffold Phase Time (minutes)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_scaffold_rt, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_scaffold_rt),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_scaffold_rt, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  ))+ labs( title = \"Distribution of SCAFFOLD Phase Response Time (minutes)\",\n        subtitle =\"\",\n        y = \"Total Study Time (minutes)\", x = \"Condition\") +\n  theme_ggdist() + theme(legend.position = \"blank\") #+\n\n\n\n\n\nCODE\n  # coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\nTime on TEST Phase\nHere we consider just the time spent on the remaining eight discriminant items of the task (the test phase).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_test_rt) %>% unlist()  %>%  favstats(),\n  \"online\"= df_online %>% dplyr::select(item_test_rt) %>% unlist() %>% favstats(),\n  \"combined\"= df_subjects %>% dplyr::select(item_test_rt) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of TEST Phase Response Latency (minutes)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of TEST Phase Response Latency (minutes)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    1.022 \n    2.97 \n    3.75 \n    4.76 \n    10.8 \n    4.00 \n    1.37 \n    126 \n    0 \n  \n  \n    online \n    0.703 \n    3.10 \n    3.89 \n    5.17 \n    13.5 \n    4.41 \n    2.24 \n    204 \n    0 \n  \n  \n    combined \n    0.703 \n    3.03 \n    3.80 \n    4.99 \n    13.5 \n    4.26 \n    1.96 \n    330 \n    0 \n  \n\n\n\n\n\nTotal time on TEST phase for in person subjects (n = 126) ranged from 1.02 to 10.85 minutes with a mean duration of (M = 4, SD = 1.37).\nTotal time on TEST phase for online replication subjects (n = 204) ranged from 0.7 to 13.49 minutes with a mean duration of (M = 4.41, SD = 2.24).\nTotal time on TEST phase for combined subjects (n = 330) ranged from 0.7 to 13.49 minutes with a mean duration of (M = 4.26, SD = 1.96).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_test_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of TEST Phase Response Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Test Phase Time (minutes)\", y = \"% subjects\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_test_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of TEST Phase Response Time (minutes)\",\n        subtitle =\"\",\n        x = \"Test Phase Time (minutes)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_test_rt, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_rt),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_rt, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  ))+ \n  labs( title = \"Distribution of TEST Phase Response Time (minutes)\",\n        subtitle =\"\",\n        y = \"Total Study Time (minutes)\", x = \"Condition\") +\n  theme_ggdist() + theme(legend.position = \"blank\") #+\n\n\n\n\n\nCODE\n  # coord_cartesian(xlim = c(0.5, NA), clip = \"off\")"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#exploring-relationships",
    "href": "analysis/SGC3A/3_sgc3A_description.html#exploring-relationships",
    "title": "5  Description",
    "section": "EXPLORING RELATIONSHIPS",
    "text": "EXPLORING RELATIONSHIPS\n\nACCURACY (VS) LATENCY\n\nTotal Task\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ rt_m, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by TOTAL Item Response Time\",\n    subtitle = \"\", \n    x = \"Total Item Response Time (minutes)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_avg_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MEAN Item Response Time\",\n    subtitle = \"\", \n    x = \"Average Item Response Time (seconds)\", y = \"Total Scaled Score\"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_max_rt, data = df_subjects %>% filter(item_max_rt < 400), alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MAX Item Response Time\",\n    subtitle = \"\", \n    x = \"MAX Item Response Time (s)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#NOTE: LOG transforms of the RT do not yield linear relationships\n\n\n\n\nPhase Specific\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_scaffold_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"TOTAL Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( item_scaffold_NABS ~ item_scaffold_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"SCAFFOLD (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"SCAFFOLD Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( item_test_NABS ~ item_scaffold_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"TEST (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"TEST Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( item_test_NABS ~ item_avg_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"TEST (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"AVERAGE Item Response Time (minutes)\", y = \"TEST Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#NOTE: LOG transforms of the RT do not yield linear relationships\n\n\n\n\nAverage Item RT by Accuracy\n\n\nCODE\nq.stats <- df_items %>% filter(q != 6) %>% dplyr::group_by(q, pretty_condition, score_niceABS) %>% dplyr::summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score_niceABS)\n)\n\ngf_line( m ~ q, group = ~group,  color = ~as.factor(score_niceABS),data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~q) %>% \n  gf_facet_wrap(~pretty_condition) + scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Absolute Score\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\n# df_items %>%\n#   ggplot(aes(y = rt_s, x = q,  fill = pretty_condition)) +\n#   stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + facet_wrap(~pretty_condition)\n\n\n\n\nCODE\nq.stats <- df_items %>% filter(q %nin% c(6,9)) %>% dplyr::group_by(q, pretty_condition, interpretation) %>% dplyr::summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score_SCALED)\n)\n\ngf_line( m ~ as.factor(q), group = ~group,  color = ~interpretation,data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~as.factor(q)) %>% \n  gf_facet_grid(interpretation~pretty_condition) + #+ scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Interpretation\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       caption=\"NOTE: Points with no ribbon indicate singular response\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Interpretation\")\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\ndf_items %>% filter(q %nin% c(6,9)) %>% mutate( interpretation = recode(interpretation, \"reference\" = \"blank\", \"frenzy\" = \"?\")) %>% \n  ggplot(aes(y = rt_s, x = q,  fill = interpretation)) +\n  stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + \n  facet_grid(interpretation ~ pretty_condition) + \n  labs( title = \"Average Response Time by Question Interpretation\", x = \"Question\", y=\"Averate Item Response Time (s)\")"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#replication-check",
    "href": "analysis/SGC3A/3_sgc3A_description.html#replication-check",
    "title": "5  Description",
    "section": "REPLICATION CHECK",
    "text": "REPLICATION CHECK\n\nData Collection Mode on Absolute Score\nDoes Mode Change Effect of Condition on Score?\nTo verify that the data collected in the lab and remotely online are comparable, we perform a t-test on group means of ABSOLUTE SCORE for each condition, and examine whether data collection modality is a significant predictor of variance in absolute score\n\n\nCODE\npaste(\"Two Sample T-Test for S_ABS LAB vs. ONLINE control condition\")\n\n\n[1] \"Two Sample T-Test for S_ABS LAB vs. ONLINE control condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 111), s_ABS ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_ABS by mode\nt = 0.5, df = 120, p-value = 0.6\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -1.09  1.84\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                   2.68                    2.30 \n\n\nCODE\npaste(\"Two Sample T-Test for S_ABS LAB vs. ONLINE impasse condition\")\n\n\n[1] \"Two Sample T-Test for S_ABS LAB vs. ONLINE impasse condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 121), s_ABS ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_ABS by mode\nt = 1, df = 135, p-value = 0.3\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -0.727  2.435\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                   5.44                    4.58 \n\n\nCODE\npaste(\"OLS Linear Regression Predicting Absolute Score by Data Collection Mode\")\n\n\n[1] \"OLS Linear Regression Predicting Absolute Score by Data Collection Mode\"\n\n\nCODE\nsummary(lm(data = df_subjects, formula = s_ABS ~ mode ))\n\n\n\nCall:\nlm(formula = s_ABS ~ mode, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.08  -3.51  -2.51   4.49   9.49 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.08       0.44    9.27   <2e-16 ***\nmodeasynch     -0.57       0.56   -1.02     0.31    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.94 on 328 degrees of freedom\nMultiple R-squared:  0.00314,   Adjusted R-squared:  0.000105 \nF-statistic: 1.03 on 1 and 328 DF,  p-value: 0.31\n\n\nBoth t-tests are non-significant with 95% confidence intervals including 0. Further, an OLS linear regression model predicting cumulative absolute score indicates that data collection mode is not a significant predictor, explaining only 0.01% of variance in absolute score, F(1,328) = 1.03, p > 0.05.\n\n\n\n\n\n\nDecision\n\n\n\nIt is reasonable to infer that data from both in-person and remote studies arise from the same data generating process.\n\n\n\n\nData Collection Mode on Cumulative Score\nAre the by-condition group means significantly different by data collection modality?\nTo verify that the data collected in the lab and remotely online are comparable, we perform a t-test on group means of SCALED SCORE for each condition.\n\n\nCODE\npaste(\"Two Sample T-Test for s_SCALED LAB vs. ONLINE control condition\")\n\n\n[1] \"Two Sample T-Test for s_SCALED LAB vs. ONLINE control condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 111), s_SCALED ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by mode\nt = -0.1, df = 117, p-value = 0.9\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -3.15  2.83\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                  -6.52                   -6.36 \n\n\nCODE\npaste(\"Two Sample T-Test for s_SCALED LAB vs. ONLINE impasse condition\")\n\n\n[1] \"Two Sample T-Test for s_SCALED LAB vs. ONLINE impasse condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 121), s_SCALED ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by mode\nt = 0.5, df = 130, p-value = 0.6\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -2.08  3.49\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                  1.008                   0.306 \n\n\nCODE\npaste(\"OLS Linear Regression Predicting Scaled Score by Data Collection Mode\")\n\n\n[1] \"OLS Linear Regression Predicting Scaled Score by Data Collection Mode\"\n\n\nCODE\nsummary(lm(data = df_subjects, formula = s_SCALED ~ mode ))\n\n\n\nCall:\nlm(formula = s_SCALED ~ mode, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10.30  -7.80  -4.42  10.33  15.83 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   -2.698      0.853   -3.16   0.0017 **\nmodeasynch    -0.135      1.085   -0.12   0.9011   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.58 on 328 degrees of freedom\nMultiple R-squared:  4.71e-05,  Adjusted R-squared:  -0.003 \nF-statistic: 0.0155 on 1 and 328 DF,  p-value: 0.901\n\n\nBoth t-tests are non-significant with 95% confidence intervals including 0. Further, an OLS linear regression model predicting cumulative scaled score indicates that data collection mode is not a significant predictor, explaining less than 0.001% of variance in absolute score, F(1,328) = 0.0078, p > 0.05.\n\n\n\n\n\n\nDecision\n\n\n\nIt is reasonable to infer that data from both in-person and remote studies arise from the same data generating process."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#archive",
    "href": "analysis/SGC3A/3_sgc3A_description.html#archive",
    "title": "5  Description",
    "section": "ARCHIVE",
    "text": "ARCHIVE\nSample ridgeplot code\n\n\nCODE\n#RIDGEPLOT\n# ggplot(data = df_subjects, aes(x = s_NABS, y = mode)) +\n#   geom_density_ridges() + xlim(0,13)+\n#   facet_wrap(~condition, labeller = label_both) +\n# labs(x = \"total number correct \",\n# y = \"proportion of subjects\",\n#        title = \"Subject Cumulative Score (Absolute)\",\n#        subtitle = \"Score distributions are comparable across modalities and different across conditions\") +\n#   theme_minimal()\n\n\n\nWhat Kind of Distribution is Total Score?\nWhat kind of distribution is the Total Absolute Score (TEST Phase) data? We use the fitdistrplus package to compare the distribution of this variable to a variety of probability distribution families. First, we transform the # correct items to % correct items by dividing it by the total number of items (n = 8).\n\n\nCODE\n#describe the distribution\ndescdist(data = df_subjects$item_test_NABS/8, discrete = FALSE, boot = 1000)\n\n\n\n\n\nsummary statistics\n------\nmin:  0   max:  1 \nmedian:  0 \nmean:  0.287 \nestimated sd:  0.405 \nestimated skewness:  0.876 \nestimated kurtosis:  1.93 \n\n\nCODE\nprint(\"FIT A NORMAL DISTRIBUTION\")\n\n\n[1] \"FIT A NORMAL DISTRIBUTION\"\n\n\nCODE\nnormal_ = fitdist(df_subjects$item_test_NABS/8,\"norm\")\nplot(normal_)\n\n\n\n\n\nCODE\nprint(\"FIT A BETA DISTRIBUTION\")\n\n\n[1] \"FIT A BETA DISTRIBUTION\"\n\n\nCODE\nbeta_ = fitdist(df_subjects$item_test_NABS/8,\"beta\", method=\"mme\" )\nplot(beta_)\n\n\n\n\n\nCODE\nsummary(beta_)\n\n\nFitting of the distribution ' beta ' by matching moments \nParameters : \n       estimate\nshape1   0.0721\nshape2   0.1786\nLoglikelihood:  Inf   AIC:  -Inf   BIC:  -Inf \n\n\nInterpreting the Cullen and Frey graph, it appears that number percentage of correct responses per subject may follow a beta distribution (u-shape tpe). If we fit this variable using both a normal and beta distribution (using method of moments), it appears that the beta distribution provides a much better fit. The parameter estimates for the beta distribution are: shape1 = 0.072, shape2 = 0.179. The beta distribution is a flexible distribution insofar as it can model a wide range of shapes with its two parameters. TODO: HOW might this be applied to the total score data?\nAnalysis Notes - This distribution is very bimodal, so OLS linear regression estimating means may not be informative, as the mean actually falls near the location of the anitmode (least common value) - Should investigate log transform to see if residuals of LM will be normal (no) - Should investigate beta regression\n\n\nWhole Task Scores\n\nAbsolute Score\nTotal Scores that include both Scaffolding Phase as well as Test Phase performance.\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total Absolute Score)\"\nabs.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(s_NABS) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(s_NABS) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(s_NABS) %>% unlist() %>% favstats()\n) \nabs.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    0 \n    0 \n    1 \n    9.00 \n    13 \n    4.11 \n    5.09 \n    126 \n    0 \n  \n  \n    online \n    0 \n    0 \n    1 \n    8.00 \n    13 \n    3.52 \n    4.89 \n    204 \n    0 \n  \n  \n    combined \n    0 \n    0 \n    1 \n    8.75 \n    13 \n    3.75 \n    4.97 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, total absolute scores (n = 126) range from 0 to 13 with a mean score of (M = 4.11, SD = 5.09).\nFor online replication, (online) total absolute accuracy scores (n = 204) range from 0 to 13 with a slightly lower mean score of (M = 3.52, SD = 4.89).\nWhen combined overall, total absolute accuracy scores (n = 330) range from 0 to 13 with a slightly lower mean score of (M = 3.75, SD = 4.97).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL ABSOLUTE\n  gf_props(~s_NABS, data = df_subjects) + \n  labs(x = \"number of correct responses\",\n       y = \"% of subjects\",\n       title = \"Distribution of Task Absolute Score\",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") + \n  theme_minimal()\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"s_NABS\", binwidth = 1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of Task Absolute Score (by Mode and Condition)\",\n        subtitle =\"Pattern of response is the same across data collection modes but differs by condition\",\n        x = \"Total Absolute Score\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = s_NABS, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    adjust = .5, \n    width = .6, \n    .width = 0, \n    justification = -.3, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter(\n      seed = 1, width = .1\n    )\n  ) + labs(\n    title = \"Distribution of Task Absolute Score\",\n    x = \"Condition\", y = \"Total Absolute Score\"\n  ) + theme_ggdist() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(s_NABS)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Task Absolute Score\",\n        x = \"Task Absolute Score [0,13]\", \n        y = \"Cumulative Probability\") + theme_minimal()\n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and likely bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018). TODO REFERENCE\n\n\nCODE\nmultimode::modetest(df_subjects$s_NABS)\n\n\nWarning in multimode::modetest(df_subjects$s_NABS): A modification of the data\nwas made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$s_NABS\nExcess mass = 0.1, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$s_NABS, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$s_NABS,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$s_NABS, mod0 = n_modes, display =\nTRUE): If the density function has an unbounded support, artificial modes may\nhave been created in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is infact multimodal (m = 0.1, p < 0.001), with two identifiable modes at 0.26 and 12.261, and an antimode at 6.985.\n\n\n\n\n\n\nNote\n\n\n\nCondition appears (through visual inspection) to yield a positive influence on Total Absolute Score (across the entire task), across data collection modalities.\n\n\n\n\nScaled Score\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total Scaled Score)\"\nscaled.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -13 \n    -12.0 \n    -7.50 \n    8.75 \n    13 \n    -2.70 \n    10.08 \n    126 \n    0 \n  \n  \n    online \n    -13 \n    -10.0 \n    -7.00 \n    6.62 \n    13 \n    -2.83 \n    9.26 \n    204 \n    0 \n  \n  \n    combined \n    -13 \n    -10.5 \n    -7.25 \n    7.50 \n    13 \n    -2.78 \n    9.56 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, total scaled scores (n = 126) range from -13 to 13 with a mean score of (M = -2.7, SD = 10.08).\nFor online replication, total scaled scores (n = 204) range from -13 to 13 with a slightly lower mean score of (M = -2.83, SD = 9.26).\nWhen combined overall, total scaled scores (n = 330) range from -13 to 13 with a slightly lower mean score of (M = -2.78, SD = 9.56).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL SCALED\ngf_props(~s_SCALED, data = df_subjects) +\n  labs(x = \"total scaled score\",\n       y = \"% of subjects\",\n       title = \"Distribution of Total Scaled Score\",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") + \n  theme_minimal()\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"s_SCALED\",binwidth=1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) + \n  labs( title = \"Distribution of Total Scaled Score (by Condition and Mode)\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"total scaled score\", y = \"number of participants\") + \n  theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = s_SCALED, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    adjust = .5, \n    width = .6, \n    .width = 0, \n    justification = -.3, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter(\n      seed = 1, width = .1\n    )\n  ) + labs(\n    title = \"Distribution of Task Scaled Score \",\n    x = \"Condition\", y = \"Total Scaled Score\"\n  ) + theme_ggdist() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(s_SCALED)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Task Scaled Score\",\n        x = \"Task Scaled Score [-13, 13]\", \n        y = \"Cumulative Probability\") + theme_minimal()\n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and perhaps perhaps bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018).\n\n\nCODE\nmultimode::modetest(df_subjects$s_SCALED)\n\n\nWarning in multimode::modetest(df_subjects$s_SCALED): A modification of the data\nwas made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$s_SCALED\nExcess mass = 0.1, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$s_SCALED, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$s_SCALED,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$s_SCALED, mod0 = n_modes, display =\nTRUE): If the density function has an unbounded support, artificial modes may\nhave been created in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is in fact multimodal (m = 0.1, p < 0.001), with two identifiable modes at -11.195 and 12.103, and an antimode at 2.942.\nAnalysis Notes - As with absolute score, the distribution of scaled score is very bimodal - Same need to investigate transformations and alternative distributions for regression\n\n\n\n\n\n\nNote\n\n\n\nCondition appears (through visual inspection) to yield a positive influence on Total Scaled Score across data collection modalities.\n\n\n\n\n\nItem Level Scores\n\nItem Absolute Score\nWhole Task Accuracy summarized over items rather than subjects\n\n\nCODE\nx <- df_items %>% mutate(score = as.logical(score_ABS))\n\ntitle = \"Proportion of Correct Items By Condition (Lab)\"\n\nitem.contingency <- df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(score_ABS, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Items By Condition (Lab)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    0 \n    0.344 \n    0.268 \n    0.613 \n  \n  \n    1 \n    0.148 \n    0.240 \n    0.387 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Items By Condition (Online)\"\nitem.contingency <- df_items %>% filter(mode == \"asynch\") %>% dplyr::select(score_ABS, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Items By Condition (Online)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    0 \n    0.342 \n    0.307 \n    0.649 \n  \n  \n    1 \n    0.128 \n    0.223 \n    0.351 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM\nstats = df_items %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(score_niceABS))\ngf_props(~score_niceABS, data = df_items) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) +\n  labs(x = \"Item Absolute Score\",\n       title = \"Item Absolute Score\",\n       subtitle=\"Across modalities, the impasse condition yielded more correct responses\")+\n  theme_minimal()\n\n\n\n\n\n\n\nItem Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1.\n\n\nCODE\ntitle = \"Descriptive Statistics of Item Response Accuracy (Scaled Score)\"\nscaled.stats.items <- rbind(\n  \"lab\"= df_items %>% filter(mode == 'lab-synch') %>% dplyr::select(score_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_items %>% filter(mode == \"asynch\") %>% dplyr::select(score_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats.items %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Item Response Accuracy (Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -1 \n    -1 \n    -0.5 \n    1 \n    1 \n    -0.128 \n    0.877 \n    1890 \n    0 \n  \n  \n    online \n    -1 \n    -1 \n    -0.5 \n    1 \n    1 \n    -0.136 \n    0.842 \n    3060 \n    0 \n  \n\n\n\n\n\n\n\nCODE\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM\nstats = df_items %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(score_SCALED))\ngf_props(~score_SCALED, data = df_items) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Scaled Score for Item\",\n       y = \"Proportion of Items\",\n       title = \"Distribution of Accuracy per Item (Scale Score)\",\n       subtitle=\"The impasse condition shifts density toward the positive score\")+\n  theme_minimal()"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#resources",
    "href": "analysis/SGC3A/3_sgc3A_description.html#resources",
    "title": "5  Description",
    "section": "RESOURCES",
    "text": "RESOURCES\n\nhttps://rpkgs.datanovia.com/ggpubr/reference/index.html\nAppropriate models for response time data. (see: https://lindeloev.github.io/shiny-rt/)]{style=“color: red;”}.\nEspecially see https://lindeloev.github.io/shiny-rt/ for ideas on modelling reaction time data\n\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] forcats_0.5.1      stringr_1.4.0      purrr_0.3.4        readr_2.1.2       \n [5] tidyr_1.2.0        tibble_3.1.7       tidyverse_1.3.1    performance_0.9.1 \n [9] fitdistrplus_1.1-8 MASS_7.3-57        multimode_1.5      ggeasy_0.1.3      \n[13] ggstatsplot_0.9.3  ggdist_3.1.1       ggpubr_0.4.0       vcd_1.4-10        \n[17] kableExtra_1.3.4   mosaic_1.8.3       ggridges_0.5.3     mosaicData_0.20.2 \n[21] ggformula_0.10.1   ggstance_0.3.5     dplyr_1.0.9        Matrix_1.4-1      \n[25] Hmisc_4.7-0        ggplot2_3.3.6      Formula_1.2-4      survival_3.3-1    \n[29] lattice_0.20-45   \n\nloaded via a namespace (and not attached):\n  [1] readxl_1.4.0           backports_1.4.1        systemfonts_1.0.4     \n  [4] plyr_1.8.7             splines_4.2.1          crosstalk_1.2.0       \n  [7] leaflet_2.1.1          TH.data_1.1-1          digest_0.6.29         \n [10] htmltools_0.5.2        fansi_1.0.3            magrittr_2.0.3        \n [13] checkmate_2.1.0        paletteer_1.4.0        cluster_2.1.3         \n [16] ks_1.13.5              tzdb_0.3.0             mosaicCore_0.9.0      \n [19] modelr_0.1.8           vroom_1.5.7            sandwich_3.0-2        \n [22] svglite_2.1.0          jpeg_0.1-9             colorspace_2.0-3      \n [25] rvest_1.0.2            ggrepel_0.9.1          haven_2.5.0           \n [28] xfun_0.31              prismatic_1.1.0        crayon_1.5.1          \n [31] jsonlite_1.8.0         zeallot_0.1.0          zoo_1.8-10            \n [34] glue_1.6.2             polyclip_1.10-0        gtable_0.3.0          \n [37] emmeans_1.7.5          MatrixModels_0.5-0     webshot_0.5.3         \n [40] statsExpressions_1.3.2 distributional_0.3.0   car_3.1-0             \n [43] abind_1.4-5            scales_1.2.0           mvtnorm_1.1-3         \n [46] DBI_1.1.3              rstatix_0.7.0          Rcpp_1.0.8.3          \n [49] viridisLite_0.4.0      xtable_1.8-4           htmlTable_2.4.0       \n [52] bit_4.0.4              mclust_5.4.10          foreign_0.8-82        \n [55] datawizard_0.4.1       htmlwidgets_1.5.4      httr_1.4.3            \n [58] RColorBrewer_1.1-3     ellipsis_0.3.2         pkgconfig_2.0.3       \n [61] farver_2.1.0           dbplyr_2.2.1           nnet_7.3-17           \n [64] utf8_1.2.2             effectsize_0.7.0       labeling_0.4.2        \n [67] tidyselect_1.1.2       rlang_1.0.3            cellranger_1.1.0      \n [70] munsell_0.5.0          tools_4.2.1            cli_3.3.0             \n [73] generics_0.1.2         broom_0.8.0            evaluate_0.15         \n [76] fastmap_1.1.0          ggdendro_0.1.23        yaml_2.3.5            \n [79] rematch2_2.1.2         bit64_4.0.5            fs_1.5.2              \n [82] knitr_1.39             rootSolve_1.8.2.3      pbapply_1.5-0         \n [85] pracma_2.3.8           xml2_1.3.3             correlation_0.8.1     \n [88] compiler_4.2.1         rstudioapi_0.13        png_0.1-7             \n [91] ggsignif_0.6.3         reprex_2.0.1           tweenr_1.0.2          \n [94] stringi_1.7.6          highr_0.9              parameters_0.18.1     \n [97] vctrs_0.4.1            pillar_1.7.0           lifecycle_1.0.1       \n[100] lmtest_0.9-40          estimability_1.4       data.table_1.14.2     \n[103] insight_0.18.0         patchwork_1.1.1        R6_2.5.1              \n[106] latticeExtra_0.6-29    KernSmooth_2.23-20     gridExtra_2.3         \n[109] BayesFactor_0.9.12-4.4 codetools_0.2-18       boot_1.3-28           \n[112] assertthat_0.2.1       withr_2.5.0            multcomp_1.4-19       \n[115] parallel_4.2.1         diptest_0.76-0         bayestestR_0.12.1     \n[118] hms_1.1.1              rpart_4.1.16           labelled_2.9.1        \n[121] coda_0.19-4            rmarkdown_2.14         carData_3.0-5         \n[124] ggforce_0.3.3          lubridate_1.8.0        base64enc_0.1-3"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_lab_hypotesting.html",
    "href": "analysis/SGC3A/4_sgc3A_lab_hypotesting.html",
    "title": "6  (Lab) Hypothesis Testing",
    "section": "",
    "text": "The purpose of this notebook is test the hypotheses that determined the design of the SGC3A study.\nResearch Questions\nIn SGC3A we set out to answer the following question: Does posing a mental impasse improve performance on the interval graph comprehension task?\nExperimental Hypothesis\nLearners posed with scenario designed to evoke a mental impasse will be more likely to correct interpret the graph.\nNull Hypothesis\nNo significant differences in performance will exist between learners in the IMPASSE and CONTROL conditions."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_lab_hypotesting.html#h1a-overall-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_lab_hypotesting.html#h1a-overall-accuracy",
    "title": "6  (Lab) Hypothesis Testing",
    "section": "H1A | OVERALL ACCURACY",
    "text": "H1A | OVERALL ACCURACY\n\n\n\n\n\n\n\nResearch Question\nDo Ss in the IMPASSE condition score higher across the entire task than those in the CONTROL group?\n\n\n\n\nHypothesis\n(H1) Participants in the IMPASSE condition will be more likely to correctly interpret the graph than those in the CONTROL condition.\n\n\nData\ndata: df_items where q nin 6,9 (the 13 discriminating Qs ), df_subjects\noutcome:\n\n[at item level] : accuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\n[subject level]: accuracy (number of test phase qs correct from total s_NABS)\n\npredictor: condition [between-subjects factor]\n\n\nAnalysis Strategy\n\nWilcoxon-Rank Sum (Mann-Whitney) test on subject-level total accuracy of test phase (s_NABS)\nMixed Logistic Regression\naccuracy ~ condition + (1 | subject ) + (1 | question)\nmodel effect of condition on probability of correct response [during test phase] while accounting for subject (and item-level?) effects\n\n\n\nAlternatives\n\nOrdinal Mixed Logistic Regression on scaled_score\nOLS Linear Regression: bimodal distribution at tails makes the mean a poor predictor; LMs violate assumptions of normally distributed residuals; both absolute and scaled scores yield non-normal residuals; no transformation of the outcome variables yield normal residuals\n\n\n\nNotes\nAlso exploring:\n\nHurdle model (mixture model w/ binomial + [poisson or negbinom count; 0s from 1 DGP)\nZero Inflated model (mixture model w/ binomial + poisson or negbinom count; 0s from 2 DGPs)\nBeta regression hurdle model? (mixture with location and scale parameters [mean, variance] and hurdles for floor and ceiling effects)\nOther way to account for the severe bimodality?\n\n\n\n\n\nSetup\n\n\nCODE\ndf_i = df_items %>% filter(q %nin% c(6,9)) %>% \n  dplyr::select(pretty_condition, accuracy, subject,q)\n\ndf_s <- df_subjects %>% \n   dplyr::select(pretty_condition, task_percent)\n\n\n\n\nVisualize\n\n\nCODE\n#:::::::: STACKED PROPORTIONAL BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  # scale_fill_manual(values = paletteer::paletteer_d(\"ggthemes::calc\", 2))+\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~pretty_mode) + \n  # coord_flip() +\n  theme(legend.position=\"bottom\")+\n   labs(title = \"DISTRIBUTION | Question Accuracy\",\n       x = \"Condition\",\n       y = \"Proportion of Questions\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART BY QUESTION\ndf_items %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~q) +\n   labs(title = \"DISTRIBUTION | Accuracy by Question\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Q6 and Q9 are non-discriminative\")\n\n\n\n\n\nCODE\n#:::::::: FACETED HISTOGRAM\nstats = df_s %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(task_percent))\ngf_props(~task_percent,\n         fill = ~pretty_condition, data = df_s) %>%\n  # gf_facet_grid(pretty_condition ~ pretty_mode) %>%\n  gf_facet_grid(~pretty_condition) %>%\n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"% Correct\",\n       y = \"proportion of subjects\",\n       title = \"DISTRIBUTION | Total Absolute Score (% Correct)\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#:::::::: RAINCLOUD WITH STATS\n  \ndf <- df_s %>% mutate(task_percent = task_percent*100)\n\np <-   ggbetweenstats(data = df, x = pretty_condition, y = task_percent,\n               plot.type = \"box\", type = \"nonparametric\", var.equal = FALSE,\n               centrality.type = \"parametric\",\n               # package = \"RColorBrewer\",\n               # palette = \"PRGn\",\n               centrality.point.args = list(color=\"black\", size = 3, shape = 1),\n               point.args = list(alpha=0), #suppress points\n               ggplot.component = ## modify further with `{ggplot2}` functions\n                list(\n                  # aes(color = pretty_condition, fill = pretty_condition),\n                  # scale_colour_manual(values = paletteer::paletteer_c(\"viridis::viridis\", 3)),\n                  # scale_fill_manual(values = paletteer::paletteer_c(\"viridis::viridis\", 3)),\n                  theme(axis.text.x = element_text(angle = 90)))\n               ) +\n  ggdist::stat_halfeye(\n    alpha = 0.7, \n    point_colour = NA,\n    adjust = .5, \n    width = .5, .width = 0, \n    justification = -.5) +\n  geom_boxplot(\n    alpha = 0.1,\n    width = .2, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 2,\n    alpha = .5,\n    position = position_jitter(\n      seed = 1, width = .08, height = 1.5\n    )\n  )  +\ncoord_flip() + theme_clean() + theme(legend.position = \"blank\")\np$layers[[3]]=NULL #remove default boxplot\ne <- statsExpressions::two_sample_test(y = task_percent, x = pretty_condition, data = df,\n                                type = \"nonparametric\", alternative = \"less\",\n                                var.equal = FALSE)\n#labels are layer 4\np + labs(title = \"DISTRIBUTION of Total Score\",\n         y = \"Percentage of correct responses across task\", x = \"\",\n         # caption=e$expression[[1]],\n         subtitle = \"Impasse condition yields greater variance and more high scores\")\n\n\n\n\n\n\n\nDescribe\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total % Correct)\"\ntbl1 <- mosaic::favstats(~task_percent, data = df_s) \ntbl1 %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total % Correct)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    0 \n    0 \n    0.077 \n    0.692 \n    1 \n    0.316 \n    0.392 \n    126 \n    0 \n  \n\n\n\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION\"\ntbl2 <- mosaic::favstats(task_percent ~ pretty_condition, data = df_s) \ntbl2 %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION\n \n  \n    pretty_condition \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    control \n    0 \n    0.000 \n    0.000 \n    0.154 \n    1 \n    0.210 \n    0.370 \n    62 \n    0 \n  \n  \n    impasse \n    0 \n    0.058 \n    0.346 \n    0.846 \n    1 \n    0.419 \n    0.387 \n    64 \n    0 \n  \n\n\n\n\n\n\nWILCOXON RANK SUM (Mann-Whitney Test)\n\nNon parametric alternative to t-test; compares median rather than mean by ranking data\nDoes not assume normality\nDoes not assume equal variance of samples (homogeneity of variance)\n\n\nTest\n\n\nCODE\n(w <- wilcox.test(df_s$task_percent ~ df_s$pretty_condition,\n                 paired = FALSE, alternative = \"less\")) #less, greater\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df_s$task_percent by df_s$pretty_condition\nW = 1243, p-value = 7e-05\nalternative hypothesis: true location shift is less than 0\n\n\nCODE\nreport(w)\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum test with continuity correction testing the difference in ranks between df_s$task_percent and df_s$pretty_condition suggests that the effect is negative, statistically significant, and large (W = 1243.00, p < .001; r (rank biserial) = -0.37, 95% CI [-1.00, -0.22])\n\n\n\n\nInference\n(not reported)\nBecause the distribution of the outcome variable is not normally distributed, we evaluate the effect of CONDITION on ACCURACY via a non-parametric test. Consistent with our hypothesis, a Wilcoxon rank sum test (with continuity correction) on ACCURACY by CONDITION indicates that data in each condition likely come from different population distributions (W = 1243, p < 0.001; one-tailed), and that the distribution of the control condition is less (i.e. shifted to the left/ lower scores) than the impasse condition (^{r} = -0.37, 95% CI [-1.00, -0.22]), a large-sized effect.\n\n\nVisualize\n\n\nCODE\nggbetweenstats( x = pretty_condition, y = task_percent, data = df_s,\n                type = \"nonparametric\", var.equal = FALSE)\n\n\n\n\n\n\n\n\nMIXED LOGISTIC REGRESSION\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.\n\nFit Model\n\n\nCODE\n## 0 | SETUP\n#confirm 13 items [all discriminating items]\nnrow(df_i) / nrow(df_s) == 13\n\n\n[1] TRUE\n\n\nCODE\n#confirm all factors \nis.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$pretty_condition) && is.factor(df_i$accuracy)\n\n\n[1] TRUE\n\n\nCODE\n## 1 | SETUP RANDOM INTERCEPT SUBJECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nprint(\"Empty fixed model\")\n\n\n[1] \"Empty fixed model\"\n\n\nCODE\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df_i) \n# summary(m0)\n\n#:: RANDOM INTERCEPT SUBJECT\nprint(\"Subject intercept random model\")\n\n\n[1] \"Subject intercept random model\"\n\n\nCODE\nmm.rS <- glmer(accuracy ~ (1|subject), data = df_i, family = \"binomial\")\n# summary(mm.rS)\n\n# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", m0$aic > AIC(logLik(mm.rS)))\n\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\n\nCODE\ntest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName  |    Model | df | df_diff |    Chi2 |      p\n--------------------------------------------------\nm0    |      glm |  1 |         |         |       \nmm.rS | glmerMod |  2 |       1 | 1011.83 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  4.82063151679782e-222\"\n\n\nCODE\n#:: RANDOM INTERCEPT SUBJECT + ITEM\nprint(\"Subject Intercept + Item intercept random model\")\n\n\n[1] \"Subject Intercept + Item intercept random model\"\n\n\nCODE\nmm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = \"binomial\")\n#summary(mm.rSQ)\n\n# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)))\n\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\n\nCODE\ntest_lrt(mm.rS, mm.rSQ) #same as anova(m0, m1, test = \"Chi\")\n\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |      p\n-------------------------------------------------\nmm.rS  | glmerMod |  2 |         |       |       \nmm.rSQ | glmerMod |  3 |       1 | 15.82 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS, mm.rSQ))$p[2])\n\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.0000697594950188617\"\n\n\nCODE\n## 2 | ADD FIXED EFFECT CONDITION\n\nprint(\"FIXED Condition + Subject & Item random intercepts\")\n\n\n[1] \"FIXED Condition + Subject & Item random intercepts\"\n\n\nCODE\nmm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q) ,\n                data = df_i, family = \"binomial\")\n#summary(mm.CrSQ)\n\npaste(\"AIC decreases w/ new model\", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.CrSQ)) )\n\n\n[1] \"AIC decreases w/ new model TRUE\"\n\n\nCODE\ntest_lrt(mm.rSQ,mm.CrSQ) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName    |    Model | df | df_diff |  Chi2 |      p\n--------------------------------------------------\nmm.rSQ  | glmerMod |  3 |         |       |       \nmm.CrSQ | glmerMod |  4 |       1 | 18.66 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rSQ,mm.CrSQ))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.0000156066263742927\"\n\n\nCODE\n# control = glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)))\n\n\nA likelihood ratio test indicates adding CONDITION as a fixed effect to a logistic regression model including a fixed effect random intercepts for SUBJECT and QUESTION explains more variance in the data than random-effects only model.\n\n\nDescribe\n\n\nCODE\n#::::::::: SETUP\nm <- mm.CrSQ\n\n#::::::::: PRINT MODEL \n\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition + (1 | subject) + (1 | q)\n   Data: df_i\n\n     AIC      BIC   logLik deviance df.resid \n    1006     1028     -499      998     1634 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.316 -0.136 -0.052  0.168  5.522 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 22.216   4.713   \n q       (Intercept)  0.308   0.555   \nNumber of obs: 1638, groups:  subject, 126; q, 13\n\nFixed effects:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                -5.51       1.02   -5.40  6.5e-08 ***\npretty_conditionimpasse     4.32       1.12    3.87  0.00011 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nprtty_cndtn -0.802\n\n\nCODE\nprint(\"SIGNIFICANCE TEST [non directional]\")\n\n\n[1] \"SIGNIFICANCE TEST [non directional]\"\n\n\nCODE\ncar::Anova(m)\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: accuracy\n                 Chisq Df Pr(>Chisq)    \npretty_condition    15  1    0.00011 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"MODEL INFO\")\n\n\n[1] \"MODEL INFO\"\n\n\nCODE\nglance(m)\n\n\n# A tibble: 1 × 7\n   nobs sigma logLik   AIC   BIC deviance df.residual\n  <int> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n1  1638     1  -499. 1006. 1028.     630.        1634\n\n\nCODE\n#:::::::: MANUAL ONE-SIDED SIGTEST \n#note: anova and chi square are always one-tailed, but that is independent of being one-sided\n#https://www.ibm.com/support/pages/can-one-get-one-tailed-tests-logistic-regression-dividing-significance-levels-half\n# one-sided (right tail) z test for B COEFFICIENT\n#SANITY CHECK 2-tailed test should match the model output\n# tt <- 2*pnorm(summary(m)$coefficients[2,3], lower.tail = F)\n# paste(\"p value for two-tailed test, null B = 0 : \",round(tt,5))\n# ot <- pnorm(summary(m)$coefficients[2,3], lower.tail = F)\n# paste(\"BUT we want a one  directional, null: B <= 0: \",round(ot,5))\n\n#:::::::: INTERPRET COEFFICIENTS\n\npaste(\"LOG ODDS\")\n\n\n[1] \"LOG ODDS\"\n\n\nCODE\n# se <- sqrt(diag(stats::vcov(m)))\n# (tab <- cbind(Est = fixef(m),\n#               LL = fixef(m) - 1.96 * se,\n#               UL = fixef(m) + 1.96 * se))\ntidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = \"Wald\")\n\n\n# A tibble: 4 × 9\n  effect   group  term  estimate std.error statistic  p.value conf.low conf.high\n  <chr>    <chr>  <chr>    <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 fixed    <NA>   (Int…   -5.51       1.02     -5.40  6.49e-8    -7.51     -3.51\n2 fixed    <NA>   pret…    4.32       1.12      3.87  1.08e-4     2.14      6.51\n3 ran_pars subje… sd__…    4.71      NA        NA    NA          NA        NA   \n4 ran_pars q      sd__…    0.555     NA        NA    NA          NA        NA   \n\n\nCODE\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\n# (e <- exp(tab))\ntidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = \"Wald\", exponentiate = TRUE)\n\n\n# A tibble: 4 × 9\n  effect   group  term  estimate std.error statistic  p.value conf.low conf.high\n  <chr>    <chr>  <chr>    <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 fixed    <NA>   (Int…  0.00406   0.00413     -5.40  6.49e-8  5.50e-4    0.0299\n2 fixed    <NA>   pret… 75.5      84.3          3.87  1.08e-4  8.46e+0  674.    \n3 ran_pars subje… sd__…  4.71     NA           NA    NA       NA         NA     \n4 ran_pars q      sd__…  0.555    NA           NA    NA       NA         NA     \n\n\nCODE\npaste(\"PROBABILITIES\")\n\n\n[1] \"PROBABILITIES\"\n\n\nCODE\n#probability control = plogis(intercept)\n#probability impasse = plogis(intercept + coefficient)\n\n#FROM predict()\n# newdata <- df_i %>% dplyr::select(pretty_condition, subject, q)\n# preds <- predict(m, newdata = newdata, type = \"response\")\n# preds <- cbind(newdata, preds)\n# p <- preds %>% \n#   dplyr::select(pretty_condition, preds) %>% \n#   group_by(pretty_condition) %>% \n#   summarise(\n#     median = median(preds),\n#     se = sd(preds)/sqrt(n()),\n#     lwr = median - 1.96*se,\n#     upr = median + 1.96*se)\n    \n  \n#FROM merTools\n#setup df \nnewdata <- df_i %>% dplyr::select(pretty_condition, subject, q)\n#make predictions\npreds <- predictInterval(m, newdata = newdata,\n                              which = \"fixed\", #full, fixed or random for those only\n                              type = \"probability\", #linear.prediction\n                              stat = \"median\",\n                              n.sims = 1000,\n                              level = 0.80) #width of prediction interval\n\n\nWarning: executing %dopar% sequentially: no parallel backend registered\n\n\nCODE\n#join predictions to the new dataframe\npreds <- cbind(newdata, preds)\n#summarize\n(summ_preds <- preds %>% \n  dplyr::select(pretty_condition, fit, lwr, upr) %>% \n  group_by(pretty_condition) %>% \n  summarise(\n    median = median(fit),\n    lower = median(lwr),\n    upper = median(upr)\n  )) \n\n\n# A tibble: 2 × 4\n  pretty_condition  median    lower  upper\n  <fct>              <dbl>    <dbl>  <dbl>\n1 control          0.00408 0.000648 0.0249\n2 impasse          0.234   0.0626   0.585 \n\n\n\n\nINFERENCE\n(In Dissertation)\nWe fit a mixed effects binomial logistic regression model with random intercepts for subjects and questions. Note that we choose to model these data at the item (i.e. question) rather than than subject (i.e. total score) level because the structure of a mixed effects model allows us to differentiate between random variance introduced by individual subjects and questions, versus the expected systematic variance of CONDITION. A likelihood ratio test indicates that a model including a fixed effect of CONDITION explains significantly more variance in the data than an intercepts-only baseline model (\\(\\chi^2 (3) = 18.66, p < 0.001\\)). The explanatory power of the entire model is substantial (\\(conditional \\ R^2 = 0.89\\)) and the part related to the fixed effect CONDITION (\\(marginal \\ R^2\\)) explains 15% of variance. Consistent with our hypothesis, the impasse condition substantially increases the odds of a correct response. Across the entire task participants in the impasse condition were 75 times more likely to offer a correct response, compared with those in the control condition ( \\(e^{\\beta_1} = 75.51, p < 0.001\\), \\(95 \\% \\  CI [8.46, 674]\\)). Based on the fixed effect of CONDITION, The model predicts that the probability of a correct response in the control condition is effectively 0% (95% CI [6.8e-4, 0.02]), and the probability of a correct response in the impasse condition increases to 24% (95% CI [0.06, 0.60]).\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.03\n<0.001\n\n\npretty condition[impasse]\n75.51\n8.46 – 673.76\n<0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 subject\n22.22\n\n\nτ00 q\n0.31\n\n\nICC\n0.87\n\n\nN subject\n126\n\n\nN q\n13\n\nObservations\n1638\n\n\nMarginal R2 / Conditional R2\n0.153 / 0.892\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m, \"(log odds)\" = m)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',\n#              notes = notes)\n#              # output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n\n\n\nVisualize\n\n\nCODE\n## | PLOT PARAMETERS \n\n\n#SJPLOT | MODEL | lOG ODDS\nplot_model(m, transform = NULL,\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.05, #manually adjust to account for directional test\n           ci.lvl = 0.95 ) + #manually adjusted for directional test   \n  labs(title = \"Model ESTIMATE | Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | ODDS RATIO\nplot_model(m, transform = \"exp\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.05, #manually adjust to account for directional test\n           ci.lvl = 0.95 ) + #manually adjusted for directional test   \n  labs(title = \"Model ESTIMATE | Log Odds\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#EASYSTATS | MODEL | ODDS RATIO\n# result <- model_parameters(m, exponentiate = TRUE, component = \"all\")\n# plot(result) + labs(\n#     title = \"Model ESTIMATE | ODDS RATIO\"\n#   )\n\n## | PLOT TESTS\n# result <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\n# plot(result)\n\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\n# plot_model(m, type=\"eff\",\n#            show.intercept = TRUE,\n#            show.values = TRUE,\n#            title = \"Model Prediction | Probability of Accurate Response\",\n#            axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type = \"pred\")[[1]]  +\n  ylim(0,1) +\n  labs(\n    title = \"Model PREDICTION | Probability of Accurate Response\",\n    subtitle = \"Impasse increases probability of correct response\",\n    y = \"Probability of Correct Response\", x = \"Condition\"\n  )\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nCODE\n#GGDIST | MODEL | PREDICTED PROBABILITIES\npreds %>% \n  ggplot(aes( x = fit, y = pretty_condition, fill = pretty_condition)) + \n  stat_halfeye(alpha = 0.5, normalize = \"xy\") + \n  xlim(0,0.3) + theme_clean() + labs(\n    title = \"Model PREDICTION | Probability of Accurate Response\",\n    subtitle = \"TODO check preds to see if fixed or includes random\"\n  )\n\n\n\n\n\nCODE\n# SIMULATE FIXED EFFECTS\n# simulate values of fixed effects \n# (feEx <- FEsim(m,  oddsRatio = FALSE, n.sims = 1000))\n# PLOT estimates of fixed effects\n# plotFEsim(feEx) +\n#   theme_bw() + labs(title = \"Coefficient Plot of InstEval Model\",\n#                     x = \"Median Effect Estimate\")\n\n# SIMULATE RANDOM EFFECTS\n# simulate values of random effects\n# reEx <- REsim(m)\n# PLOT estimates of random effects\n# plotREsim(reEx)\n\n\n\n\nDiagnostics\n\n\nCODE\n# print(\"SANITY CHECK REPORTING\")\n# report(m)\n\n# print(\"MODEL PERFORMANCE\")\n# performance(m)\n\nprint(\"DIAGNOSTICS\")\n\n\n[1] \"DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m)\n\n\n\n\n\n\n\nSanity Check :: Bayesian\n\n\nCODE\n# ## 0 | SETUP\n# #confirm 13 items [all discriminating items]\n# nrow(df_i) / nrow(df_s) == 13\n# #confirm all factors \n# is.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$pretty_condition) && is.factor(df_i$accuracy)\n# \n#\n# \n# print(\"FIXED Condition + Subject & Item random intercepts\")\n# Bmm.CrSQ <- brm( accuracy ~ pretty_condition + (1|subject) + (1|q), \n#                  data = df_i, \n#                  family = \"bernoulli\",\n#                  chains = 4, iter = 2000, warmup = 1000,\n#                  cores = 4, seed = 1234,\n#                  backend = \"cmdstanr\",\n#                  file =\"analysis/SGC3A/models/sgc3a_brms_acc_Bmm.CrSQ_LAB.rds\")\n# \n# #get Priors \n# # describe_priors(Bmm.CrSQ)\n# \n# #GRAPHICAL POSTERIOR PREDICTION CHECKS\n# pp_check(Bmm.CrSQ)\n# \n# #DESCRIBE MODEL\n# (d <- describe_posterior(ci=.95, Bmm.CrSQ))\n# \n# #SEE MODEL\n# plot(pd(Bmm.CrSQ))\n# #convert to a pd value\n# (pds <- pd_to_p(d$pd))\n\n\nA likelihood ratio test indicates adding CONDITION as a fixed effect to a logistic regression model including a fixed effect random intercepts for SUBJECT and QUESTION explains more variance in the data than random-effects only model.\n\n\nCODE\n# #::::: GGDIST POSTERIOR PROBABILITY OF RESPONSE\n# ##WORKING\n# ## VIS probability of correct response\n# #TAKES A REALLY LONG TIME\n# \n# #1 | get draws\n# draws <- df_i %>%\n#   data_grid(pretty_condition, subject, q) %>%\n#   add_fitted_draws(Bmm.CrSQ,\n#                    # n = 100,\n#                    # dpar = TRUE,\n#                    # transform = TRUE, #gives prob%, otherwise OR\n#                    re_formula = NA)\n# # draws %>% write_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n# \n# #OR load from file\n# # draws <- read_rds(\"analysis/SGC3A/models/draws_BB.catCrSQ.rds\")\n# \n# #2| VISUALIZE PREDICTIONS | GGDIST\n# ##TODO figure out height normalization.\n# ##do it with much smaller number of draws \n# #TODO adjust bandwidth/smoothing? + put on same line + \n# #TAKES A REAAALY LONG TIME\n# # draws %>% sample_n(1000) %>% \n# #   ggplot(aes(x = .value,  y = 0, fill = pretty_condition)) +\n# #   stat_slab(width = c(.95), alpha = 1, normalize=\"xy\") +\n# #   #normalize = all, panels, xy, groups, none\n# #   xlim(0,1) + labs(\n# #     title = \"Model Predicted Probability of Correct Response\",\n# #     x = \"probability of correct response\",\n# #     y = \"Interpretation\"\n# #   ) +  theme_clean() #+ ggeasy::easy_remove_legend() + ggeasy::easy_remove_y_axis()\n# # #TO PLOT ON THE SAME LINE, INCLUDE Y = 0 in aes and ggeasy::remove_y_axis()"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_lab_hypotesting.html#h1a-overall-interpretation-state",
    "href": "analysis/SGC3A/4_sgc3A_lab_hypotesting.html#h1a-overall-interpretation-state",
    "title": "6  (Lab) Hypothesis Testing",
    "section": "H1A | OVERALL INTERPRETATION STATE",
    "text": "H1A | OVERALL INTERPRETATION STATE\nDo Ss in the IMPASSE condition offer less-orthogonal interpretations across the test phase questions?\nWhile absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn’t allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject’s response, and determine if these interpretations differ by experimental condition. State is a 3-category variable that groups the following interpretations:\n\n“orthogonal” [reference category] includes orthogonal and satisficing responses ==> indicates a primarily orthogonal state of coordinate system understanding\n“other” includes: blank, reference point, responses that can’t be classified (including selecting all datapoints), => indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly not orthogonal nor triangular\n“angular” includes ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n“triangular” includes correct triangular and ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n\n\n\n\n\n\n\n\nResearch Question\nDoes Ss in the impasse condition produce less orthogonal responses across questions?\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of producing unknown and triangle-like response states across all items\n\n\nData\n\ndata: df_items where q nin 6,9 (13 discriminant test phase items)\noutcome: state ( 3 level factor from high_interpretation )\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMIXED Multinomial (Logistic) Regression on state predicted by condition\n\nAlternative:\n\nMIXED Ordinal regression on state (doesn’t meet proportional odds assumption-I think)\nMIXED Multinomial or Ordinal regression on high_interpretation (some cells are 0, produces problems)\n\n\n\n\n\nSetup\n\n\nCODE\ndf_i = df_items %>% filter(q %nin% c(6,9)) %>% dplyr::select(q,subject,state,pretty_condition)\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_manual(values = paletteer::paletteer_d(\"ggthemes::calc\", 4))+\n  # facet_wrap(~pretty_mode) + \n  theme(legend.position = \"bottom\")+\n   labs(title = \"DISTRIBUTION of Interpretation\",\n       x = \"Condition\",\n       y = \"Proportion of Questions\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_manual(values = paletteer::paletteer_d(\"ggthemes::calc\", 4))+\n  facet_wrap(~q) +\n   labs(title = \"Interpretation by Question\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nA proportional bar chart visualizing the proportion of each interpretations type by condition for each data collection modality (left/right facet) reveals that the pattern of responses are similar regardless of the data collection modality, by differ by condition. In the impasse condition, there are more triangular responses than in control. In the impasse condition, there are also more positive transition (i.e. triangle-like) and neutral (ie. blank or uncertain response types) than in the control condition.\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\ntable(df_i$state, df_i$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n            \n             control impasse     Sum\n  orthogonal 0.69727 0.32692 0.50916\n  other      0.07320 0.19231 0.13370\n  angular    0.00993 0.03846 0.02442\n  triangular 0.21960 0.44231 0.33272\n  Sum        1.00000 1.00000 1.00000\n\n\nCODE\n(t <- table(df_i$state, df_i$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n            \n             control impasse  Sum\n  orthogonal     562     272  834\n  other           59     160  219\n  angular          8      32   40\n  triangular     177     368  545\n  Sum            806     832 1638\n\n\n\n\nMIXED MULTINOMIAL REGRESSION\nDoes condition affect the response state of of items across the task?\nFit a MIXED logistic regression predicting interpretation state (k=3) by condition(k = 2).\n\nCan use mclogit mblogit() with random effect or bayesian brms package b/c nlme, lme4 don’t support random effects on multinomial (ie no categorical family on glmer())\nAlternative would be to manually run [k-1] X binomial mixed models [should compare outcomes]\n[k-1] equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) [essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing [reference category] vs [this category])\nFor each equation:\n\n\\(\\beta_{0}\\) = Log Odds of [this category type vs. reference category type) response in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of [this category type vs. reference category type] response in CONTROL condition\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for [this category] type response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of [this. vs reference category type] response in IMPASSE (vs) CONTROL\nTwo-tailed NHST Null hypothesis: \\(\\beta_{impasse} = 0\\) the odds for [this category of response vs. reference] are not different for IMPASSE condition\nAlternative hypothesis: \\(\\beta_{impasse} \\ne 0\\) the odds of [this category of response vs. reference] increases or decreases for IMPASSE condition\n\n\n\nFit Model [mblogit]\n\n\nCODE\n#https://www.elff.eu/software/mclogit/manual/mblogit/\n#\"baseline category logit\" model matches multinom()\n\n#check reference level \nprint(\"Categories (first is reference)\")\n\n\n[1] \"Categories (first is reference)\"\n\n\nCODE\nlevels(df_i$state)\n\n\n[1] \"orthogonal\" \"other\"      \"angular\"    \"triangular\"\n\n\nCODE\n#FIT EMPTY MODEL\n# print(\"EMPTY MODEL\")\nmm.cat.rSQ <- mblogit(state ~ 1 , \n                      random = list( ~ 1|subject, ~1|q), \n                      data = df_i)\n\n\n\nIteration 1 - deviance = 2540 - criterion = 0.821\nIteration 2 - deviance = 2130 - criterion = 0.0764\nIteration 3 - deviance = 2018 - criterion = 0.0241\nIteration 4 - deviance = 1942 - criterion = 0.00947\nIteration 5 - deviance = 1890 - criterion = 0.00588\nIteration 6 - deviance = 1908 - criterion = 0.00469\nIteration 7 - deviance = 1881 - criterion = 0.0216\nIteration 8 - deviance = 1912 - criterion = 0.0101\nIteration 9 - deviance = 1897 - criterion = 0.0089\nIteration 10 - deviance = 1905 - criterion = 0.00172\nIteration 11 - deviance = 1934 - criterion = 0.000253\nIteration 12 - deviance = 1887 - criterion = 0.0168\nIteration 13 - deviance = 1906 - criterion = 0.00498\nIteration 14 - deviance = 1917 - criterion = 0.0023\nIteration 15 - deviance = 1828 - criterion = 0.0177\nIteration 16 - deviance = 1851 - criterion = 0.00603\nIteration 17 - deviance = 1903 - criterion = 0.000756\nIteration 18 - deviance = 1885 - criterion = 0.0197\nIteration 19 - deviance = 1899 - criterion = 0.00449\nIteration 20 - deviance = 1915 - criterion = 0.00144\nIteration 21 - deviance = 1879 - criterion = 0.00235\nIteration 22 - deviance = 1911 - criterion = 0.00166\nIteration 23 - deviance = 1888 - criterion = 0.0101\nIteration 24 - deviance = 1921 - criterion = 0.00392\nIteration 25 - deviance = 1898 - criterion = 0.00654\n\n\nWarning: Algorithm did not converge\n\n\nCODE\n#summary(mm.cat.rSQ)\n\n#FIT PREDICTOR MODEL\n# print(\"PREDICTOR MODEL\")\nmm.cat.CrSQ <- mblogit(state ~ pretty_condition , \n                  random = list( ~ 1|subject, ~1|q), \n                  data = df_i)\n\n\n\nIteration 1 - deviance = 2516 - criterion = 0.818\nIteration 2 - deviance = 2152 - criterion = 0.0677\nIteration 3 - deviance = 1988 - criterion = 0.0332\nIteration 4 - deviance = 1943 - criterion = 0.00696\nIteration 5 - deviance = 1920 - criterion = 0.00226\nIteration 6 - deviance = 1903 - criterion = 0.000947\nIteration 7 - deviance = 1895 - criterion = 0.00044\nIteration 8 - deviance = 1891 - criterion = 0.000212\nIteration 9 - deviance = 1891 - criterion = 0.0000998\nIteration 10 - deviance = 1891 - criterion = 0.0000456\nIteration 11 - deviance = 1891 - criterion = 0.0000207\nIteration 12 - deviance = 1891 - criterion = 9.59e-06\nIteration 13 - deviance = 1891 - criterion = 4.67e-06\nIteration 14 - deviance = 1891 - criterion = 2.41e-06\nIteration 15 - deviance = 1891 - criterion = 1.32e-06\nIteration 16 - deviance = 1891 - criterion = 7.49e-07\nIteration 17 - deviance = 1891 - criterion = 4.38e-07\nIteration 18 - deviance = 1891 - criterion = 2.6e-07\nIteration 19 - deviance = 1891 - criterion = 1.55e-07\nIteration 20 - deviance = 1891 - criterion = 9.29e-08\nIteration 21 - deviance = 1891 - criterion = 5.56e-08\nIteration 22 - deviance = 1891 - criterion = 3.32e-08\nIteration 23 - deviance = 1891 - criterion = 1.98e-08\nIteration 24 - deviance = 1891 - criterion = 1.18e-08\nIteration 25 - deviance = 1891 - criterion = 7.04e-09\nconverged\n\n\nCODE\n# summary(mm.cat.CrSQ)\n\n#COMPARE MODEL FIT\npaste(\"AIC wth predictor is lower than empty model?\", AIC(mm.cat.rSQ) > AIC(mm.cat.CrSQ))\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(mm.cat.rSQ, mm.cat.CrSQ)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName        |    Model | df | df_diff |  Chi2 |      p\n------------------------------------------------------\nmm.cat.rSQ  | mmblogit | 15 |         |       |       \nmm.cat.CrSQ | mmblogit | 18 |       3 | 18.39 | < .001\n\n\n\n\nDescribe\n\n\nCODE\nm <- mm.cat.CrSQ\n\n#DESCRIBE MODEL\nsummary(m)\n\n\nWarning in sqrt(diag(vcov.phi)): NaNs produced\n\n\n\nCall:\nmblogit(formula = state ~ pretty_condition, data = df_i, random = list(~1 | \n    subject, ~1 | q))\n\nEquation for other vs orthogonal:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -2.574      0.527   -4.88  1.1e-06 ***\npretty_conditionimpasse    2.488      0.385    6.47  9.9e-11 ***\n\nEquation for angular vs orthogonal:\n                           Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                  -4.804      0.800   -6.00  1.9e-09 ***\ntri(Intercept)               -2.924      0.782   -3.74  0.00018 ***\npretty_conditionimpasse       2.989      0.614    4.87  1.1e-06 ***\ntripretty_conditionimpasse    3.454      0.795    4.34  1.4e-05 ***\n\nEquation for triangular vs orthogonal:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -2.924      0.782   -3.74  0.00018 ***\npretty_conditionimpasse    3.454      0.795    4.34 0.000014 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Co-)Variances:\nGrouping level: subject \n             Estimate            Std.Err.      \nother~1       2.93                NaN          \nangular~1     3.15  5.29         1.25 2.60     \ntriangular~1  5.43  8.25 16.08   3.26 5.21 8.49\n\nGrouping level: q \n             Estimate         Std.Err.      \nother~1      2.54             12.0          \nangular~1    2.88 4.91        19.0 29.7     \ntriangular~1 2.13 3.54 3.36   15.1 23.6 18.6\n\nNull Deviance:     4540 \nResidual Deviance: 1890 \nNumber of Fisher Scoring iterations:  25\nNumber of observations\n  Groups by subject: 126\n  Groups by q: 13\n  Individual observations:  1638\n\n\nCODE\n#INTERPRET COEFFICIENTS\npaste(\"LOG ODDS\")\n\n\n[1] \"LOG ODDS\"\n\n\nCODE\ntidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = \"Wald\")\n\n\nWarning: The `tidy()` method for objects of class mmblogit is not maintained by\nthe broom team, and is only supported through the lm tidier method. Please be\ncautious in interpreting and reporting broom output.\n\nWarning: NaNs produced\n\n\n# A tibble: 6 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  <chr>                    <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 other~(Intercept)        -2.57     0.527     -4.88 1.06e- 6    -3.61     -1.54\n2 angular~(Intercept)      -4.80     0.800     -6.00 1.92e- 9    -6.37     -3.24\n3 triangular~(Intercep…    -2.92     0.782     -3.74 1.85e- 4    -4.46     -1.39\n4 other~pretty_conditi…     2.49     0.385      6.47 9.94e-11     1.73      3.24\n5 angular~pretty_condi…     2.99     0.614      4.87 1.13e- 6     1.78      4.19\n6 triangular~pretty_co…     3.45     0.795      4.34 1.40e- 5     1.90      5.01\n\n\nCODE\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\ntidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = \"Wald\", exponentiate = TRUE)\n\n\nWarning: The `tidy()` method for objects of class mmblogit is not maintained by\nthe broom team, and is only supported through the lm tidier method. Please be\ncautious in interpreting and reporting broom output.\n\nWarning: NaNs produced\n\n\n# A tibble: 6 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  <chr>                    <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 other~(Intercept)        -2.57     0.527     -4.88 1.06e- 6    -3.61     -1.54\n2 angular~(Intercept)      -4.80     0.800     -6.00 1.92e- 9    -6.37     -3.24\n3 triangular~(Intercep…    -2.92     0.782     -3.74 1.85e- 4    -4.46     -1.39\n4 other~pretty_conditi…     2.49     0.385      6.47 9.94e-11     1.73      3.24\n5 angular~pretty_condi…     2.99     0.614      4.87 1.13e- 6     1.78      4.19\n6 triangular~pretty_co…     3.45     0.795      4.34 1.40e- 5     1.90      5.01\n\n\nCODE\n# paste(\"MODEL INFO\")\n# glance(m)\n\n#PERFORMANCE\nperformance(m)\n\n\n# Indices of model performance\n\nAIC       |       BIC |  RMSE | Sigma\n-------------------------------------\n21103.626 | 21200.848 | 0.242 | 1.077\n\n\n\n\nTODO Inference\n\nBeing in the IMPASSE condition increases the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 12 (z = 6.48, p < 0.001) . Participants in the impasse condition were 12x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 29 (z = 4.63, p < 0.001 ). Participants in the impasse condition were more than 29x as likely to give an a triangular response rather than an orthogonal response compared to participants in control.\nAs with the (binary) logistic regression on accuracy ~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 in the control condition. (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \nstate\n\n\nPredictors\nEstimates\nCI\np\n\n\nother~(Intercept)\n-2.57\n-3.61 – -1.54\n<0.001\n\n\nangular~(Intercept)\n-4.80\n-6.37 – -3.24\n<0.001\n\n\ntriangular~(Intercept)\n-2.92\n-4.46 – -1.39\n<0.001\n\n\nother~prettyconditionimpasse\n2.49\n1.73 – 3.24\n<0.001\n\n\nangular~prettyconditionimpasse\n2.99\n1.78 – 4.19\n<0.001\n\n\ntriangular~prettyconditionimpasse\n3.45\n1.90 – 5.01\n<0.001\n\n\n\nN subject\n126\n\n\nN q\n13\n\nObservations\n1638\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m, \"(log odds)\" = m)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',\n#              notes = notes)\n#              # output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n\n\n\nVisualize\n\n\nCODE\n## | PLOT PARAMETERS \n\n#SJPLOT | MODEL | ODDS RATIO\nplot_model(m, \n           transform = \"exp\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE)\n\n\n\n\n\nCODE\n#TODO SEPARATE THIS BY EQUATION \n# ms <- model_parameters(Bmm.cat.CrSQ, component = \"conditional\")\n# m1 <- ms %>% filter(str_detect(Parameter, \"muother\"))\n# plot(m1)\n\n#EASYSTATS | MODEL | ODDS RATIO\nresult <- model_parameters(m, exponentiate = TRUE, component = \"all\")\nplot(result, show_labels = TRUE, n_columns = 3)\n\n\n\n\n\nCODE\n# result <- simulate_parameters(m)\n# plot(result, stack = FALSE)\n\n## | PLOT TESTS\nresult <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\n\n\nWarning: Could not estimate a good default ROPE range. Using 'c(-0.1, 0.1)'.\n\n\nCODE\nplot(result)\n\n\n\n\n\nCODE\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\n# plot_model(m, type=\"eff\",\n#            show.intercept = TRUE,\n#            show.values = TRUE,\n#            title = \"Model Prediction | Probability of Accurate Response\",\n#            axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n# \n# #PLOT MODEL PREDICTION\n# plot_model(m, type = \"pred\")[[1]] + \n#   ylim(0,1) + labs(\n#     title = \"Model Prediction | Probability of Accurate Response\",\n#     subtitle = \"Impasse increases Probability of Correct Response\"\n#   )\n\n#TODO EMMEANS for the estimated marginal means\n\n\n\n\nDiagnostics\n\n\nCODE\n# check_model(m)\n\n\n\n\nFit Model [brms]\n\n\nCODE\n#BAYESIAN RANDOM ONLY\nBmm.cat.rSQ <- brm( state ~ 1 + (1|subject) + (1|q), \n                 data = df_i, \n                 family = \"categorical\",\n                 chains = 4, iter = 2500, warmup = 1000,\n                 cores = 4, seed = 1234,\n                 save_pars = save_pars(all = TRUE),\n                 # backend = \"cmdstanr\",\n                 file =\"analysis/SGC3A/models/sgc3a_brms_state_Bmm.cat.rSQ_LAB.rds\")\n\n\n#UNINFORMATIVE PRIOR BAYESIAN MIXED VERSION\n# flat_Bmm.cat.CrSQ <- brm( state ~ pretty_condition + (1|subject) + (1|q), \n#                  data = df_i, \n#                  family = \"categorical\",\n#                  chains = 4, iter = 2500, warmup = 1000,\n#                  cores = 4, seed = 1234,\n#                  save_pars = save_pars(all = TRUE),\n#                  # backend = \"cmdstanr\",\n#                  file =\"analysis/SGC3A/models/sgc3a_brms_state_FLAT_Bmm.cat.CrSQ_LAB.rds\")\n\n\n# determine default priors \n# prior_summary(flat_Bmm.cat.CrSQ)\n\n#set priors [see justification, below]\ninf_priors <- c(\n  # too strong?\n  # prior(normal(-6.91, 0.201),  class = \"Intercept\", dpar = \"muangular\"),\n  # prior(normal(-6.91, 0.201),  class = \"Intercept\", dpar = \"muother\"),\n  # prior(normal(-6.91, 0.201),  class = \"Intercept\", dpar = \"mutriangular\"),\n  #prior on INTERCEPTS \n  #25% chance of each answer in control, scale = from 0.01 to 62%\n  prior(normal(-1.1, 1.5),  class = \"Intercept\", dpar = \"muangular\"),\n  prior(normal(-1.1, 1.5),  class = \"Intercept\", dpar = \"muother\"),\n  prior(normal(-1.1, 1.5),  class = \"Intercept\", dpar = \"mutriangular\"),\n  #prior on COEFFICIENT\n  #likely to change odds between 0 and 2.4\n  prior(normal(0, 2.42), class = b, coef=\"pretty_conditionimpasse\", dpar = \"muangular\"),\n  prior(normal(0, 2.42), class = b, coef=\"pretty_conditionimpasse\", dpar = \"muother\"),\n  prior(normal(0, 2.42), class = b, coef=\"pretty_conditionimpasse\", dpar = \"mutriangular\")\n)\n\n#INFORMATIVE PRIORS\nBmm.cat.CrSQ <- brm( state ~ pretty_condition + (1|subject) + (1|q), \n                 data = df_i, \n                 family = \"categorical\",\n                 prior = inf_priors,\n                 chains = 4, iter = 2500, warmup = 1000,\n                 cores = 4, seed = 1234,\n                 save_pars = save_pars(all = TRUE),\n                 control = list(adapt_delta = 0.98),  # to deal with divergent transitions\n                 # backend = \"cmdstanr\",\n                 file =\"analysis/SGC3A/models/sgc3a_brms_state_Bmm.cat.CrSQ_LAB.rds\"\n                 )\n\n#a bayes factor model comparison of the flat vs informative prior models suggest convicing evidence that \n#informative prior model is a better fit\n# bayesfactor(Bmm.cat.CrSQ, flat_Bmm.cat.CrSQ)\n\n# PRIORS LOGIC \n# https://www.bayesrulesbook.com/chapter-13.html#building-the-logistic-regression-model\n\n#expectation for probability of _better_ response [in control]?\n#very low probability center: 0.1% [very low]; as logodds = logit(0.001) = -6.91\n#range from 0 to 55%  logit(0.55) = 0.201\n#probability of 0.1 to 55% is equivalent to [logodds] -6.91 +/ 2* 0.201\n#therefore... prior for intercept => Normal(−6.91, 0)\n\n\n#expectation for probability of _better_ response [in impasse]?\n#increases probablity from 0 % \n# 0 [very low]; as OR  = exp(0) = 1\n#range from 0 to 90%  exp(0.9) = 2.46\n#probability of 0 to 90% is equivalent to [ODDS scale] 1 +/ 2* 2.42\n#on log odds scale ? [0, ]\n#therefore... prior for intercept => Normal(1, 2.42)\n                             # prior = normal(0.07, 0.035),\n\n\n\n\nDescribe\n\n\nCODE\n# best model\nm <- Bmm.cat.CrSQ\n\n#::::::::: PRINT MODEL \n\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m)\n\n\n Family: categorical \n  Links: muother = logit; muangular = logit; mutriangular = logit \nFormula: state ~ pretty_condition + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 1638) \n  Draws: 4 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 6000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          1.43      0.36     0.89     2.29 1.00     1738\nsd(muangular_Intercept)        2.24      0.85     1.08     4.35 1.00     1351\nsd(mutriangular_Intercept)     0.99      0.27     0.59     1.59 1.00     1710\n                           Tail_ESS\nsd(muother_Intercept)          3069\nsd(muangular_Intercept)        2388\nsd(mutriangular_Intercept)     2989\n\n~subject (Number of levels: 126) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          1.32      0.19     0.99     1.71 1.00     1738\nsd(muangular_Intercept)        1.78      0.40     1.05     2.63 1.00     1839\nsd(mutriangular_Intercept)     4.51      0.50     3.63     5.59 1.00     1084\n                           Tail_ESS\nsd(muother_Intercept)          3080\nsd(muangular_Intercept)        2074\nsd(mutriangular_Intercept)     2016\n\nPopulation-Level Effects: \n                                     Estimate Est.Error l-95% CI u-95% CI Rhat\nmuother_Intercept                       -3.09      0.47    -4.02    -2.18 1.00\nmuangular_Intercept                     -5.71      0.81    -7.29    -4.14 1.00\nmutriangular_Intercept                  -3.63      0.74    -5.13    -2.19 1.00\nmuother_pretty_conditionimpasse          2.46      0.35     1.80     3.14 1.00\nmuangular_pretty_conditionimpasse        2.74      0.61     1.59     3.99 1.00\nmutriangular_pretty_conditionimpasse     3.58      0.88     1.91     5.41 1.01\n                                     Bulk_ESS Tail_ESS\nmuother_Intercept                        1223     2382\nmuangular_Intercept                      1676     3000\nmutriangular_Intercept                    985     1988\nmuother_pretty_conditionimpasse          2101     3314\nmuangular_pretty_conditionimpasse        3170     3521\nmutriangular_pretty_conditionimpasse      658     1468\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n(d <- describe_posterior(ci=.95, Bmm.cat.CrSQ))\n\n\nSummary of Posterior Distribution\n\nParameter                            | Median |         95% CI |   pd |          ROPE | % in ROPE |  Rhat |     ESS\n-------------------------------------------------------------------------------------------------------------------\nmuother_Intercept                    |  -3.09 | [-4.02, -2.18] | 100% | [-0.18, 0.18] |        0% | 1.003 | 1218.00\nmuangular_Intercept                  |  -5.69 | [-7.29, -4.14] | 100% | [-0.18, 0.18] |        0% | 1.002 | 1620.00\nmutriangular_Intercept               |  -3.60 | [-5.13, -2.19] | 100% | [-0.18, 0.18] |        0% | 1.004 |  983.00\nmuother_pretty_conditionimpasse      |   2.45 | [ 1.80,  3.14] | 100% | [-0.18, 0.18] |        0% | 1.000 | 2080.00\nmuangular_pretty_conditionimpasse    |   2.73 | [ 1.59,  3.99] | 100% | [-0.18, 0.18] |        0% | 1.000 | 3066.00\nmutriangular_pretty_conditionimpasse |   3.55 | [ 1.91,  5.41] | 100% | [-0.18, 0.18] |        0% | 1.006 |  652.00\n\n\nCODE\nprint(\"BAYES FACTOR [comparison to null]\")\n\n\n[1] \"BAYES FACTOR [comparison to null]\"\n\n\nCODE\n#think of this like the anova(model) to get p values for each predictor\n#has to recompile the models with rstan. total drag\n(b <- bayesfactor(Bmm.cat.rSQ, m))\n\n\nWarning: Bayes factors might not be precise.\nFor precise Bayes factors, sampling at least 40,000 posterior samples is recommended.\n\n\nComputation of Bayes factors: estimating marginal likelihood, please wait...\n\n\nWarning: logml could not be estimated within maxiter, rerunning with adjusted starting value. \nEstimate might be more variable than usual.\n\n\nWarning: logml could not be estimated within maxiter, rerunning with adjusted starting value. \nEstimate might be more variable than usual.\n\n\nBayes Factors for Model Comparison\n\n    Model                                            BF\n[2] pretty_condition + (1 | subject) + (1 | q) 9.14e+14\n\n* Against Denominator: [1] 1 + (1 | subject) + (1 | q)\n*   Bayes Factor Type: marginal likelihoods (bridgesampling)\n\n\nCODE\nprint(\"DESCRIBE POSTERIOR\")\n\n\n[1] \"DESCRIBE POSTERIOR\"\n\n\nCODE\n#:::::::: INTERPRET COEFFICIENTS\n# se <- sqrt(diag(stats::vcov(m)))\n# # table of estimates with 95% CI\n# (tab <- cbind(Est = fixef(m),\n#               LL = fixef(m) - 1.96 * se,\n#               UL = fixef(m) + 1.96 * se))\n\npaste(\"LOG ODDS\")\n\n\n[1] \"LOG ODDS\"\n\n\nCODE\n(l <- describe_posterior(m))\n\n\nSummary of Posterior Distribution\n\nParameter                            | Median |         95% CI |   pd |          ROPE | % in ROPE |  Rhat |     ESS\n-------------------------------------------------------------------------------------------------------------------\nmuother_Intercept                    |  -3.09 | [-4.02, -2.18] | 100% | [-0.18, 0.18] |        0% | 1.003 | 1218.00\nmuangular_Intercept                  |  -5.69 | [-7.29, -4.14] | 100% | [-0.18, 0.18] |        0% | 1.002 | 1620.00\nmutriangular_Intercept               |  -3.60 | [-5.13, -2.19] | 100% | [-0.18, 0.18] |        0% | 1.004 |  983.00\nmuother_pretty_conditionimpasse      |   2.45 | [ 1.80,  3.14] | 100% | [-0.18, 0.18] |        0% | 1.000 | 2080.00\nmuangular_pretty_conditionimpasse    |   2.73 | [ 1.59,  3.99] | 100% | [-0.18, 0.18] |        0% | 1.000 | 3066.00\nmutriangular_pretty_conditionimpasse |   3.55 | [ 1.91,  5.41] | 100% | [-0.18, 0.18] |        0% | 1.006 |  652.00\n\n\nCODE\n# (tm <- tidy(m,   conf.int = TRUE))\n\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\n(e <- model_parameters(m, exponentiate = TRUE))\n\n\nParameter                            |   Median |         95% CI |   pd | % in ROPE |  Rhat |     ESS\n-----------------------------------------------------------------------------------------------------\nmuother_Intercept                    |     0.05 | [0.02,   0.11] | 100% |        0% | 1.003 | 1218.00\nmuangular_Intercept                  | 3.38e-03 | [0.00,   0.02] | 100% |        0% | 1.002 | 1620.00\nmutriangular_Intercept               |     0.03 | [0.01,   0.11] | 100% |        0% | 1.004 |  983.00\nmuother_pretty_conditionimpasse      |    11.58 | [6.03,  23.19] | 100% |        0% | 1.000 | 2080.00\nmuangular_pretty_conditionimpasse    |    15.30 | [4.89,  54.22] | 100% |        0% | 1.000 | 3066.00\nmutriangular_pretty_conditionimpasse |    34.71 | [6.72, 223.29] | 100% |        0% | 1.006 |  652.00\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\nCODE\n# tidy(m,   conf.int = TRUE, exponentiate = TRUE)\n# tm %>% mutate(\n#   OR.est = exp(estimate),\n#   exp.low = exp(conf.low),\n#   exp.high = exp(conf.high)\n# ) %>% dplyr::select(effect, component, group, term, OR.est, exp.low, exp.high)\n\npaste(\"PROBABILITIES\")\n\n\n[1] \"PROBABILITIES\"\n\n\nCODE\n#PREDICT METHOD\nnewdata <- df_i %>% dplyr::select(pretty_condition, subject, q)\npreds <- predict(m, newdata = newdata, type = \"response\")\npreds <- cbind(newdata, preds)\n#lengthen data frame to handle multinomial\npreds <- preds %>% \n  dplyr::select(-subject, -q) %>% #marginalize over subject and q\n  pivot_longer(\n  cols = !pretty_condition,\n  values_to = \"preds\",\n  names_to = \"state\",\n) \n\n(p <- preds %>% \n  group_by(pretty_condition, state ) %>%\n  summarise(\n    median = median(preds),\n    se = sd(preds)/sqrt(n()),\n    lwr = median - 1.96*se,\n    upr = median + 1.96*se))\n\n\n# A tibble: 8 × 6\n# Groups:   pretty_condition [2]\n  pretty_condition state             median       se      lwr     upr\n  <fct>            <chr>              <dbl>    <dbl>    <dbl>   <dbl>\n1 control          P(Y = angular)    0.003  0.000735  0.00156 0.00444\n2 control          P(Y = orthogonal) 0.879  0.0123    0.855   0.903  \n3 control          P(Y = other)      0.0365 0.00361   0.0294  0.0436 \n4 control          P(Y = triangular) 0.0152 0.0127   -0.00967 0.0400 \n5 impasse          P(Y = angular)    0.0132 0.00210   0.00912 0.0174 \n6 impasse          P(Y = orthogonal) 0.241  0.0100    0.222   0.261  \n7 impasse          P(Y = other)      0.126  0.00656   0.114   0.139  \n8 impasse          P(Y = triangular) 0.372  0.0131    0.346   0.397  \n\n\nCODE\n##DRAWS METHOD\n# GENERATE draws from model\n# draws <- df_i %>%\n#   data_grid(pretty_condition, subject, q) %>% \n#   add_fitted_draws(Bmm.cat.CrSQ,\n#                    # n = 100,\n#                    # dpar = TRUE,\n#                    # transform = TRUE, #gives prob%, otherwise OR\n#                    re_formula = NA)\n# # draws %>% write_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n# \n# #OR load from file\n# # draws <- read_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n# \n# # SUMMARIZE draws from model\n# (k <- kable(draws %>%\n#   dplyr::select(pretty_condition, .category, .value) %>%\n#   group_by(pretty_condition, .category) %>%\n#   median_hdci(.value), digits = 4, col.names =\n#     c(\"Condition\",\"Category\", \"Probability\",\"Lower Cred.I\",\"Upper Cred.I\", \"CI Width\", \"Point Type\", \"Interval Type\")) %>%\n#   kable_styling())\n\n\n\n\nINFERENCE\n[REPORT POSTERIOR MEDIAN \\(\\exp_{beta}\\), 95 % credible interval, % probability of direction]\nWe fit a (bayesian) multinomial logistic regression model with random intercepts for subjects and questions. A Bayes Factor model comparison (against a random intercepts-only model) indicates extreme evidence for a main effect of CONDITION (BF = 9.31e+15). Consistent with our hypothesis, the impasse condition substantially increases the odds of transitional interpretations.\nAcross the entire task participants in the impasse condition were 12 times more likely to offer an ‘unknown’ rather than orthogonal response compared with those in the control condition ( \\(e^{\\beta_1} = 11.58, 95 \\% CI [6.03, 23.19], pd = 100\\%\\)). Participants in the impasse condition were 15 times more likely to offer an ‘angular’ rather than orthogonal response compared with those in the control condition ( \\(e^{\\beta_1} = 15.30, 95 \\% CI [4.89, 54.22], pd = 100\\%\\)), and 35 times more likely to offer an ‘triangular’ rather than orthogonal response compared with those in the control condition ( \\(e^{\\beta_1} = 34.71, 95 \\% CI [6.72, 223.29], pd = 100\\%\\)).\nThe model estimates that probability of an orthogonal response in the control condition is 92% vs only 38% in IMPASSE The probability of an unknown/uncertain response in the control condition is 4%, vs 20% in impasse The probability of an angular response in the control condition is less than 1%, vs 2% in impasse. The probability of an triangular response in the control condition is 3%, vs 40% in impasse.\nThe impasse condition decreases the probability of an orthogonal response by 54% The impasse condition increases the probability of an uknown/uncertain response by 16% The impasse condition increases the probability of an angular response by 1% the impasse condition increase the probability of a triangular response by 37%\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n'bayes_R2' is not defined for unordered categorical models.\n\n\n\n\n \nstate: other\nstate: angular\nstate: triangular\nstate: other_pretty\nstate: angular_pretty\nstate: triangular_pretty\n\n\nPredictors\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\n\n\nIntercept\n0.00\n0.00 – 0.02\n\n\n0.05\n0.02 – 0.11\n\n\n0.03\n0.01 – 0.11\n\n\n\n\nconditionimpasse\n\n\n15.30\n4.89 – 54.22\n\n\n11.58\n6.03 – 23.19\n\n\n34.71\n6.72 – 223.29\n\n\nRandom Effects\n\n\n\nσ2\n0.40\n\n\n\nτ00\n1.43\n\n\nICC\n0.22\n\n\nN subject\n126\n\n\nN q\n13\n\nObservations\n1638\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m, \"(log odds)\" = m)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',\n#              notes = notes)\n#              # output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n# \n# \n# ## POINT ESTIMATES IN PROBABILITY\n# #1 | get draws\n# draws <- df_i %>%\n#   data_grid(pretty_condition, subject, q) %>%\n#   add_fitted_draws(Bmm.cat.CrSQ,\n#                    # n = 100,\n#                    # dpar = TRUE,\n#                    # transform = TRUE, #gives prob%, otherwise OR\n#                    re_formula = NA)\n# draws %>% write_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n# \n# #2 | SUMMARIZE draws \n# k <- kable(draws %>%\n#   select(pretty_condition, .category, .value) %>%\n#   group_by(pretty_condition, .category) %>%\n#   median_hdci(.value), digits = 2, col.names = \n#     c(\"Condition\",\"Category\", \"Probability\",\"Lower Cred.I\",\"Upper Cred.I\", \"CI Width\", \"Point Type\", \"Interval Type\")) %>% \n#   kable_styling()\n# k\n\n#COMPARISONS\n# c <- draws %>% \n#   # dplyr::select(pretty_condition, .category, .value) %>%\n#   compare_levels(variable = .value, by = pretty_condition, \n#                  comparison = list(c(\"control\",\"impasse\"))) \n#                                    # c(\"adots\",\"interval\"),\n#                                    # c(\"adots\",\"mean\"),\n#                                    # c(\"adots\",\"text\"),\n#                                    # c(\"density\",\"interval\"),\n#                                    # c(\"density\",\"mean\"),\n#                                    # c(\"density\",\"text\"),\n#                                    # c(\"interval\",\"mean\"),\n#                                    # c(\"interval\",\"text\"),\n#                                    # c(\"mean\",\"text\")\n#                                    \n# c %>%\n#   ggplot(aes(x = .value, y = reorder(x =pretty_condition, X = .value)))+\n#   stat_interval(.width = .95, color = \"black\") +\n#   geom_vline(xintercept = 0)+\n#   theme_bw()+\n#   # coord_cartesian(xlim = c(-.5,1)) +\n#   theme_tidybayes() \n# comps\n\n\n\n\nVisualize\n\n\nCODE\n## | PLOT PARAMETERS \n\n#SJPLOT | MODEL | ODDS RATIO\n# plot_model(m, vline.color = \"red\",\n#            show.intercept = TRUE,\n#            show.values = TRUE,\n#            p.threshold = 0.1, #manually adjust to account for directional test\n#            ci.lvl = 0.90 ) + #manually adjusted for directional test\n#   labs(title = \"Model Estimate | Odds Ratio\",\n#        subtitle = \"\",\n#        x = \"Condition\")\n\n\n#EASYSTATS | MODEL | ODDS RATIO\nresult <- model_parameters(Bmm.cat.CrSQ, exponentiate = TRUE, component = \"all\")\nplot(result, show_intercept = TRUE, show_labels = TRUE) \n\n\n\n\n\nCODE\n# + theme_clean()\n\n# \n# result <- estimate_density(m,exponentiate = TRUE)\n# plot(result,  stack = FALSE, priors = TRUE)\n\n## | PLOT TESTS\nresult <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\nplot(result)\n\n\nPicking joint bandwidth of 0.0804\n\n\nWarning: Removed 3600 rows containing non-finite values (stat_density_ridges).\n\n\n\n\n\nCODE\nresult <- rope(m)\nplot(result)\n\n\n\n\n\nCODE\n(result <- pd(m,exponentiate = TRUE))\n\n\nProbability of Direction\n\nParameter                            |   pd\n-------------------------------------------\nmuother_Intercept                    | 100%\nmuangular_Intercept                  | 100%\nmutriangular_Intercept               | 100%\nmuother_pretty_conditionimpasse      | 100%\nmuangular_pretty_conditionimpasse    | 100%\nmutriangular_pretty_conditionimpasse | 100%\n\n\nCODE\nplot(result, show_intercept = TRUE, show_labels = TRUE)\n\n\n\n\n\nCODE\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\n# plot_model(m, type=\"eff\",\n#            show.intercept = TRUE,\n#            show.values = TRUE,\n#            title = \"Model Prediction | Probability of Accurate Response\",\n#            axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n# \n# #PLOT MODEL PREDICTION\nplot_model(m, type = \"pred\")\n\n\nNote: uncertainty of error terms are not taken into account. You may want to use `rstantools::posterior_predict()`.\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#TODO EMMEANS for the estimated marginal means\n\n#TODO OUTPUT TABLE \n#https://arelbundock.com/posts/modelsummary_multinomial_logit/\n# modelsummary(m)\n\n\n\n\nCODE\n#::::: GGDIST POSTERIOR PROBABILITY OF RESPONSE\n##WORKING\n# https://mjskay.github.io/ggdist/reference/stat_slab.html\n## VIS probability of correct response\n#TAKES A REALLY LONG TIME\n\n#1 | get draws\n# draws <- df_i %>%\n#   data_grid(pretty_condition, subject, q) %>% \n#   add_fitted_draws(Bmm.cat.CrSQ,\n#                    # n = 100,\n#                    # dpar = TRUE,\n#                    # transform = TRUE, #gives prob%, otherwise OR\n#                    re_formula = NA) \n# draws %>% write_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n\n#OR load from file\n# draws <- read_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n\n#2| VISUALIZE PREDICTIONS | GGDIST\n##TODO figure out height normalization.\n##do it with much smaller number of draws \n#TODO adjust bandwidth/smoothing? + put on same line + \n#TAKES A REAAALY LONG TIME\n# d <- draws %>%\n#   ggplot(aes(x = .value,  y = pretty_condition, fill = .category)) +\n#   stat_slab(width = c(.95), alpha = 1, normalize=\"xy\") +\n#   #   #normalize = all, panels, xy, groups, none\n#   xlim(0,1) + labs(\n#     title = \"Model Predicted Probability of Correct Response\",\n#     x = \"probability of correct response\",\n#     y = \"Interpretation\"\n#   ) +  theme_clean() #+ ggeasy::easy_remove_legend() + ggeasy::easy_remove_y_axis()\n# # #TO PLOT ON THE SAME LINE, INCLUDE Y = 0 in aes and ggeasy::remove_y_axis()\n# # \n# # ggsave(d, filename = \"figures/sgc3a_BBm.cat.CrSQ_lab_posterior.svg\", width = 6, height =4)\n# d\n\n\n\n\nDiagnostics\n\n\nCODE\n#CHECK Fit of posterior predictive to data\npp_check(Bmm.cat.CrSQ, ndraws=1000)\n\n\n\n\n\nCODE\n#CHECK posterior vs. priors\nresult <- estimate_density(Bmm.cat.CrSQ)\nplot(result, stack = FALSE, priors= TRUE)\n\n\n\n\n\nCODE\n#CHECK model\nplot(Bmm.cat.CrSQ)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOMPARE MBLOGIT to BRMS\n\n\nCODE\ncompare_models(mm.cat.CrSQ, Bmm.cat.CrSQ)\n\n\nParameter                            |          mm.cat.CrSQ |         Bmm.cat.CrSQ\n----------------------------------------------------------------------------------\nother~(Intercept)                    | -2.57 (-3.61, -1.54) |                     \nangular~(Intercept)                  | -4.80 (-6.37, -3.24) |                     \ntriangular~(Intercept)               | -2.92 (-4.46, -1.39) |                     \nother~pretty conditionimpasse        |  2.49 ( 1.73,  3.24) |                     \nangular~pretty conditionimpasse      |  2.99 ( 1.78,  4.19) |                     \ntriangular~pretty conditionimpasse   |  3.45 ( 1.90,  5.01) |                     \nmuother_Intercept                    |                      | -3.09 (-4.02, -2.18)\nmuangular_Intercept                  |                      | -5.69 (-7.29, -4.14)\nmutriangular_Intercept               |                      | -3.60 (-5.13, -2.19)\nmuother_pretty_conditionimpasse      |                      |  2.45 ( 1.80,  3.14)\nmuangular_pretty_conditionimpasse    |                      |  2.73 ( 1.59,  3.99)\nmutriangular_pretty_conditionimpasse |                      |  3.55 ( 1.91,  5.41)\n----------------------------------------------------------------------------------\nObservations                         |                 1638 |                 1638\n\n\nThe predictions of the manual, frequentist mixed multinomial and bayesian mixed multinomial models are comparable."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_lab_hypotesting.html#h1b-q1-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_lab_hypotesting.html#h1b-q1-accuracy",
    "title": "6  (Lab) Hypothesis Testing",
    "section": "H1B | Q1 ACCURACY",
    "text": "H1B | Q1 ACCURACY\nDo Ss in the IMPASSE condition have a higher likelihood of producing a correct response to the first question?\nThe graph comprehension task includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their first exposure to the unconventional triangular coordinate system.\n\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of correctly responding to the first question than those in the CONTROL condition\n\n\nData\n\ndata: df_items where q == 1\noutcome: accuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nLogistic Regression on accuracy predicted by condition\n\naccount for difference in odds of correct score by condition\n\n\nAlternatives:\n\nChi-Square test of independence on outcome accuracy by condition\n\n\n\nNotes\n\nCHIQ SQR is simplest method to examine independence of two categorical factors; LOGISTIC REGRESSION is recommended for binomial ~ continuous; though with regression we can quantify the size of the effect and overall model fit\nindependence assumption : (CHI SQR) as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\ncell frequency : (CHI SQR) expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\n\n\n\nSetup\n\n\nCODE\n#:::::::: PREP DATA\ndf <- df_items %>% filter(q==1)  %>% dplyr::select(accuracy, pretty_condition)\n\n\n\n\nDescribe\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Q1 Accuracy\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nA proportional bar chart visualizing the proportion of incorrect (vs) correct responses in each condition for each data collection modality (left/right facet) reveals that the pattern of responses appear the same regardless of the data collection modality. In both data collection sessions, the proportion of incorrect responses is much greater than the proportion of correct responses, regardless of condition (marginal probability of incorrect). In the impasse condition, the difference in proportions is smaller than the control condition (conditional probability of success in impasse; (i.e) There are more correct responses in the impasse condition than the control condition).\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\npaste(\"Proportions of Correct Responses by Condition\")\n\n\n[1] \"Proportions of Correct Responses by Condition\"\n\n\nCODE\ntable(df$accuracy, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse   Sum\n  incorrect   0.839   0.703 0.770\n  correct     0.161   0.297 0.230\n  Sum         1.000   1.000 1.000\n\n\nCODE\npaste(\"Number of Correct Responses by Condition\")\n\n\n[1] \"Number of Correct Responses by Condition\"\n\n\nCODE\ntable(df$accuracy, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse Sum\n  incorrect      52      45  97\n  correct        10      19  29\n  Sum            62      64 126\n\n\n\n\nLOGISTIC REGRESSION\nFit a logistic regression predicting accuracy (absolute score) (n = 126) by condition (k = 2).\n\n\nParameter estimate: \\(\\beta_{0}\\) = Log Odds of (correct) responses in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of correct response in CONTROL condition\nParameter estimate: \\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for correct response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of correct response in IMPASSE (vs) CONTROL\nNull hypothesis:\\(\\beta_{impasse} \\le 0\\) the odds for a correct response does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\gt 0\\) the odds of a correct response increases\n\n\nFit Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\n\nCODE\n# MODEL FITTING ::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\n# print(\"EMPTY MODEL\")\n# summary(m0)\n\n#: 2 CONDITION model\nm1 <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\n# print(\"PREDICTOR MODEL\")\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.839  -0.839  -0.593  -0.593   1.910  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.649      0.345   -4.77  1.8e-06 ***\npretty_conditionimpasse    0.786      0.441    1.79    0.074 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 135.95  on 125  degrees of freedom\nResidual deviance: 132.63  on 124  degrees of freedom\nAIC: 136.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\ncar::Anova(m1)\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: accuracy\n                 LR Chisq Df Pr(>Chisq)  \npretty_condition     3.31  1      0.069 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > m1$aic)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff | Chi2 |     p\n------------------------------------------\nm0   |   glm |  1 |         |      |      \nm1   |   glm |  2 |       1 | 3.31 | 0.069\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.0687084837283363\"\n\n\nThe Condition predictor decreases AIC, but the Likelihood Ratio Test is marginal. We proceed to examine the predictor model, as we plan to do a 1-tailed NHST .\n\n\nDescribe\n\n\nCODE\n# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL [default two-tailed sig test]\")\n\n\n[1] \"PREDICTOR MODEL [default two-tailed sig test]\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.839  -0.839  -0.593  -0.593   1.910  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.649      0.345   -4.77  1.8e-06 ***\npretty_conditionimpasse    0.786      0.441    1.79    0.074 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 135.95  on 125  degrees of freedom\nResidual deviance: 132.63  on 124  degrees of freedom\nAIC: 136.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\ncar::Anova(m1)\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: accuracy\n                 LR Chisq Df Pr(>Chisq)  \npretty_condition     3.31  1      0.069 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: \n\n# one-sided (right tail) z test for B COEFFICIENT\n#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients\n\n#SANITY CHECK 2-tailed test should match the model output\ntt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"p value for two-tailed test, null B = 0 : \",round(tt,3))\n\n\n[1] \"p value for two-tailed test, null B = 0 :  0.074\"\n\n\nCODE\not <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"BUT we want a one tailed directional, null: B <= 0: \",round(ot,3))\n\n\n[1] \"BUT we want a one tailed directional, null: B <= 0:  0.037\"\n\n\nCODE\npaste(\"adjusted confint for directional hypothesis\")\n\n\n[1] \"adjusted confint for directional hypothesis\"\n\n\nCODE\n(dcint <- confint(m1, level = 0.90)) # get 90% for right side))\n\n\nWaiting for profiling to be done...\n\n\n                            5 %  95 %\n(Intercept)             -2.2578 -1.11\npretty_conditionimpasse  0.0749  1.53\n\n\nCODE\n# https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte\n\n#:::::::: INTERPRET COEFFICIENTS\n\n# print(\"Confidence Interval —- LOG ODDS\")\n# confint(m1) #not adjusted for 1-tailed\nprint(\"Coefficients —- ODDS RATIOS\")\n\n\n[1] \"Coefficients —- ODDS RATIOS\"\n\n\nCODE\n# (e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiated, not adjusted\n(e <- cbind( exp(coef(m1)), exp(dcint))) #exponentiated, adjusted\n\n\n                                5 %  95 %\n(Intercept)             0.192 0.105 0.329\npretty_conditionimpasse 2.196 1.078 4.631\n\n\nCODE\nprint(\"MODEL PREDICTIONS\")\n\n\n[1] \"MODEL PREDICTIONS\"\n\n\nCODE\n# Retrieve predictions as probabilities \n# (for each level of the predictor)\npred.control <- predict(m1,data.frame(pretty_condition=\"control\"),type=\"response\")\n#this should match : plogis(intercept coefficient)\npaste(\"Probability of success in control,\", pred.control)\n\n\n[1] \"Probability of success in control, 0.161290322580645\"\n\n\nCODE\npred.impasse <- predict(m1,data.frame(pretty_condition=\"impasse\"),type=\"response\")\n#this should match : plogis(intercept coefficient + predictor coeff)\npaste(\"Probability of success in impasse,\", pred.impasse)\n\n\n[1] \"Probability of success in impasse, 0.296875000000275\"\n\n\n\n\nInference\nIn Dissertation\nWe fit a logistic regression model to explore the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 1.79, p = 0.04, one-tailed). The model predicts that the odds of a correct response on the first question in the impasse condition increase by nearly 120% (\\(e^{beta_{impasse}}\\) = 2.19, 95% CI [1.08, 4.63]) over the control condition. The intercept \\(\\beta_{0}\\) parameter is also significant, (\\(e^{b_{0}}\\) = 0.19, p < 0.001, 95% CI [0.11, 0.33]) indicating that the odds of a correct response in the control condition are significantly less than even (less than 50/50 chance of correct response in control condition).\nEquivalent statements:\n\nbeing in impasse condition increases log odds of correct response by 0.79 (over control)\nbeing in impasse increases odds of correct response in impasse over control increases by factor of 2.19\nprobability of correct response in impasse predicted as 30%, vs only 16% in control condition\n\n\n\nVisualize\n\n\nCODE\n#SET MODEL\nm <- m1\n\n#GGSTATS | MODEL | LOG ODDS \n# ggcoefstats(m1, output = \"plot\", \n#               conf.level = 0.90) + \n#   labs(x = \"Log Odds Estimate\", \n#        title = \"LOGODDS | Q1 Accuracy ~ condition\",\n#        subtitle = \"(p is for two tailed test)\")\n\n\n#PARAMETERS | MODEL | SIMULATED PARAMETERS\n# similar to bayesian dist of estimate\n# result <- simulate_parameters(m1)\n# #rename params so intercept is plotted \n# result$Parameter[1] <- \"condition [control]\"\n# result$Parameter[2] <- \"condition [impasse]\"\n# plot(result) \n\n#EQUIVALENCE TEST [not sure if appropriate for log model?]\n# https://journals.sagepub.com/doi/10.1177/2515245918770963#:~:text=Consequently%2C%20when%20reporting%20an%20equivalence,values%20is%20smaller%20than%20alpha.\n# https://easystats.github.io/parameters/reference/equivalence_test.lm.html\n# (result <- equivalence_test(m1, rule = \"classic\", component = c(\"all\")))\n# plot(result,   show_intercept = TRUE) + \n#   scale_y_discrete(labels = c(\"impasse\", \"control\")) + \n#   labs( title = \"Equivalence Test for Model Parameter Estimates\")\n\n\n#PARAMETERS | MODEL | ODDS RATIO \n# result <- model_parameters(m1,exponentiate = TRUE)\n# #rename params \n# result$Parameter[1] <- \"condition [control]\"\n# result$Parameter[2] <- \"condition [impasse]\"\n# plot(result,   show_intercept = TRUE) +  labs(\n#   title = \"Model Parameter Estimates\"\n# ) + theme_clean() + theme(legend.position=\"blank\")\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m, type = \"est\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) +  #manually adjusted for directional test   \n  scale_y_continuous() + #remove to put on log scale x axis \n  scale_x_discrete(labels=c(\"control\",\"impasse\"))+\n  labs(title = \"MODEL ESTIMATE | Q1 Accuracy ~ condition\",\n       subtitle = \"Impasse increases odds of correct response on Q1\",\n       x = \"Condition\") + theme_clean()\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type=\"pred\")[[1]] +\n  ylim(0,1) + #scale y axis to actual range\n  labs(title = \"MODEL PREDICTION  | Q1 Accuracy ~ condition\",\n       subtitle = \"Impasse increases probability of correct response on Q1\",\n       x = \"Condition\") + theme_clean()\n\n\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.19\n0.09 – 0.36\n<0.001\n\n\npretty condition[impasse]\n2.20\n0.94 – 5.38\n0.074\n\n\nObservations\n126\n\n\nR2 Tjur\n0.026\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m1, \"(log odds)\" = m1)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', \n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n\n\n\nDiagnostics\n\n\nCODE\n# print(\"SANITY CHECK REPORTING\")\n# report::report(m1)\n\n#print(\"MODEL PERFORMANCE\")\n# performance(m1)\n\nprint(\"MODEL DIAGNOSTICS\")\n\n\n[1] \"MODEL DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m1)"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_lab_hypotesting.html#h1b-q1-interpretation-state",
    "href": "analysis/SGC3A/4_sgc3A_lab_hypotesting.html#h1b-q1-interpretation-state",
    "title": "6  (Lab) Hypothesis Testing",
    "section": "H1B | Q1 INTERPRETATION STATE",
    "text": "H1B | Q1 INTERPRETATION STATE\nDo Ss in the IMPASSE condition offer less-orthogonal interpretations on first question?\nWhile absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn’t allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject’s response, and determine if these interpretations differ by experimental condition. State is a 3-category derived response variable that groups the following interpretations:\n\n“orthogonal” [reference category] includes orthogonal and satisficing responses ==> indicates a primarily orthogonal state of coordinate system understanding\n“other” includes: blank, reference point, responses that can’t be classified (including selecting all datapoints), => indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly not orthogonal nor triangular\n“angular” includes ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n“triangular” includes correct triangular and ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n\n\n\n\n\n\n\n\nResearch Question\nDoes Ss in the impasse condition produce less orthogonal responses on the first question?\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of producing unknown and and triangle-like response states, relative to orthogonal response states, on the first question\n\n\nData\n\ndata: df_items where q == 1\noutcome: state ( 4 level factor from 5 level high_interpretation )\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMultinomial (Logistic) Regression on state predicted by condition\n\nAlternative:\n\nOrdinal regression on state; but model doesn’t satisfy proportional odds assumption (parallel slopes)\nMultinomial or Ordinal regression on high_interpretation (5 category interpretation state which distinguishes between uncertain (blank, reference) unclassifiable, triangle-like and true triangular.) There are some cells with zeros, however (no uncertain responses in control) which means the model can’t accurately estimate those comparisons\n\n\n\n\n\nSetup\n\n\nCODE\n#:::::::: PREP DATA\ndf <- df_items %>% filter(q==1) %>% dplyr::select(pretty_condition, state)\n\n\n\n\nDescribe\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Q1 Interpretation\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nA proportional bar chart visualizing the proportion of each interpretations type by condition for each data collection modality (left/right facet) reveals that the pattern of responses are similar regardless of the data collection modality, by differ by condition. In the impasse condition, there are more triangular responses than in control. We see that around half of the ‘incorrect’ (i.e. not triangular) responses in the impasse condition are not orthogonal-like, but “other/unknown”.\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\ntable(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n            \n             control impasse    Sum\n  orthogonal  0.8065  0.3125 0.5556\n  other       0.0161  0.2812 0.1508\n  angular     0.0161  0.1094 0.0635\n  triangular  0.1613  0.2969 0.2302\n  Sum         1.0000  1.0000 1.0000\n\n\nCODE\n(t <- table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n            \n             control impasse Sum\n  orthogonal      50      20  70\n  other            1      18  19\n  angular          1       7   8\n  triangular      10      19  29\n  Sum             62      64 126\n\n\n\n\nMULTINOMIAL REGRESSION\nDoes condition affect the response state of Q1?\nFit a logistic regression predicting interpretation state (k=3) by condition(k = 2).\n\n3 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) [essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing [reference category] vs [this category])\nFor each equation:\n\n\\(\\beta_{0}\\) = Log Odds of [this category type vs. reference category type) response in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of [this category type vs. reference category type] response in CONTROL condition\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for [this category] type response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of [this. vs reference category type] response in IMPASSE (vs) CONTROL\nTwo-tailed NHST Null hypothesis: \\(\\beta_{impasse} = 0\\) the odds for [this category of response vs. reference] are not different for IMPASSE condition\nAlternative hypothesis: \\(\\beta_{impasse} \\ne 0\\) the odds of [this category of response vs. reference] increases or decreases for IMPASSE condition\n\n\n\nFit Model\n\n\nCODE\n#check reference level \nprint(\"Categories (first is reference)\")\n\n\n[1] \"Categories (first is reference)\"\n\n\nCODE\nlevels(df$state)\n\n\n[1] \"orthogonal\" \"other\"      \"angular\"    \"triangular\"\n\n\nCODE\n#FIT EMPTY MODEL\n# print(\"EMPTY MODEL\")\ncatm.0 <- multinom(state ~ 1, data = df)\n\n\n# weights:  8 (3 variable)\ninitial  value 174.673090 \niter  10 value 141.745541\nfinal  value 141.745401 \nconverged\n\n\nCODE\n# summary(catm.0)\n\n#FIT PREDICTOR MODEL\n# print(\"PREDICTOR MODEL\")\ncatm <- multinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\n\n# weights:  12 (6 variable)\ninitial  value 174.673090 \niter  10 value 121.936407\nfinal  value 121.916800 \nconverged\n\n\nCODE\n# summary(catm)\n\n#COMPARE MODEL FIT\npaste(\"AIC wth predictor is lower than empty model?\", catm.0$AIC > catm$AIC)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(catm.0, catm)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |      p\n-------------------------------------------------\ncatm.0 | multinom |  3 |         |       |       \ncatm   | multinom |  6 |       3 | 39.66 | < .001\n\n\nCODE\n##compare bayesian version\n#library(brms)\n# b.cat <- brm( state2 ~ pretty_condition, data = df, family = \"categorical\", backend = \"cmdstanr\")\n# summary(b.cat)\n# plot_model(b.cat)\n# report(b.cat)\n# coefficient estimates are very simliar to catm. super cool!\n\n##compare mclogit version\n#\"baseline-category logit model\n# https://www.elff.eu/software/mclogit/manual/mblogit/\n# blm1 <- mblogit(state2 ~ pretty_condition , data = df)\n# summary(blm1)\n#identical to catm. super cool!\n\n\nAIC in predictor model is less than empty model, and likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.\n\n\nDescribe\n\n\nCODE\n#set model\nm <- catm\n\n#::::::::INTERPRETATION\npaste(\"MODEL SUMMARY\")\n\n\n[1] \"MODEL SUMMARY\"\n\n\nCODE\nsummary(m)\n\n\nCall:\nmultinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\nCoefficients:\n           (Intercept) pretty_conditionimpasse\nother            -3.91                    3.81\nangular          -3.91                    2.86\ntriangular       -1.61                    1.56\n\nStd. Errors:\n           (Intercept) pretty_conditionimpasse\nother            1.009                   1.060\nangular          1.010                   1.101\ntriangular       0.346                   0.472\n\nResidual Deviance: 244 \nAIC: 256 \n\n\nCODE\ncar::Anova(m)\n\n\n# weights:  8 (3 variable)\ninitial  value 174.673090 \niter  10 value 141.745541\nfinal  value 141.745401 \nconverged\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: state\n                 LR Chisq Df Pr(>Chisq)    \npretty_condition     39.7  3    1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n# calculate z-statistics of coefficients\nz_stats <- summary(m)$coefficients/summary(m)$standard.errors\n# convert to p-values\np_values <- (1 - pnorm(abs(z_stats)))*2\n# display p-values in transposed data frame\n(p_values <- data.frame(p = (p_values)))\n\n\n           p..Intercept. p.pretty_conditionimpasse\nother           1.07e-04                  0.000332\nangular         1.07e-04                  0.009354\ntriangular      3.38e-06                  0.000959\n\n\nCODE\n# display odds ratios in transposed data frame\n\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\nodds_ratios <- data.frame(OR = exp(summary(m)$coefficients))\noptions(scipen = 2)\n(results <- cbind(odds_ratios, p_values))\n\n\n           OR..Intercept. OR.pretty_conditionimpasse p..Intercept.\nother                0.02                      44.96    0.00010690\nangular              0.02                      17.51    0.00010746\ntriangular           0.20                       4.75    0.00000338\n           p.pretty_conditionimpasse\nother                       0.000332\nangular                     0.009354\ntriangular                  0.000959\n\n\n\n\nInference\n\nBeing in the IMPASSE condition increases the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 45 (z = 3.81, p < 0.001) . Participants in the impasse condition were 45x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 17.5 (z = 2.60, p < 0.001 ). Participants in the impasse condition were almost 6x as likely to give an a triangular response rather than an orthogonal response compared to participants in control.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 4.8 (z = 3.30, p < 0.001 ). Participants in the impasse condition were almost 6x as likely to give an a triangular response rather than an orthogonal response compared to participants in control.\nAs with the (binary) logistic regression on accuracy ~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 in the control condition. (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)\n[need to to double check interpretation, but I think that the OR intercepts converted to probabilities equate to the marginal probability of being in each state in the control condition. which makes sense. I think.?]\nIF I change reference category for condition… then the intercepts should no longer be significant. The b1 coefficients should still be significant, but with changed sign (much less likely) [Yup! this works!]\n\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \n# ggcoefstats(m, output = \"plot\", \n#               # conf.level = 0.90,\n#               exclude.intercept = FALSE) + \n#   labs(x = \"Log Odds Estimate\", \n#        title = \"LOGODDS | Q1 State ~ condition\",\n#        subtitle = \"(p is for two tailed test)\")\n#:::::::: PLOT\n\n#PARAMETERS | MODEL | SIMULATED PARAMETERS\n# similar to bayesian dist of estimate\n# result <- simulate_parameters(m)\n# plot(result, show_intercept = TRUE, stack=FALSE)\n\n#PARAMETERS | MODEL | ODDS RATIO \n# result <- model_parameters(m1,exponentiate = TRUE)\n# #rename params \n# result$Parameter[1] <- \"condition [control]\"\n# result$Parameter[2] <- \"condition [impasse]\"\n# plot(result,   show_intercept = TRUE) +  labs(\n#   title = \"Model Parameter Estimates\"\n# ) + theme_clean() + theme(legend.position=\"blank\")\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m, type = \"est\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) +  #manually adjusted for directional test   \n  # scale_y_continuous() + #remove to put on log scale x axis \n  scale_x_discrete(labels=c(\"control\",\"impasse\"))+\n  labs(title = \"MODEL ESTIMATE  | Q1 Accuracy ~ condition\",\n       subtitle = \"Impasse increases odds of correct response on Q1\",\n       x = \"Condition\") + theme_clean()\n\n\nScale for 'x' is already present. Adding another scale for 'x', which will\nreplace the existing scale.\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type=\"eff\", ci.lvl = 0.95)[[1]] +\n  ylim(0,1) +\n  labs(title = \"MODEL PREDICTION  | Q1 State ~ condition\",\n       subtitle = \"Impasse increases probability of more accurate response states Q1\",\n       x = \"Condition\") + theme_clean()\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nCODE\n#MANUALLY BUILD PREDICTION PLOT FACET BY CONDITION RATHER THAN STATE\np <-plot_model(m, type=\"eff\")[[1]]\nd <- ggplot_build(p)[[1]]  \npoints <- d[[2]]\npoints <- points %>% mutate(\n  state = recode(PANEL, \"1\" =\"orth\", \"2\"=\"other\", \"3\" = \"trilike\", \"4\"=\"tri\"),\n  condition = recode(x, \"1\"=\"control\",\"2\"=\"impasse\"),\n  prob = y\n)\ngf_point( prob ~ state, group = ~x, data = points) + \n  geom_errorbar(aes( x = state, ymin = ymin, ymax = ymax)) + facet_grid(~condition) +ylim(0,1)\n\n\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \nstate\n\n\nPredictors\nOdds Ratios\nCI\np\nResponse\n\n\n(Intercept)\n0.02\n0.00 – 0.15\n<0.001\nother\n\n\npretty condition[impasse]\n44.96\n5.51 – 366.97\n<0.001\nother\n\n\n(Intercept)\n0.02\n0.00 – 0.15\n<0.001\nangular\n\n\npretty condition[impasse]\n17.51\n1.98 – 155.00\n0.011\nangular\n\n\n(Intercept)\n0.20\n0.10 – 0.40\n<0.001\ntriangular\n\n\npretty condition[impasse]\n4.75\n1.87 – 12.09\n0.001\ntriangular\n\n\nObservations\n126\n\n\nR2 / R2 adjusted\n0.140 / 0.133\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m1, \"(log odds)\" = m1)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', \n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n# modelsummary(mixcat.1, s)\n#TODO OUTPUT TABLE \n#https://arelbundock.com/posts/modelsummary_multinomial_logit/\n\n\n\n\nDiagnostics\n\n\nCODE\n#EXAMINE PREDICTIONS\n#create sample data frame\ntest <- data.frame(pretty_condition = c(\"control\", \"impasse\"))\npred <- predict(catm, newdata = test, \"probs\")\npaste(\"Predicted Probability of Being in Each State\")\n\n\n[1] \"Predicted Probability of Being in Each State\"\n\n\nCODE\n( x <- cbind(test, pred))\n\n\n  pretty_condition orthogonal  other angular triangular\n1          control      0.806 0.0161  0.0161      0.161\n2          impasse      0.313 0.2813  0.1094      0.297\n\n\nCODE\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(catm)\n\n\nCan't calculate log-loss.\nCan't calculate proper scoring rules for ordinal, multinomial or cumulative link models.\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n-----------------------------------------------------\n255.834 | 272.851 | 0.140 |     0.133 | 0.363 | 1.425\n\n\nCODE\nDescTools::PseudoR2(catm, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n\n\n  McFadden   CoxSnell Nagelkerke \n     0.140      0.270      0.302 \n\n\nCODE\n#General Goodness of Fit\n#library(generalhoslem)\n#logitgof(df$state, catm$fitted.values, g = 3)\n#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).\n#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables\n# chisq.test(df$state, predict(catm)) #actual states VS predicted states\n# The chi-square test tests the decrease in unexplained variance from the baseline model to the final model\n\n# print(\"MODEL DIAGNOSTICS\")\n# check_model(m) can't do overall diagnostics, have to do them on individual model equations\n\n\n\n\nTODO\n\ninterpretation/reporting of model fit?\nsanity check correct interpretation of coefficients & reporting\ndiagnostics on individual model equations\ncalculate 1-tailed p values to match directional hypothesis"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html",
    "href": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html",
    "title": "7  (Lab) Hypothesis Testing",
    "section": "",
    "text": "The purpose of this notebook is test the hypotheses that determined the design of the SGC3A study (online replication, without OSPAN).\nResearch Questions\nIn SGC3A we set out to answer the following question: Does posing a mental impasse improve performance on the interval graph comprehension task?\nExperimental Hypothesis\nLearners posed with scenario designed to evoke a mental impasse will be more likely to correct interpret the graph.\nNull Hypothesis\nNo significant differences in performance will exist between learners in the IMPASSE and CONTROL conditions."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html#sample",
    "href": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html#sample",
    "title": "7  (Lab) Hypothesis Testing",
    "section": "SAMPLE",
    "text": "SAMPLE\n\nData Collection\nData was collected (online, via SONA) in Fall 2021 and Winter 2022, for the purpose of verifying the use of the graph comprehension task for online, asynchronous data collection.\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_subjects$term, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    fall21 \n    3 \n    3 \n    6 \n  \n  \n    winter22 \n    28 \n    37 \n    65 \n  \n  \n    Sum \n    31 \n    40 \n    71 \n  \n\n\n\n\n\n\n\nParticipants\n\n\nCODE\n#Describe participants\nsubject.stats <-df_subjects %>% dplyr::select(age) %>% unlist() %>% favstats()\nsubject.stats$percent.male <- ((df_subjects %>% filter(gender==\"Male\") %>% count())/count(df_subjects))$n\nsubject.stats$percent.female <- ((df_subjects %>% filter(gender==\"Female\") %>% count())/count(df_subjects))$n\nsubject.stats$percent.other <- ((df_subjects %>% filter(gender %nin% c(\"Female\",\"Male\")) %>% count())/count(df_subjects))$n\n\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()%>% \n  footnote(general = \"Age in Years\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    percent.male \n    percent.female \n    percent.other \n  \n \n\n  \n     \n    18 \n    20 \n    20 \n    21 \n    27 \n    20.6 \n    1.61 \n    71 \n    0 \n    0.282 \n    0.676 \n    0.042 \n  \n\n\nNote:   Age in Years\n\n\n\n\nOverall 71 participants (28 % male, 68 % female, 4 % other) undergraduate STEM majors at a public American University participated in exchange for course credit (age: 18 - 27 years)."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html#h1a-overall-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html#h1a-overall-accuracy",
    "title": "7  (Lab) Hypothesis Testing",
    "section": "H1A | OVERALL ACCURACY",
    "text": "H1A | OVERALL ACCURACY\n\n\n\n\n\n\n\nResearch Question\nDo Ss in the IMPASSE condition score higher across the entire task than those in the CONTROL group?\n\n\n\n\nHypothesis\n(H1) Participants in the IMPASSE condition will be more likely to correctly interpret the graph than those in the CONTROL condition.\n\n\nData\ndata: df_items where q nin 6,9 (the 13 discriminating Qs ), df_subjects\noutcome:\n\n[at item level] : accuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\n[subject level]: accuracy (number of test phase qs correct from total s_NABS)\n\npredictor: condition [between-subjects factor]\n\n\nAnalysis Strategy\n\nWilcoxon-Rank Sum (Mann-Whitney) test on subject-level total accuracy of test phase (s_NABS)\nMixed Logistic Regression\naccuracy ~ condition + (1 | subject ) + (1 | question)\nmodel effect of condition on probability of correct response [during test phase] while accounting for subject (and item-level?) effects\n\n\n\nAlternatives\n\nOrdinal Mixed Logistic Regression on scaled_score\nOLS Linear Regression: bimodal distribution at tails makes the mean a poor predictor; LMs violate assumptions of normally distributed residuals; both absolute and scaled scores yield non-normal residuals; no transformation of the outcome variables yield normal residuals\n\n\n\nNotes\nAlso exploring:\n\nHurdle model (mixture model w/ binomial + [poisson or negbinom count; 0s from 1 DGP)\nZero Inflated model (mixture model w/ binomial + poisson or negbinom count; 0s from 2 DGPs)\nBeta regression hurdle model? (mixture with location and scale parameters [mean, variance] and hurdles for floor and ceiling effects)\nOther way to account for the severe bimodality?\n\n\n\n\n\nSetup\n\n\nCODE\ndf_i = df_items %>% filter(q %nin% c(6,9)) %>% \n  dplyr::select(pretty_condition, accuracy, subject,q)\n\ndf_s <- df_subjects %>% \n   dplyr::select(pretty_condition, task_percent)\n\n\n\n\nVisualize\n\n\nCODE\n#:::::::: STACKED PROPORTIONAL BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  # scale_fill_manual(values = paletteer::paletteer_d(\"ggthemes::calc\", 2))+\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~pretty_mode) + \n  # coord_flip() +\n  theme(legend.position=\"bottom\")+\n   labs(title = \"DISTRIBUTION | Question Accuracy\",\n       x = \"Condition\",\n       y = \"Proportion of Questions\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART BY QUESTION\ndf_items %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~q) +\n   labs(title = \"DISTRIBUTION | Accuracy by Question\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Q6 and Q9 are non-discriminative\")\n\n\n\n\n\nCODE\n#:::::::: FACETED HISTOGRAM\nstats = df_s %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(task_percent))\ngf_props(~task_percent,\n         fill = ~pretty_condition, data = df_s) %>%\n  # gf_facet_grid(pretty_condition ~ pretty_mode) %>%\n  gf_facet_grid(~pretty_condition) %>%\n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"% Correct\",\n       y = \"proportion of subjects\",\n       title = \"DISTRIBUTION | Total Absolute Score (% Correct)\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#:::::::: RAINCLOUD WITH STATS\n  \ndf <- df_s %>% mutate(task_percent = task_percent*100)\n\np <-   ggbetweenstats(data = df, x = pretty_condition, y = task_percent,\n               plot.type = \"box\", type = \"nonparametric\", var.equal = FALSE,\n               centrality.type = \"parametric\",\n               # package = \"RColorBrewer\",\n               # palette = \"PRGn\",\n               centrality.point.args = list(color=\"black\", size = 3, shape = 1),\n               point.args = list(alpha=0), #suppress points\n               ggplot.component = ## modify further with `{ggplot2}` functions\n                list(\n                  # aes(color = pretty_condition, fill = pretty_condition),\n                  # scale_colour_manual(values = paletteer::paletteer_c(\"viridis::viridis\", 3)),\n                  # scale_fill_manual(values = paletteer::paletteer_c(\"viridis::viridis\", 3)),\n                  theme(axis.text.x = element_text(angle = 90)))\n               ) +\n  ggdist::stat_halfeye(\n    alpha = 0.7, \n    point_colour = NA,\n    adjust = .5, \n    width = .5, .width = 0, \n    justification = -.5) +\n  geom_boxplot(\n    alpha = 0.1,\n    width = .2, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 2,\n    alpha = .5,\n    position = position_jitter(\n      seed = 1, width = .08, height = 1.5\n    )\n  )  +\ncoord_flip() + theme_clean() + theme(legend.position = \"blank\")\np$layers[[3]]=NULL #remove default boxplot\ne <- statsExpressions::two_sample_test(y = task_percent, x = pretty_condition, data = df,\n                                type = \"nonparametric\", alternative = \"less\",\n                                var.equal = FALSE)\n#labels are layer 4\np + labs(title = \"DISTRIBUTION of Total Score\",\n         y = \"Percentage of correct responses across task\", x = \"\",\n         # caption=e$expression[[1]],\n         subtitle = \"Impasse condition yields greater variance and more high scores\")\n\n\n\n\n\n\n\nDescribe\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total % Correct)\"\ntbl1 <- mosaic::favstats(~task_percent, data = df_s) \ntbl1 %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total % Correct)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    0 \n    0 \n    0.077 \n    0.769 \n    1 \n    0.315 \n    0.39 \n    71 \n    0 \n  \n\n\n\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION\"\ntbl2 <- mosaic::favstats(task_percent ~ pretty_condition, data = df_s) \ntbl2 %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION\n \n  \n    pretty_condition \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    control \n    0 \n    0 \n    0.000 \n    0.231 \n    1 \n    0.233 \n    0.372 \n    31 \n    0 \n  \n  \n    impasse \n    0 \n    0 \n    0.115 \n    0.788 \n    1 \n    0.379 \n    0.397 \n    40 \n    0 \n  \n\n\n\n\n\n\nWILCOXON RANK SUM (Mann-Whitney Test)\n\nNon parametric alternative to t-test; compares median rather than mean by ranking data\nDoes not assume normality\nDoes not assume equal variance of samples (homogeneity of variance)\n\n\nTest\n\n\nCODE\n(w <- wilcox.test(df_s$task_percent ~ df_s$pretty_condition,\n                 paired = FALSE, alternative = \"less\")) #less, greater\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df_s$task_percent by df_s$pretty_condition\nW = 468, p-value = 0.03\nalternative hypothesis: true location shift is less than 0\n\n\nCODE\nreport(w)\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum test with continuity correction testing the difference in ranks between df_s$task_percent and df_s$pretty_condition suggests that the effect is negative, statistically significant, and medium (W = 467.50, p = 0.034; r (rank biserial) = -0.25, 95% CI [-1.00, -0.02])\n\n\n\n\nInference – DIRECTIONAL EFFECT\n\n\nVisualize\n\n\nCODE\nggbetweenstats( x = pretty_condition, y = task_percent, data = df_s,\n                type = \"nonparametric\", var.equal = FALSE)\n\n\n\n\n\n\n\n\nMIXED LOGISTIC REGRESSION\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.\n\nFit Model\n\n\nCODE\n## 0 | SETUP\n#confirm 13 items [all discriminating items]\nnrow(df_i) / nrow(df_s) == 13\n\n\n[1] TRUE\n\n\nCODE\n#confirm all factors \nis.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$pretty_condition) && is.factor(df_i$accuracy)\n\n\n[1] TRUE\n\n\nCODE\n## 1 | SETUP RANDOM INTERCEPT SUBJECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nprint(\"Empty fixed model\")\n\n\n[1] \"Empty fixed model\"\n\n\nCODE\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df_i) \n# summary(m0)\n\n#:: RANDOM INTERCEPT SUBJECT\nprint(\"Subject intercept random model\")\n\n\n[1] \"Subject intercept random model\"\n\n\nCODE\nmm.rS <- glmer(accuracy ~ (1|subject), data = df_i, family = \"binomial\")\n# summary(mm.rS)\n\n# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", m0$aic > AIC(logLik(mm.rS)))\n\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\n\nCODE\ntest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName  |    Model | df | df_diff |   Chi2 |      p\n-------------------------------------------------\nm0    |      glm |  1 |         |        |       \nmm.rS | glmerMod |  2 |       1 | 542.09 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  6.62439101233609e-120\"\n\n\nCODE\n#:: RANDOM INTERCEPT SUBJECT + ITEM\nprint(\"Subject Intercept + Item intercept random model\")\n\n\n[1] \"Subject Intercept + Item intercept random model\"\n\n\nCODE\nmm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = \"binomial\")\n#summary(mm.rSQ)\n\n# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)))\n\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\n\nCODE\ntest_lrt(mm.rS, mm.rSQ) #same as anova(m0, m1, test = \"Chi\")\n\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |      p\n-------------------------------------------------\nmm.rS  | glmerMod |  2 |         |       |       \nmm.rSQ | glmerMod |  3 |       1 | 13.43 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS, mm.rSQ))$p[2])\n\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.00024829256662973\"\n\n\nCODE\n## 2 | ADD FIXED EFFECT CONDITION\n\nprint(\"FIXED Condition + Subject & Item random intercepts\")\n\n\n[1] \"FIXED Condition + Subject & Item random intercepts\"\n\n\nCODE\nmm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q) ,\n                data = df_i, family = \"binomial\")\n#summary(mm.CrSQ)\n\npaste(\"AIC decreases w/ new model\", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.CrSQ)) )\n\n\n[1] \"AIC decreases w/ new model TRUE\"\n\n\nCODE\ntest_lrt(mm.rSQ,mm.CrSQ) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName    |    Model | df | df_diff | Chi2 |     p\n------------------------------------------------\nmm.rSQ  | glmerMod |  3 |         |      |      \nmm.CrSQ | glmerMod |  4 |       1 | 3.81 | 0.051\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rSQ,mm.CrSQ))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.0510053431608199\"\n\n\nCODE\n# control = glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)))\n\n\nA likelihood ratio test indicates adding CONDITION as a fixed effect to a logistic regression model including a fixed effect random intercepts for SUBJECT and QUESTION explains more variance in the data than random-effects only model.\n\n\nDescribe\n\n\nCODE\n#::::::::: SETUP\nm <- mm.CrSQ\n\n#::::::::: PRINT MODEL \n\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition + (1 | subject) + (1 | q)\n   Data: df_i\n\n     AIC      BIC   logLik deviance df.resid \n     599      618     -296      591      919 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-6.192 -0.230 -0.091  0.195  4.828 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 15.137   3.891   \n q       (Intercept)  0.474   0.689   \nNumber of obs: 923, groups:  subject, 71; q, 13\n\nFixed effects:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -3.583      0.951   -3.77  0.00017 ***\npretty_conditionimpasse    2.124      1.114    1.91  0.05657 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nprtty_cndtn -0.769\n\n\nCODE\nprint(\"SIGNIFICANCE TEST [non directional]\")\n\n\n[1] \"SIGNIFICANCE TEST [non directional]\"\n\n\nCODE\ncar::Anova(m)\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: accuracy\n                 Chisq Df Pr(>Chisq)  \npretty_condition  3.64  1      0.057 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"MODEL INFO\")\n\n\n[1] \"MODEL INFO\"\n\n\nCODE\nglance(m)\n\n\n# A tibble: 1 × 7\n   nobs sigma logLik   AIC   BIC deviance df.residual\n  <int> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n1   923     1  -296.  599.  619.     368.         919\n\n\nCODE\n#:::::::: MANUAL ONE-SIDED SIGTEST \n#note: anova and chi square are always one-tailed, but that is independent of being one-sided\n#https://www.ibm.com/support/pages/can-one-get-one-tailed-tests-logistic-regression-dividing-significance-levels-half\n# one-sided (right tail) z test for B COEFFICIENT\n#SANITY CHECK 2-tailed test should match the model output\n# tt <- 2*pnorm(summary(m)$coefficients[2,3], lower.tail = F)\n# paste(\"p value for two-tailed test, null B = 0 : \",round(tt,5))\n# ot <- pnorm(summary(m)$coefficients[2,3], lower.tail = F)\n# paste(\"BUT we want a one  directional, null: B <= 0: \",round(ot,5))\n\n#:::::::: INTERPRET COEFFICIENTS\n\npaste(\"LOG ODDS\")\n\n\n[1] \"LOG ODDS\"\n\n\nCODE\n# se <- sqrt(diag(stats::vcov(m)))\n# (tab <- cbind(Est = fixef(m),\n#               LL = fixef(m) - 1.96 * se,\n#               UL = fixef(m) + 1.96 * se))\ntidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = \"Wald\")\n\n\n# A tibble: 4 × 9\n  effect   group  term  estimate std.error statistic  p.value conf.low conf.high\n  <chr>    <chr>  <chr>    <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 fixed    <NA>   (Int…   -3.58      0.951     -3.77  1.65e-4  -5.45       -1.72\n2 fixed    <NA>   pret…    2.12      1.11       1.91  5.66e-2  -0.0594      4.31\n3 ran_pars subje… sd__…    3.89     NA         NA    NA        NA          NA   \n4 ran_pars q      sd__…    0.689    NA         NA    NA        NA          NA   \n\n\nCODE\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\n# (e <- exp(tab))\ntidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = \"Wald\", exponentiate = TRUE)\n\n\n# A tibble: 4 × 9\n  effect   group  term  estimate std.error statistic  p.value conf.low conf.high\n  <chr>    <chr>  <chr>    <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 fixed    <NA>   (Int…   0.0278    0.0264     -3.77  1.65e-4  0.00431     0.179\n2 fixed    <NA>   pret…   8.36      9.31        1.91  5.66e-2  0.942      74.2  \n3 ran_pars subje… sd__…   3.89     NA          NA    NA       NA          NA    \n4 ran_pars q      sd__…   0.689    NA          NA    NA       NA          NA    \n\n\nCODE\npaste(\"PROBABILITIES\")\n\n\n[1] \"PROBABILITIES\"\n\n\nCODE\n#probability control = plogis(intercept)\n#probability impasse = plogis(intercept + coefficient)\n\n#FROM predict()\n# newdata <- df_i %>% dplyr::select(pretty_condition, subject, q)\n# preds <- predict(m, newdata = newdata, type = \"response\")\n# preds <- cbind(newdata, preds)\n# p <- preds %>% \n#   dplyr::select(pretty_condition, preds) %>% \n#   group_by(pretty_condition) %>% \n#   summarise(\n#     median = median(preds),\n#     se = sd(preds)/sqrt(n()),\n#     lwr = median - 1.96*se,\n#     upr = median + 1.96*se)\n    \n  \n#FROM merTools\n#setup df \nnewdata <- df_i %>% dplyr::select(pretty_condition, subject, q)\n#make predictions\npreds <- predictInterval(m, newdata = newdata,\n                              which = \"fixed\", #full, fixed or random for those only\n                              type = \"probability\", #linear.prediction\n                              stat = \"median\",\n                              n.sims = 1000,\n                              level = 0.80) #width of prediction interval\n#join predictions to the new dataframe\npreds <- cbind(newdata, preds)\n#summarize\n(summ_preds <- preds %>% \n  dplyr::select(pretty_condition, fit, lwr, upr) %>% \n  group_by(pretty_condition) %>% \n  summarise(\n    median = median(fit),\n    lower = median(lwr),\n    upper = median(upr)\n  )) \n\n\n# A tibble: 2 × 4\n  pretty_condition median   lower upper\n  <fct>             <dbl>   <dbl> <dbl>\n1 control          0.0267 0.00468 0.142\n2 impasse          0.185  0.0444  0.523\n\n\n\n\nINFERENCE — DIRECTIONAL EFFECT\np = 0.06; but we should be performing a directional test, so actually OK.\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.03\n0.00 – 0.18\n<0.001\n\n\npretty condition[impasse]\n8.36\n0.94 – 74.18\n0.057\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 subject\n15.14\n\n\nτ00 q\n0.47\n\n\nICC\n0.83\n\n\nN subject\n71\n\n\nN q\n13\n\nObservations\n923\n\n\nMarginal R2 / Conditional R2\n0.055 / 0.836\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m, \"(log odds)\" = m)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',\n#              notes = notes)\n#              # output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n\n\n\nVisualize\n\n\nCODE\n## | PLOT PARAMETERS \n\n\n#SJPLOT | MODEL | lOG ODDS\nplot_model(m, transform = NULL,\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.05, #manually adjust to account for directional test\n           ci.lvl = 0.95 ) + #manually adjusted for directional test   \n  labs(title = \"Model ESTIMATE | Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | ODDS RATIO\nplot_model(m, transform = \"exp\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.05, #manually adjust to account for directional test\n           ci.lvl = 0.95 ) + #manually adjusted for directional test   \n  labs(title = \"Model ESTIMATE | Log Odds\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#EASYSTATS | MODEL | ODDS RATIO\n# result <- model_parameters(m, exponentiate = TRUE, component = \"all\")\n# plot(result) + labs(\n#     title = \"Model ESTIMATE | ODDS RATIO\"\n#   )\n\n## | PLOT TESTS\n# result <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\n# plot(result)\n\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\n# plot_model(m, type=\"eff\",\n#            show.intercept = TRUE,\n#            show.values = TRUE,\n#            title = \"Model Prediction | Probability of Accurate Response\",\n#            axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type = \"pred\")[[1]]  +\n  ylim(0,1) +\n  labs(\n    title = \"Model PREDICTION | Probability of Accurate Response\",\n    subtitle = \"Impasse increases probability of correct response\",\n    y = \"Probability of Correct Response\", x = \"Condition\"\n  )\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nCODE\n#GGDIST | MODEL | PREDICTED PROBABILITIES\npreds %>% \n  ggplot(aes( x = fit, y = pretty_condition, fill = pretty_condition)) + \n  stat_halfeye(alpha = 0.5, normalize = \"xy\") + \n  xlim(0,0.3) + theme_clean() + labs(\n    title = \"Model PREDICTION | Probability of Accurate Response\",\n    subtitle = \"TODO check preds to see if fixed or includes random\"\n  )\n\n\n\n\n\nCODE\n# SIMULATE FIXED EFFECTS\n# simulate values of fixed effects \n# (feEx <- FEsim(m,  oddsRatio = FALSE, n.sims = 1000))\n# PLOT estimates of fixed effects\n# plotFEsim(feEx) +\n#   theme_bw() + labs(title = \"Coefficient Plot of InstEval Model\",\n#                     x = \"Median Effect Estimate\")\n\n# SIMULATE RANDOM EFFECTS\n# simulate values of random effects\n# reEx <- REsim(m)\n# PLOT estimates of random effects\n# plotREsim(reEx)\n\n\n\n\nDiagnostics\n\n\nCODE\n# print(\"SANITY CHECK REPORTING\")\n# report(m)\n\n# print(\"MODEL PERFORMANCE\")\n# performance(m)\n\nprint(\"DIAGNOSTICS\")\n\n\n[1] \"DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m)\n\n\n\n\n\n\n\nSanity Check :: Bayesian\n\n\nCODE\n# ## 0 | SETUP\n# #confirm 13 items [all discriminating items]\n# nrow(df_i) / nrow(df_s) == 13\n# #confirm all factors \n# is.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$pretty_condition) && is.factor(df_i$accuracy)\n# \n#\n# \n# print(\"FIXED Condition + Subject & Item random intercepts\")\n# Bmm.CrSQ <- brm( accuracy ~ pretty_condition + (1|subject) + (1|q), \n#                  data = df_i, \n#                  family = \"bernoulli\",\n#                  chains = 4, iter = 2000, warmup = 1000,\n#                  cores = 4, seed = 1234,\n#                  backend = \"cmdstanr\",\n#                  file =\"analysis/SGC3A/models/sgc3a_brms_acc_Bmm.CrSQ_REP.rds\")\n# \n# #get Priors \n# # describe_priors(Bmm.CrSQ)\n# \n# #GRAPHICAL POSTERIOR PREDICTION CHECKS\n# pp_check(Bmm.CrSQ)\n# \n# #DESCRIBE MODEL\n# (d <- describe_posterior(ci=.95, Bmm.CrSQ))\n# \n# #SEE MODEL\n# plot(pd(Bmm.CrSQ))\n# #convert to a pd value\n# (pds <- pd_to_p(d$pd))\n\n\nA likelihood ratio test indicates adding CONDITION as a fixed effect to a logistic regression model including a fixed effect random intercepts for SUBJECT and QUESTION explains more variance in the data than random-effects only model.\n\n\nCODE\n# #::::: GGDIST POSTERIOR PROBABILITY OF RESPONSE\n# ##WORKING\n# ## VIS probability of correct response\n# #TAKES A REALLY LONG TIME\n# \n# #1 | get draws\n# draws <- df_i %>%\n#   data_grid(pretty_condition, subject, q) %>%\n#   add_fitted_draws(Bmm.CrSQ,\n#                    # n = 100,\n#                    # dpar = TRUE,\n#                    # transform = TRUE, #gives prob%, otherwise OR\n#                    re_formula = NA)\n# # draws %>% write_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n# \n# #OR load from file\n# # draws <- read_rds(\"analysis/SGC3A/models/draws_BB.catCrSQ.rds\")\n# \n# #2| VISUALIZE PREDICTIONS | GGDIST\n# ##TODO figure out height normalization.\n# ##do it with much smaller number of draws \n# #TODO adjust bandwidth/smoothing? + put on same line + \n# #TAKES A REAAALY LONG TIME\n# # draws %>% sample_n(1000) %>% \n# #   ggplot(aes(x = .value,  y = 0, fill = pretty_condition)) +\n# #   stat_slab(width = c(.95), alpha = 1, normalize=\"xy\") +\n# #   #normalize = all, panels, xy, groups, none\n# #   xlim(0,1) + labs(\n# #     title = \"Model Predicted Probability of Correct Response\",\n# #     x = \"probability of correct response\",\n# #     y = \"Interpretation\"\n# #   ) +  theme_clean() #+ ggeasy::easy_remove_legend() + ggeasy::easy_remove_y_axis()\n# # #TO PLOT ON THE SAME LINE, INCLUDE Y = 0 in aes and ggeasy::remove_y_axis()"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html#h1a-overall-interpretation-state",
    "href": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html#h1a-overall-interpretation-state",
    "title": "7  (Lab) Hypothesis Testing",
    "section": "H1A | OVERALL INTERPRETATION STATE",
    "text": "H1A | OVERALL INTERPRETATION STATE\nDo Ss in the IMPASSE condition offer less-orthogonal interpretations across the test phase questions?\nWhile absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn’t allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject’s response, and determine if these interpretations differ by experimental condition. State is a 3-category variable that groups the following interpretations:\n\n“orthogonal” [reference category] includes orthogonal and satisficing responses ==> indicates a primarily orthogonal state of coordinate system understanding\n“other” includes: blank, reference point, responses that can’t be classified (including selecting all datapoints), => indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly not orthogonal nor triangular\n“angular” includes ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n“triangular” includes correct triangular and ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n\n\n\n\n\n\n\n\nResearch Question\nDoes Ss in the impasse condition produce less orthogonal responses across questions?\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of producing unknown and triangle-like response states across all items\n\n\nData\n\ndata: df_items where q nin 6,9 (13 discriminant test phase items)\noutcome: state ( 3 level factor from high_interpretation )\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMIXED Multinomial (Logistic) Regression on state predicted by condition\n\nAlternative:\n\nMIXED Ordinal regression on state (doesn’t meet proportional odds assumption-I think)\nMIXED Multinomial or Ordinal regression on high_interpretation (some cells are 0, produces problems)\n\n\n\n\n\nSetup\n\n\nCODE\ndf_i = df_items %>% filter(q %nin% c(6,9)) %>% dplyr::select(q,subject,state,pretty_condition)\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_manual(values = paletteer::paletteer_d(\"ggthemes::calc\", 4))+\n  # facet_wrap(~pretty_mode) + \n  theme(legend.position = \"bottom\")+\n   labs(title = \"DISTRIBUTION of Interpretation\",\n       x = \"Condition\",\n       y = \"Proportion of Questions\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_manual(values = paletteer::paletteer_d(\"ggthemes::calc\", 4))+\n  facet_wrap(~q) +\n   labs(title = \"Interpretation by Question\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nA proportional bar chart visualizing the proportion of each interpretations type by condition for each data collection modality (left/right facet) reveals that the pattern of responses are similar regardless of the data collection modality, by differ by condition. In the impasse condition, there are more triangular responses than in control. In the impasse condition, there are also more positive transition (i.e. triangle-like) and neutral (ie. blank or uncertain response types) than in the control condition.\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\ntable(df_i$state, df_i$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n            \n             control impasse    Sum\n  orthogonal  0.5682  0.3096 0.4225\n  other       0.1489  0.2250 0.1918\n  angular     0.0323  0.0500 0.0423\n  triangular  0.2506  0.4154 0.3434\n  Sum         1.0000  1.0000 1.0000\n\n\nCODE\n(t <- table(df_i$state, df_i$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n            \n             control impasse Sum\n  orthogonal     229     161 390\n  other           60     117 177\n  angular         13      26  39\n  triangular     101     216 317\n  Sum            403     520 923\n\n\n\n\nMIXED MULTINOMIAL REGRESSION\nDoes condition affect the response state of of items across the task?\nFit a MIXED logistic regression predicting interpretation state (k=3) by condition(k = 2).\n\nFit Model [mblogit]\n\n\nCODE\n#https://www.elff.eu/software/mclogit/manual/mblogit/\n#\"baseline category logit\" model matches multinom()\n\n#check reference level \nprint(\"Categories (first is reference)\")\n\n\n[1] \"Categories (first is reference)\"\n\n\nCODE\nlevels(df_i$state)\n\n\n[1] \"orthogonal\" \"other\"      \"angular\"    \"triangular\"\n\n\nCODE\n#FIT EMPTY MODEL\n# print(\"EMPTY MODEL\")\nmm.cat.rSQ <- mblogit(state ~ 1 , \n                      random = list( ~ 1|subject, ~1|q), \n                      data = df_i)\n\n\n\nIteration 1 - deviance = 1692 - criterion = 0.811\nIteration 2 - deviance = 1480 - criterion = 0.0727\nIteration 3 - deviance = 1411 - criterion = 0.0226\nIteration 4 - deviance = 1360 - criterion = 0.0156\nIteration 5 - deviance = 1352 - criterion = 0.00284\nIteration 6 - deviance = 1361 - criterion = 0.000387\nIteration 7 - deviance = 1362 - criterion = 0.000149\nIteration 8 - deviance = 1362 - criterion = 0.0000855\nIteration 9 - deviance = 1361 - criterion = 0.0000554\nIteration 10 - deviance = 1360 - criterion = 0.0000361\nIteration 11 - deviance = 1360 - criterion = 0.000023\nIteration 12 - deviance = 1360 - criterion = 0.0000143\nIteration 13 - deviance = 1359 - criterion = 8.81e-06\nIteration 14 - deviance = 1359 - criterion = 5.36e-06\nIteration 15 - deviance = 1359 - criterion = 3.25e-06\nIteration 16 - deviance = 1359 - criterion = 1.96e-06\nIteration 17 - deviance = 1359 - criterion = 1.19e-06\nIteration 18 - deviance = 1359 - criterion = 7.27e-07\nIteration 19 - deviance = 1359 - criterion = 4.45e-07\nIteration 20 - deviance = 1359 - criterion = 2.74e-07\nIteration 21 - deviance = 1359 - criterion = 1.7e-07\nIteration 22 - deviance = 1359 - criterion = 1.06e-07\nIteration 23 - deviance = 1359 - criterion = 6.62e-08\nIteration 24 - deviance = 1359 - criterion = 4.16e-08\nIteration 25 - deviance = 1359 - criterion = 2.63e-08\n\n\nWarning: Algorithm did not converge\n\n\nCODE\n#summary(mm.cat.rSQ)\n\n#FIT PREDICTOR MODEL\n# print(\"PREDICTOR MODEL\")\nmm.cat.CrSQ <- mblogit(state ~ pretty_condition , \n                  random = list( ~ 1|subject, ~1|q), \n                  data = df_i)\n\n\n\nIteration 1 - deviance = 1667 - criterion = 0.809\nIteration 2 - deviance = 1473 - criterion = 0.0698\nIteration 3 - deviance = 1400 - criterion = 0.0263\nIteration 4 - deviance = 1378 - criterion = 0.00458\nIteration 5 - deviance = 1365 - criterion = 0.00112\nIteration 6 - deviance = 1356 - criterion = 0.000422\nIteration 7 - deviance = 1350 - criterion = 0.000212\nIteration 8 - deviance = 1347 - criterion = 0.000121\nIteration 9 - deviance = 1346 - criterion = 0.000071\nIteration 10 - deviance = 1345 - criterion = 0.0000412\nIteration 11 - deviance = 1344 - criterion = 0.0000235\nIteration 12 - deviance = 1344 - criterion = 0.0000134\nIteration 13 - deviance = 1344 - criterion = 7.57e-06\nIteration 14 - deviance = 1343 - criterion = 4.31e-06\nIteration 15 - deviance = 1343 - criterion = 2.47e-06\nIteration 16 - deviance = 1343 - criterion = 1.43e-06\nIteration 17 - deviance = 1343 - criterion = 8.37e-07\nIteration 18 - deviance = 1343 - criterion = 4.95e-07\nIteration 19 - deviance = 1343 - criterion = 2.96e-07\nIteration 20 - deviance = 1343 - criterion = 1.79e-07\nIteration 21 - deviance = 1343 - criterion = 1.09e-07\nIteration 22 - deviance = 1343 - criterion = 6.71e-08\nIteration 23 - deviance = 1343 - criterion = 4.15e-08\nIteration 24 - deviance = 1343 - criterion = 2.59e-08\nIteration 25 - deviance = 1343 - criterion = 1.62e-08\n\n\nWarning: Algorithm did not converge\n\n\nCODE\n# summary(mm.cat.CrSQ)\n\n#COMPARE MODEL FIT\npaste(\"AIC wth predictor is lower than empty model?\", AIC(mm.cat.rSQ) > AIC(mm.cat.CrSQ))\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(mm.cat.rSQ, mm.cat.CrSQ)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName        |    Model | df | df_diff |  Chi2 |      p\n------------------------------------------------------\nmm.cat.rSQ  | mmblogit | 15 |         |       |       \nmm.cat.CrSQ | mmblogit | 18 |       3 | 23.60 | < .001\n\n\n\n\nDescribe\n\n\nCODE\nm <- mm.cat.CrSQ\n\n#DESCRIBE MODEL\nsummary(m)\n\n\nWarning in sqrt(diag(vcov.phi)): NaNs produced\n\nWarning in sqrt(diag(vcov.phi)): NaNs produced\n\n\n\nCall:\nmblogit(formula = state ~ pretty_condition, data = df_i, random = list(~1 | \n    subject, ~1 | q))\n\nEquation for other vs orthogonal:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.587      0.571   -2.78  0.00549 ** \npretty_conditionimpasse    1.573      0.419    3.75  0.00017 ***\n\nEquation for angular vs orthogonal:\n                           Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                  -3.282      0.743   -4.42  9.9e-06 ***\ntri(Intercept)               -1.904      0.817   -2.33    0.020 *  \npretty_conditionimpasse       1.643      0.653    2.52    0.012 *  \ntripretty_conditionimpasse    2.296      0.919    2.50    0.012 *  \n\nEquation for triangular vs orthogonal:\n                        Estimate Std. Error z value Pr(>|z|)  \n(Intercept)               -1.904      0.817   -2.33    0.020 *\npretty_conditionimpasse    2.296      0.919    2.50    0.012 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Co-)Variances:\nGrouping level: subject \n             Estimate            Std.Err.            \nother~1       1.89                0.693              \nangular~1     1.86  3.97          1.536  2.510       \ntriangular~1  3.24  5.48 12.28      NaN  5.293 10.878\n\nGrouping level: q \n             Estimate         Std.Err.         \nother~1      2.91               NaN            \nangular~1    1.79 3.62         8.31 13.14      \ntriangular~1 1.51 1.96 2.25    4.92  9.42  6.19\n\nNull Deviance:     2560 \nResidual Deviance: 1340 \nNumber of Fisher Scoring iterations:  25\nNumber of observations\n  Groups by subject: 71\n  Groups by q: 13\n  Individual observations:  923\nNote: Algorithm did not converge.\n\n\nCODE\n#INTERPRET COEFFICIENTS\npaste(\"LOG ODDS\")\n\n\n[1] \"LOG ODDS\"\n\n\nCODE\ntidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = \"Wald\")\n\n\nWarning: The `tidy()` method for objects of class mmblogit is not maintained by\nthe broom team, and is only supported through the lm tidier method. Please be\ncautious in interpreting and reporting broom output.\n\nWarning: NaNs produced\n\nWarning: NaNs produced\n\n\n# A tibble: 6 × 7\n  term                   estimate std.error statistic p.value conf.low conf.high\n  <chr>                     <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n1 other~(Intercept)         -1.59     0.571     -2.78 5.49e-3   -2.71     -0.466\n2 angular~(Intercept)       -3.28     0.743     -4.42 9.91e-6   -4.74     -1.83 \n3 triangular~(Intercept)    -1.90     0.817     -2.33 1.97e-2   -3.51     -0.303\n4 other~pretty_conditio…     1.57     0.419      3.75 1.74e-4    0.751     2.39 \n5 angular~pretty_condit…     1.64     0.653      2.52 1.18e-2    0.363     2.92 \n6 triangular~pretty_con…     2.30     0.919      2.50 1.25e-2    0.494     4.10 \n\n\nCODE\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\ntidy(m,   conf.int = TRUE, conf.level = 0.95, conf.method = \"Wald\", exponentiate = TRUE)\n\n\nWarning: The `tidy()` method for objects of class mmblogit is not maintained by\nthe broom team, and is only supported through the lm tidier method. Please be\ncautious in interpreting and reporting broom output.\n\nWarning: NaNs produced\n\nWarning: NaNs produced\n\n\n# A tibble: 6 × 7\n  term                   estimate std.error statistic p.value conf.low conf.high\n  <chr>                     <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n1 other~(Intercept)         -1.59     0.571     -2.78 5.49e-3   -2.71     -0.466\n2 angular~(Intercept)       -3.28     0.743     -4.42 9.91e-6   -4.74     -1.83 \n3 triangular~(Intercept)    -1.90     0.817     -2.33 1.97e-2   -3.51     -0.303\n4 other~pretty_conditio…     1.57     0.419      3.75 1.74e-4    0.751     2.39 \n5 angular~pretty_condit…     1.64     0.653      2.52 1.18e-2    0.363     2.92 \n6 triangular~pretty_con…     2.30     0.919      2.50 1.25e-2    0.494     4.10 \n\n\nCODE\n# paste(\"MODEL INFO\")\n# glance(m)\n\n#PERFORMANCE\nperformance(m)\n\n\n# Indices of model performance\n\nAIC       |       BIC |  RMSE | Sigma\n-------------------------------------\n12552.758 | 12639.656 | 0.276 | 1.210\n\n\n\n\nTODO Inference\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \nstate\n\n\nPredictors\nEstimates\nCI\np\n\n\nother~(Intercept)\n-1.59\n-2.71 – -0.47\n0.006\n\n\nangular~(Intercept)\n-3.28\n-4.74 – -1.83\n<0.001\n\n\ntriangular~(Intercept)\n-1.90\n-3.51 – -0.30\n0.020\n\n\nother~prettyconditionimpasse\n1.57\n0.75 – 2.39\n<0.001\n\n\nangular~prettyconditionimpasse\n1.64\n0.36 – 2.92\n0.012\n\n\ntriangular~prettyconditionimpasse\n2.30\n0.49 – 4.10\n0.013\n\n\n\nN subject\n71\n\n\nN q\n13\n\nObservations\n923\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m, \"(log odds)\" = m)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',\n#              notes = notes)\n#              # output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n\n\n\nVisualize\n\n\nCODE\n## | PLOT PARAMETERS \n\n#SJPLOT | MODEL | ODDS RATIO\nplot_model(m, \n           transform = \"exp\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE)\n\n\n\n\n\nCODE\n#TODO SEPARATE THIS BY EQUATION \n# ms <- model_parameters(Bmm.cat.CrSQ, component = \"conditional\")\n# m1 <- ms %>% filter(str_detect(Parameter, \"muother\"))\n# plot(m1)\n\n#EASYSTATS | MODEL | ODDS RATIO\nresult <- model_parameters(m, exponentiate = TRUE, component = \"all\")\nplot(result, show_labels = TRUE, n_columns = 3)\n\n\n\n\n\nCODE\n# result <- simulate_parameters(m)\n# plot(result, stack = FALSE)\n\n## | PLOT TESTS\nresult <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\n\n\nWarning: Could not estimate a good default ROPE range. Using 'c(-0.1, 0.1)'.\n\n\nCODE\nplot(result)\n\n\n\n\n\nCODE\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\n# plot_model(m, type=\"eff\",\n#            show.intercept = TRUE,\n#            show.values = TRUE,\n#            title = \"Model Prediction | Probability of Accurate Response\",\n#            axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n# \n# #PLOT MODEL PREDICTION\n# plot_model(m, type = \"pred\")[[1]] + \n#   ylim(0,1) + labs(\n#     title = \"Model Prediction | Probability of Accurate Response\",\n#     subtitle = \"Impasse increases Probability of Correct Response\"\n#   )\n\n#TODO EMMEANS for the estimated marginal means\n\n\n\n\nDiagnostics\n\n\nCODE\n# check_model(m)\n\n\n\n\nFit Model [brms]\n\n\nCODE\n#BAYESIAN RANDOM ONLY\nBmm.cat.rSQ <- brm( state ~ 1 + (1|subject) + (1|q), \n                 data = df_i, \n                 family = \"categorical\",\n                 chains = 4, iter = 2500, warmup = 1000,\n                 cores = 4, seed = 1234,\n                 save_pars = save_pars(all = TRUE),\n                 # backend = \"cmdstanr\",\n                 file =\"analysis/SGC3A/models/sgc3a_brms_state_Bmm.cat.rSQ_REP.rds\")\n\n\n#UNINFORMATIVE PRIOR BAYESIAN MIXED VERSION\n# flat_Bmm.cat.CrSQ <- brm( state ~ pretty_condition + (1|subject) + (1|q), \n#                  data = df_i, \n#                  family = \"categorical\",\n#                  chains = 4, iter = 2500, warmup = 1000,\n#                  cores = 4, seed = 1234,\n#                  save_pars = save_pars(all = TRUE),\n#                  # backend = \"cmdstanr\",\n#                  file =\"analysis/SGC3A/models/sgc3a_brms_state_FLAT_Bmm.cat.CrSQ_REP.rds\")\n\n\n# determine default priors \n# prior_summary(flat_Bmm.cat.CrSQ)\n\n#set priors [see justification, below]\ninf_priors <- c(\n  # too strong?\n  # prior(normal(-6.91, 0.201),  class = \"Intercept\", dpar = \"muangular\"),\n  # prior(normal(-6.91, 0.201),  class = \"Intercept\", dpar = \"muother\"),\n  # prior(normal(-6.91, 0.201),  class = \"Intercept\", dpar = \"mutriangular\"),\n  #prior on INTERCEPTS \n  #25% chance of each answer in control, scale = from 0.01 to 62%\n  prior(normal(-1.1, 1.5),  class = \"Intercept\", dpar = \"muangular\"),\n  prior(normal(-1.1, 1.5),  class = \"Intercept\", dpar = \"muother\"),\n  prior(normal(-1.1, 1.5),  class = \"Intercept\", dpar = \"mutriangular\"),\n  #prior on COEFFICIENT\n  #likely to change odds between 0 and 2.4\n  prior(normal(0, 2.42), class = b, coef=\"pretty_conditionimpasse\", dpar = \"muangular\"),\n  prior(normal(0, 2.42), class = b, coef=\"pretty_conditionimpasse\", dpar = \"muother\"),\n  prior(normal(0, 2.42), class = b, coef=\"pretty_conditionimpasse\", dpar = \"mutriangular\")\n)\n\n#INFORMATIVE PRIORS\nBmm.cat.CrSQ <- brm( state ~ pretty_condition + (1|subject) + (1|q), \n                 data = df_i, \n                 family = \"categorical\",\n                 prior = inf_priors,\n                 chains = 4, iter = 2500, warmup = 1000,\n                 cores = 4, seed = 1234,\n                 save_pars = save_pars(all = TRUE),\n                 control = list(adapt_delta = 0.98),  # to deal with divergent transitions\n                 # backend = \"cmdstanr\",\n                 file =\"analysis/SGC3A/models/sgc3a_brms_state_Bmm.cat.CrSQ_REP.rds\"\n                 )\n\n#a bayes factor model comparison of the flat vs informative prior models suggest convicing evidence that \n#informative prior model is a better fit\n# bayesfactor(Bmm.cat.CrSQ, flat_Bmm.cat.CrSQ)\n\n# PRIORS LOGIC \n# https://www.bayesrulesbook.com/chapter-13.html#building-the-logistic-regression-model\n\n#expectation for probability of _better_ response [in control]?\n#very low probability center: 0.1% [very low]; as logodds = logit(0.001) = -6.91\n#range from 0 to 55%  logit(0.55) = 0.201\n#probability of 0.1 to 55% is equivalent to [logodds] -6.91 +/ 2* 0.201\n#therefore... prior for intercept => Normal(−6.91, 0)\n\n\n#expectation for probability of _better_ response [in impasse]?\n#increases probablity from 0 % \n# 0 [very low]; as OR  = exp(0) = 1\n#range from 0 to 90%  exp(0.9) = 2.46\n#probability of 0 to 90% is equivalent to [ODDS scale] 1 +/ 2* 2.42\n#on log odds scale ? [0, ]\n#therefore... prior for intercept => Normal(1, 2.42)\n                             # prior = normal(0.07, 0.035),\n\n\n\n\nDescribe\n\n\nCODE\n# best model\nm <- Bmm.cat.CrSQ\n\n#::::::::: PRINT MODEL \n\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m)\n\n\n Family: categorical \n  Links: muother = logit; muangular = logit; mutriangular = logit \nFormula: state ~ pretty_condition + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 923) \n  Draws: 4 chains, each with iter = 2500; warmup = 1000; thin = 1;\n         total post-warmup draws = 6000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          1.54      0.40     0.92     2.50 1.00     2162\nsd(muangular_Intercept)        1.68      0.61     0.82     3.21 1.00     1992\nsd(mutriangular_Intercept)     1.12      0.31     0.66     1.88 1.00     2299\n                           Tail_ESS\nsd(muother_Intercept)          3445\nsd(muangular_Intercept)        3351\nsd(mutriangular_Intercept)     3097\n\n~subject (Number of levels: 71) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          1.00      0.19     0.65     1.41 1.00     2354\nsd(muangular_Intercept)        1.47      0.37     0.80     2.26 1.01     2019\nsd(mutriangular_Intercept)     4.17      0.58     3.20     5.47 1.00     1646\n                           Tail_ESS\nsd(muother_Intercept)          3083\nsd(muangular_Intercept)        2903\nsd(mutriangular_Intercept)     3070\n\nPopulation-Level Effects: \n                                     Estimate Est.Error l-95% CI u-95% CI Rhat\nmuother_Intercept                       -1.96      0.49    -2.93    -1.01 1.00\nmuangular_Intercept                     -3.94      0.70    -5.40    -2.64 1.00\nmutriangular_Intercept                  -2.42      0.81    -4.02    -0.89 1.00\nmuother_pretty_conditionimpasse          1.43      0.36     0.77     2.14 1.00\nmuangular_pretty_conditionimpasse        1.36      0.60     0.21     2.60 1.00\nmutriangular_pretty_conditionimpasse     2.21      0.96     0.33     4.11 1.00\n                                     Bulk_ESS Tail_ESS\nmuother_Intercept                        1482     3068\nmuangular_Intercept                      2553     3560\nmutriangular_Intercept                   1548     2956\nmuother_pretty_conditionimpasse          3270     3916\nmuangular_pretty_conditionimpasse        4023     4413\nmutriangular_pretty_conditionimpasse     1284     2271\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n(d <- describe_posterior(ci=.95, Bmm.cat.CrSQ))\n\n\nSummary of Posterior Distribution\n\nParameter                            | Median |         95% CI |     pd |          ROPE | % in ROPE |  Rhat |     ESS\n---------------------------------------------------------------------------------------------------------------------\nmuother_Intercept                    |  -1.95 | [-2.93, -1.01] | 99.92% | [-0.18, 0.18] |        0% | 1.001 | 1479.00\nmuangular_Intercept                  |  -3.91 | [-5.40, -2.64] |   100% | [-0.18, 0.18] |        0% | 1.002 | 2525.00\nmutriangular_Intercept               |  -2.40 | [-4.02, -0.89] | 99.95% | [-0.18, 0.18] |        0% | 1.000 | 1547.00\nmuother_pretty_conditionimpasse      |   1.43 | [ 0.77,  2.14] | 99.97% | [-0.18, 0.18] |        0% | 1.000 | 3248.00\nmuangular_pretty_conditionimpasse    |   1.35 | [ 0.21,  2.60] | 99.08% | [-0.18, 0.18] |        0% | 1.000 | 3986.00\nmutriangular_pretty_conditionimpasse |   2.21 | [ 0.33,  4.11] | 98.80% | [-0.18, 0.18] |        0% | 1.001 | 1282.00\n\n\nCODE\nprint(\"BAYES FACTOR [comparison to null]\")\n\n\n[1] \"BAYES FACTOR [comparison to null]\"\n\n\nCODE\n#think of this like the anova(model) to get p values for each predictor\n#has to recompile the models with rstan. total drag\n(b <- bayesfactor(Bmm.cat.rSQ, m))\n\n\nWarning: Bayes factors might not be precise.\nFor precise Bayes factors, sampling at least 40,000 posterior samples is recommended.\n\n\nComputation of Bayes factors: estimating marginal likelihood, please wait...\n\n\nBayes Factors for Model Comparison\n\n    Model                                            BF\n[2] pretty_condition + (1 | subject) + (1 | q) 1.59e+04\n\n* Against Denominator: [1] 1 + (1 | subject) + (1 | q)\n*   Bayes Factor Type: marginal likelihoods (bridgesampling)\n\n\nCODE\nprint(\"DESCRIBE POSTERIOR\")\n\n\n[1] \"DESCRIBE POSTERIOR\"\n\n\nCODE\n#:::::::: INTERPRET COEFFICIENTS\n\npaste(\"LOG ODDS\")\n\n\n[1] \"LOG ODDS\"\n\n\nCODE\n(l <- describe_posterior(m))\n\n\nSummary of Posterior Distribution\n\nParameter                            | Median |         95% CI |     pd |          ROPE | % in ROPE |  Rhat |     ESS\n---------------------------------------------------------------------------------------------------------------------\nmuother_Intercept                    |  -1.95 | [-2.93, -1.01] | 99.92% | [-0.18, 0.18] |        0% | 1.001 | 1479.00\nmuangular_Intercept                  |  -3.91 | [-5.40, -2.64] |   100% | [-0.18, 0.18] |        0% | 1.002 | 2525.00\nmutriangular_Intercept               |  -2.40 | [-4.02, -0.89] | 99.95% | [-0.18, 0.18] |        0% | 1.000 | 1547.00\nmuother_pretty_conditionimpasse      |   1.43 | [ 0.77,  2.14] | 99.97% | [-0.18, 0.18] |        0% | 1.000 | 3248.00\nmuangular_pretty_conditionimpasse    |   1.35 | [ 0.21,  2.60] | 99.08% | [-0.18, 0.18] |        0% | 1.000 | 3986.00\nmutriangular_pretty_conditionimpasse |   2.21 | [ 0.33,  4.11] | 98.80% | [-0.18, 0.18] |        0% | 1.001 | 1282.00\n\n\nCODE\n# (tm <- tidy(m,   conf.int = TRUE))\n\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\n(e <- model_parameters(m, exponentiate = TRUE))\n\n\nParameter                            | Median |        95% CI |     pd | % in ROPE |  Rhat |     ESS\n----------------------------------------------------------------------------------------------------\nmuother_Intercept                    |   0.14 | [0.05,  0.36] | 99.92% |        0% | 1.001 | 1479.00\nmuangular_Intercept                  |   0.02 | [0.00,  0.07] |   100% |        0% | 1.002 | 2525.00\nmutriangular_Intercept               |   0.09 | [0.02,  0.41] | 99.95% |        0% | 1.000 | 1547.00\nmuother_pretty_conditionimpasse      |   4.16 | [2.16,  8.51] | 99.97% |        0% | 1.000 | 3248.00\nmuangular_pretty_conditionimpasse    |   3.84 | [1.23, 13.45] | 99.08% |        0% | 1.000 | 3986.00\nmutriangular_pretty_conditionimpasse |   9.07 | [1.39, 60.87] | 98.80% |        0% | 1.001 | 1282.00\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\nCODE\n# tidy(m,   conf.int = TRUE, exponentiate = TRUE)\n# tm %>% mutate(\n#   OR.est = exp(estimate),\n#   exp.low = exp(conf.low),\n#   exp.high = exp(conf.high)\n# ) %>% dplyr::select(effect, component, group, term, OR.est, exp.low, exp.high)\n\n# paste(\"PROBABILITIES\")\n# \n# #PREDICT METHOD\n# newdata <- df_i %>% dplyr::select(pretty_condition, subject, q)\n# preds <- predict(m, newdata = newdata, type = \"response\")\n# preds <- cbind(newdata, preds)\n# #lengthen data frame to handle multinomial\n# preds <- preds %>% \n#   dplyr::select(-subject, -q) %>% #marginalize over subject and q\n#   pivot_longer(\n#   cols = !pretty_condition,\n#   values_to = \"preds\",\n#   names_to = \"state\",\n# ) \n# \n# (p <- preds %>% \n#   group_by(pretty_condition, state ) %>%\n#   summarise(\n#     median = median(preds),\n#     se = sd(preds)/sqrt(n()),\n#     lwr = median - 1.96*se,\n#     upr = median + 1.96*se))\n\n##DRAWS METHOD\n# GENERATE draws from model\n# draws <- df_i %>%\n#   data_grid(pretty_condition, subject, q) %>% \n#   add_fitted_draws(Bmm.cat.CrSQ,\n#                    # n = 100,\n#                    # dpar = TRUE,\n#                    # transform = TRUE, #gives prob%, otherwise OR\n#                    re_formula = NA)\n# # draws %>% write_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n# \n# #OR load from file\n# # draws <- read_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n# \n# # SUMMARIZE draws from model\n# (k <- kable(draws %>%\n#   dplyr::select(pretty_condition, .category, .value) %>%\n#   group_by(pretty_condition, .category) %>%\n#   median_hdci(.value), digits = 4, col.names =\n#     c(\"Condition\",\"Category\", \"Probability\",\"Lower Cred.I\",\"Upper Cred.I\", \"CI Width\", \"Point Type\", \"Interval Type\")) %>%\n#   kable_styling())\n\n\n\n\nINFERENCE — EFFECT\n[REPORT POSTERIOR MEDIAN \\(\\exp_{beta}\\), 95 % credible interval, % probability of direction]\nWe fit a (bayesian) multinomial logistic regression model with random intercepts for subjects and questions. A Bayes Factor model comparison (against a random intercepts-only model) indicates extreme evidence for a main effect of CONDITION (BF = 3.65e+04). Consistent with our hypothesis, the impasse condition substantially increases the odds of transitional interpretations.\nAcross the entire task participants in the impasse condition were 4 times more likely to offer an ‘unknown’ rather than orthogonal response compared with those in the control condition ( \\(e^{\\beta_1} = 4.16, 95 \\% CI [2.16, 8.51], pd = 99\\%\\)). Participants in the impasse condition were 4 times more likely to offer an ‘angular’ rather than orthogonal response compared with those in the control condition ( \\(e^{\\beta_1} = 3.84, 95 \\% CI [1.23, 13.45], pd = 99\\%\\)), and 9 times more likely to offer an ‘triangular’ rather than orthogonal response compared with those in the control condition ( \\(e^{\\beta_1} = 9.07, 95 \\% CI [1.39, 60.87], pd = 99\\%\\)).\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n'bayes_R2' is not defined for unordered categorical models.\n\n\n\n\n \nstate: other\nstate: angular\nstate: triangular\nstate: other_pretty\nstate: angular_pretty\nstate: triangular_pretty\n\n\nPredictors\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\n\n\nIntercept\n0.02\n0.00 – 0.07\n\n\n0.14\n0.05 – 0.36\n\n\n0.09\n0.02 – 0.41\n\n\n\n\nconditionimpasse\n\n\n3.84\n1.23 – 13.45\n\n\n4.16\n2.16 – 8.51\n\n\n9.07\n1.39 – 60.87\n\n\nRandom Effects\n\n\n\nσ2\n0.29\n\n\n\nτ00\n1.46\n\n\nICC\n0.17\n\n\nN subject\n71\n\n\nN q\n13\n\nObservations\n923\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m, \"(log odds)\" = m)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',\n#              notes = notes)\n#              # output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n# \n# \n# ## POINT ESTIMATES IN PROBABILITY\n# #1 | get draws\n# draws <- df_i %>%\n#   data_grid(pretty_condition, subject, q) %>%\n#   add_fitted_draws(Bmm.cat.CrSQ,\n#                    # n = 100,\n#                    # dpar = TRUE,\n#                    # transform = TRUE, #gives prob%, otherwise OR\n#                    re_formula = NA)\n# draws %>% write_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n# \n# #2 | SUMMARIZE draws \n# k <- kable(draws %>%\n#   select(pretty_condition, .category, .value) %>%\n#   group_by(pretty_condition, .category) %>%\n#   median_hdci(.value), digits = 2, col.names = \n#     c(\"Condition\",\"Category\", \"Probability\",\"Lower Cred.I\",\"Upper Cred.I\", \"CI Width\", \"Point Type\", \"Interval Type\")) %>% \n#   kable_styling()\n# k\n\n#COMPARISONS\n# c <- draws %>% \n#   # dplyr::select(pretty_condition, .category, .value) %>%\n#   compare_levels(variable = .value, by = pretty_condition, \n#                  comparison = list(c(\"control\",\"impasse\"))) \n#                                    # c(\"adots\",\"interval\"),\n#                                    # c(\"adots\",\"mean\"),\n#                                    # c(\"adots\",\"text\"),\n#                                    # c(\"density\",\"interval\"),\n#                                    # c(\"density\",\"mean\"),\n#                                    # c(\"density\",\"text\"),\n#                                    # c(\"interval\",\"mean\"),\n#                                    # c(\"interval\",\"text\"),\n#                                    # c(\"mean\",\"text\")\n#                                    \n# c %>%\n#   ggplot(aes(x = .value, y = reorder(x =pretty_condition, X = .value)))+\n#   stat_interval(.width = .95, color = \"black\") +\n#   geom_vline(xintercept = 0)+\n#   theme_bw()+\n#   # coord_cartesian(xlim = c(-.5,1)) +\n#   theme_tidybayes() \n# comps\n\n\n\n\nVisualize\n\n\nCODE\n## | PLOT PARAMETERS \n\n#SJPLOT | MODEL | ODDS RATIO\n# plot_model(m, vline.color = \"red\",\n#            show.intercept = TRUE,\n#            show.values = TRUE,\n#            p.threshold = 0.1, #manually adjust to account for directional test\n#            ci.lvl = 0.90 ) + #manually adjusted for directional test\n#   labs(title = \"Model Estimate | Odds Ratio\",\n#        subtitle = \"\",\n#        x = \"Condition\")\n\n\n#EASYSTATS | MODEL | ODDS RATIO\nresult <- model_parameters(Bmm.cat.CrSQ, exponentiate = TRUE, component = \"all\")\nplot(result, show_intercept = TRUE, show_labels = TRUE) \n\n\n\n\n\nCODE\n# + theme_clean()\n\n# \n# result <- estimate_density(m,exponentiate = TRUE)\n# plot(result,  stack = FALSE, priors = TRUE)\n\n## | PLOT TESTS\nresult <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\nplot(result)\n\n\nPicking joint bandwidth of 0.0827\n\n\nWarning: Removed 3600 rows containing non-finite values (stat_density_ridges).\n\n\n\n\n\nCODE\nresult <- rope(m)\nplot(result)\n\n\n\n\n\nCODE\n(result <- pd(m,exponentiate = TRUE))\n\n\nProbability of Direction\n\nParameter                            |     pd\n---------------------------------------------\nmuother_Intercept                    | 99.92%\nmuangular_Intercept                  |   100%\nmutriangular_Intercept               | 99.95%\nmuother_pretty_conditionimpasse      | 99.97%\nmuangular_pretty_conditionimpasse    | 99.08%\nmutriangular_pretty_conditionimpasse | 98.80%\n\n\nCODE\nplot(result, show_intercept = TRUE, show_labels = TRUE)\n\n\n\n\n\nCODE\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\n# plot_model(m, type=\"eff\",\n#            show.intercept = TRUE,\n#            show.values = TRUE,\n#            title = \"Model Prediction | Probability of Accurate Response\",\n#            axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n# \n# #PLOT MODEL PREDICTION\nplot_model(m, type = \"pred\")\n\n\nNote: uncertainty of error terms are not taken into account. You may want to use `rstantools::posterior_predict()`.\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#TODO OUTPUT TABLE \n#https://arelbundock.com/posts/modelsummary_multinomial_logit/\n# modelsummary(m)\n\n\n\n\nCODE\n#::::: GGDIST POSTERIOR PROBABILITY OF RESPONSE\n##WORKING\n# https://mjskay.github.io/ggdist/reference/stat_slab.html\n## VIS probability of correct response\n#TAKES A REALLY LONG TIME\n\n#1 | get draws\n# draws <- df_i %>%\n#   data_grid(pretty_condition, subject, q) %>% \n#   add_fitted_draws(Bmm.cat.CrSQ,\n#                    # n = 100,\n#                    # dpar = TRUE,\n#                    # transform = TRUE, #gives prob%, otherwise OR\n#                    re_formula = NA) \n# draws %>% write_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n\n#OR load from file\n# draws <- read_rds(file = \"analysis/SGC3A/models/draws/draws_BB.catCrSQ.rds\")\n\n#2| VISUALIZE PREDICTIONS | GGDIST\n##TODO figure out height normalization.\n##do it with much smaller number of draws \n#TODO adjust bandwidth/smoothing? + put on same line + \n#TAKES A REAAALY LONG TIME\n# d <- draws %>%\n#   ggplot(aes(x = .value,  y = pretty_condition, fill = .category)) +\n#   stat_slab(width = c(.95), alpha = 1, normalize=\"xy\") +\n#   #   #normalize = all, panels, xy, groups, none\n#   xlim(0,1) + labs(\n#     title = \"Model Predicted Probability of Correct Response\",\n#     x = \"probability of correct response\",\n#     y = \"Interpretation\"\n#   ) +  theme_clean() #+ ggeasy::easy_remove_legend() + ggeasy::easy_remove_y_axis()\n# # #TO PLOT ON THE SAME LINE, INCLUDE Y = 0 in aes and ggeasy::remove_y_axis()\n# # \n# # ggsave(d, filename = \"figures/sgc3a_BBm.cat.CrSQ_lab_posterior.svg\", width = 6, height =4)\n# d\n\n\n\n\nDiagnostics\n\n\nCODE\n#CHECK Fit of posterior predictive to data\npp_check(Bmm.cat.CrSQ, ndraws=1000)\n\n\n\n\n\nCODE\n#CHECK posterior vs. priors\nresult <- estimate_density(Bmm.cat.CrSQ)\nplot(result, stack = FALSE, priors = TRUE)\n\n\n\n\n\nCODE\n#CHECK model\nplot(Bmm.cat.CrSQ)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOMPARE MBLOGIT to BRMS\n\n\nCODE\ncompare_models(mm.cat.CrSQ, Bmm.cat.CrSQ)\n\n\nParameter                            |          mm.cat.CrSQ |         Bmm.cat.CrSQ\n----------------------------------------------------------------------------------\nother~(Intercept)                    | -1.59 (-2.71, -0.47) |                     \nangular~(Intercept)                  | -3.28 (-4.74, -1.83) |                     \ntriangular~(Intercept)               | -1.90 (-3.51, -0.30) |                     \nother~pretty conditionimpasse        |  1.57 ( 0.75,  2.39) |                     \nangular~pretty conditionimpasse      |  1.64 ( 0.36,  2.92) |                     \ntriangular~pretty conditionimpasse   |  2.30 ( 0.49,  4.10) |                     \nmuother_Intercept                    |                      | -1.95 (-2.93, -1.01)\nmuangular_Intercept                  |                      | -3.91 (-5.40, -2.64)\nmutriangular_Intercept               |                      | -2.40 (-4.02, -0.89)\nmuother_pretty_conditionimpasse      |                      |  1.43 ( 0.77,  2.14)\nmuangular_pretty_conditionimpasse    |                      |  1.35 ( 0.21,  2.60)\nmutriangular_pretty_conditionimpasse |                      |  2.21 ( 0.33,  4.11)\n----------------------------------------------------------------------------------\nObservations                         |                  923 |                  923\n\n\nThe predictions of the manual, frequentist mixed multinomial and bayesian mixed multinomial models are comparable."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html#h1b-q1-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html#h1b-q1-accuracy",
    "title": "7  (Lab) Hypothesis Testing",
    "section": "H1B | Q1 ACCURACY",
    "text": "H1B | Q1 ACCURACY\nDo Ss in the IMPASSE condition have a higher likelihood of producing a correct response to the first question?\nThe graph comprehension task includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their first exposure to the unconventional triangular coordinate system.\n\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of correctly responding to the first question than those in the CONTROL condition\n\n\nData\n\ndata: df_items where q == 1\noutcome: accuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nLogistic Regression on accuracy predicted by condition\n\naccount for difference in odds of correct score by condition\n\n\nAlternatives:\n\nChi-Square test of independence on outcome accuracy by condition\n\n\n\nNotes\n\nCHIQ SQR is simplest method to examine independence of two categorical factors; LOGISTIC REGRESSION is recommended for binomial ~ continuous; though with regression we can quantify the size of the effect and overall model fit\nindependence assumption : (CHI SQR) as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\ncell frequency : (CHI SQR) expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\n\n\n\nSetup\n\n\nCODE\n#:::::::: PREP DATA\ndf <- df_items %>% filter(q==1)  %>% dplyr::select(accuracy, pretty_condition)\n\n\n\n\nDescribe\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Q1 Accuracy\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nA proportional bar chart visualizing the proportion of incorrect (vs) correct responses in each condition for each data collection modality (left/right facet) reveals that the pattern of responses appear the same regardless of the data collection modality. In both data collection sessions, the proportion of incorrect responses is much greater than the proportion of correct responses, regardless of condition (marginal probability of incorrect). In the impasse condition, the difference in proportions is smaller than the control condition (conditional probability of success in impasse; (i.e) There are more correct responses in the impasse condition than the control condition).\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\npaste(\"Proportions of Correct Responses by Condition\")\n\n\n[1] \"Proportions of Correct Responses by Condition\"\n\n\nCODE\ntable(df$accuracy, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse   Sum\n  incorrect   0.839   0.650 0.732\n  correct     0.161   0.350 0.268\n  Sum         1.000   1.000 1.000\n\n\nCODE\npaste(\"Number of Correct Responses by Condition\")\n\n\n[1] \"Number of Correct Responses by Condition\"\n\n\nCODE\ntable(df$accuracy, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse Sum\n  incorrect      26      26  52\n  correct         5      14  19\n  Sum            31      40  71\n\n\n\n\nLOGISTIC REGRESSION\nFit a logistic regression predicting accuracy (absolute score) (n = 71) by condition (k = 2).\n\n\nParameter estimate: \\(\\beta_{0}\\) = Log Odds of (correct) responses in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of correct response in CONTROL condition\nParameter estimate: \\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for correct response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of correct response in IMPASSE (vs) CONTROL\nNull hypothesis:\\(\\beta_{impasse} \\le 0\\) the odds for a correct response does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\gt 0\\) the odds of a correct response increases\n\n\nFit Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\n\nCODE\n# MODEL FITTING ::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\n# print(\"EMPTY MODEL\")\n# summary(m0)\n\n#: 2 CONDITION model\nm1 <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\n# print(\"PREDICTOR MODEL\")\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.928  -0.928  -0.593   1.449   1.910  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.649      0.488   -3.38  0.00074 ***\npretty_conditionimpasse    1.030      0.590    1.74  0.08107 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 82.483  on 70  degrees of freedom\nResidual deviance: 79.188  on 69  degrees of freedom\nAIC: 83.19\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\ncar::Anova(m1)\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: accuracy\n                 LR Chisq Df Pr(>Chisq)  \npretty_condition     3.29  1      0.069 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > m1$aic)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff | Chi2 |     p\n------------------------------------------\nm0   |   glm |  1 |         |      |      \nm1   |   glm |  2 |       1 | 3.29 | 0.069\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.069492127779542\"\n\n\nThe Condition predictor decreases AIC, but the Likelihood Ratio Test is marginal. We proceed to examine the predictor model, as we plan to do a 1-tailed NHST .\n\n\nDescribe\n\n\nCODE\n# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL [default two-tailed sig test]\")\n\n\n[1] \"PREDICTOR MODEL [default two-tailed sig test]\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.928  -0.928  -0.593   1.449   1.910  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.649      0.488   -3.38  0.00074 ***\npretty_conditionimpasse    1.030      0.590    1.74  0.08107 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 82.483  on 70  degrees of freedom\nResidual deviance: 79.188  on 69  degrees of freedom\nAIC: 83.19\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\ncar::Anova(m1)\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: accuracy\n                 LR Chisq Df Pr(>Chisq)  \npretty_condition     3.29  1      0.069 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: \n\n# one-sided (right tail) z test for B COEFFICIENT\n#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients\n\n#SANITY CHECK 2-tailed test should match the model output\ntt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"p value for two-tailed test, null B = 0 : \",round(tt,3))\n\n\n[1] \"p value for two-tailed test, null B = 0 :  0.081\"\n\n\nCODE\not <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"BUT we want a one tailed directional, null: B <= 0: \",round(ot,3))\n\n\n[1] \"BUT we want a one tailed directional, null: B <= 0:  0.041\"\n\n\nCODE\npaste(\"adjusted confint for directional hypothesis\")\n\n\n[1] \"adjusted confint for directional hypothesis\"\n\n\nCODE\n(dcint <- confint(m1, level = 0.90)) # get 90% for right side))\n\n\nWaiting for profiling to be done...\n\n\n                           5 %   95 %\n(Intercept)             -2.538 -0.907\npretty_conditionimpasse  0.094  2.058\n\n\nCODE\n# https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte\n\n#:::::::: INTERPRET COEFFICIENTS\n\n# print(\"Confidence Interval —- LOG ODDS\")\n# confint(m1) #not adjusted for 1-tailed\nprint(\"Coefficients —- ODDS RATIOS\")\n\n\n[1] \"Coefficients —- ODDS RATIOS\"\n\n\nCODE\n# (e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiated, not adjusted\n(e <- cbind( exp(coef(m1)), exp(dcint))) #exponentiated, adjusted\n\n\n                                5 %  95 %\n(Intercept)             0.192 0.079 0.404\npretty_conditionimpasse 2.800 1.099 7.832\n\n\nCODE\nprint(\"MODEL PREDICTIONS\")\n\n\n[1] \"MODEL PREDICTIONS\"\n\n\nCODE\n# Retrieve predictions as probabilities \n# (for each level of the predictor)\npred.control <- predict(m1,data.frame(pretty_condition=\"control\"),type=\"response\")\n#this should match : plogis(intercept coefficient)\npaste(\"Probability of success in control,\", pred.control)\n\n\n[1] \"Probability of success in control, 0.161290322580645\"\n\n\nCODE\npred.impasse <- predict(m1,data.frame(pretty_condition=\"impasse\"),type=\"response\")\n#this should match : plogis(intercept coefficient + predictor coeff)\npaste(\"Probability of success in impasse,\", pred.impasse)\n\n\n[1] \"Probability of success in impasse, 0.350000000000014\"\n\n\n\n\nInference — DIRECTIONAL EFFECT\n\n\nVisualize\n\n\nCODE\n#SET MODEL\nm <- m1\n\n#GGSTATS | MODEL | LOG ODDS \n# ggcoefstats(m1, output = \"plot\", \n#               conf.level = 0.90) + \n#   labs(x = \"Log Odds Estimate\", \n#        title = \"LOGODDS | Q1 Accuracy ~ condition\",\n#        subtitle = \"(p is for two tailed test)\")\n\n\n#PARAMETERS | MODEL | SIMULATED PARAMETERS\n# similar to bayesian dist of estimate\n# result <- simulate_parameters(m1)\n# #rename params so intercept is plotted \n# result$Parameter[1] <- \"condition [control]\"\n# result$Parameter[2] <- \"condition [impasse]\"\n# plot(result) \n\n#EQUIVALENCE TEST [not sure if appropriate for log model?]\n# https://journals.sagepub.com/doi/10.1177/2515245918770963#:~:text=Consequently%2C%20when%20reporting%20an%20equivalence,values%20is%20smaller%20than%20alpha.\n# https://easystats.github.io/parameters/reference/equivalence_test.lm.html\n# (result <- equivalence_test(m1, rule = \"classic\", component = c(\"all\")))\n# plot(result,   show_intercept = TRUE) + \n#   scale_y_discrete(labels = c(\"impasse\", \"control\")) + \n#   labs( title = \"Equivalence Test for Model Parameter Estimates\")\n\n\n#PARAMETERS | MODEL | ODDS RATIO \n# result <- model_parameters(m1,exponentiate = TRUE)\n# #rename params \n# result$Parameter[1] <- \"condition [control]\"\n# result$Parameter[2] <- \"condition [impasse]\"\n# plot(result,   show_intercept = TRUE) +  labs(\n#   title = \"Model Parameter Estimates\"\n# ) + theme_clean() + theme(legend.position=\"blank\")\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m, type = \"est\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) +  #manually adjusted for directional test   \n  scale_y_continuous() + #remove to put on log scale x axis \n  scale_x_discrete(labels=c(\"control\",\"impasse\"))+\n  labs(title = \"MODEL ESTIMATE | Q1 Accuracy ~ condition\",\n       subtitle = \"Impasse increases odds of correct response on Q1\",\n       x = \"Condition\") + theme_clean()\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type=\"pred\")[[1]] +\n  ylim(0,1) + #scale y axis to actual range\n  labs(title = \"MODEL PREDICTION  | Q1 Accuracy ~ condition\",\n       subtitle = \"Impasse increases probability of correct response on Q1\",\n       x = \"Condition\") + theme_clean()\n\n\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.19\n0.07 – 0.46\n0.001\n\n\npretty condition[impasse]\n2.80\n0.92 – 9.70\n0.081\n\n\nObservations\n71\n\n\nR2 Tjur\n0.045\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m1, \"(log odds)\" = m1)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', \n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n\n\n\nDiagnostics\n\n\nCODE\nprint(\"MODEL DIAGNOSTICS\")\n\n\n[1] \"MODEL DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m1)"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html#h1b-q1-interpretation-state",
    "href": "analysis/SGC3A/4_sgc3A_replication_hypotesting.html#h1b-q1-interpretation-state",
    "title": "7  (Lab) Hypothesis Testing",
    "section": "H1B | Q1 INTERPRETATION STATE",
    "text": "H1B | Q1 INTERPRETATION STATE\nDo Ss in the IMPASSE condition offer less-orthogonal interpretations on first question?\nWhile absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn’t allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject’s response, and determine if these interpretations differ by experimental condition. State is a 3-category derived response variable that groups the following interpretations:\n\n“orthogonal” [reference category] includes orthogonal and satisficing responses ==> indicates a primarily orthogonal state of coordinate system understanding\n“other” includes: blank, reference point, responses that can’t be classified (including selecting all datapoints), => indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly not orthogonal nor triangular\n“angular” includes ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n“triangular” includes correct triangular and ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n\n\n\n\n\n\n\n\nResearch Question\nDoes Ss in the impasse condition produce less orthogonal responses on the first question?\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of producing unknown and and triangle-like response states, relative to orthogonal response states, on the first question\n\n\nData\n\ndata: df_items where q == 1\noutcome: state ( 4 level factor from 5 level high_interpretation )\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMultinomial (Logistic) Regression on state predicted by condition\n\nAlternative:\n\nOrdinal regression on state; but model doesn’t satisfy proportional odds assumption (parallel slopes)\nMultinomial or Ordinal regression on high_interpretation (5 category interpretation state which distinguishes between uncertain (blank, reference) unclassifiable, triangle-like and true triangular.) There are some cells with zeros, however (no uncertain responses in control) which means the model can’t accurately estimate those comparisons\n\n\n\n\n\nSetup\n\n\nCODE\n#:::::::: PREP DATA\ndf <- df_items %>% filter(q==1) %>% dplyr::select(pretty_condition, state)\n\n\n\n\nDescribe\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Q1 Interpretation\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nA proportional bar chart visualizing the proportion of each interpretations type by condition for each data collection modality (left/right facet) reveals that the pattern of responses are similar regardless of the data collection modality, by differ by condition. In the impasse condition, there are more triangular responses than in control. We see that around half of the ‘incorrect’ (i.e. not triangular) responses in the impasse condition are not orthogonal-like, but “other/unknown”.\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\ntable(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n            \n             control impasse    Sum\n  orthogonal  0.8387  0.1750 0.4648\n  other       0.0000  0.4000 0.2254\n  angular     0.0000  0.0750 0.0423\n  triangular  0.1613  0.3500 0.2676\n  Sum         1.0000  1.0000 1.0000\n\n\nCODE\n(t <- table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n            \n             control impasse Sum\n  orthogonal      26       7  33\n  other            0      16  16\n  angular          0       3   3\n  triangular       5      14  19\n  Sum             31      40  71\n\n\n\n\nMULTINOMIAL REGRESSION\nDoes condition affect the response state of Q1?\nFit a logistic regression predicting interpretation state (k=3) by condition(k = 2).\n\n3 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) [essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing [reference category] vs [this category])\nFor each equation:\n\n\\(\\beta_{0}\\) = Log Odds of [this category type vs. reference category type) response in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of [this category type vs. reference category type] response in CONTROL condition\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for [this category] type response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of [this. vs reference category type] response in IMPASSE (vs) CONTROL\nTwo-tailed NHST Null hypothesis: \\(\\beta_{impasse} = 0\\) the odds for [this category of response vs. reference] are not different for IMPASSE condition\nAlternative hypothesis: \\(\\beta_{impasse} \\ne 0\\) the odds of [this category of response vs. reference] increases or decreases for IMPASSE condition\n\n\n\nFit Model\n\n\nCODE\n#check reference level \nprint(\"Categories (first is reference)\")\n\n\n[1] \"Categories (first is reference)\"\n\n\nCODE\nlevels(df$state)\n\n\n[1] \"orthogonal\" \"other\"      \"angular\"    \"triangular\"\n\n\nCODE\n#FIT EMPTY MODEL\n# print(\"EMPTY MODEL\")\ncatm.0 <- multinom(state ~ 1, data = df)\n\n\n# weights:  8 (3 variable)\ninitial  value 98.426900 \niter  10 value 83.663951\nfinal  value 83.663925 \nconverged\n\n\nCODE\n# summary(catm.0)\n\n#FIT PREDICTOR MODEL\n# print(\"PREDICTOR MODEL\")\ncatm <- multinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\n\n# weights:  12 (6 variable)\ninitial  value 98.426900 \niter  10 value 63.286429\niter  20 value 63.042636\niter  30 value 63.026275\nfinal  value 63.026272 \nconverged\n\n\nCODE\n# summary(catm)\n\n#COMPARE MODEL FIT\npaste(\"AIC wth predictor is lower than empty model?\", catm.0$AIC > catm$AIC)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(catm.0, catm)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |      p\n-------------------------------------------------\ncatm.0 | multinom |  3 |         |       |       \ncatm   | multinom |  6 |       3 | 41.28 | < .001\n\n\nCODE\n##compare bayesian version\n#library(brms)\n# b.cat <- brm( state2 ~ pretty_condition, data = df, family = \"categorical\", backend = \"cmdstanr\")\n# summary(b.cat)\n# plot_model(b.cat)\n# report(b.cat)\n# coefficient estimates are very simliar to catm. super cool!\n\n##compare mclogit version\n#\"baseline-category logit model\n# https://www.elff.eu/software/mclogit/manual/mblogit/\n# blm1 <- mblogit(state2 ~ pretty_condition , data = df)\n# summary(blm1)\n#identical to catm. super cool!\n\n\nAIC in predictor model is less than empty model, and likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.\n\n\nDescribe\n\n\nCODE\n#set model\nm <- catm\n\n#::::::::INTERPRETATION\npaste(\"MODEL SUMMARY\")\n\n\n[1] \"MODEL SUMMARY\"\n\n\nCODE\nsummary(m)\n\n\nCall:\nmultinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\nCoefficients:\n           (Intercept) pretty_conditionimpasse\nother           -12.20                   13.02\nangular         -10.88                   10.03\ntriangular       -1.65                    2.34\n\nStd. Errors:\n           (Intercept) pretty_conditionimpasse\nother           87.342                  87.343\nangular         45.215                  45.221\ntriangular       0.488                   0.673\n\nResidual Deviance: 126 \nAIC: 138 \n\n\nCODE\ncar::Anova(m)\n\n\n# weights:  8 (3 variable)\ninitial  value 98.426900 \niter  10 value 83.663951\nfinal  value 83.663925 \nconverged\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: state\n                 LR Chisq Df Pr(>Chisq)    \npretty_condition     41.3  3    5.7e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n# calculate z-statistics of coefficients\nz_stats <- summary(m)$coefficients/summary(m)$standard.errors\n# convert to p-values\np_values <- (1 - pnorm(abs(z_stats)))*2\n# display p-values in transposed data frame\n(p_values <- data.frame(p = (p_values)))\n\n\n           p..Intercept. p.pretty_conditionimpasse\nother           0.888933                  0.881461\nangular         0.809829                  0.824406\ntriangular      0.000735                  0.000501\n\n\nCODE\n# display odds ratios in transposed data frame\n\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\nodds_ratios <- data.frame(OR = exp(summary(m)$coefficients))\noptions(scipen = 2)\n(results <- cbind(odds_ratios, p_values))\n\n\n           OR..Intercept. OR.pretty_conditionimpasse p..Intercept.\nother          0.00000504                   453336.0      0.888933\nangular        0.00001881                    22779.9      0.809829\ntriangular     0.19230417                       10.4      0.000735\n           p.pretty_conditionimpasse\nother                       0.881461\nangular                     0.824406\ntriangular                  0.000501\n\n\n\n\nInference — CAN’T MODEL, SPARSE CELLS\nsample size is too low to estimate this model. There are no participants in the control condition with ‘other’ or ‘angular’ answers.\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \n# ggcoefstats(m, output = \"plot\", \n#               # conf.level = 0.90,\n#               exclude.intercept = FALSE) + \n#   labs(x = \"Log Odds Estimate\", \n#        title = \"LOGODDS | Q1 State ~ condition\",\n#        subtitle = \"(p is for two tailed test)\")\n#:::::::: PLOT\n\n#PARAMETERS | MODEL | SIMULATED PARAMETERS\n# similar to bayesian dist of estimate\n# result <- simulate_parameters(m)\n# plot(result, show_intercept = TRUE, stack=FALSE)\n\n#PARAMETERS | MODEL | ODDS RATIO \n# result <- model_parameters(m1,exponentiate = TRUE)\n# #rename params \n# result$Parameter[1] <- \"condition [control]\"\n# result$Parameter[2] <- \"condition [impasse]\"\n# plot(result,   show_intercept = TRUE) +  labs(\n#   title = \"Model Parameter Estimates\"\n# ) + theme_clean() + theme(legend.position=\"blank\")\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m, type = \"est\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) +  #manually adjusted for directional test   \n  # scale_y_continuous() + #remove to put on log scale x axis \n  scale_x_discrete(labels=c(\"control\",\"impasse\"))+\n  labs(title = \"MODEL ESTIMATE  | Q1 Accuracy ~ condition\",\n       subtitle = \"Impasse increases odds of correct response on Q1\",\n       x = \"Condition\") + theme_clean()\n\n\nScale for 'x' is already present. Adding another scale for 'x', which will\nreplace the existing scale.\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type=\"eff\", ci.lvl = 0.95)[[1]] +\n  ylim(0,1) +\n  labs(title = \"MODEL PREDICTION  | Q1 State ~ condition\",\n       subtitle = \"Impasse increases probability of more accurate response states Q1\",\n       x = \"Condition\") + theme_clean()\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nCODE\n#MANUALLY BUILD PREDICTION PLOT FACET BY CONDITION RATHER THAN STATE\np <-plot_model(m, type=\"eff\")[[1]]\nd <- ggplot_build(p)[[1]]  \npoints <- d[[2]]\npoints <- points %>% mutate(\n  state = recode(PANEL, \"1\" =\"orth\", \"2\"=\"other\", \"3\" = \"trilike\", \"4\"=\"tri\"),\n  condition = recode(x, \"1\"=\"control\",\"2\"=\"impasse\"),\n  prob = y\n)\ngf_point( prob ~ state, group = ~x, data = points) + \n  geom_errorbar(aes( x = state, ymin = ymin, ymax = ymax)) + facet_grid(~condition) +ylim(0,1)\n\n\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \nstate\n\n\nPredictors\nOdds Ratios\nCI\np\nResponse\n\n\n(Intercept)\n0.00\n0.00 – 28722943722689885056297507068099247428705629324489501913190908976693248.00\n0.889\nother\n\n\npretty condition[impasse]\n453336.04\n0.00 – 2588676395945885478672530837777748631114813950974385474275281551223317278062804992.00\n0.882\nother\n\n\n(Intercept)\n0.00\n0.00 – 31041424057090752611383649685733376.00\n0.811\nangular\n\n\npretty condition[impasse]\n22779.87\n0.00 – 37983240834656997126437600879159232069369856.00\n0.825\nangular\n\n\n(Intercept)\n0.19\n0.07 – 0.51\n0.001\ntriangular\n\n\npretty condition[impasse]\n10.40\n2.71 – 39.87\n0.001\ntriangular\n\n\nObservations\n71\n\n\nR2 / R2 adjusted\n0.247 / 0.235\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m1, \"(log odds)\" = m1)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', \n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n# modelsummary(mixcat.1, s)\n#TODO OUTPUT TABLE \n#https://arelbundock.com/posts/modelsummary_multinomial_logit/\n\n\n\n\nDiagnostics\n\n\nCODE\n#EXAMINE PREDICTIONS\n#create sample data frame\ntest <- data.frame(pretty_condition = c(\"control\", \"impasse\"))\npred <- predict(catm, newdata = test, \"probs\")\npaste(\"Predicted Probability of Being in Each State\")\n\n\n[1] \"Predicted Probability of Being in Each State\"\n\n\nCODE\n( x <- cbind(test, pred))\n\n\n  pretty_condition orthogonal      other   angular triangular\n1          control      0.839 0.00000423 0.0000158      0.161\n2          impasse      0.175 0.39999611 0.0750004      0.350\n\n\nCODE\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(catm)\n\n\nCan't calculate log-loss.\nCan't calculate proper scoring rules for ordinal, multinomial or cumulative link models.\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n-----------------------------------------------------\n138.053 | 151.629 | 0.247 |     0.235 | 0.354 | 1.393\n\n\nCODE\nDescTools::PseudoR2(catm, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n\n\n  McFadden   CoxSnell Nagelkerke \n     0.247      0.441      0.487 \n\n\nCODE\n#General Goodness of Fit\n#library(generalhoslem)\n#logitgof(df$state, catm$fitted.values, g = 3)\n#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).\n#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables\n# chisq.test(df$state, predict(catm)) #actual states VS predicted states\n# The chi-square test tests the decrease in unexplained variance from the baseline model to the final model\n\n# print(\"MODEL DIAGNOSTICS\")\n# check_model(m) can't do overall diagnostics, have to do them on individual model equations"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html",
    "href": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html",
    "title": "8  (Online) Hypothesis Testing",
    "section": "",
    "text": "The purpose of this notebook is test the hypotheses that determined the design of the SGC3A study WITH OSPAN.\nTODO UPDATE Research Questions\nIn SGC3A-OSPAN we set out to answer the following question: Does posing a mental impasse improve performance on the interval graph comprehension task? Does WORKING MEMORY CAPACITY (as measured by the OSPAN task) explain performance on the graph comprehension task?\nExperimental Hypothesis\nLearners posed with scenario designed to evoke a mental impasse will be more likely to correct interpret the graph.\nWe had no specific hypotheses with respect to how the OSPAN task affects performance. It is plausible that higher working memory allows one to persist in interpreting the coordinate system. It is also plausibe that higher working memory capacity facilitates incorrect interpretations that are more taxing on working memory (for example: end-time questions that require multiple visual projections.)\nNull Hypothesis\nNo significant differences in performance will exist between learners in the IMPASSE and CONTROL conditions."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#sample",
    "href": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#sample",
    "title": "8  (Online) Hypothesis Testing",
    "section": "SAMPLE",
    "text": "SAMPLE\n\nData Collection\nData was collected (online, via SONA) in Fall 2021. TODO ADDITIONAL SUMMER 2022? Note that approximately 200 subjects were run in Fall 2021, but only 133 of them completed the OSPAN task. Therefore subjects who did not complete the task are discarded from analysis.\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_subjects$term, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    fall21 \n    65 \n    68 \n    133 \n  \n  \n    Sum \n    65 \n    68 \n    133 \n  \n\n\n\n\n\n\n\nParticipants\n\n\nCODE\n#Describe participants\nsubject.stats <-df_subjects %>% dplyr::select(age) %>% unlist() %>% favstats()\nsubject.stats$percent.male <- ((df_subjects %>% filter(gender==\"Male\") %>% count())/count(df_subjects))$n\nsubject.stats$percent.female <- ((df_subjects %>% filter(gender==\"Female\") %>% count())/count(df_subjects))$n\nsubject.stats$percent.other <- ((df_subjects %>% filter(gender %nin% c(\"Female\",\"Male\")) %>% count())/count(df_subjects))$n\n\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()%>% \n  footnote(general = \"Age in Years\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    percent.male \n    percent.female \n    percent.other \n  \n \n\n  \n     \n    18 \n    19 \n    20 \n    21 \n    31 \n    20.6 \n    2.18 \n    133 \n    0 \n    0.316 \n    0.669 \n    0.015 \n  \n\n\nNote:   Age in Years\n\n\n\n\nOverall 133 participants (32 % male, 67 % female, 2 % other) undergraduate STEM majors at a public American University participated in exchange for course credit (age: 18 - 31 years).\n\n\nOSPAN\n\n\nCODE\ntitle = \"Descriptive Statistics of OSPAN Task Accuracy\"\nospan.stats <- rbind(\n  \"MATH\" = df_subjects %>% dplyr::select(OSPAN.math_acc) %>% unlist() %>% favstats(),\n  \"ORDER\" = df_subjects %>%  dplyr::select(OSPAN.order_acc) %>% unlist() %>% favstats(),\n  \"WEIGHTED\" = df_subjects %>% dplyr::select(OSPAN.weighted) %>% unlist() %>% favstats()\n\n)\nospan.stats %>% kbl (caption = title) %>% kable_classic() %>%\n  footnote(general = \"MATH = %correct of all math questions;\n           ORDER = % correct of OSPAN ordering (out of 30); WEIGHTED = math*ospan number correct\", general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nDescriptive Statistics of OSPAN Task Accuracy\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    MATH \n    0.517 \n    0.897 \n    0.931 \n    0.966 \n    1 \n    0.924 \n    0.085 \n    133 \n    0 \n  \n  \n    ORDER \n    0.000 \n    0.533 \n    0.733 \n    0.867 \n    1 \n    0.678 \n    0.253 \n    133 \n    0 \n  \n  \n    WEIGHTED \n    0.000 \n    13.448 \n    20.276 \n    24.828 \n    30 \n    19.082 \n    7.391 \n    133 \n    0 \n  \n\n\nNote:   MATH = %correct of all math questions;           ORDER = % correct of OSPAN ordering (out of 30); WEIGHTED = math*ospan number correct\n\n\n\n\n\n\nCODE\n# #GGFORMULA | DENSITY HISTOGRAM \n med = median(df_subjects$OSPAN.weighted)\n  gf_dhistogram(~OSPAN.weighted, data = df_subjects) %>% \n  gf_vline(xintercept = ~med, color = \"red\") +\n  labs(x = \"OSPAN (weighted) score\",\n       y = \"% of subjects\",\n       title = \"Distribution of OSPAN SCORE\",\n       subtitle = \"line indicates median split\")\n\n\n\n\n\nCODE\n#:::::::: STACKED PROPORTIONAL BAR CHART\ndf_subjects %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = ospan_split)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~ospan_split) + \n   labs(title = \"OSPAN SPLIT\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"After taking a median split, comparable high(vs) low in each condition\")"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#h1a-overall-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#h1a-overall-accuracy",
    "title": "8  (Online) Hypothesis Testing",
    "section": "H1A | OVERALL ACCURACY",
    "text": "H1A | OVERALL ACCURACY\n\n\n\n\n\n\n\nResearch Question\nDo Ss in the IMPASSE condition score higher across the entire task than those in the CONTROL group?\n\n\n\n\nHypothesis\n(H1) Participants in the IMPASSE condition will be more likely to correctly interpret the graph than those in the CONTROL condition.\n\n\nData\ndata: df_items where q nin 6,9 (the 13 discriminating Qs ), df_subjects\noutcome:\n\n[at item level] : accuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\n[subject level]: accuracy (number of test phase qs correct from total s_NABS)\n\npredictor: condition [between-subjects factor]\n\n\nAnalysis Strategy\n\nWilcoxon-Rank Sum (Mann-Whitney) test on subject-level total accuracy of test phase (s_NABS)\nMixed Logistic Regression\naccuracy ~ condition + (1 | subject ) + (1 | question)\nmodel effect of condition on probability of correct response [during test phase] while accounting for subject (and item-level?) effects\n\n\n\nAlternatives\n\nOrdinal Mixed Logistic Regression on scaled_score\nOLS Linear Regression: bimodal distribution at tails makes the mean a poor predictor; LMs violate assumptions of normally distributed residuals; both absolute and scaled scores yield non-normal residuals; no transformation of the outcome variables yield normal residuals\n\n\n\nNotes\nAlso exploring:\n\nHurdle model (mixture model w/ binomial + [poisson or negbinom count; 0s from 1 DGP)\nZero Inflated model (mixture model w/ binomial + poisson or negbinom count; 0s from 2 DGPs)\nBeta regression hurdle model? (mixture with location and scale parameters [mean, variance] and hurdles for floor and ceiling effects)\nOther way to account for the severe bimodality?\n\n\n\n\n\nSetup\n\n\nCODE\ndf_i = df_items %>% filter(q %nin% c(6,9)) %>% \n  dplyr::select(pretty_condition, accuracy, subject,q, ospan_split)\n\ndf_s <- df_subjects %>% \n  dplyr::select(pretty_condition, ospan_split, task_percent)\n\n\n\n\nDescribe\n\n\nCODE\n#:::::::: STACKED PROPORTIONAL BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~ospan_split) + \n   labs(title = \"Overall Accuracy\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses for HIGH WM\")\n\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART BY QUESTION\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap( q ~ ospan_split) +\n   labs(title = \"Accuracy by Question\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Q6 and Q9 are non-discriminative\")\n\n\n\n\n\nCODE\n#:::::::: FACETED HISTOGRAM\nstats = df_s %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(task_percent))\ngf_props(~task_percent,\n         fill = ~pretty_condition, data = df_s) %>%\n  gf_facet_grid(pretty_condition ~ ospan_split) %>%\n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"% Correct\",\n       y = \"proportion of subjects\",\n       title = \"Overall Absolute Score (% Correct)\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total % Correct)\"\ntbl1 <- mosaic::favstats(~task_percent, data = df_s) \ntbl1 %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total % Correct)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    0 \n    0 \n    0 \n    0.462 \n    1 \n    0.248 \n    0.368 \n    133 \n    0 \n  \n\n\n\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION\"\ntbl2 <- mosaic::favstats(task_percent ~ pretty_condition, data = df_s) \ntbl2 %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION\n \n  \n    pretty_condition \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    control \n    0 \n    0 \n    0.000 \n    0.077 \n    1 \n    0.15 \n    0.301 \n    65 \n    0 \n  \n  \n    impasse \n    0 \n    0 \n    0.077 \n    0.769 \n    1 \n    0.34 \n    0.402 \n    68 \n    0 \n  \n\n\n\n\n\nAcross both conditions, overall accuracy on the task ranges from 0 to 100 with a mean of 24.754. We see that the distribution of this outcome variable is clearly bimodal: with modes near the floor (0% correct) and ceiling (100% correct) of the scale. This bimodality is sensical considering the nature of the task, where each item in the task indexes a different information extraction operation over the same coordinate system.\nA score of 100% indicates that the participant correctly interpreted the interval-coordinate system throughout the task, starting at the first question. A score of 0% indicates the individual never correctly interpreted the coordinate system. A score somewhere inbetween indicates that an individual deciphered the coordinate system sometime over the course the task.\n\n\nWILCOXON RANK SUM (Mann-Whitney Test)\n\nNon parametric alternative to t-test; compares median rather than mean by ranking data\nDoes not assume normality\nDoes not assume equal variance of samples (homogeneity of variance)\n\n\nTest\n\n\nCODE\n#WILCOXON ON ACCURACY X OSPAN-SPLIT in LOW WORKING MEMORY\ndf_low <- df_s %>% filter(ospan_split == \"low-memory\")\n(w <- wilcox.test(df_low$task_percent ~ df_low$pretty_condition,\n                 paired = FALSE, alternative = \"less\")) #less, greater\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df_low$task_percent by df_low$pretty_condition\nW = 465, p-value = 0.1\nalternative hypothesis: true location shift is less than 0\n\n\nCODE\n#WILCOXON ON ACCURACY X OSPAN-SPLIT in HIGH\ndf_high <- df_s %>% filter(ospan_split == \"high-memory\")\n(w <- wilcox.test(df_high$task_percent ~ df_high$pretty_condition,\n                 paired = FALSE, alternative = \"less\")) #less, greater\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df_high$task_percent by df_high$pretty_condition\nW = 292, p-value = 0.0003\nalternative hypothesis: true location shift is less than 0\n\n\nCODE\ngrouped_ggbetweenstats( data = df_s, type = \"nonparametric\",\n                        y = task_percent, x = pretty_condition, grouping.var = ospan_split)\n\n\n\n\n\n\n\nInference\nA Wilcoxon-Rank sum test on task accuracy x condition for the low-memory participants indicate that impasse is not significantly higher. A Wilcoxon rank sum test on task accuracy x condition for high-memory participants indicate that impasse IS higher. Taken together, this indiates there may be an interaction between working memory and condition.\n\n\nVisualize\n\n\nCODE\n#TODO   \ngrouped_ggbetweenstats(data = df_s,\n                       y = task_percent, x = pretty_condition, grouping.var = ospan_split,\n               plot.type = \"box\", type = \"nonparametric\", var.equal = FALSE,\n               centrality.type = \"parametric\",\n               package = \"RColorBrewer\",\n               palette = \"PRGn\",\n               centrality.point.args = list(color=\"black\", size = 3, shape = 1),\n               # point.args = list(alpha=0), #suppress points\n               ggplot.component = ## modify further with `{ggplot2}` functions\n                list(\n                  aes(color = pretty_condition, fill = pretty_condition),\n                  scale_colour_manual(values = paletteer::paletteer_c(\"viridis::viridis\", 3)),\n                  scale_fill_manual(values = paletteer::paletteer_c(\"viridis::viridis\", 3))\n                  # theme(axis.text.x = element_text(angle = 90)\n                )) \n\n\nScale for 'colour' is already present. Adding another scale for 'colour',\nwhich will replace the existing scale.\nScale for 'colour' is already present. Adding another scale for 'colour',\nwhich will replace the existing scale.\n\n\n\n\n\nCODE\n# # ggplot(data = df_s, aes( x = pretty_condition, y = task_percent)) + \n#   ggdist::stat_halfeye(\n#     alpha = 0.7,\n#     point_colour = NA,\n#     adjust = .5,\n#     width = .5, .width = 0,\n#     justification = -.5) +\n#   geom_boxplot(\n#     alpha = 0.1,\n#     width = .2,\n#     outlier.shape = NA\n#   ) +\n#   geom_point(\n#     size = 2,\n#     alpha = .5,\n#     position = position_jitter(\n#       seed = 1, width = .05, height = .02\n#     )\n#   ) \n# coord_flip() + theme_clean() + theme(legend.position = \"blank\")\n# p$layers[[3]]=NULL #remove default boxplot\n# e <- statsExpressions::two_sample_test(y = task_percent, x = pretty_condition, data = df_s,\n#                                 type = \"nonparametric\", alternative = \"less\",\n#                                 var.equal = FALSE)\n# #labels are layer 4\n# p + labs(title = \"Distribution of Total Accuracy\",\n#          y = \"Proportion of correct responses across task\", x = \"\",\n#          subtitle = \"Impasse condition yields higher scores and greater variance\",\n#          caption=e$expression[[1]])\n\n\n\n\n\nMIXED LOGISTIC REGRESSION [IXN!!]\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.\n\nFit Model\n\n\nCODE\n## 0 | SETUP\n#confirm 13 items [all discriminating items]\nnrow(df_i) / nrow(df_s) == 13\n\n\n[1] TRUE\n\n\nCODE\n#confirm all factors \nis.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$pretty_condition) && is.factor(df_i$accuracy) && is.factor(df_i$ospan_split)\n\n\n[1] TRUE\n\n\nCODE\n## 1 | SETUP RANDOM INTERCEPT SUBJECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nprint(\"Empty fixed model\")\n\n\n[1] \"Empty fixed model\"\n\n\nCODE\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df_i) \n# summary(m0)\n\n#:: RANDOM INTERCEPT SUBJECT\nprint(\"Subject intercept random model\")\n\n\n[1] \"Subject intercept random model\"\n\n\nCODE\nmm.rS <- glmer(accuracy ~ (1|subject), data = df_i, family = \"binomial\")\n# summary(mm.rS)\n\n# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", m0$aic > AIC(logLik(mm.rS)))\n\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\n\nCODE\ntest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName  |    Model | df | df_diff |   Chi2 |      p\n-------------------------------------------------\nm0    |      glm |  1 |         |        |       \nmm.rS | glmerMod |  2 |       1 | 996.89 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  8.51171579642736e-219\"\n\n\nCODE\n#:: RANDOM INTERCEPT SUBJECT + ITEM\nprint(\"Subject Intercept + Item intercept random model\")\n\n\n[1] \"Subject Intercept + Item intercept random model\"\n\n\nCODE\nmm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = \"binomial\")\n#summary(mm.rSQ)\n\n# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)))\n\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\n\nCODE\ntest_lrt(mm.rS, mm.rSQ) #same as anova(m0, m1, test = \"Chi\")\n\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |      p\n-------------------------------------------------\nmm.rS  | glmerMod |  2 |         |       |       \nmm.rSQ | glmerMod |  3 |       1 | 68.40 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS, mm.rSQ))$p[2])\n\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\n\n[1] \"Likelihood Ratio test is significant? p =  1.33590849571387e-16\"\n\n\nCODE\n## 2 | ADD FIXED EFFECT CONDITION\n\nprint(\"FIXED Condition + Subject & Item random intercepts\")\n\n\n[1] \"FIXED Condition + Subject & Item random intercepts\"\n\n\nCODE\nmm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q) ,\n                data = df_i, family = \"binomial\")\n#summary(mm.CrSQ)\n\npaste(\"AIC decreases w/ new model\", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.CrSQ)) )\n\n\n[1] \"AIC decreases w/ new model TRUE\"\n\n\nCODE\ntest_lrt(mm.rSQ,mm.CrSQ) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName    |    Model | df | df_diff |  Chi2 |      p\n--------------------------------------------------\nmm.rSQ  | glmerMod |  3 |         |       |       \nmm.CrSQ | glmerMod |  4 |       1 | 17.84 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rSQ,mm.CrSQ))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.0000240599850715097\"\n\n\nCODE\n## 3 | ADD INTERACTION OSPAN\n\nprint(\"FIXED Condition * FIXED OSPAN + Subject & Item random intercepts\")\n\n\n[1] \"FIXED Condition * FIXED OSPAN + Subject & Item random intercepts\"\n\n\nCODE\nmm.COrSQ <- glmer(accuracy ~ pretty_condition *ospan_split + (1|subject) + (1|q) ,\n                data = df_i, family = \"binomial\")\nsummary(mm.COrSQ)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition * ospan_split + (1 | subject) + (1 |  \n    q)\n   Data: df_i\n\n     AIC      BIC   logLik deviance df.resid \n     856      889     -422      844     1723 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-6.035 -0.109 -0.026 -0.007 11.450 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 34.85    5.90    \n q       (Intercept)  1.14    1.07    \nNumber of obs: 1729, groups:  subject, 133; q, 13\n\nFixed effects:\n                                               Estimate Std. Error z value\n(Intercept)                                      -7.200      1.474   -4.88\npretty_conditionimpasse                           2.119      1.648    1.29\nospan_splithigh-memory                           -0.845      1.412   -0.60\npretty_conditionimpasse:ospan_splithigh-memory    5.585      2.387    2.34\n                                               Pr(>|z|)    \n(Intercept)                                       1e-06 ***\npretty_conditionimpasse                           0.199    \nospan_splithigh-memory                            0.550    \npretty_conditionimpasse:ospan_splithigh-memory    0.019 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) prtty_ ospn_-\nprtty_cndtn -0.495              \nospn_splth- -0.612  0.513       \nprtty_cn:_-  0.112 -0.691 -0.566\n\n\nCODE\ncar::Anova(mm.COrSQ)\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: accuracy\n                             Chisq Df Pr(>Chisq)    \npretty_condition             16.12  1      6e-05 ***\nospan_split                   0.78  1      0.378    \npretty_condition:ospan_split  5.47  1      0.019 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"AIC decreases w/ new model\", AIC(logLik(mm.CrSQ)) > AIC(logLik(mm.COrSQ)) )\n\n\n[1] \"AIC decreases w/ new model TRUE\"\n\n\nCODE\ntest_lrt(mm.CrSQ,mm.COrSQ) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName     |    Model | df | df_diff | Chi2 |     p\n-------------------------------------------------\nmm.CrSQ  | glmerMod |  4 |         |      |      \nmm.COrSQ | glmerMod |  6 |       2 | 7.84 | 0.020\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.CrSQ,mm.COrSQ))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.0198769795815358\"\n\n\nA likelihood ratio test indicates adding OSPAN as a fixed effect to a logistic regression model including a fixed effect for CONDITION and random intercepts for SUBJECT and QUESTION explains more variance in the data than the CONDITION + random-effects only model.\n\n\nDescribe\n\n\nCODE\n# best model\nm <- mm.COrSQ\n\n#::::::::: PRINT MODEL \n\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition * ospan_split + (1 | subject) + (1 |  \n    q)\n   Data: df_i\n\n     AIC      BIC   logLik deviance df.resid \n     856      889     -422      844     1723 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-6.035 -0.109 -0.026 -0.007 11.450 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 34.85    5.90    \n q       (Intercept)  1.14    1.07    \nNumber of obs: 1729, groups:  subject, 133; q, 13\n\nFixed effects:\n                                               Estimate Std. Error z value\n(Intercept)                                      -7.200      1.474   -4.88\npretty_conditionimpasse                           2.119      1.648    1.29\nospan_splithigh-memory                           -0.845      1.412   -0.60\npretty_conditionimpasse:ospan_splithigh-memory    5.585      2.387    2.34\n                                               Pr(>|z|)    \n(Intercept)                                       1e-06 ***\npretty_conditionimpasse                           0.199    \nospan_splithigh-memory                            0.550    \npretty_conditionimpasse:ospan_splithigh-memory    0.019 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) prtty_ ospn_-\nprtty_cndtn -0.495              \nospn_splth- -0.612  0.513       \nprtty_cn:_-  0.112 -0.691 -0.566\n\n\nCODE\nprint(\"SIGNIFICANCE TEST [non directional]\")\n\n\n[1] \"SIGNIFICANCE TEST [non directional]\"\n\n\nCODE\ncar::Anova(m, type=3) #TYPE 3 SS FOR IXNS\n\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: accuracy\n                             Chisq Df Pr(>Chisq)    \n(Intercept)                  23.85  1      1e-06 ***\npretty_condition              1.65  1      0.199    \nospan_split                   0.36  1      0.550    \npretty_condition:ospan_split  5.47  1      0.019 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n#:::::::: MANUAL ONE-SIDED SIGTEST \n#note: anova and chi square are always one-tailed, but that is independent of being one-sided\n#https://www.ibm.com/support/pages/can-one-get-one-tailed-tests-logistic-regression-dividing-significance-levels-half\n\n# one-sided (right tail) z test for B COEFFICIENT\n#SANITY CHECK 2-tailed test should match the model output\n#NOTE ... NEED TO DO THIS FOR _EACH_ COEFFICIENT\n# tt <- 2*pnorm(summary(m)$coefficients[2,3], lower.tail = F)\n# paste(\"p value for two-tailed test, null B = 0 : \",round(tt,5))\n# ot <- pnorm(summary(m)$coefficients[2,3], lower.tail = F)\n# paste(\"BUT we want a one  directional, null: B <= 0: \",round(ot,5))\n\n#:::::::: INTERPRET COEFFICIENTS\n\nse <- sqrt(diag(stats::vcov(m)))\n# table of estimates with 95% CI\npaste(\"LOG ODDS\")\n\n\n[1] \"LOG ODDS\"\n\n\nCODE\n(tab <- cbind(Est = fixef(m), LL = fixef(m) - 1.96 * se, UL = fixef(m) + 1.96 *\n    se))\n\n\n                                                  Est      LL    UL\n(Intercept)                                    -7.200 -10.090 -4.31\npretty_conditionimpasse                         2.119  -1.111  5.35\nospan_splithigh-memory                         -0.845  -3.613  1.92\npretty_conditionimpasse:ospan_splithigh-memory  5.585   0.907 10.26\n\n\nCODE\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\n(e <- exp(tab))\n\n\n                                                    Est        LL       UL\n(Intercept)                                    7.47e-04 0.0000415 1.34e-02\npretty_conditionimpasse                        8.32e+00 0.3290805 2.10e+02\nospan_splithigh-memory                         4.30e-01 0.0269834 6.84e+00\npretty_conditionimpasse:ospan_splithigh-memory 2.66e+02 2.4758207 2.87e+04\n\n\n\n\nInference\n(TODO Dissertation)\nTo quantify the effect of CONDITION and OSPAN on ACCURACY, we fit a mixed-effect binomial logistic regression model with random intercepts for subjects and questions. The structure of this model allows us to differentiate between random variance introduced by individual subjects and questions, versus the expected systematic variance of CONDITION and OSPAN. A likelihood ratio test indicates that a model including a fixed effect of CONDITION is explains significantly more variance in the data than an intercepts-only baseline model (\\(\\chi^2 (3,4) = 17.84, p < 0.001\\)). The final model including main effect and interaction term of OSPAN is a significantly better fit than the CONDITION-only model (\\(\\chi^2 (4,6) = 8.84, p < 0.05\\)).\nThe explanatory power of the entire model is substantial (\\(conditional \\ R^2 = 0/93\\)) and the part related to the fixed effects CONDITION and OSPAN (\\(marginal \\ R^2\\)) explains 18% of variance. There were no significant main effects of condition or impasse, but a rather a significant interaction between condition and ospan, ($e^{_1} = 266.00 p < 0.05  95 %   CI [2.48, 28700] $)).\nTODO INTERPRETING INTERACTIONS IN REGRESSION\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.01\n<0.001\n\n\npretty condition[impasse]\n8.32\n0.33 – 210.45\n0.199\n\n\nospan split [high-memory]\n0.43\n0.03 – 6.84\n0.550\n\n\npretty condition[impasse] * ospan split[high-memory]\n266.48\n2.48 – 28680.38\n0.019\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 subject\n34.85\n\n\nτ00 q\n1.14\n\n\nICC\n0.92\n\n\nN subject\n133\n\n\nN q\n13\n\nObservations\n1729\n\n\nMarginal R2 / Conditional R2\n0.181 / 0.931\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m1, \"(log odds)\" = m1)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', \n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n# \n# \n# \n\n\n\n\nVisualize\n\n\nCODE\n## | PLOT PARAMETERS \n\n#SJPLOT | MODEL | ODDS RATIO\nplot_model(m, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) + #manually adjusted for directional test   \n  labs(title = \"Model Estimate | Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#EASYSTATS | MODEL | ODDS RATIO\n# result <- model_parameters(m, exponentiate = TRUE, component = \"all\")\n# plot(result)\n\n\n## | PLOT TESTS\n\nresult <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\nplot(result)\n\n\n\n\n\nCODE\n## | PLOT TESTS\nresult <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\nplot(result)\n\n\n\n\n\nCODE\n#ONLY FOR BAYESIAN VERSION\n# result <- rope(m)\n# plot(result)\n# \n# result <- pd(m)\n# plot(result)\n\n\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type=\"int\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Prediction | Probability of Accurate Response\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\n\n\n\nCODE\n#PLOT MODEL PREDICTION\nplot_model(m, type = \"pred\")  \n\n\n$pretty_condition\n\n\n\n\n\n\n$ospan_split\n\n\n\n\n\nCODE\nplot_model(m, type = \"eff\")  \n\n\n$pretty_condition\n\n\n\n\n\n\n$ospan_split\n\n\n\n\n\nCODE\n  # ylim(0,1) + \n  # labs(\n  #   title = \"Model Prediction | Probability of Accurate Response\",\n  #   subtitle = \"Impasse increases Probability of Correct Response\"\n  # )\n\n#TODO EMMEANS for the estimated marginal means OR USE IXN PLOT\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.01\n<0.001\n\n\npretty condition[impasse]\n8.32\n0.33 – 210.45\n0.199\n\n\nospan split [high-memory]\n0.43\n0.03 – 6.84\n0.550\n\n\npretty condition[impasse] * ospan split[high-memory]\n266.48\n2.48 – 28680.38\n0.019\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 subject\n34.85\n\n\nτ00 q\n1.14\n\n\nICC\n0.92\n\n\nN subject\n133\n\n\nN q\n13\n\nObservations\n1729\n\n\nMarginal R2 / Conditional R2\n0.181 / 0.931\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m, \"(log odds)\" = m)\n# notes = list('\"* p < 0.05, ** p < 0.01, *** p < 0.001\"',\n#                'N(subject) = 133 $\\tau_{00}$(subject) = 34.85',\n#              'N(question) = 13 $\\tau_{00}$(question) = 1.14')\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',\n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_online.tex\")\n# # #              # coef_omit = \"Intercept\",\n\n\n\n\nDiagnostics\n\n\nCODE\n# print(\"SANITY CHECK REPORTING\")\n# report(m)\n\n# print(\"MODEL PERFORMANCE\")\n# performance(m)\n\nprint(\"DIAGNOSTICS\")\n\n\n[1] \"DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m)"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#h1a-overall-interpretation-state",
    "href": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#h1a-overall-interpretation-state",
    "title": "8  (Online) Hypothesis Testing",
    "section": "H1A | OVERALL INTERPRETATION STATE",
    "text": "H1A | OVERALL INTERPRETATION STATE\nDo Ss in the IMPASSE condition offer less-orthogonal interpretations across the test phase questions?\nWhile absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn’t allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject’s response, and determine if these interpretations differ by experimental condition. State is a 3-category variable that groups the following interpretations:\n\n“orthogonal” [reference category] includes orthogonal and satisficing responses ==> indicates a primarily orthogonal state of coordinate system understanding\n“other” includes: blank, reference point, responses that can’t be classified (including selecting all datapoints), => indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly not orthogonal nor triangular\n“angular” includes ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n“triangular” includes correct triangular and ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n\n\n\n\n\n\n\n\nResearch Question\nDoes Ss in the impasse condition produce less orthogonal responses across questions?\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of producing unknown and triangle-like response states across all items\n\n\nData\n\ndata: df_items where q nin 6,9 (13 discriminant test phase items)\noutcome: state ( 3 level factor from high_interpretation )\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMIXED Multinomial (Logistic) Regression on state predicted by condition\n\nAlternative:\n\nMIXED Ordinal regression on state (doesn’t meet proportional odds assumption-I think)\nMIXED Multinomial or Ordinal regression on high_interpretation (some cells are 0, produces problems)\n\n\n\n\n\nSetup\n\n\nCODE\n#:::::::: PREP DATA\ndf_i = df_items %>% filter(q %nin% c(6,9)) %>% dplyr::select(q,subject,state,pretty_condition, ospan_split)\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~ospan_split) +\n   labs(title = \"Interpretation across all Questions\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(q ~ ospan_split) +\n   labs(title = \"Interpretation by Question\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\ntable(df_i$state, df_i$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n            \n             control impasse    Sum\n  orthogonal  0.6710  0.3235 0.4933\n  other       0.1396  0.2636 0.2030\n  tri-like    0.0260  0.0509 0.0388\n  triangular  0.1633  0.3620 0.2649\n  Sum         1.0000  1.0000 1.0000\n\n\nCODE\n(t <- table(df_i$state, df_i$pretty_condition, df_i$ospan_split) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n, ,  = low-memory\n\n            \n             control impasse Sum\n  orthogonal     269     173 442\n  other           61     147 208\n  tri-like        10      30  40\n  triangular      63     118 181\n  Sum            403     468 871\n\n, ,  = high-memory\n\n            \n             control impasse Sum\n  orthogonal     298     113 411\n  other           57      86 143\n  tri-like        12      15  27\n  triangular      75     202 277\n  Sum            442     416 858\n\n\n\n\nMIXED MULTINOMIAL REGRESSION\nDoes condition affect the response state of of items across the task?\nFit a MIXED logistic regression predicting interpretation state (k=3) by condition(k = 2).\n\nmblogit version wouldn’t coverge, so using brms\n\n\nFit Model [brms]\n\n\nCODE\n# CONDITION ONLY MODEL\nmm.cat.CrSQ <- brm( state ~ pretty_condition + (1|subject) + (1|q), \n                 data = df_i, \n                 family = \"categorical\",\n                 chains = 4, iter = 2000, warmup = 1000,\n                 cores = 4, seed = 1234,\n                 backend = \"cmdstanr\",\n                 file =\"analysis/SGC3A/models/sgc3a_brms_mixedcat.condition._state_ONLINE.rds\")\n\nsummary(mm.cat.CrSQ)\n\n\n Family: categorical \n  Links: muother = logit; mutrilike = logit; mutriangular = logit \nFormula: state ~ pretty_condition + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 1729) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          1.48      0.37     0.92     2.31 1.00      982\nsd(mutrilike_Intercept)        2.43      0.78     1.35     4.36 1.00     1104\nsd(mutriangular_Intercept)     1.46      0.36     0.91     2.34 1.00     1334\n                           Tail_ESS\nsd(muother_Intercept)          1577\nsd(mutrilike_Intercept)        1718\nsd(mutriangular_Intercept)     2217\n\n~subject (Number of levels: 133) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          0.91      0.13     0.66     1.19 1.00     1572\nsd(mutrilike_Intercept)        1.79      0.31     1.23     2.46 1.00     1259\nsd(mutriangular_Intercept)     5.21      0.61     4.17     6.52 1.00      903\n                           Tail_ESS\nsd(muother_Intercept)          2399\nsd(mutrilike_Intercept)        1941\nsd(mutriangular_Intercept)     1805\n\nPopulation-Level Effects: \n                                     Estimate Est.Error l-95% CI u-95% CI Rhat\nmuother_Intercept                       -2.09      0.43    -2.97    -1.27 1.02\nmutrilike_Intercept                     -5.27      0.91    -7.13    -3.61 1.00\nmutriangular_Intercept                  -4.90      0.96    -6.90    -3.13 1.01\nmuother_pretty_conditionimpasse          1.78      0.24     1.32     2.28 1.00\nmutrilike_pretty_conditionimpasse        2.25      0.52     1.25     3.29 1.00\nmutriangular_pretty_conditionimpasse     4.16      1.06     2.23     6.40 1.00\n                                     Bulk_ESS Tail_ESS\nmuother_Intercept                         510      585\nmutrilike_Intercept                       880     1107\nmutriangular_Intercept                    551     1089\nmuother_pretty_conditionimpasse          1930     2330\nmutrilike_pretty_conditionimpasse        1655     2106\nmutriangular_pretty_conditionimpasse      498     1109\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n# CONDITION + OSPAN MODEL\nmm.cat.COrSQ <- brm( state ~ pretty_condition*ospan_split + (1|subject) + (1|q), \n                 data = df_i, \n                 family = \"categorical\",\n                 chains = 4, iter = 2000, warmup = 1000,\n                 cores = 4, seed = 1234,\n                 backend = \"cmdstanr\",\n                 file =\"analysis/SGC3A/models/sgc3a_brms_mixedcat.conditionospan._state_ONLINE.rds\")\nsummary(mm.cat.COrSQ)\n\n\n Family: categorical \n  Links: muother = logit; mutrilike = logit; mutriangular = logit \nFormula: state ~ pretty_condition * ospan_split + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 1729) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          1.51      0.38     0.94     2.41 1.00     1399\nsd(mutrilike_Intercept)        2.43      0.74     1.37     4.17 1.00     1613\nsd(mutriangular_Intercept)     1.46      0.37     0.91     2.33 1.00     1675\n                           Tail_ESS\nsd(muother_Intercept)          2299\nsd(mutrilike_Intercept)        2529\nsd(mutriangular_Intercept)     2488\n\n~subject (Number of levels: 133) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          0.94      0.13     0.69     1.22 1.00     1578\nsd(mutrilike_Intercept)        1.86      0.33     1.29     2.58 1.00     1502\nsd(mutriangular_Intercept)     5.18      0.63     4.12     6.57 1.00     1177\n                           Tail_ESS\nsd(muother_Intercept)          2466\nsd(mutrilike_Intercept)        2340\nsd(mutriangular_Intercept)     2176\n\nPopulation-Level Effects: \n                                                            Estimate Est.Error\nmuother_Intercept                                              -1.93      0.47\nmutrilike_Intercept                                            -5.59      1.04\nmutriangular_Intercept                                         -4.63      1.23\nmuother_pretty_conditionimpasse                                 1.76      0.32\nmuother_ospan_splithighMmemory                                 -0.23      0.34\nmuother_pretty_conditionimpasse:ospan_splithighMmemory          0.03      0.49\nmutrilike_pretty_conditionimpasse                               2.61      0.77\nmutrilike_ospan_splithighMmemory                                0.40      0.79\nmutrilike_pretty_conditionimpasse:ospan_splithighMmemory       -0.65      1.05\nmutriangular_pretty_conditionimpasse                            2.32      1.48\nmutriangular_ospan_splithighMmemory                            -0.36      1.55\nmutriangular_pretty_conditionimpasse:ospan_splithighMmemory     3.71      2.08\n                                                            l-95% CI u-95% CI\nmuother_Intercept                                              -2.86    -1.01\nmutrilike_Intercept                                            -7.73    -3.71\nmutriangular_Intercept                                         -7.20    -2.34\nmuother_pretty_conditionimpasse                                 1.12     2.40\nmuother_ospan_splithighMmemory                                 -0.93     0.42\nmuother_pretty_conditionimpasse:ospan_splithighMmemory         -0.91     1.00\nmutrilike_pretty_conditionimpasse                               1.21     4.23\nmutrilike_ospan_splithighMmemory                               -1.05     2.01\nmutrilike_pretty_conditionimpasse:ospan_splithighMmemory       -2.83     1.32\nmutriangular_pretty_conditionimpasse                           -0.48     5.23\nmutriangular_ospan_splithighMmemory                            -3.27     2.73\nmutriangular_pretty_conditionimpasse:ospan_splithighMmemory    -0.38     7.76\n                                                            Rhat Bulk_ESS\nmuother_Intercept                                           1.00     1036\nmutrilike_Intercept                                         1.00     1293\nmutriangular_Intercept                                      1.00      894\nmuother_pretty_conditionimpasse                             1.00     2125\nmuother_ospan_splithighMmemory                              1.00     1940\nmuother_pretty_conditionimpasse:ospan_splithighMmemory      1.00     2133\nmutrilike_pretty_conditionimpasse                           1.00     2096\nmutrilike_ospan_splithighMmemory                            1.00     2042\nmutrilike_pretty_conditionimpasse:ospan_splithighMmemory    1.00     1997\nmutriangular_pretty_conditionimpasse                        1.00      752\nmutriangular_ospan_splithighMmemory                         1.01      580\nmutriangular_pretty_conditionimpasse:ospan_splithighMmemory 1.00      668\n                                                            Tail_ESS\nmuother_Intercept                                               1871\nmutrilike_Intercept                                             2118\nmutriangular_Intercept                                          1609\nmuother_pretty_conditionimpasse                                 2283\nmuother_ospan_splithighMmemory                                  2385\nmuother_pretty_conditionimpasse:ospan_splithighMmemory          2540\nmutrilike_pretty_conditionimpasse                               2429\nmutrilike_ospan_splithighMmemory                                2250\nmutrilike_pretty_conditionimpasse:ospan_splithighMmemory        2359\nmutriangular_pretty_conditionimpasse                            1484\nmutriangular_ospan_splithighMmemory                              881\nmutriangular_pretty_conditionimpasse:ospan_splithighMmemory     1053\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n#which model is better?\ncompare_models(mm.cat.CrSQ, mm.cat.COrSQ)\n\n\nPossible multicollinearity between b_mutriangular_pretty_conditionimpasse and b_mutriangular_Intercept (r = 0.72). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nPossible multicollinearity between b_muother_pretty_conditionimpasse:ospan_splithighMmemory and b_muother_ospan_splithighMmemory (r = 0.71), b_mutrilike_pretty_conditionimpasse:ospan_splithighMmemory and b_mutrilike_ospan_splithighMmemory (r = 0.75), b_mutriangular_pretty_conditionimpasse:ospan_splithighMmemory and b_mutriangular_ospan_splithighMmemory (r = 0.73). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nParameter                                                   |          mm.cat.CrSQ |         mm.cat.COrSQ\n---------------------------------------------------------------------------------------------------------\nmuother_Intercept                                           | -2.09 (-2.97, -1.27) | -1.92 (-2.86, -1.01)\nmutrilike_Intercept                                         | -5.24 (-7.13, -3.61) | -5.53 (-7.73, -3.71)\nmutriangular_Intercept                                      | -4.82 (-6.90, -3.13) | -4.58 (-7.20, -2.34)\nmuother_pretty_conditionimpasse                             |  1.78 ( 1.32,  2.28) |  1.76 ( 1.12,  2.40)\nmutrilike_pretty_conditionimpasse                           |  2.24 ( 1.25,  3.29) |  2.56 ( 1.21,  4.23)\nmutriangular_pretty_conditionimpasse                        |  4.10 ( 2.23,  6.40) |  2.30 (-0.48,  5.23)\nmuother_ospan_splithighMmemory                              |                      | -0.23 (-0.93,  0.42)\nmuother_pretty_conditionimpasse:ospan_splithighMmemory      |                      |  0.02 (-0.91,  1.00)\nmutriangular_ospan_splithighMmemory                         |                      | -0.40 (-3.27,  2.73)\nmutrilike_ospan_splithighMmemory                            |                      |  0.38 (-1.05,  2.01)\nmutrilike_pretty_conditionimpasse:ospan_splithighMmemory    |                      | -0.61 (-2.83,  1.32)\nmutriangular_pretty_conditionimpasse:ospan_splithighMmemory |                      |  3.69 (-0.38,  7.76)\n---------------------------------------------------------------------------------------------------------\nObservations                                                |                 1729 |                 1729\n\n\nCODE\ncompare_performance(mm.cat.CrSQ, mm.cat.COrSQ)\n\n\n'bayes_R2' is not defined for unordered categorical models.\n'bayes_R2' is not defined for unordered categorical models.\n\n\n# Comparison of Model Performance Indices\n\nName         |   Model |      ELPD | ELPD_SE |    LOOIC | LOOIC weights | LOOIC_SE |     WAIC | WAIC weights | Sigma\n--------------------------------------------------------------------------------------------------------------------\nmm.cat.CrSQ  | brmsfit | -1119.416 |  35.865 | 2238.832 |         1.000 |   71.731 | 2215.126 |        0.751 | 1.000\nmm.cat.COrSQ | brmsfit | -1121.594 |  36.122 | 2243.188 |       < 0.001 |   72.243 | 2217.331 |        0.249 | 1.000\n\n\nCODE\n# test_lrt(mm.cat.CrSQ, mm.cat.COrSQ) #doesn't run?\n# car::Anova(mm.cat.CrSQ, mm.cat.COrSQ)\n# car::Anova(mm.cat.COrSQ)\n\n\n\n\nDescribe\n\n\nCODE\nplot(pd(mm.cat.CrSQ))\n\n\n\n\n\nCODE\nplot(pd(mm.cat.COrSQ))\n\n\n\n\n\n\n\nCODE\n#set model \nm <- mm.cat.COrSQ\nsummary(m)\n\n\n Family: categorical \n  Links: muother = logit; mutrilike = logit; mutriangular = logit \nFormula: state ~ pretty_condition * ospan_split + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 1729) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          1.51      0.38     0.94     2.41 1.00     1399\nsd(mutrilike_Intercept)        2.43      0.74     1.37     4.17 1.00     1613\nsd(mutriangular_Intercept)     1.46      0.37     0.91     2.33 1.00     1675\n                           Tail_ESS\nsd(muother_Intercept)          2299\nsd(mutrilike_Intercept)        2529\nsd(mutriangular_Intercept)     2488\n\n~subject (Number of levels: 133) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          0.94      0.13     0.69     1.22 1.00     1578\nsd(mutrilike_Intercept)        1.86      0.33     1.29     2.58 1.00     1502\nsd(mutriangular_Intercept)     5.18      0.63     4.12     6.57 1.00     1177\n                           Tail_ESS\nsd(muother_Intercept)          2466\nsd(mutrilike_Intercept)        2340\nsd(mutriangular_Intercept)     2176\n\nPopulation-Level Effects: \n                                                            Estimate Est.Error\nmuother_Intercept                                              -1.93      0.47\nmutrilike_Intercept                                            -5.59      1.04\nmutriangular_Intercept                                         -4.63      1.23\nmuother_pretty_conditionimpasse                                 1.76      0.32\nmuother_ospan_splithighMmemory                                 -0.23      0.34\nmuother_pretty_conditionimpasse:ospan_splithighMmemory          0.03      0.49\nmutrilike_pretty_conditionimpasse                               2.61      0.77\nmutrilike_ospan_splithighMmemory                                0.40      0.79\nmutrilike_pretty_conditionimpasse:ospan_splithighMmemory       -0.65      1.05\nmutriangular_pretty_conditionimpasse                            2.32      1.48\nmutriangular_ospan_splithighMmemory                            -0.36      1.55\nmutriangular_pretty_conditionimpasse:ospan_splithighMmemory     3.71      2.08\n                                                            l-95% CI u-95% CI\nmuother_Intercept                                              -2.86    -1.01\nmutrilike_Intercept                                            -7.73    -3.71\nmutriangular_Intercept                                         -7.20    -2.34\nmuother_pretty_conditionimpasse                                 1.12     2.40\nmuother_ospan_splithighMmemory                                 -0.93     0.42\nmuother_pretty_conditionimpasse:ospan_splithighMmemory         -0.91     1.00\nmutrilike_pretty_conditionimpasse                               1.21     4.23\nmutrilike_ospan_splithighMmemory                               -1.05     2.01\nmutrilike_pretty_conditionimpasse:ospan_splithighMmemory       -2.83     1.32\nmutriangular_pretty_conditionimpasse                           -0.48     5.23\nmutriangular_ospan_splithighMmemory                            -3.27     2.73\nmutriangular_pretty_conditionimpasse:ospan_splithighMmemory    -0.38     7.76\n                                                            Rhat Bulk_ESS\nmuother_Intercept                                           1.00     1036\nmutrilike_Intercept                                         1.00     1293\nmutriangular_Intercept                                      1.00      894\nmuother_pretty_conditionimpasse                             1.00     2125\nmuother_ospan_splithighMmemory                              1.00     1940\nmuother_pretty_conditionimpasse:ospan_splithighMmemory      1.00     2133\nmutrilike_pretty_conditionimpasse                           1.00     2096\nmutrilike_ospan_splithighMmemory                            1.00     2042\nmutrilike_pretty_conditionimpasse:ospan_splithighMmemory    1.00     1997\nmutriangular_pretty_conditionimpasse                        1.00      752\nmutriangular_ospan_splithighMmemory                         1.01      580\nmutriangular_pretty_conditionimpasse:ospan_splithighMmemory 1.00      668\n                                                            Tail_ESS\nmuother_Intercept                                               1871\nmutrilike_Intercept                                             2118\nmutriangular_Intercept                                          1609\nmuother_pretty_conditionimpasse                                 2283\nmuother_ospan_splithighMmemory                                  2385\nmuother_pretty_conditionimpasse:ospan_splithighMmemory          2540\nmutrilike_pretty_conditionimpasse                               2429\nmutrilike_ospan_splithighMmemory                                2250\nmutrilike_pretty_conditionimpasse:ospan_splithighMmemory        2359\nmutriangular_pretty_conditionimpasse                            1484\nmutriangular_ospan_splithighMmemory                              881\nmutriangular_pretty_conditionimpasse:ospan_splithighMmemory     1053\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n## | PLOT PARAMETERS \n\n#SJPLOT | MODEL | ODDS RATIO\nplot_model(m, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) + #manually adjusted for directional test   \n  labs(title = \"Model Estimate | Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\nWarning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Set1 is 9\nReturning the palette you asked for with that many colors\n\nWarning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Set1 is 9\nReturning the palette you asked for with that many colors\n\n\n\n\n\nCODE\n#EASYSTATS | MODEL | ODDS RATIO\n# result <- model_parameters(m, exponentiate = TRUE, component = \"all\")\n# plot(result)\n\n\n## | PLOT TESTS\n\nresult <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\n\n\nPossible multicollinearity between b_muother_pretty_conditionimpasse:ospan_splithighMmemory and b_muother_ospan_splithighMmemory (r = 0.71), b_mutrilike_pretty_conditionimpasse:ospan_splithighMmemory and b_mutrilike_ospan_splithighMmemory (r = 0.75), b_mutriangular_pretty_conditionimpasse:ospan_splithighMmemory and b_mutriangular_ospan_splithighMmemory (r = 0.73). This might lead to inappropriate results. See 'Details' in '?equivalence_test'.\n\n\nCODE\nplot(result)\n\n\nPicking joint bandwidth of 0.132\n\n\nWarning: Removed 4800 rows containing non-finite values (stat_density_ridges).\n\n\n\n\n\nCODE\nresult <- rope(m)\n\n\nPossible multicollinearity between b_muother_pretty_conditionimpasse:ospan_splithighMmemory and b_muother_ospan_splithighMmemory (r = 0.71), b_mutrilike_pretty_conditionimpasse:ospan_splithighMmemory and b_mutrilike_ospan_splithighMmemory (r = 0.75), b_mutriangular_pretty_conditionimpasse:ospan_splithighMmemory and b_mutriangular_ospan_splithighMmemory (r = 0.73). This might lead to inappropriate results. See 'Details' in '?rope'.\n\n\nCODE\nplot(result)\n\n\n\n\n\nCODE\nresult <- pd(m) \nplot(result)\n\n\n\n\n\nCODE\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type=\"int\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Prediction | Probability of Accurate Response\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\nNote: uncertainty of error terms are not taken into account. You may want to use `rstantools::posterior_predict()`.\n\n\n\n\n\nCODE\n#PLOT MODEL PREDICTION\n# plot_model(m, type = \"pred\")  \n# plot_model(m, type = \"eff\")  \n  # ylim(0,1) + \n  # labs(\n  #   title = \"Model Prediction | Probability of Accurate Response\",\n  #   subtitle = \"Impasse increases Probability of Correct Response\"\n  # )\n\n\n\n\nCODE\n#DISPLAY MODEL AS TABLE\ntab_model(mm.cat.CrSQ)\n\n\n'bayes_R2' is not defined for unordered categorical models.\n\n\n\n\n \nstate: other\nstate: trilike\nstate: triangular\nstate: other_pretty\nstate: trilike_pretty\nstate: triangular_pretty\n\n\nPredictors\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\nOdds Ratios\nCI (95%)\n\n\nIntercept\n0.12\n0.05 – 0.28\n\n\n0.01\n0.00 – 0.04\n\n\n0.01\n0.00 – 0.03\n\n\n\n\nconditionimpasse\n\n\n5.91\n3.75 – 9.76\n\n\n60.33\n9.34 – 600.68\n\n\n9.40\n3.48 – 26.78\n\n\nRandom Effects\n\n\n\nσ2\n0.64\n\n\n\nτ00\n0.95\n\n\nICC\n0.40\n\n\nN subject\n133\n\n\nN q\n13\n\nObservations\n1729\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m, \"(log odds)\" = m)\n# notes = list('\"* p < 0.05, ** p < 0.01, *** p < 0.001\"',\n#                'N(subject) = 133 $\\tau_{00}$(subject) = 34.85',\n#              'N(question) = 13 $\\tau_{00}$(question) = 1.14')\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)',\n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_online.tex\")\n# # #              # coef_omit = \"Intercept\",\n#TODO OUTPUT TABLE \n\n#https://arelbundock.com/posts/modelsummary_multinomial_logit/\n# modelsummary(m)"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#h1b-q1-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#h1b-q1-accuracy",
    "title": "8  (Online) Hypothesis Testing",
    "section": "H1B | Q1 ACCURACY",
    "text": "H1B | Q1 ACCURACY\nDo Ss in the IMPASSE condition have a higher likelihood of producing a correct response to the first question?\nThe graph comprehension task includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their first exposure to the unconventional triangular coordinate system.\n\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of correctly responding to the first question than those in the CONTROL condition\n\n\nData\n\ndata: df_items where q == 1\noutcome: accuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nLogistic Regression on accuracy predicted by condition\n\naccount for difference in odds of correct score by condition\n\n\nAlternatives:\n\nChi-Square test of independence on outcome accuracy by condition\n\n\n\nNotes\n\nCHIQ SQR is simplest method to examine independence of two categorical factors; LOGISTIC REGRESSION is recommended for binomial ~ continuous; though with regression we can quantify the size of the effect and overall model fit\nindependence assumption : (CHI SQR) as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\ncell frequency : (CHI SQR) expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\n\n\n\nSetup\n\n\nCODE\n#:::::::: PREP DATA\ndf <- df_items %>% filter(q==1) \n# %>% dplyr::select(accuracy, pretty_condition, ospan_split)\n\n\n\n\nDescribe\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~ospan_split) +\n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Q1 Accuracy\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nTODO DESCRIPTIONS\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\npaste(\"Proportions of Correct Responses by Condition\")\n\n\n[1] \"Proportions of Correct Responses by Condition\"\n\n\nCODE\ntable(df$accuracy, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            control impasse   Sum\n  incorrect   0.892   0.765 0.827\n  correct     0.108   0.235 0.173\n  Sum         1.000   1.000 1.000\n\n\nCODE\npaste(\"Number of Correct Responses by Condition\")\n\n\n[1] \"Number of Correct Responses by Condition\"\n\n\nCODE\ntable(df$accuracy, df$pretty_condition, df$ospan_split) %>% addmargins(2) %>% #display sum for row\n      addmargins(1) #sanity check sum of columns\n\n\n, ,  = low-memory\n\n           \n            control impasse Sum\n  incorrect      28      31  59\n  correct         3       5   8\n  Sum            31      36  67\n\n, ,  = high-memory\n\n           \n            control impasse Sum\n  incorrect      30      21  51\n  correct         4      11  15\n  Sum            34      32  66\n\n\n\n\nCHI SQUARE [YES]\n\n\nCODE\n#CHI SQUARE ON ACCURACY X OSPAN-SPLIT in LOW WORKING MEMORY\ndf_low <- df %>% filter(ospan_split == \"low-memory\")\n# table(df_low$pretty_condition, df_low$accuracy)\nchisq.test( x = df_low$pretty_condition, y = df_low$accuracy, correct = TRUE)\n\n\nWarning in chisq.test(x = df_low$pretty_condition, y = df_low$accuracy, : Chi-\nsquared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  df_low$pretty_condition and df_low$accuracy\nX-squared = 0.02, df = 1, p-value = 0.9\n\n\nCODE\n#CHI SQUARE ON ACCURACY X OSPAN-SPLIT in HIGH WORKING MEMORY\ndf_high <- df %>% filter(ospan_split == \"high-memory\")\n#table(df_high$pretty_condition, df_high$accuracy)\nchisq.test( x = df_high$pretty_condition, y = df_high$accuracy,correct = TRUE)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  df_high$pretty_condition and df_high$accuracy\nX-squared = 4, df = 1, p-value = 0.06\n\n\nCODE\n#significant if correct = FALSE\n\n\nTODO why do these chisqrs not match the grouped bar stats? ::: {.cell}\n\nCODE\n# INTERACTION (OSPAN X CONDITION)\ngrouped_ggbarstats( data = df, x = accuracy, y = pretty_condition, \n                    grouping.var = ospan_split,\n                    type = \"parametric\")\n\n\n\n\n\nCODE\n# MAIN EFFECT CONDITION (yes)\n# ggbarstats( data = df, x = accuracy, y = pretty_condition, \n#                     type = \"nonparametric\")\n\n# MAIN EFFECT OSPAN (none)\n# ggbarstats( data = df, x = accuracy, y = ospan_split, \n#                     type = \"nonparametric\")\n\n:::\nThere is no non-parametric version of two-way ANOVA, so we perform individual CHI-SQR tests. We split the data into two groups (low memory, and high memory, based on the median split). For each, we run a CHI SQR test of independence testing the null hypothesis that Q1 ACCURACY is independent of CONDITION. In the low-working memory group, we cannot reject the null hypothesis, suggesting that accuracy does not differ by condition. But in the HIGH working memory group we do reject the null hypothesis. The proportion of correct responses in IMPASSE is much higher than in CONTROL, but only in the HIGH WORKING MEMORY group.\n\n\nLOGISTIC REGRESSION (MAIN EFFECT CONDITION)\nTODO:: consider weighted(centered) continuous vs ospan split\nFit a logistic regression predicting accuracy (absolute score) (n = 133) by condition (k = 2).\n\n\nParameter estimate: \\(\\beta_{0}\\) = Log Odds of (correct) responses in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of correct response in CONTROL condition\nParameter estimate: \\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for correct response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of correct response in IMPASSE (vs) CONTROL\nNull hypothesis:\\(\\beta_{impasse} \\le 0\\) the odds for a correct response does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\gt 0\\) the odds of a correct response increases\n\n\nFit CONDITION Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\n\nCODE\n# MODEL FITTING ::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm.0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\n# print(\"EMPTY MODEL\")\n# summary(m0)\n\n#: 2 CONDITION model\nm.C <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\n# print(\"PREDICTOR MODEL\")\n# summary(m1)\n\n#: 2 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m.0$aic > m.C$aic)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(m.0,m.C) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff | Chi2 |     p\n------------------------------------------\nm.0  |   glm |  1 |         |      |      \nm.C  |   glm |  2 |       1 | 3.88 | 0.049\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m.0,m.C))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.0489409367734944\"\n\n\nCODE\nsummary(m.C)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.733  -0.733  -0.477  -0.477   2.111  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -2.115      0.400   -5.28  1.3e-07 ***\npretty_conditionimpasse    0.936      0.492    1.90    0.057 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 122.49  on 132  degrees of freedom\nResidual deviance: 118.62  on 131  degrees of freedom\nAIC: 122.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\n##### Fit OSPAN Models\n\n#: 3 OSPAN ONLY MODEL \nm.O = glm(accuracy ~ ospan_split, data = df, family = \"binomial\")\n# print(\"EMPTY MODEL\")\n# summary(m0)\n\n#: 3 TEST SUPERIOR FIT\npaste(\"AIC OSPAN predictor is lower than CONDITION model?\", m.C$aic > m.O$aic)\n\n\n[1] \"AIC OSPAN predictor is lower than CONDITION model? FALSE\"\n\n\nADD OSPAN ::: {.cell}\n\nCODE\n#: 4 OSPAN + CONDITION model\nm.CO <- glm( accuracy ~ pretty_condition + ospan_split, data = df, family = \"binomial\")\n# print(\"PREDICTOR MODEL\")\nsummary(m.CO)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition + ospan_split, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.870  -0.599  -0.559  -0.373   2.323  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -2.629      0.523   -5.03    5e-07 ***\npretty_conditionimpasse    1.002      0.499    2.01    0.045 *  \nospan_splithigh-memory     0.851      0.487    1.75    0.081 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 122.49  on 132  degrees of freedom\nResidual deviance: 115.42  on 130  degrees of freedom\nAIC: 121.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\ncar::Anova(m.CO, type=3)\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: accuracy\n                 LR Chisq Df Pr(>Chisq)  \npretty_condition     4.33  1      0.037 *\nospan_split          3.19  1      0.074 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n#: 4 TEST SUPERIOR FIT\npaste(\"AIC wth OSPAN is lower than CONDITION only model?\", m.C$aic > m.CO$aic)\n\n\n[1] \"AIC wth OSPAN is lower than CONDITION only model? TRUE\"\n\n\nCODE\ntest_lrt(m.C,m.CO) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff | Chi2 |     p\n------------------------------------------\nm.C  |   glm |  2 |         |      |      \nm.CO |   glm |  3 |       1 | 3.19 | 0.074\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m.C,m.CO))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.0739762476630023\"\n\n::: Adding OSPAN as a predictor (no interaction) decreases AIC, but does not improve fit (LRT)\nINTERACTION MODEL ::: {.cell}\n\nCODE\n#: 5 OSPAN + CONDITION INTERACTION model\nm.C.O <- glm( accuracy ~ pretty_condition * ospan_split, data = df, family = \"binomial\")\nx <- glm( accuracy ~ pretty_condition + ospan_split + pretty_condition*ospan_split, data = df, family = \"binomial\")\n# print(\"PREDICTOR MODEL\")\nsummary(m.C.O)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition * ospan_split, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.918  -0.547  -0.500  -0.451   2.161  \n\nCoefficients:\n                                               Estimate Std. Error z value\n(Intercept)                                      -2.234      0.607   -3.68\npretty_conditionimpasse                           0.409      0.775    0.53\nospan_splithigh-memory                            0.219      0.808    0.27\npretty_conditionimpasse:ospan_splithigh-memory    0.959      1.011    0.95\n                                               Pr(>|z|)    \n(Intercept)                                     0.00024 ***\npretty_conditionimpasse                         0.59782    \nospan_splithigh-memory                          0.78656    \npretty_conditionimpasse:ospan_splithigh-memory  0.34295    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 122.49  on 132  degrees of freedom\nResidual deviance: 114.54  on 129  degrees of freedom\nAIC: 122.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\ncar::Anova(m.C.O, type =3)\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: accuracy\n                             LR Chisq Df Pr(>Chisq)\npretty_condition                0.284  1       0.59\nospan_split                     0.074  1       0.79\npretty_condition:ospan_split    0.887  1       0.35\n\n\nCODE\n#: 5 TEST SUPERIOR FIT\npaste(\"AIC wth OSPAN IXN lower than CONDITION + OSPAN only model?\", m.CO$aic > m.C.O$aic)\n\n\n[1] \"AIC wth OSPAN IXN lower than CONDITION + OSPAN only model? FALSE\"\n\n\nCODE\ntest_lrt(m.CO,m.C.O) \n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName  | Model | df | df_diff | Chi2 |     p\n-------------------------------------------\nm.CO  |   glm |  3 |         |      |      \nm.C.O |   glm |  4 |       1 | 0.89 | 0.346\n\n\nCODE\npaste(\"AIC wth OSPAN IXN is lower than CONDITION only model?\", m.C$aic > m.C.O$aic)\n\n\n[1] \"AIC wth OSPAN IXN is lower than CONDITION only model? TRUE\"\n\n\nCODE\ntest_lrt(m.C,m.C.O) \n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName  | Model | df | df_diff | Chi2 |     p\n-------------------------------------------\nm.C   |   glm |  2 |         |      |      \nm.C.O |   glm |  4 |       2 | 4.08 | 0.130\n\n:::\nAdding OSPAN interaction does not improve model fit over condition-only model, or main effects only model.\n\n\nDescribe\n\n\nCODE\n#set model\nm <- m.C.O\n\n# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL [default two-tailed sig test]\")\n\n\n[1] \"PREDICTOR MODEL [default two-tailed sig test]\"\n\n\nCODE\nsummary(m)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition * ospan_split, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.918  -0.547  -0.500  -0.451   2.161  \n\nCoefficients:\n                                               Estimate Std. Error z value\n(Intercept)                                      -2.234      0.607   -3.68\npretty_conditionimpasse                           0.409      0.775    0.53\nospan_splithigh-memory                            0.219      0.808    0.27\npretty_conditionimpasse:ospan_splithigh-memory    0.959      1.011    0.95\n                                               Pr(>|z|)    \n(Intercept)                                     0.00024 ***\npretty_conditionimpasse                         0.59782    \nospan_splithigh-memory                          0.78656    \npretty_conditionimpasse:ospan_splithigh-memory  0.34295    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 122.49  on 132  degrees of freedom\nResidual deviance: 114.54  on 129  degrees of freedom\nAIC: 122.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\ncar::Anova(m, type=3)\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: accuracy\n                             LR Chisq Df Pr(>Chisq)\npretty_condition                0.284  1       0.59\nospan_split                     0.074  1       0.79\npretty_condition:ospan_split    0.887  1       0.35\n\n\nCODE\n# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: \n\n# one-sided (right tail) z test for B COEFFICIENT\n#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients\n\n#SANITY CHECK 2-tailed test should match the model output\n# tt <- 2*pnorm(summary(m)$coefficients[2:4], lower.tail = F)\n# paste(\"p value for two-tailed test, null B = 0 : \",round(tt,3))\n# ot <- pnorm(summary(m)$coefficients[2:4], lower.tail = F)\n# paste(\"BUT we want a one tailed directional, null: B <= 0: \",round(ot,3))\n# paste(\"adjusted confint for directional hypothesis\")\n# (dcint <- confint(m, level = 0.90)) # get 90% for right side))\n# # https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte\n\n#:::::::: INTERPRET COEFFICIENTS\n\n# print(\"Confidence Interval —- LOG ODDS\")\n# confint(m1) #not adjusted for 1-tailed\nprint(\"Coefficients —- ODDS RATIOS\")\n\n\n[1] \"Coefficients —- ODDS RATIOS\"\n\n\nCODE\n(e <- cbind( exp(coef(m)), exp(confint(m)))) #exponentiated, not adjusted\n\n\nWaiting for profiling to be done...\n\n\n                                                      2.5 % 97.5 %\n(Intercept)                                    0.107 0.0256  0.302\npretty_conditionimpasse                        1.505 0.3380  7.881\nospan_splithigh-memory                         1.244 0.2528  6.786\npretty_conditionimpasse:ospan_splithigh-memory 2.610 0.3409 19.436\n\n\nCODE\n# (e <- cbind( exp(coef(m)), exp(dcint))) #exponentiated, adjusted\n\n#TODO INTERACTIONS & ESTIMATED MARGINAL MEANS \n# print(\"MODEL PREDICTIONS\")\n# Retrieve predictions as probabilities \n# (for each level of the predictor)\n# pred.control <- predict(m,data.frame(pretty_condition=\"control\"),type=\"response\")\n#this should match : plogis(intercept coefficient)\n# paste(\"Probability of success in control,\", pred.control)\n# pred.impasse <- predict(m,data.frame(pretty_condition=\"impasse\"),type=\"response\")\n#this should match : plogis(intercept coefficient + predictor coeff)\n# paste(\"Probability of success in impasse,\", pred.impasse)\n\n\n\n\nInference\nTODO double check chisqrs vs grouped_barstats. Why is the tests not the same. Otherwise report mm.C as ospan didn’t improve fit\n\n\nVisualize\n\n\nCODE\n## | PLOT PARAMETERS \n\n#SJPLOT | MODEL | ODDS RATIO\nplot_model(m, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) + #manually adjusted for directional test   \n  labs(title = \"Model Estimate | Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\n\n\nCODE\n#EASYSTATS | MODEL | ODDS RATIO\n# result <- model_parameters(m, exponentiate = TRUE, component = \"all\")\n# plot(result)\n\n\n\n## | PLOT TESTS\nresult <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\nplot(result)\n\n\n\n\n\nCODE\n#ONLY FOR BAYESIAN VERSION\n# result <- rope(m)\n# plot(result)\n# \n# result <- pd(m)\n# plot(result)\n\n\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type=\"int\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Prediction | Probability of Accurate Response\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\n\n\n\nCODE\n#PLOT MODEL PREDICTION\nplot_model(m, type = \"pred\")  \n\n\n$pretty_condition\n\n\n\n\n\n\n$ospan_split\n\n\n\n\n\nCODE\nplot_model(m, type = \"eff\")  \n\n\n$pretty_condition\n\n\n\n\n\n\n$ospan_split\n\n\n\n\n\nCODE\n  # ylim(0,1) + \n  # labs(\n  #   title = \"Model Prediction | Probability of Accurate Response\",\n  #   subtitle = \"Impasse increases Probability of Correct Response\"\n  # )\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.11\n0.03 – 0.30\n<0.001\n\n\npretty condition[impasse]\n1.51\n0.34 – 7.88\n0.598\n\n\nospan split [high-memory]\n1.24\n0.25 – 6.79\n0.787\n\n\npretty condition[impasse] * ospan split[high-memory]\n2.61\n0.34 – 19.44\n0.343\n\n\nObservations\n133\n\n\nR2 Tjur\n0.066\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m1, \"(log odds)\" = m1)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', \n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n\n\n\nDiagnostics\n\n\nCODE\n# print(\"SANITY CHECK REPORTING\")\n# report::report(m)\n\n#print(\"MODEL PERFORMANCE\")\n# performance(m)\n\nprint(\"MODEL DIAGNOSTICS\")\n\n\n[1] \"MODEL DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m)"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#h1b-q1-interpretation-state",
    "href": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#h1b-q1-interpretation-state",
    "title": "8  (Online) Hypothesis Testing",
    "section": "H1B | Q1 INTERPRETATION STATE",
    "text": "H1B | Q1 INTERPRETATION STATE\nDo Ss in the IMPASSE condition offer less-orthogonal interpretations on first question?\nWhile absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn’t allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject’s response, and determine if these interpretations differ by experimental condition. State is a 3-category derived response variable that groups the following interpretations:\n\n“orthogonal” [reference category] includes orthogonal and satisficing responses ==> indicates a primarily orthogonal state of coordinate system understanding\n“other” includes: blank, reference point, responses that can’t be classified (including selecting all datapoints), => indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly not orthogonal nor triangular\n“angular” includes ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n“triangular” includes correct triangular and ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n\n\n\n\n\n\n\n\nResearch Question\nDoes Ss in the impasse condition produce less orthogonal responses on the first question?\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of producing unknown and and triangle-like response states, relative to orthogonal response states, on the first question\n\n\nData\n\ndata: df_items where q == 1\noutcome: state ( 4 level factor from 5 level high_interpretation )\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMultinomial (Logistic) Regression on state predicted by condition\n\nAlternative:\n\nOrdinal regression on state; but model doesn’t satisfy proportional odds assumption (parallel slopes)\nMultinomial or Ordinal regression on high_interpretation (5 category interpretation state which distinguishes between uncertain (blank, reference) unclassifiable, triangle-like and true triangular.) There are some cells with zeros, however (no uncertain responses in control) which means the model can’t accurately estimate those comparisons\n\n\n\n\n\nSetup\n\n\nCODE\n#:::::::: PREP DATA\ndf <- df_items %>% filter(q==1) %>% \n  dplyr::select(pretty_condition, ospan_split, state)\n\n\n\n\nDescribe\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~ospan_split) +\n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Q1 Interpretation\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\ntable(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n            \n             control impasse    Sum\n  orthogonal  0.8154  0.2647 0.5338\n  other       0.0462  0.3824 0.2180\n  tri-like    0.0308  0.1176 0.0752\n  triangular  0.1077  0.2353 0.1729\n  Sum         1.0000  1.0000 1.0000\n\n\nCODE\n(t <- table(df$state, df$pretty_condition, df$ospan_split) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n, ,  = low-memory\n\n            \n             control impasse Sum\n  orthogonal      26      15  41\n  other            1      13  14\n  tri-like         1       3   4\n  triangular       3       5   8\n  Sum             31      36  67\n\n, ,  = high-memory\n\n            \n             control impasse Sum\n  orthogonal      27       3  30\n  other            2      13  15\n  tri-like         1       5   6\n  triangular       4      11  15\n  Sum             34      32  66\n\n\n\n\nMULTINOMIAL REGRESSION\nTODO:: USE MBLOGIT VERSION WITH P VALUES IN MODEL\nDoes condition affect the response state of Q1?\nFit a logistic regression predicting interpretation state (k=3) by condition(k = 2).\n\n3 equations will be estimated (# categories - 1); each representing the odds of answering in that particular interpretation (vs) the reference category (orthogonal answer) [essentially a series of binary logistic regressions, but instead of incorrect/correct, comparing [reference category] vs [this category])\nFor each equation:\n\n\\(\\beta_{0}\\) = Log Odds of [this category type vs. reference category type) response in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of [this category type vs. reference category type] response in CONTROL condition\n\\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for [this category] type response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of [this. vs reference category type] response in IMPASSE (vs) CONTROL\nTwo-tailed NHST Null hypothesis: \\(\\beta_{impasse} = 0\\) the odds for [this category of response vs. reference] are not different for IMPASSE condition\nAlternative hypothesis: \\(\\beta_{impasse} \\ne 0\\) the odds of [this category of response vs. reference] increases or decreases for IMPASSE condition\n\n\n\nFit CONDITION Model\n\n\nCODE\n#check reference level \nprint(\"Categories (first is reference)\")\n\n\n[1] \"Categories (first is reference)\"\n\n\nCODE\nlevels(df$state)\n\n\n[1] \"orthogonal\" \"other\"      \"tri-like\"   \"triangular\"\n\n\nCODE\n#FIT EMPTY MODEL\n# print(\"EMPTY MODEL\")\ncatm.0 <- multinom(state ~ 1, data = df)\n\n\n# weights:  8 (3 variable)\ninitial  value 184.377150 \niter  10 value 155.544397\nfinal  value 154.972366 \nconverged\n\n\nCODE\n# summary(catm.0)\n\n#FIT PREDICTOR MODEL\n# print(\"PREDICTOR MODEL\")\ncatm.C <- multinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\n\n# weights:  12 (6 variable)\ninitial  value 184.377150 \niter  10 value 131.798993\nfinal  value 131.798569 \nconverged\n\n\nCODE\n# summary(catm.C)\n\n#COMPARE MODEL FIT\npaste(\"AIC wth predictor is lower than empty model?\", catm.0$AIC > catm.C$AIC)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(catm.0, catm.C)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |      p\n-------------------------------------------------\ncatm.0 | multinom |  3 |         |       |       \ncatm.C | multinom |  6 |       3 | 46.35 | < .001\n\n\nCODE\n##compare bayesian version\n#library(brms)\n# b.cat <- brm( state2 ~ pretty_condition, data = df, family = \"categorical\", backend = \"cmdstanr\")\n# summary(b.cat)\n# plot_model(b.cat)\n# report(b.cat)\n# coefficient estimates are very simliar to catm. super cool!\n\n##compare mclogit version\n#\"baseline-category logit model\n# https://www.elff.eu/software/mclogit/manual/mblogit/\n# blm1 <- mblogit(state2 ~ pretty_condition , data = df)\n# summary(blm1)\n#identical to catm. super cool!\n\n\nAIC in predictor model is less than empty model, and likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.\n\n\nFit OSPAN Model\n\n\nCODE\n#FIT OSPAN only MODEL\n# print(\"OSPAN ONLY MODEL\")\ncatm.O <- multinom(state ~ ospan_split, data = df)\n\n\n# weights:  12 (6 variable)\ninitial  value 184.377150 \niter  10 value 152.820024\nfinal  value 152.819667 \nconverged\n\n\nCODE\n# summary(catm.O) \n# car::Anova(catm.O) \nprint(\"OSPAN ONLY better than empty?\")\n\n\n[1] \"OSPAN ONLY better than empty?\"\n\n\nCODE\ntest_lrt(catm.0, catm.O)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff | Chi2 |     p\n-----------------------------------------------\ncatm.0 | multinom |  3 |         |      |      \ncatm.O | multinom |  6 |       3 | 4.31 | 0.230\n\n\nCODE\n#FIT OSPAN + CONDITION\n# print(\"OSPAN + CONDITION MODEL\")\ncatm.CO <- multinom(formula = state ~ pretty_condition + ospan_split, data = df, model = TRUE)\n\n\n# weights:  16 (9 variable)\ninitial  value 184.377150 \niter  10 value 128.125771\nfinal  value 128.076360 \nconverged\n\n\nCODE\n# summary(catm.CO)\ncar::Anova(catm.CO) #MainEff condition, marginal ospan\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: state\n                 LR Chisq Df Pr(>Chisq)    \npretty_condition     49.5  3      1e-10 ***\nospan_split           7.4  3      0.059 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n#COMPARE MODEL FIT\npaste(\"Adding OSPAN to CONDITION lowers AIC?\", catm.C$AIC > catm.CO$AIC)\n\n\n[1] \"Adding OSPAN to CONDITION lowers AIC? TRUE\"\n\n\nCODE\ntest_lrt(catm.C, catm.CO)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName    |    Model | df | df_diff | Chi2 |     p\n------------------------------------------------\ncatm.C  | multinom |  6 |         |      |      \ncatm.CO | multinom |  9 |       3 | 7.44 | 0.059\n\n\nAdding (main effect) predictor of OSPAN decreases AIC and is a marginally better fit. In this model, there is still only a main effect of condition. OSPAN is not a significant main effect.\n\n\nCODE\n#FIT OSPAN * CONDITION\n# print(\"OSPAN * CONDITION MODEL\")\ncatm.C.O <- multinom(formula = state ~ pretty_condition * ospan_split, data = df, model = TRUE)\n\n\n# weights:  20 (12 variable)\ninitial  value 184.377150 \niter  10 value 126.168657\niter  20 value 125.962345\nfinal  value 125.962341 \nconverged\n\n\nCODE\ncar::Anova(catm.C.O, type = 3)\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: state\n                             LR Chisq Df Pr(>Chisq)    \npretty_condition                16.37  3    0.00095 ***\nospan_split                      0.36  3    0.94767    \npretty_condition:ospan_split     4.23  3    0.23787    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n# summary(catm.C.O)\n# car::Anova(catm.C.O) #MainEff condition, marginal ospan\n#COMPARE MODEL FIT\npaste(\"Adding INTERACTION lowers AIC?\", catm.CO$AIC > catm.C.O$AIC)\n\n\n[1] \"Adding INTERACTION lowers AIC? FALSE\"\n\n\nCODE\ntest_lrt(catm.CO, catm.C.O)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName     |    Model | df | df_diff | Chi2 |     p\n-------------------------------------------------\ncatm.CO  | multinom |  9 |         |      |      \ncatm.C.O | multinom | 12 |       3 | 4.23 | 0.238\n\n\nAdding interaction of OSPAN does not improve fit and does not lower AIC. In the IXN model, only the main effect of condition is significant.\n\n\nCODE\n##compare bayesian version\n# library(brms)\n# b.cat <- brm( state ~ pretty_condition*ospan_split, data = df, family = \"categorical\", backend = \"cmdstanr\")\n# summary(b.cat)\n# plot_model(b.cat) \n# plot(equivalence_test(b.cat))\n# plot(rope(b.cat))\n# report(b.cat)\n# coefficient estimates are very simliar to catm. super cool!\n\n##compare mclogit version\n#\"baseline-category logit model\n# https://www.elff.eu/software/mclogit/manual/mblogit/\nblm1 <- mblogit(state ~ pretty_condition *ospan_split , data = df)\n\n\n\nIteration 1 - deviance = 253 - criterion = 0.272\nIteration 2 - deviance = 252 - criterion = 0.00566\nIteration 3 - deviance = 252 - criterion = 0.000157\nIteration 4 - deviance = 252 - criterion = 2.98e-07\nIteration 5 - deviance = 252 - criterion = 2.53e-12\nconverged\n\n\nCODE\nsummary(blm1)\n\n\n\nCall:\nmblogit(formula = state ~ pretty_condition * ospan_split, data = df)\n\nEquation for other vs orthogonal:\n                                               Estimate Std. Error z value\n(Intercept)                                      -3.258      1.019   -3.20\npretty_conditionimpasse                           3.115      1.087    2.87\nospan_splithigh-memory                            0.655      1.255    0.52\npretty_conditionimpasse:ospan_splithigh-memory    0.954      1.459    0.65\n                                               Pr(>|z|)   \n(Intercept)                                      0.0014 **\npretty_conditionimpasse                          0.0042 **\nospan_splithigh-memory                           0.6016   \npretty_conditionimpasse:ospan_splithigh-memory   0.5132   \n\nEquation for tri-like vs orthogonal:\n                                               Estimate Std. Error z value\n(Intercept)                                     -3.2581     1.0190   -3.20\npretty_conditionimpasse                          1.6487     1.1994    1.37\nospan_splithigh-memory                          -0.0377     1.4407   -0.03\npretty_conditionimpasse:ospan_splithigh-memory   2.1580     1.7346    1.24\n                                               Pr(>|z|)   \n(Intercept)                                      0.0014 **\npretty_conditionimpasse                          0.1693   \nospan_splithigh-memory                           0.9791   \npretty_conditionimpasse:ospan_splithigh-memory   0.2135   \n\nEquation for triangular vs orthogonal:\n                                               Estimate Std. Error z value\n(Intercept)                                      -2.159      0.610   -3.54\npretty_conditionimpasse                           1.061      0.799    1.33\nospan_splithigh-memory                            0.250      0.812    0.31\npretty_conditionimpasse:ospan_splithigh-memory    2.148      1.162    1.85\n                                               Pr(>|z|)    \n(Intercept)                                      0.0004 ***\npretty_conditionimpasse                          0.1843    \nospan_splithigh-memory                           0.7581    \npretty_conditionimpasse:ospan_splithigh-memory   0.0645 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNull Deviance:     369 \nResidual Deviance: 252 \nNumber of Fisher Scoring iterations:  5 \nNumber of observations:  133 \n\n\nCODE\n# car::Anova(blm1) #todo need to separate by individual equation\n#identical to catm. super cool!\n\n\nAIC in predictor model is less than empty model, and likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.\n\n\nDescribe\n\n\nCODE\n#set model\nm <- catm.C.O\n\n#::::::::INTERPRETATION\npaste(\"MODEL SUMMARY\")\n\n\n[1] \"MODEL SUMMARY\"\n\n\nCODE\nsummary(m)\n\n\nCall:\nmultinom(formula = state ~ pretty_condition * ospan_split, data = df, \n    model = TRUE)\n\nCoefficients:\n           (Intercept) pretty_conditionimpasse ospan_splithigh-memory\nother            -3.26                    3.11                 0.6553\ntri-like         -3.26                    1.65                -0.0377\ntriangular       -2.16                    1.06                 0.2500\n           pretty_conditionimpasse:ospan_splithigh-memory\nother                                               0.954\ntri-like                                            2.158\ntriangular                                          2.148\n\nStd. Errors:\n           (Intercept) pretty_conditionimpasse ospan_splithigh-memory\nother             1.02                   1.087                  1.255\ntri-like          1.02                   1.199                  1.441\ntriangular        0.61                   0.799                  0.812\n           pretty_conditionimpasse:ospan_splithigh-memory\nother                                                1.46\ntri-like                                             1.73\ntriangular                                           1.16\n\nResidual Deviance: 252 \nAIC: 276 \n\n\nCODE\ncar::Anova(m, type =3) #always type 3 for ixns \n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: state\n                             LR Chisq Df Pr(>Chisq)    \npretty_condition                16.37  3    0.00095 ***\nospan_split                      0.36  3    0.94767    \npretty_condition:ospan_split     4.23  3    0.23787    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n# calculate z-statistics of coefficients\nz_stats <- summary(m)$coefficients/summary(m)$standard.errors\n# convert to p-values\np_values <- (1 - pnorm(abs(z_stats)))*2\n# display p-values in transposed data frame\n(p_values <- data.frame(p = (p_values)))\n\n\n           p..Intercept. p.pretty_conditionimpasse p.ospan_splithigh.memory\nother           0.001388                   0.00417                    0.602\ntri-like        0.001387                   0.16927                    0.979\ntriangular      0.000398                   0.18428                    0.758\n           p.pretty_conditionimpasse.ospan_splithigh.memory\nother                                                0.5132\ntri-like                                             0.2134\ntriangular                                           0.0645\n\n\nCODE\n# display odds ratios in transposed data frame\n\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\nodds_ratios <- data.frame(OR = exp(summary(m)$coefficients))\noptions(scipen = 2)\n(results <- cbind(odds_ratios, p_values))\n\n\n           OR..Intercept. OR.pretty_conditionimpasse OR.ospan_splithigh.memory\nother              0.0385                      22.53                     1.926\ntri-like           0.0385                       5.20                     0.963\ntriangular         0.1154                       2.89                     1.284\n           OR.pretty_conditionimpasse.ospan_splithigh.memory p..Intercept.\nother                                                   2.60      0.001388\ntri-like                                                8.65      0.001387\ntriangular                                              8.57      0.000398\n           p.pretty_conditionimpasse p.ospan_splithigh.memory\nother                        0.00417                    0.602\ntri-like                     0.16927                    0.979\ntriangular                   0.18428                    0.758\n           p.pretty_conditionimpasse.ospan_splithigh.memory\nother                                                0.5132\ntri-like                                             0.2134\ntriangular                                           0.0645\n\n\n\n\nInference\nlooking at detailed p values\n… OTHER: only main effect of condition TRI-LIKE: no effects TRI: IXN condition * impasse\nTODO\n\nBeing in the IMPASSE condition increases the odds of giving an unknown (potentially nonsense) or blank/uncertain response rather than an orthogonal (or satisficing) response by a factor of 45 (z = 3.81, p < 0.001) . Participants in the impasse condition were 45x as likely to give an unknown/uncertain response rather than an orthogonal response compared to participants in control.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 17.5 (z = 2.60, p < 0.001 ). Participants in the impasse condition were almost 6x as likely to give an a triangular response rather than an orthogonal response compared to participants in control.\nBeing in the IMPASSE condition increases the odds of giving ‘triangle-like’ response rather than an orthogonal (or satisficing) response by a factor of 4.8 (z = 3.30, p < 0.001 ). Participants in the impasse condition were almost 6x as likely to give an a triangular response rather than an orthogonal response compared to participants in control.\nAs with the (binary) logistic regression on accuracy ~ condition, significant model intercepts indicate that the odds of being in any particular response state (vs) orthogonal are significantly different than 1 in the control condition. (i.e. not 1:1 odds or 50% chance of being in that response state. Orthogonal is a much more probable response state in control)\n[need to to double check interpretation, but I think that the OR intercepts converted to probabilities equate to the marginal probability of being in each state in the control condition. which makes sense. I think.?]\nIF I change reference category for condition… then the intercepts should no longer be significant. The b1 coefficients should still be significant, but with changed sign (much less likely) [Yup! this works!]\n\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m, type = \"est\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) +  #manually adjusted for directional test   \n  # scale_y_continuous() + #remove to put on log scale x axis \n  # scale_x_discrete(labels=c(\"control\",\"impasse\"))+\n  labs(title = \"MODEL ESTIMATE  | Q1 Accuracy ~ condition\",\n       subtitle = \"Impasse increases odds of correct response on Q1\",\n       x = \"Condition\") + theme_clean()\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type = \"int\", ci.lvl = 0.95) \n\n\n\n\n\nCODE\nplot_model(m, type=\"eff\", ci.lvl = 0.95) \n\n\n$pretty_condition\n\n\n\n\n\n\n$ospan_split\n\n\n\n\n\nCODE\n# +  ylim(0,1) +\n#   labs(title = \"MODEL PREDICTION  | Q1 State ~ condition\",\n#        subtitle = \"Impasse increases probability of more accurate response states Q1\",\n#        x = \"Condition\") + theme_clean()\n\n#TODO ESTIMAED MARGINALS AND IXN PLOTS \n# https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \nstate\n\n\nPredictors\nOdds Ratios\nCI\np\nResponse\n\n\n(Intercept)\n0.04\n0.01 – 0.29\n0.002\nother\n\n\npretty condition[impasse]\n22.53\n2.62 – 193.88\n0.005\nother\n\n\nospan split [high-memory]\n1.93\n0.16 – 23.11\n0.603\nother\n\n\npretty condition[impasse] * ospan split[high-memory]\n2.60\n0.14 – 46.67\n0.514\nother\n\n\n(Intercept)\n0.04\n0.01 – 0.29\n0.002\ntri-like\n\n\npretty condition[impasse]\n5.20\n0.48 – 55.86\n0.172\ntri-like\n\n\nospan split [high-memory]\n0.96\n0.06 – 16.68\n0.979\ntri-like\n\n\npretty condition[impasse] * ospan split[high-memory]\n8.65\n0.28 – 268.24\n0.216\ntri-like\n\n\n(Intercept)\n0.12\n0.03 – 0.39\n0.001\ntriangular\n\n\npretty condition[impasse]\n2.89\n0.59 – 14.05\n0.187\ntriangular\n\n\nospan split [high-memory]\n1.28\n0.26 – 6.40\n0.759\ntriangular\n\n\npretty condition[impasse] * ospan split[high-memory]\n8.57\n0.86 – 85.46\n0.067\ntriangular\n\n\nObservations\n133\n\n\nR2 / R2 adjusted\n0.187 / 0.181\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m1, \"(log odds)\" = m1)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', \n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n# modelsummary(mixcat.1, s)\n#TODO OUTPUT TABLE \n#https://arelbundock.com/posts/modelsummary_multinomial_logit/\n\n\n\n\nDiagnostics\n\n\nCODE\n#EXAMINE PREDICTIONS\n#create sample data frame\n# test <- data.frame(pretty_condition = c(\"control\", \"impasse\"))\n# pred <- predict(m, newdata = test, \"probs\")\n# paste(\"Predicted Probability of Being in Each State\")\n# ( x <- cbind(test, pred))\n\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(m)\n\n\nCan't calculate log-loss.\nCan't calculate proper scoring rules for ordinal, multinomial or cumulative link models.\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n-----------------------------------------------------\n275.925 | 310.609 | 0.187 |     0.181 | 0.354 | 1.443\n\n\nCODE\nDescTools::PseudoR2(m, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n\n\n  McFadden   CoxSnell Nagelkerke \n     0.187      0.354      0.392 \n\n\nCODE\n#General Goodness of Fit\n#library(generalhoslem)\n#logitgof(df$state, catm$fitted.values, g = 3)\n#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).\n#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables\nchisq.test(df$state,predict(m)) #actual states VS predicted states\n\n\nWarning in chisq.test(df$state, predict(m)): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  df$state and predict(m)\nX-squared = 33, df = 3, p-value = 3e-07\n\n\nCODE\n# The chi-square test tests the decrease in unexplained variance from the baseline model to the final model\n\n# print(\"MODEL DIAGNOSTICS\")\n# check_model(m) can't do overall diagnostics, have to do them on individual model equations"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#explore-specific-question",
    "href": "analysis/SGC3A/4_sgc3A_ospan_hypotesting.html#explore-specific-question",
    "title": "8  (Online) Hypothesis Testing",
    "section": "EXPLORE specific question",
    "text": "EXPLORE specific question\n\n\nCODE\ndf <- df_items %>% filter(q==10)\ngrouped_ggbarstats( data = df, x = accuracy, y = pretty_condition, grouping.var = ospan_split)"
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html",
    "title": "9  Introduction",
    "section": "",
    "text": "TODO UPDATE ALL\nIn Study 3B we compare the efficacy of the explicit [interaction] scaffold and the implicit [impasse] scaffold."
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html#methods",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html#methods",
    "title": "9  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Scaffold: control,impasse)\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 19.1. The list of questions can be found here.\n\n\n\nFigure 9.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nNote that across both control and impasse conditions, both the question, response options and graph structure were identical. The experimental manipulation (posing a mental impasse) was accomplished by changing the position of datapoints in the impasse-condition graph, such that for any given question, there was no available response option if the reader were to interpret the graph as cartesian (making an orthogonal rather than diagonal projection from the x-axis.)\nThe green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. We hypothesize that this presents the reader with an obstacle, at which point they are forced to confront their interpretation of the coordinate system and (ideally) develop a new strategy.\n\n\n\nFigure 9.2: Sample Question (Q=1) graphs for each condition\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items.\n(3B) The first five items in the task are defined as the SCAFFOLDING block. In the IMPASSE condition, the first five questions included an IMPASSE problem state. For participants in the CONTROL condition, the dataset was structure such that there was always an available ‘orthogonal answer’ for the first 5 questions.\n(3B) The remaining 10 items are defined as the TESTING block. In both conditions, these questions were not structured as impasse (i.e. contained an available orthogonal answer)\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData was collected by convenience sample of a university subject pool. Initial data (Fall 2017, Spring 2018) were collected in-person, with large groups of students simultaneously completing the study (independently) in a computer lab. In Fall 2021 and Winter 2022 we collected additional data to replicate results in a remote format (students completing the study asynchronously on their own computers)."
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html#analysis",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html#analysis",
    "title": "9  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\nBefore analysis, data files from individual data collection periods are harmonized into a common data format.\n\n\n\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nspring17_clean_data.Rmd  spring18_clean_data.Rmd  fall21_clean_data.Rmd  winter2022_clean_sgc3b.Rmd\n2_sgc3B_scoring.qmd\n\n\n\nData for study SGC_3B were collected across four time periods, interrupted by the Covid-19 pandemic.\n\n\n\nPeriod\nModality\n\n\n\n\nFall 2017\nin person, SONA groups in computer lab\n\n\nSpring 2018\nin person, SONA groups in computer lab\n\n\nFall 2021\nasynchronous, online, SONA\n\n\n\nData collected in Fall 2017, Spring 2018 constitute the original SGC_3B study, conducted in person. Data collected in Fall 2021 constitute the web-based replication, conducted online (asynchronously). In all cases, the experiment was administered via a web application.\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3B/data/0-session-level/fall17_sgc3b_participants.csv\"\nspring18 <- \"analysis/SGC3B/data/0-session-level/spring18_sgc3b_participants.csv\"\nfall21 <- \"analysis/SGC3B/data/0-session-level/fall21_sgc3b_participants.csv\"\nmeta <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_participants.csv\" #FOR SCHEMA ONLY\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\nmeta <- read_csv(meta)\n\n#SAVE METADATA FROM SGC3A, but no rows \ndf_subjects <- meta %>% filter(condition=='X') %>% \n  dplyr::select(\n  subject,condition,term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m, \n  # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n  #exploratory factors\n  violations, browser, width, height\n)\n\n#COMPARE COLS\n# janitor::compare_df_cols(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21,meta)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\",\n    violations = \"NULL\",\n    browser = \"NULL\",\n    width = \"NULL\",\n    height = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, \n                # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                #exploratory factors\n                violations, browser, width, height)\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\"),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#CLEANUP\nrm(df_subjects_fall17,df_subjects_fall21, df_subjects_spring18, df_subjects_before)\nrm(fall17,fall21,spring18,meta)\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3B/data/0-session-level/fall17_sgc3b_blocks.csv\"\nspring18 <- \"analysis/SGC3B/data/0-session-level/spring18_sgc3b_blocks.csv\"\nfall21 <- \"analysis/SGC3B/data/0-session-level/fall21_sgc3b_blocks.csv\"\nmeta <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_items.rds\" #FOR SCHEMA ONLY\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\nmeta <- read_rds(meta) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- meta %>% group_by(q) %>% dplyr::select(q,relation) %>% unique()\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- meta %>% filter(condition=='X') %>% dplyr::select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n  \n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18, df_items_fall21) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  dplyr::select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n\n#COMPARE COLS\njanitor::compare_df_cols(df_items_before, df_items)\n\n\n  column_name df_items_before  df_items\n1      answer       character character\n2   condition         numeric    factor\n3     correct         logical    factor\n4        mode       character    factor\n5           q         numeric    factor\n6    question       character character\n7        rt_s         numeric   numeric\n8     subject       character    factor\n9        term       character    factor\n\n\nCODE\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% dplyr::select(-question,-correct,-rt_s,-num_o)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n  question = \"Please describe how to determine what event(s) start at 12pm?\",\n  response = as.character(response) #doesn't need to be factor\n)\n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#add back pretty condition \ndf_items <- df_items %>% mutate(\n  pretty_condition = recode_factor(condition, \n                                   \"111\" = \"none-none\", \"121\" =  \"none-impasse\", \n                                   \"211\" = \"img-none\", \"221\" =  \"img-impasse\", \n                                   \"311\" = \"ixv-none\", \"321\" =  \"ixv-impasse\", \n                                   ),\n  pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_items_fall17,df_items_fall21, df_items_spring18, df_items_before)\nrm(fall17,fall21,spring18,meta, map_relations)\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3B/data/1-study-level/sgc3b_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3B/data/1-study-level/sgc3b_items.csv\", row.names = FALSE)\nwrite.csv(df_freeresponse,\"analysis/SGC3B/data/1-study-level/sgc3b_freeresponse.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3B/data/1-study-level/sgc3b_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3B/data/1-study-level/sgc3b_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html#resources",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html#resources",
    "title": "9  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.3        bit64_4.0.5       vroom_1.5.7       jsonlite_1.8.0   \n [5] viridisLite_0.4.0 modelr_0.1.8      assertthat_0.2.1  highr_0.9        \n [9] cellranger_1.1.0  yaml_2.3.5        pillar_1.7.0      backports_1.4.1  \n[13] glue_1.6.2        digest_0.6.29     rvest_1.0.2       snakecase_0.11.0 \n[17] colorspace_2.0-3  htmltools_0.5.2   pkgconfig_2.0.3   broom_0.8.0      \n[21] labelled_2.9.1    haven_2.5.0       scales_1.2.0      webshot_0.5.3    \n[25] svglite_2.1.0     openxlsx_4.2.5    rio_0.5.29        tzdb_0.3.0       \n[29] generics_0.1.2    ellipsis_0.3.2    withr_2.5.0       janitor_2.1.0    \n[33] cli_3.3.0         magrittr_2.0.3    crayon_1.5.1      readxl_1.4.0     \n[37] evaluate_0.15     fs_1.5.2          fansi_1.0.3       xml2_1.3.3       \n[41] foreign_0.8-82    tools_4.2.1       data.table_1.14.2 hms_1.1.1        \n[45] lifecycle_1.0.1   munsell_0.5.0     reprex_2.0.1      zip_2.2.0        \n[49] compiler_4.2.1    systemfonts_1.0.4 rlang_1.0.3       grid_4.2.1       \n[53] rstudioapi_0.13   htmlwidgets_1.5.4 rmarkdown_2.14    gtable_0.3.0     \n[57] DBI_1.1.3         curl_4.3.2        R6_2.5.1          lubridate_1.8.0  \n[61] knitr_1.39        fastmap_1.1.0     bit_4.0.4         utf8_1.2.2       \n[65] stringi_1.7.6     parallel_4.2.1    Rcpp_1.0.8.3      vctrs_0.4.1      \n[69] dbplyr_2.2.1      tidyselect_1.1.2  xfun_0.31"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html",
    "title": "10  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC3B study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#score-sgc-data",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#score-sgc-data",
    "title": "10  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n# backup <- read_rds('analysis/SGC3B/data/1-study-level/sgc3b_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC3B/data/1-study-level/sgc3b_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#summarize-by-subject",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#summarize-by-subject",
    "title": "10  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC3B/data/1-study-level/sgc3b_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#explore-distributions",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#explore-distributions",
    "title": "10  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"Impasse Condition (blue) yields more correct responses across the entire task\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(pretty_condition~q) + \n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields more correct responses on each item\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>% \ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\", \n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher total absolute scores\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE TEST PHASE\ngf_histogram(~item_test_NABS, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Absolute Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores across the entire task\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(q~pretty_condition) + \n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores on each item\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) + \n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>% \n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\", \n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher cumulative scaled scores\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE TEST PHASE\ngf_histogram(~item_test_SCALED, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Scaled Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Scaled Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\nTODO: INVESTIGATE if some of the scores assigned to 0 should be assigned to -0.5 to balance\nTODO: INVESTIGATE DISTRIBUTIONS of each subscore type\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"Impasse condition (blue) yields fewer Orthogonal and more Triangular responses\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more Triangular responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more positive trending responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"Impasse shifts density toward higher Triagular scores\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"Impasse shifts density toward lower Orthogonal scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#export",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#export",
    "title": "10  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3B/data/2-scored-data/sgc3b_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3B/data/2-scored-data/sgc3b_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC3B/data/2-scored-data/sgc3b_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC3B/data/2-scored-data/sgc3b_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures\n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3B/data/2-scored-data/sgc3b_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3B/data/2-scored-data/sgc3b_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#resources",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#resources",
    "title": "10  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     \n [5] readr_2.1.2      tidyr_1.2.0      tibble_3.1.7     tidyverse_1.3.1 \n [9] Hmisc_4.7-0      Formula_1.2-4    survival_3.3-1   lattice_0.20-45 \n[13] pbapply_1.5-0    ggformula_0.10.1 ggridges_0.5.3   scales_1.2.0    \n[17] ggstance_0.3.5   ggplot2_3.3.6    kableExtra_1.3.4\n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            bit64_4.0.5         lubridate_1.8.0    \n [4] webshot_0.5.3       RColorBrewer_1.1-3  httr_1.4.3         \n [7] tools_4.2.1         backports_1.4.1     utf8_1.2.2         \n[10] R6_2.5.1            rpart_4.1.16        DBI_1.1.3          \n[13] colorspace_2.0-3    nnet_7.3-17         withr_2.5.0        \n[16] tidyselect_1.1.2    gridExtra_2.3       curl_4.3.2         \n[19] bit_4.0.4           compiler_4.2.1      cli_3.3.0          \n[22] rvest_1.0.2         htmlTable_2.4.0     xml2_1.3.3         \n[25] labeling_0.4.2      mosaicCore_0.9.0    checkmate_2.1.0    \n[28] systemfonts_1.0.4   digest_0.6.29       foreign_0.8-82     \n[31] rmarkdown_2.14      svglite_2.1.0       rio_0.5.29         \n[34] base64enc_0.1-3     jpeg_0.1-9          pkgconfig_2.0.3    \n[37] htmltools_0.5.2     labelled_2.9.1      dbplyr_2.2.1       \n[40] fastmap_1.1.0       readxl_1.4.0        htmlwidgets_1.5.4  \n[43] rlang_1.0.3         rstudioapi_0.13     farver_2.1.0       \n[46] generics_0.1.2      jsonlite_1.8.0      vroom_1.5.7        \n[49] zip_2.2.0           magrittr_2.0.3      Matrix_1.4-1       \n[52] Rcpp_1.0.8.3        munsell_0.5.0       fansi_1.0.3        \n[55] lifecycle_1.0.1     stringi_1.7.6       yaml_2.3.5         \n[58] MASS_7.3-57         plyr_1.8.7          grid_4.2.1         \n[61] parallel_4.2.1      crayon_1.5.1        haven_2.5.0        \n[64] splines_4.2.1       hms_1.1.1           knitr_1.39         \n[67] pillar_1.7.0        reprex_2.0.1        glue_1.6.2         \n[70] evaluate_0.15       latticeExtra_0.6-29 data.table_1.14.2  \n[73] modelr_0.1.8        tzdb_0.3.0          png_0.1-7          \n[76] vctrs_0.4.1         tweenr_1.0.2        cellranger_1.1.0   \n[79] gtable_0.3.0        polyclip_1.10-0     assertthat_0.2.1   \n[82] openxlsx_4.2.5      xfun_0.31           ggforce_0.3.3      \n[85] broom_0.8.0         viridisLite_0.4.0   cluster_2.1.3      \n[88] ellipsis_0.3.2     \n\n\nTODO sample from sgc3a to combine with sgc3b ::: {.cell}\n\nCODE\n# \n# table(df_subjects$condition)\n# \n# set.seed(8)\n# r111  <- df_subjects %>% filter(condition == \"111\") %>% sample(50) %>% dplyr::select(-orig.id)\n# r121  <- df_subjects %>% filter(condition == \"121\") %>% sample(50) %>% dplyr::select(-orig.id)\n# df <- df_subjects %>% filter(condition %nin% c(111,121)) \n# df <- rbind(df,r111,r121)\n# \n# df <- df %>% mutate(\n#   explicit = as_factor(str_sub(condition,1,1)),\n#   explicit = recode_factor(explicit, \"1\" = \"none\", \"2\" = \"image\", \"3\"=\"ixv\"),\n#   impasse = str_sub(condition,2,2),\n#   impasse = recode_factor(impasse, \"1\" = \"none\", \"2\"=\"impasse\")\n# )\n# \n# table(df$explicit , df$impasse)\n# table(df$condition)\n# \n# gf_histogram(~s_NABS, data = df) %>% \n#   gf_facet_grid(df$explicit~df$impasse)\n# \n# m <- lm(s_NABS ~ explicit+impasse+explicit*impasse, data = df)\n# summary(m)\n# summ(m)\n# check_model(m)\n# \n# gf_boxplot(~s_NABS, data = df) %>% gf_facet_grid(explicit~impasse)\n# \n# library(interactions)\n# library(jtools) #nice presentation of restuls\n# summ(m)\n# cat_plot(model = m, pred=explicit, modx = impasse, geom=\"line\")+\n#   lims(y=c(0,13))\n# \n# \n# means <- favstats(s_NABS~condition, data = df)\n# means\n# \n# x <- favstats(absolute_score~condition , data = df4c)\n# x\n# \n# means <- rbind(means,x)\n# means\n# \n# df <- df %>% dplyr::select(s_NABS, condition) %>% rename(score=s_NABS)\n# c4 <- df4c %>% dplyr::select(absolute_score, condition) %>% rename(score = absolute_score)\n# \n# df <- rbind(df,c4)\n# gf_boxplot(score ~ condition, data = df)\n\n:::"
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html",
    "title": "11  Introduction",
    "section": "",
    "text": "In Study 4A we explore the extent to which the design of the axes and gridlines of the graph influence how a reader interprets its underlying coordinate system.\nExperimental Hypothesis:\nWe hypothesize that the design of the major axes (specifically orthogonal) axes establish for the learner the basis of the coordinate system. Differently oriented axes should lead the reader to be more open to alternative coordinate systems.\nExploratory Questions"
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html#methods",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html#methods",
    "title": "11  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 4 levels (Graphical Framework: ORTH-FULL, ORTH-SPARSE, ORTH-GRID, TRI-SPARSE) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Graphical Framework: ORTH-FULL, ORTH-SPARSE, ORTH-GRID, TRI-SPARSE)\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 19.1. The list of questions can be found here.\n\n\n\nFigure 11.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items : the Graph Comprehension Task\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData were collected by convenience sample of a university subject pool during the winter of 2022. Participants accessed the study via a web browser (asynchronously). The stimulus application required the participant stay in full-screen mode for the entirety of the study."
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html#analysis",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html#analysis",
    "title": "11  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nwinter2022_clean_sgc4a.Rmd\n2_sgc4A_scoring.qmd\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC4A/data/0-session-level/fall17_sgc4a_participants.csv\"\nspring18 <- \"analysis/SGC4A/data/0-session-level/spring18_sgc4a_participants.csv\"\nwinter22 <- \"analysis/SGC4A/data/0-session-level/winter22_sgc4a_participants.rds\"\nsummer22 <- \"analysis/SGC4A/data/0-session-level/su22_sgc4a_participants.rds\"\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\ndf_subjects_summer22 <- read_rds(summer22) #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% \n  dplyr::select(\n  subject,condition,pretty_condition, term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m, \n  # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n  #exploratory factors\n  violations, browser, width, height\n)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    condition = as.factor(condition),\n    pretty_condition = \"NULL\",\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\",\n    violations = \"NULL\",\n    browser = \"NULL\",\n    width = \"NULL\",\n    height = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, pretty_condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, \n                # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                #exploratory factors\n                violations, browser, width, height)\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_winter22_q16 <- df_subjects_winter22 %>% \n  dplyr::select(subject, condition, pretty_condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% dplyr::select(-explanation)\n\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects_winter22 <- df_subjects_winter22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors \n                 violations, browser, width, height)\n\ndf_subjects_summer22 <- df_subjects_summer22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors \n                 violations, browser, width, height)\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#compare dataframe columns\n# janitor::compare_df_cols(df_subjects, df_subjects_winter22, df_subjects_before)\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_winter22, df_subjects_summer22, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\"),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\",\"summer22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#REFACTOR CONDITIONS\ndf_subjects <- df_subjects %>% mutate(\n    condition = recode_factor(condition, \"11111\" = \"111\", \"112\" = \"112\", \"111\" = \"111\", \"113\" = \"113\", \"114\" = \"114\", \"115\"=\"115\"),\n    pretty_condition = recode_factor(condition, \"111\" = \"Orth-Full\", \"114\" =  \"Orth-Sparse\", \"115\"=\"Orth-Grid\",\"113\"=\"Tri-Sparse\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_subjects_fall17, df_subjects_spring18, df_subjects_winter22,df_subjects_before, df_subjects_summer22)\nrm(fall17,spring18,winter22, summer22)\n\n#FINALLY DROP CONDITION 112 (partial orthog with y axis lines extending only to right end of triangle)\n#this was an incomplete [pilot only] condition collected in FA17 SP18 for pilot purposes\ndf_subjects <- df_subjects %>% filter(condition != \"112\") %>% \n  mutate(\n    condition = droplevels(condition),\n    pretty_condition = droplevels(pretty_condition)\n  )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC4A/data/0-session-level/fall17_sgc4a_blocks.csv\"\nspring18 <- \"analysis/SGC4A/data/0-session-level/spring18_sgc4a_blocks.csv\"\nwinter22 <- \"analysis/SGC4A/data/0-session-level/winter22_sgc4a_items.rds\"\nsummer22 <- \"analysis/SGC4A/data/0-session-level/su22_sgc4a_items.rds\"\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\ndf_items_summer22 <- read_rds(summer22) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- df_items_winter22 %>% group_by(q) %>% select(q,relation) %>% unique()\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- df_items_winter22 %>% filter(condition=='X') %>% select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n\n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n  \n#reduce data collected using new webapp\ndf_items_winter22 <- df_items_winter22 %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\ndf_items_summer22 <- df_items_summer22 %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\n\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_winter22, df_items_summer22, df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\",\"summer22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% select(-question,-correct,-rt_s,-num_o)\n#add data from wi22 [stored on subject data]\ndf_winter22_q16 <- df_winter22_q16 %>% dplyr::select(-pretty_condition)\ndf_freeresponse <- rbind(df_freeresponse, df_winter22_q16)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n    question = \"Please describe how to determine what event(s) start at 12pm?\",\n    response = as.character(response) #doesn't need to be factor\n  ) \n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#REFACTOR CONDITIONS\ndf_items <- df_items %>% mutate(\n    condition = recode_factor(condition, \"11111\" = \"111\", \"112\" = \"112\", \"111\" = \"111\", \"113\" = \"113\", \"114\" = \"114\", \"115\"=\"115\"),\n    pretty_condition = recode_factor(condition, \"111\" = \"Orth-Full\", \"114\" =  \"Orth-Sparse\", \"115\"=\"Orth-Grid\",\"113\"=\"Tri-Sparse\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_items_fall17, df_items_spring18, df_items_winter22, df_items_before, df_winter22_q16, df_items_summer22)\nrm(fall17,spring18,winter22, map_relations, summer22)\n\n#FINALLY DROP CONDITION 112 (partial orthog with y axis lines extending only to right end of triangle)\n#this was an incomplete [pilot only] condition collected in FA17 SP18 for pilot purposes\ndf_items <- df_items %>% filter(condition != \"112\") %>% \n  mutate(\n    condition = droplevels(condition),\n    pretty_condition = droplevels(pretty_condition)\n  )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4A/data/1-study-level/sgc4a_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4A/data/1-study-level/sgc4a_items.csv\", row.names = FALSE)\nwrite.csv(df_freeresponse,\"analysis/SGC4A/data/1-study-level/sgc4a_freeresponse.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4A/data/1-study-level/sgc4a_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4A/data/1-study-level/sgc4a_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html#resources",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html#resources",
    "title": "11  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     \n [5] readr_2.1.2      tidyr_1.2.0      tibble_3.1.7     ggplot2_3.3.6   \n [9] tidyverse_1.3.1  kableExtra_1.3.4 codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] httr_1.4.3        highr_0.9         pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] bit_4.0.4         munsell_0.5.0     broom_0.8.0       compiler_4.2.1   \n[29] modelr_0.1.8      xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4\n[33] htmltools_0.5.2   tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3      \n[37] viridisLite_0.4.0 crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1     \n[41] withr_2.5.0       grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0     \n[45] lifecycle_1.0.1   DBI_1.1.3         magrittr_2.0.3    scales_1.2.0     \n[49] zip_2.2.0         cli_3.3.0         stringi_1.7.6     vroom_1.5.7      \n[53] fs_1.5.2          xml2_1.3.3        ellipsis_0.3.2    generics_0.1.2   \n[57] vctrs_0.4.1       openxlsx_4.2.5    tools_4.2.1       bit64_4.0.5      \n[61] glue_1.6.2        hms_1.1.1         parallel_4.2.1    fastmap_1.1.0    \n[65] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[69] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html",
    "title": "12  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC4A study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#score-sgc-data",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#score-sgc-data",
    "title": "12  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#backup <- read_rds('analysis/SGC4A/data/1-study-level/sgc4a_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC4A/data/1-study-level/sgc4a_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#summarize-by-subject",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#summarize-by-subject",
    "title": "12  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC4A/data/1-study-level/sgc4a_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\nWarning in subjects_summary$subject == subjects$subject: longer object length is\nnot a multiple of shorter object length\n\n\n[1]  TRUE FALSE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#explore-distributions",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#explore-distributions",
    "title": "12  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_condition = as.factor(condition),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n                \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(pretty_condition~q) +\n  labs( x = \"Absolute Score\",\n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>%\ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\",\n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(q~pretty_condition) +\n  labs( x = \"Scaled Score\",\n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) +\n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>%\n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\",\n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.05), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.05), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"Impasse shifts density toward higher Triagular scores\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"Impasse shifts density toward lower Orthogonal scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#peeking",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#peeking",
    "title": "12  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\n# m1 <- lm(s_SCALED ~ pretty_condition, data = df_subjects)\n# summary(m1)\n# anova(m1)\n# report(m1)"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#export",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#export",
    "title": "12  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4A/data/2-scored-data/sgc4a_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4A/data/2-scored-data/sgc4a_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC4A/data/2-scored-data/sgc4a_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC4A/data/2-scored-data/sgc4a_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4A/data/2-scored-data/sgc4a_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4A/data/2-scored-data/sgc4a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#resources",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#resources",
    "title": "12  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            bit64_4.0.5         lubridate_1.8.0    \n [4] insight_0.18.0      webshot_0.5.3       RColorBrewer_1.1-3 \n [7] httr_1.4.3          tools_4.2.1         backports_1.4.1    \n[10] utf8_1.2.2          R6_2.5.1            rpart_4.1.16       \n[13] DBI_1.1.3           colorspace_2.0-3    nnet_7.3-17        \n[16] withr_2.5.0         tidyselect_1.1.2    gridExtra_2.3      \n[19] curl_4.3.2          bit_4.0.4           compiler_4.2.1     \n[22] cli_3.3.0           rvest_1.0.2         htmlTable_2.4.0    \n[25] xml2_1.3.3          labeling_0.4.2      mosaicCore_0.9.0   \n[28] checkmate_2.1.0     systemfonts_1.0.4   digest_0.6.29      \n[31] foreign_0.8-82      rmarkdown_2.14      svglite_2.1.0      \n[34] rio_0.5.29          base64enc_0.1-3     jpeg_0.1-9         \n[37] pkgconfig_2.0.3     htmltools_0.5.2     labelled_2.9.1     \n[40] dbplyr_2.2.1        fastmap_1.1.0       readxl_1.4.0       \n[43] htmlwidgets_1.5.4   rlang_1.0.3         rstudioapi_0.13    \n[46] farver_2.1.0        generics_0.1.2      jsonlite_1.8.0     \n[49] vroom_1.5.7         zip_2.2.0           magrittr_2.0.3     \n[52] Matrix_1.4-1        Rcpp_1.0.8.3        munsell_0.5.0      \n[55] fansi_1.0.3         lifecycle_1.0.1     stringi_1.7.6      \n[58] yaml_2.3.5          MASS_7.3-57         plyr_1.8.7         \n[61] grid_4.2.1          parallel_4.2.1      crayon_1.5.1       \n[64] haven_2.5.0         splines_4.2.1       hms_1.1.1          \n[67] knitr_1.39          pillar_1.7.0        reprex_2.0.1       \n[70] glue_1.6.2          evaluate_0.15       latticeExtra_0.6-29\n[73] data.table_1.14.2   modelr_0.1.8        tzdb_0.3.0         \n[76] png_0.1-7           vctrs_0.4.1         tweenr_1.0.2       \n[79] cellranger_1.1.0    gtable_0.3.0        polyclip_1.10-0    \n[82] datawizard_0.4.1    assertthat_0.2.1    openxlsx_4.2.5     \n[85] xfun_0.31           ggforce_0.3.3       broom_0.8.0        \n[88] viridisLite_0.4.0   cluster_2.1.3       ellipsis_0.3.2"
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html",
    "href": "analysis/SGC4A/3_sgc4A_description.html",
    "title": "13  Description",
    "section": "",
    "text": "TODO check term cell counts and decide if data is pilot or included ? The purpose of this notebook is describe the distributions of dependent variables for Study SGC4A.\nTEMP REMOVE IN PERSON DATA ::: {.cell}\n:::"
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html#sample",
    "href": "analysis/SGC4A/3_sgc4A_description.html#sample",
    "title": "13  Description",
    "section": "SAMPLE",
    "text": "SAMPLE\n\nData Collection\nData were collected online in Winter 2022.\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Orth-Full\", \"Orth-Sparse\",\"Orth-Grid\", \"Tri-Sparse\",\"Total for Period\")\ncont <- table(df_subjects$term, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Orth-Full \n    Orth-Sparse \n    Orth-Grid \n    Tri-Sparse \n    Total for Period \n  \n \n\n  \n    winter22 \n    88 \n    86 \n    88 \n    98 \n    360 \n  \n  \n    Sum \n    88 \n    86 \n    88 \n    98 \n    360 \n  \n\n\n\n\n\n\n\nParticipants\n\n\nCODE\n#Describe participants\nsubject.stats <-df_subjects %>% dplyr::select(age) %>% unlist() %>% favstats()\nsubject.stats$percent.female <- df_subjects %>% filter(gender==\"Female\") %>% count() %>% unlist()/nrow(df_subjects)\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()%>% \n  footnote(general = \"Age in Years\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    percent.female \n  \n \n\n  \n     \n    18 \n    19 \n    20 \n    21 \n    37 \n    20.5 \n    2.29 \n    360 \n    0 \n    0.722 \n  \n\n\nNote:   Age in Years\n\n\n\n\n360 participants (72 % female ) undergraduate STEM majors at a public American University participated in exchange for course credit (age: 18 - 37 years)."
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html#response-accuracy",
    "href": "analysis/SGC4A/3_sgc4A_description.html#response-accuracy",
    "title": "13  Description",
    "section": "RESPONSE ACCURACY",
    "text": "RESPONSE ACCURACY\n\nSubject Level Scores\nSubject level scores summarize the the response accuracy by a particular participant across all discriminant items in the graph comprehension task.\n\nTask Absolute Score\nRecall from Section 3.1.2.1 that the absolute score (following the dichotomous scoring approach) s_NABS indicates if the subject’s response for a particular item was perfectly correct: whether they selected all correct answer options and no others (excluding certain allowed exceptions, such as also selecting the data point referenced in the question). Across the entire task, there ae 13 strategy discriminating questions.\n\n\nCODE\ntitle = \"Descriptive Statistics of Task Response Accuracy (Total Absolute Score)\"\nabs.stats <- df_subjects %>% dplyr::select(s_NABS) %>% unlist() %>% favstats()\nabs.stats %>% kbl (caption = title) %>% kable_classic() %>% \n  footnote(general = \"# questions correct [0,13]\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Task Response Accuracy (Total Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    0 \n    0 \n    0 \n    2 \n    13 \n    2.22 \n    4.02 \n    360 \n    0 \n  \n\n\nNote:   # questions correct [0,13]\n\n\n\n\nWhen combined overall, total absolute accuracy scores in the TEST phase (n = 360) range from 0 to 13 with a slightly lower mean score of (M = 2.22, SD = 4.02).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL ABSOLUTE\n  gf_props(~s_NABS, data = df_subjects) + \n  labs(x = \"number of correct responses\",\n       y = \"% of subjects\",\n       title = \"Distribution of Task Absolute Score \",\n       subtitle = \"\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"s_NABS\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of Task Absolute Score\",\n        subtitle =\"\",\n        x = \"Total Absolute Score\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = s_NABS,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_NABS),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_NABS, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Task Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score \") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbetweenstats(y = s_NABS, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(s_NABS)) + \n  stat_ecdf(geom = \"step\") +\n  facet_grid(pretty_condition~pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Task Absolute Score \",\n        x = \"Total Absolute Score [0,13]\", \n        y = \"Cumulative Probability\")\n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and perhaps bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018).\n\n\nCODE\nmultimode::modetest(df_subjects$s_NABS)\n\n\nWarning in multimode::modetest(df_subjects$s_NABS): A modification of the data\nwas made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$s_NABS\nExcess mass = 0.06, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\n# n_modes = multimode::nmodes(df_subjects$s_NABS, bw=2) #bw = 2questions/15 = 0.15%\n# l_modes = multimode::locmodes(df_subjects$s_NABS,mod0 =  n_modes, display = TRUE)\n\n\nThe excess mass test for multimodality suggests there is not enough mass at the positive end of the score distribution to be considered multimodal.\n\n\nTask Scaled Scores\nThe total scaled score s_SCALED summarizes the scaled score on the 13 strategy-discriminant questions, for each subject. This score ranges from from -13 (all orthogonal) to 13 (all triangular). Recall that the s_SCALED score for an item is a numeric representation of the strategy-consistent response, scaled from -1 to +1 (see Section 4.1.4)\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy\"\nscaled.stats <- df_subjects %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats()\nscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    -13 \n    -12.5 \n    -10.5 \n    -5 \n    13 \n    -6.58 \n    8.23 \n    360 \n    0 \n  \n\n\n\n\n\nOverall, task scaled scores (n = 360) range from -13 to 13 with a slightly lower mean score of (M = -6.58, SD = 8.23).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL SCALED\ngf_props(~item_test_SCALED, data = df_subjects) +\n  labs(x = \"total scaled score\",\n       y = \"% of subjects\",\n       title = \"Distribution of Scaled Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"s_SCALED\",binwidth=1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\")) + \n  labs( title = \"Distribution of Scaled Score\",\n        subtitle =\"\",\n        x = \"total scaled score\", y = \"number of participants\") + \n theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = s_SCALED,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_SCALED),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_SCALED, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Task Scaled Score\",\n    x = \"Condition\", y = \"Total Scaled \") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbetweenstats(y = s_SCALED, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(s_SCALED)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Task Scaled Score\",\n        x = \"Test Phase Scaled Score [-13,13]\", \n        y = \"Cumulative Probability\") \n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and perhaps perhaps bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018).\n\n\nCODE\nmultimode::modetest(df_subjects$s_SCALED)\n\n\nWarning in multimode::modetest(df_subjects$s_SCALED): A modification of the data\nwas made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$s_SCALED\nExcess mass = 0.07, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\n# n_modes = multimode::nmodes(df_subjects$s_SCALED, bw=2) #bw = 2questions/15 = 0.15%\n# l_modes = multimode::locmodes(df_subjects$s_SCALED,mod0 =  n_modes, display = TRUE)\n\n\nThe excess mass test for multimodality suggests there is not enough mass at the positive end of the score distribution to be considered multimodal.\n\n\n\nFirst Item Scores\nNext we consider the response accuracy on just the first question of the graph comprehension task: a subject’s first exposure to the TM graph.\n\nFirst Item Absolute Score\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Combined)\"\nitem.contingency <- df_subjects %>%  dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>%  addmargins(2) %>% prop.table(margin=2) %>% addmargins(1)\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Combined)\n \n  \n      \n    Orth-Full \n    Orth-Sparse \n    Orth-Grid \n    Tri-Sparse \n    Sum \n  \n \n\n  \n    0 \n    0.943 \n    0.886 \n    0.959 \n    0.86 \n    0.914 \n  \n  \n    1 \n    0.057 \n    0.114 \n    0.041 \n    0.14 \n    0.086 \n  \n  \n    Sum \n    1.000 \n    1.000 \n    1.000 \n    1.00 \n    1.000 \n  \n\n\n\n\n\nAcross data collection sessions, first-item accuracy is consistent across experimental conditions. Incorrect answers are far more frequent (91%) than correct answers (9%). Highest accuracy is achieved in the Triangular gridlines condition, with roughly 14% correct response rate, compared to only 6% in the orthogonal axis control.\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_props(~item_q1_NABS, data = df_subjects) +\n  labs(x = \"response accuracy\",\n       y = \"% subjects\",\n       title = \"Proportion of Correct Responses on First Item\",\n       subtitle=\"\")+\n  theme(legend.position = \"none\")+theme_ggdist()\n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_subjects %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(item_q1_NABS))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  labs(x = \"response accuracy\",\n       title = \"Proportion of Correct Responses on First Item\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Proportion of Correct Responses on First Item\",\n            data = df_subjects, item_q1_NABS ~ pretty_condition, rot_labels=c(0,90,0,0), \n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\"))) \n\n\n\n\n\nCODE\n#STATSPLOT\nggbarstats(\n  x = item_q1_NABS,\n  y = pretty_condition, \n  data = df_subjects\n)\n\n\n\n\n\n\n\nFirst Item Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1. (note: we evaluate scaled_score on the first item rather than interpretation, because no orthogonal interpretation is available in the impasse condition)\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (First Item Scaled Score)\"\nfirstscaled.stats <- df_subjects %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats()\nfirstscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (First Item Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    -0.751 \n    0.621 \n    360 \n    0 \n  \n\n\n\n\n\nWhen combined overall, first item scaled scores (n = 360) range from -1 to 1 with a slightly lower mean score of (M = -0.75, SD = 0.62).\n\n\nCODE\n#GGFORMULA | PROPORTIONAL HISTOGRAM SUBJECT FIRST SCALED\ngf_props(~item_q1_SCALED, data = df_subjects) +\n  labs(x = \"scaled score (first item)\",\n       y = \"% of subjects\",\n       title = \"Distribution of First Item Scaled Score\",\n       subtitle = \"\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_q1_SCALED\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\")) + \n  labs( title = \"Distribution of First Item Scaled Score (by Mode and Condition)\",\n        subtitle =\"\",\n        x = \"scaled score (firt item) \", y = \"number of participants\") + \n  theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_subjects %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(item_q1_SCALED))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  labs(x = \"response accuracy\",\n       title = \"Type of Responses on First Item (by Modality and Condition)\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#STATSPLOT\nggbarstats(\n  x = item_q1_SCALED,\n  y = pretty_condition, \n  data = df_subjects\n)\n\n\n\n\n\n\n\n\nInterpretation Scores\nNext we consider the the interpretations assigned to each response. For each response given by a participant to a question, we assign an interpretation label based on the interpretation the response most closely matches (see Section 3.2.3).\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition\"\nitem.contingency <- df_items %>%  dplyr::select(interpretation, pretty_condition) %>% table() %>%  addmargins(2) %>% prop.table(margin=2) %>% addmargins(1)\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition\n \n  \n      \n    Orth-Full \n    Orth-Sparse \n    Tri-Sparse \n    Orth-Grid \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.568 \n    0.558 \n    0.484 \n    0.608 \n    0.556 \n  \n  \n    frenzy \n    0.005 \n    0.008 \n    0.004 \n    0.005 \n    0.006 \n  \n  \n    ? \n    0.125 \n    0.089 \n    0.114 \n    0.114 \n    0.111 \n  \n  \n    reference \n    0.000 \n    0.002 \n    0.001 \n    0.003 \n    0.001 \n  \n  \n    blank \n    0.018 \n    0.023 \n    0.025 \n    0.035 \n    0.026 \n  \n  \n    both tri + orth \n    0.120 \n    0.119 \n    0.115 \n    0.120 \n    0.118 \n  \n  \n    Tversky \n    0.025 \n    0.018 \n    0.031 \n    0.020 \n    0.024 \n  \n  \n    Triangular \n    0.139 \n    0.183 \n    0.227 \n    0.094 \n    0.158 \n  \n  \n    Sum \n    1.000 \n    1.000 \n    1.000 \n    1.000 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_propsh(~interpretation, data = df_items, fill = ~pretty_condition) %>% \n  gf_facet_grid(pretty_condition~pretty_mode) +\n  labs(x = \"% of items\",\n       title = \"Proportion of Interpretations Across Items\",\n       subtitle=\"\")+\n  theme(legend.position = \"none\")\n\n\n\n\n\nCODE\n#STACKED BAR CHART\ndf_items %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = as.factor(interpretation))) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  labs(x = \"response accuracy\",\n       title = \"Response Types on All Items (by Condition)\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\n# vcd::mosaic(main=\"Proportion of Interpretations across Conditions\",\n#             data = df_items, pretty_condition ~ interpretation, rot_labels=c(0,90,0,0), \n#             offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n#             spacing = spacing_dimequal(unit(1:2, \"lines\"))) \n\n#STATSPLOT\nggbarstats(\n  x = interpretation,\n  y = pretty_condition, \n  data = df_items\n)\n\n\n\n\n\n\n\nCumulative Task Performance\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html#response-latency",
    "href": "analysis/SGC4A/3_sgc4A_description.html#response-latency",
    "title": "13  Description",
    "section": "RESPONSE LATENCY",
    "text": "RESPONSE LATENCY\n\nTime on First Item\nHere we consider the time spent on just the first individual item (first exposure to graph).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- df_subjects %>% dplyr::select(item_q1_rt) %>% unlist() %>% favstats()\ntitle = \"Descriptive Statistics of First Response Time (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of First Response Time (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    5.6 \n    13.8 \n    21.6 \n    35.6 \n    536 \n    31.1 \n    39.1 \n    360 \n    0 \n  \n\n\n\n\n\nResponse time on the first item for subjects (n = 360) ranged from 5.6 to 536.39 minutes with a mean duration of (M = 31.08, SD = 39.13).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_q1_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of First Item Response Time (seconds)\", subtitle = \"fit by gamma distribution\", x = \"First Item Response Time (seconds)\", y = \"% items\")\n\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\n\nWarning: Removed 360 rows containing missing values (geom_vline).\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_q1_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"First Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#recode as boolean correct\ndf_subjects <- df_subjects %>% mutate(\n  item_q1_NABS = as.logical(item_q1_NABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_q1_rt, color = item_q1_NABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .1\n  )) + \n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"First Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\n\n\nCODE\n# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n#TEMP REMOVE OUTLIERS\ndf <- df_subjects %>% filter(item_q1_rt < 300)\n\n#STATSPLOT\nggbetweenstats(y = item_q1_rt, x = pretty_condition, data = df,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\n\n\nTime on Item\nHere we consider the time spent on an individual item (across all items).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- df_items %>%   dplyr::select(rt_s) %>% unlist() %>% favstats()\ntitle = \"Descriptive Statistics of Item Response Latency (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Item Response Latency (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    0.785 \n    12.1 \n    21.8 \n    40.6 \n    536 \n    33.1 \n    36 \n    5400 \n    0 \n  \n\n\n\n\n\nTime on an individual item for subjects (n = 5400) ranged from 0.78 to 536.39 minutes with a mean duration of (M = 33.13, SD = 35.95).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~rt_s, data = df_items) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of Item Response Time (seconds)\", \n       subtitle = \"fit by gamma distribution\", x = \"Item Response Time (seconds)\", y = \"% items\") \n\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\n\nWarning: Removed 5400 rows containing missing values (geom_vline).\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_items, x = \"rt_s\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#recode as boolean correct\ndf_items <- df_items %>% mutate(\n  score_niceABS = as.logical(score_niceABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_items, aes(x = pretty_condition, y = rt_s, color = score_niceABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    # position = position_dodgejust(),\n    justification = 1.5, \n    # adjust = .5, \n    width = .5, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA,\n    position = position_dodge2()\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitterdodge(\n      # seed = 1,\n      dodge.width = 0.5,\n      jitter.width = 0.075\n  )) +\n  labs( title = \"Distribution of Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\n\n\nCODE\n# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n                \n#STATSPLOT\nggbetweenstats(y = rt_s, x = pretty_condition, data = df_items,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )\n\n\n\n\n\n\n\nTime on Task\nHere we consider the time spent on the entire experimental task.\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- df_subjects %>% dplyr::select(rt_m) %>% unlist() %>% favstats()\ntitle = \"Descriptive Statistics of Total Task Response Latency (minutes)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Total Task Response Latency (minutes)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    1.19 \n    5.08 \n    6.85 \n    9.1 \n    26.8 \n    7.48 \n    3.52 \n    360 \n    0 \n  \n\n\n\n\n\nTotal time on task for subjects (n = 360) ranged from 1.19 to 26.82 minutes with a mean duration of (M = 7.48, SD = 3.52).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~rt_m, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of Total Response Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Total Response Time (minutes)\", y = \"% subjects\") \n\n\nWarning: Removed 360 rows containing missing values (geom_vline).\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"rt_m\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of Total Response Time (minutes)\",\n        subtitle =\"\",\n        x = \"Scaffold Phase Time (minutes)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = rt_m, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = rt_m),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = rt_m, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  ))+ labs( title = \"Distribution of Total Response Time (minutes)\",\n        subtitle =\"\",\n        y = \"Total Response Time (minutes)\", x = \"Condition\") +\n  theme_ggdist() + theme(legend.position = \"blank\") #+\n\n\n\n\n\nCODE\n  # coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n#STATSPLOT\nggbetweenstats(y = rt_m, x = pretty_condition, data = df_subjects,\n               type = \"nonparametric\", var.equal = FALSE,\n               pairwide.display = \"significant\", )"
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html#exploring-relationships",
    "href": "analysis/SGC4A/3_sgc4A_description.html#exploring-relationships",
    "title": "13  Description",
    "section": "EXPLORING RELATIONSHIPS",
    "text": "EXPLORING RELATIONSHIPS\n\nACCURACY (VS) LATENCY\n\nTotal Task\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ rt_m, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by TOTAL Item Response Time\",\n    subtitle = \"\", \n    x = \"Total Item Response Time (minutes)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_avg_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MEAN Item Response Time\",\n    subtitle = \"\", \n    x = \"Average Item Response Time (seconds)\", y = \"Total Scaled Score\"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_max_rt, data = df_subjects %>% filter(item_max_rt < 400), alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MAX Item Response Time\",\n    subtitle = \"\", \n    x = \"MAX Item Response Time (s)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#NOTE: LOG transforms of the RT do not yield linear relationships\n\n\n\n\nAverage Item RT by Accuracy\n\n\nCODE\nq.stats <- df_items %>% filter(q != 6) %>% dplyr::group_by(q, pretty_condition, score_niceABS) %>% dplyr::summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score_niceABS)\n)\n\ngf_line( m ~ q, group = ~group,  color = ~as.factor(score_niceABS),data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~q) %>% \n  gf_facet_wrap(~pretty_condition) + scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Absolute Score\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\n# df_items %>%\n#   ggplot(aes(y = rt_s, x = q,  fill = pretty_condition)) +\n#   stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + facet_wrap(~pretty_condition)\n\n\n\n\nCODE\nq.stats <- df_items %>% filter(q %nin% c(6,9)) %>% dplyr::group_by(q, pretty_condition, interpretation) %>% dplyr::summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score_SCALED)\n)\n\ngf_line( m ~ as.factor(q), group = ~group,  color = ~interpretation,data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~as.factor(q)) %>% \n  gf_facet_grid(interpretation~pretty_condition) + #+ scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Interpretation\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       caption=\"NOTE: Points with no ribbon indicate singular response\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Interpretation\")\n\n\ngeom_path: Each group consists of only one observation. Do you need to adjust\nthe group aesthetic?\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\ndf_items %>% filter(q %nin% c(6,9)) %>% mutate( interpretation = recode(interpretation, \"reference\" = \"blank\", \"frenzy\" = \"?\")) %>% \n  ggplot(aes(y = rt_s, x = q,  fill = interpretation)) +\n  stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + \n  facet_grid(interpretation ~ pretty_condition) + \n  labs( title = \"Average Response Time by Question Interpretation\", x = \"Question\", y=\"Averate Item Response Time (s)\")"
  },
  {
    "objectID": "analysis/SGC4A/3_sgc4A_description.html#resources",
    "href": "analysis/SGC4A/3_sgc4A_description.html#resources",
    "title": "13  Description",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] forcats_0.5.1      stringr_1.4.0      purrr_0.3.4        readr_2.1.2       \n [5] tidyr_1.2.0        tibble_3.1.7       tidyverse_1.3.1    performance_0.9.1 \n [9] fitdistrplus_1.1-8 MASS_7.3-57        multimode_1.5      ggeasy_0.1.3      \n[13] ggstatsplot_0.9.3  ggdist_3.1.1       ggpubr_0.4.0       vcd_1.4-10        \n[17] kableExtra_1.3.4   mosaic_1.8.3       ggridges_0.5.3     mosaicData_0.20.2 \n[21] ggformula_0.10.1   ggstance_0.3.5     dplyr_1.0.9        Matrix_1.4-1      \n[25] Hmisc_4.7-0        ggplot2_3.3.6      Formula_1.2-4      survival_3.3-1    \n[29] lattice_0.20-45   \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.2.2             ks_1.13.5              tidyselect_1.1.2      \n  [4] htmlwidgets_1.5.4      gmp_0.6-5              munsell_0.5.0         \n  [7] codetools_0.2-18       effectsize_0.7.0       withr_2.5.0           \n [10] colorspace_2.0-3       highr_0.9              knitr_1.39            \n [13] rstudioapi_0.13        ggsignif_0.6.3         labeling_0.4.2        \n [16] emmeans_1.7.5          polyclip_1.10-0        bit64_4.0.5           \n [19] farver_2.1.0           datawizard_0.4.1       coda_0.19-4           \n [22] vctrs_0.4.1            generics_0.1.2         TH.data_1.1-1         \n [25] xfun_0.31              BWStest_0.2.2          diptest_0.76-0        \n [28] R6_2.5.1               BayesFactor_0.9.12-4.4 cachem_1.0.6          \n [31] assertthat_0.2.1       scales_1.2.0           vroom_1.5.7           \n [34] multcomp_1.4-19        nnet_7.3-17            rootSolve_1.8.2.3     \n [37] gtable_0.3.0           multcompView_0.1-8     sandwich_3.0-2        \n [40] MatrixModels_0.5-0     rlang_1.0.3            zeallot_0.1.0         \n [43] systemfonts_1.0.4      PMCMRplus_1.9.5        splines_4.2.1         \n [46] rstatix_0.7.0          broom_0.8.0            prismatic_1.1.0       \n [49] mosaicCore_0.9.0       checkmate_2.1.0        yaml_2.3.5            \n [52] abind_1.4-5            modelr_0.1.8           crosstalk_1.2.0       \n [55] backports_1.4.1        tools_4.2.1            ellipsis_0.3.2        \n [58] RColorBrewer_1.1-3     ggdendro_0.1.23        Rcpp_1.0.8.3          \n [61] plyr_1.8.7             base64enc_0.1-3        rpart_4.1.16          \n [64] pbapply_1.5-0          correlation_0.8.1      zoo_1.8-10            \n [67] haven_2.5.0            ggrepel_0.9.1          cluster_2.1.3         \n [70] fs_1.5.2               magrittr_2.0.3         data.table_1.14.2     \n [73] lmtest_0.9-40          reprex_2.0.1           mvtnorm_1.1-3         \n [76] hms_1.1.1              patchwork_1.1.1        evaluate_0.15         \n [79] xtable_1.8-4           leaflet_2.1.1          jpeg_0.1-9            \n [82] mclust_5.4.10          readxl_1.4.0           gridExtra_2.3         \n [85] compiler_4.2.1         KernSmooth_2.23-20     crayon_1.5.1          \n [88] htmltools_0.5.2        tzdb_0.3.0             lubridate_1.8.0       \n [91] DBI_1.1.3              SuppDists_1.1-9.7      kSamples_1.2-9        \n [94] tweenr_1.0.2           dbplyr_2.2.1           boot_1.3-28           \n [97] car_3.1-0              cli_3.3.0              parallel_4.2.1        \n[100] insight_0.17.1         pkgconfig_2.0.3        statsExpressions_1.3.2\n[103] foreign_0.8-82         xml2_1.3.3             paletteer_1.4.0       \n[106] svglite_2.1.0          webshot_0.5.3          estimability_1.4      \n[109] rvest_1.0.2            distributional_0.3.0   digest_0.6.29         \n[112] parameters_0.18.1      pracma_2.3.8           rmarkdown_2.14        \n[115] cellranger_1.1.0       htmlTable_2.4.0        lifecycle_1.0.1       \n[118] jsonlite_1.8.0         carData_3.0-5          viridisLite_0.4.0     \n[121] fansi_1.0.3            labelled_2.9.1         pillar_1.7.0          \n[124] fastmap_1.1.0          httr_1.4.3             glue_1.6.2            \n[127] bayestestR_0.12.1      png_0.1-7              bit_4.0.4             \n[130] ggforce_0.3.3          stringi_1.7.6          rematch2_2.1.2        \n[133] latticeExtra_0.6-29    memoise_2.0.1          Rmpfr_0.8-9"
  },
  {
    "objectID": "analysis/SGC4A/4_sgc4A_hypotesting.html",
    "href": "analysis/SGC4A/4_sgc4A_hypotesting.html",
    "title": "14  Hypothesis Testing",
    "section": "",
    "text": "TODO\nThe purpose of this notebook is test the hypotheses that determined the designs of the SGC4A studies.\nResearch Questions\nExperimental Hypothesis\nNull Hypothesis"
  },
  {
    "objectID": "analysis/SGC4A/4_sgc4A_hypotesting.html#h1a-overall-task-accuracy",
    "href": "analysis/SGC4A/4_sgc4A_hypotesting.html#h1a-overall-task-accuracy",
    "title": "14  Hypothesis Testing",
    "section": "H1A | OVERALL TASK ACCURACY",
    "text": "H1A | OVERALL TASK ACCURACY\n\nSetup\n\n\nCODE\ndf_i = df_items %>% filter(q %nin% c(6,9)) %>% \n  dplyr::select(pretty_condition, accuracy, subject,q)\n\ndf_s <- df_subjects %>% \n   dplyr::select(pretty_condition, task_percent)\n\n\n\n\nDescribe\n\n\nCODE\n#:::::::: STACKED PROPORTIONAL BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~pretty_mode) + \n   labs(title = \"Overall Accuracy\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART BY QUESTION\ndf_items %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~q) +\n   labs(title = \"Accuracy by Question\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Q6 and Q9 are non-discriminative\")\n\n\n\n\n\nCODE\n#:::::::: FACETED HISTOGRAM\nstats = df_s %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(task_percent))\ngf_props(~task_percent,\n         fill = ~pretty_condition, data = df_s) %>%\n  # gf_facet_grid(pretty_condition ~ pretty_mode) %>%\n  gf_facet_grid(~pretty_condition) %>%\n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"% Correct\",\n       y = \"proportion of subjects\",\n       title = \"Overall Absolute Score (% Correct)\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total % Correct)\"\ntbl1 <- mosaic::favstats(~task_percent, data = df_s) \ntbl1 %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total % Correct)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n     \n    0 \n    0 \n    0 \n    0.154 \n    1 \n    0.184 \n    0.324 \n    480 \n    0 \n  \n\n\n\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION\"\ntbl2 <- mosaic::favstats(task_percent ~ pretty_condition, data = df_s) \ntbl2 %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION\n \n  \n    pretty_condition \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    Orth-Full \n    0 \n    0 \n    0 \n    0.077 \n    1 \n    0.169 \n    0.319 \n    150 \n    0 \n  \n  \n    Orth-Sparse \n    0 \n    0 \n    0 \n    0.154 \n    1 \n    0.202 \n    0.350 \n    88 \n    0 \n  \n  \n    Orth-Grid \n    0 \n    0 \n    0 \n    0.077 \n    1 \n    0.101 \n    0.233 \n    98 \n    0 \n  \n  \n    Tri-Sparse \n    0 \n    0 \n    0 \n    0.538 \n    1 \n    0.245 \n    0.355 \n    144 \n    0 \n  \n\n\n\n\n\n\n\nTESTS\n\nAligned Ranks Transformation\n\n\n\nCODE\nm.art = art(task_percent ~ pretty_condition, data = df_s)\nanova(m.art)\n\n\nAnalysis of Variance of Aligned Rank Transformed Data\n\nTable Type: Anova Table (Type III tests) \nModel: No Repeated Measures (lm)\nResponse: art(task_percent)\n\n                   Df Df.res F value Pr(>F)  \n1 pretty_condition  3    476  2.9176   0.03 *\n---\nSignif. codes:   0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\n\n\n\nKruskal Wallis Test\n\n\nCODE\n(k <- kruskal.test(df_s$task_percent ~ df_s$pretty_condition))\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  df_s$task_percent by df_s$pretty_condition\nKruskal-Wallis chi-squared = 9, df = 3, p-value = 0.03\n\n\n\n\nVisualize\n\n\nCODE\n(results <- statsExpressions::oneway_anova(data = df_s, \n          x = pretty_condition, y = task_percent,\n          type = \"nonparametric\", alternative = \"less\"))\n\n\n# A tibble: 1 × 15\n  parameter1   parameter2  statistic df.error p.value method effectsize estimate\n  <chr>        <chr>           <dbl>    <int>   <dbl> <chr>  <chr>         <dbl>\n1 task_percent pretty_con…      8.65        3  0.0343 Krusk… Epsilon2 …   0.0181\n# … with 7 more variables: conf.level <dbl>, conf.low <dbl>, conf.high <dbl>,\n#   conf.method <chr>, conf.iterations <int>, n.obs <int>, expression <list>\n\n\nCODE\n#:::::::: STATSPLOT | VIOLIN\nggbetweenstats(y = task_percent, x = pretty_condition, \n               data = df_s, type = \"nonparametric\")\n\n\n\n\n\n\n\n\nMIXED LOGISTIC REGRESSION\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.\n\nFit Model\nTODO model with item as random doesn’t converge unless use bobyqa optimizer. ::: {.cell}\n\nCODE\n## 0 | SETUP\n#confirm 13 items [all discriminating items]\nnrow(df_i) / nrow(df_s) == 13\n\n\n[1] TRUE\n\n\nCODE\n#confirm all factors \nis.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$pretty_condition) && is.factor(df_i$accuracy)\n\n\n[1] TRUE\n\n\nCODE\n## 1 | SETUP RANDOM INTERCEPT SUBJECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nprint(\"Empty fixed model\")\n\n\n[1] \"Empty fixed model\"\n\n\nCODE\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df_i) \n# summary(m0)\n\n#:: RANDOM INTERCEPT SUBJECT\nprint(\"Subject intercept random model\")\n\n\n[1] \"Subject intercept random model\"\n\n\nCODE\nmm.rS <- glmer(accuracy ~ (1|subject), data = df_i, family = \"binomial\")\n# summary(mm.rS)\n\n# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", m0$aic > AIC(logLik(mm.rS)))\n\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\n\nCODE\ntest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName  |    Model | df | df_diff |    Chi2 |      p\n--------------------------------------------------\nm0    |      glm |  1 |         |         |       \nmm.rS | glmerMod |  2 |       1 | 3019.81 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0\"\n\n\nCODE\n#:: RANDOM INTERCEPT SUBJECT + ITEM\nprint(\"Subject Intercept + Item intercept random model\")\n\n\n[1] \"Subject Intercept + Item intercept random model\"\n\n\nCODE\nmm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = \"binomial\",\n                control = glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)))\n#summary(mm.rSQ)\n\n# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)))\n\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\n\nCODE\ntest_lrt(mm.rS, mm.rSQ) #same as anova(m0, m1, test = \"Chi\")\n\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |   Chi2 |      p\n--------------------------------------------------\nmm.rS  | glmerMod |  2 |         |        |       \nmm.rSQ | glmerMod |  3 |       1 | 282.10 | < .001\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS, mm.rSQ))$p[2])\n\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\n\n[1] \"Likelihood Ratio test is significant? p =  2.61584550644328e-63\"\n\n\nCODE\n## 2 | ADD FIXED EFFECT CONDITION\n\nprint(\"FIXED Condition + Subject & Item random intercepts\")\n\n\n[1] \"FIXED Condition + Subject & Item random intercepts\"\n\n\nCODE\nmm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q) ,\n                data = df_i, family = binomial,\n                control = glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)))\n# summary(mm.CrSQ)\n\npaste(\"AIC decreases w/ new model\", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.CrSQ)) )\n\n\n[1] \"AIC decreases w/ new model FALSE\"\n\n\nCODE\ntest_lrt(mm.rSQ,mm.CrSQ) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName    |    Model | df | df_diff | Chi2 |     p\n------------------------------------------------\nmm.rSQ  | glmerMod |  3 |         |      |      \nmm.CrSQ | glmerMod |  6 |       3 | 2.55 | 0.466\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rSQ,mm.CrSQ))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.466121170373078\"\n\n:::\n\n\nDescribe\n\n\nCODE\n# best model\nm <- mm.CrSQ\n\n#::::::::: PRINT MODEL \n\nprint(\"PREDICTOR MODEL\")\n\n\n[1] \"PREDICTOR MODEL\"\n\n\nCODE\nsummary(m)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition + (1 | subject) + (1 | q)\n   Data: df_i\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 200000))\n\n     AIC      BIC   logLik deviance df.resid \n    2662     2702    -1325     2650     6234 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-10.213  -0.032  -0.010  -0.004   8.369 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 88.403   9.402   \n q       (Intercept)  0.658   0.811   \nNumber of obs: 6240, groups:  subject, 480; q, 13\n\nFixed effects:\n                            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                  -10.212      0.671  -15.22   <2e-16 ***\npretty_conditionOrth-Sparse    0.440      0.814    0.54     0.59    \npretty_conditionTri-Sparse     0.914      0.749    1.22     0.22    \npretty_conditionOrth-Grid     -0.311      0.780   -0.40     0.69    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) pr_O-S pr_T-S\nprtty_cnO-S -0.400              \nprtty_cnT-S -0.375  0.391       \nprtty_cnO-G -0.466  0.367  0.395\n\n\nCODE\nprint(\"SIGNIFICANCE TEST [non directional]\")\n\n\n[1] \"SIGNIFICANCE TEST [non directional]\"\n\n\nCODE\ncar::Anova(m, type = 2)\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: accuracy\n                 Chisq Df Pr(>Chisq)\npretty_condition  2.51  3       0.47\n\n\nCODE\n#:::::::: MANUAL ONE-SIDED SIGTEST \n#note: anova and chi square are always one-tailed, but that is independent of being one-sided\n#https://www.ibm.com/support/pages/can-one-get-one-tailed-tests-logistic-regression-dividing-significance-levels-half\n# one-sided (right tail) z test for B COEFFICIENT\n#SANITY CHECK 2-tailed test should match the model output\n# tt <- 2*pnorm(summary(m)$coefficients[2,3], lower.tail = F)\n# paste(\"p value for two-tailed test, null B = 0 : \",round(tt,5))\n# ot <- pnorm(summary(m)$coefficients[2,3], lower.tail = F)\n# paste(\"BUT we want a one  directional, null: B <= 0: \",round(ot,5))\n\n\n#TODO POSTHOCS\n#:::::::: INTERPRET COEFFICIENTS\n\n# se <- sqrt(diag(stats::vcov(m)))\n# table of estimates with 95% CI\n# paste(\"LOG ODDS\")\n# (tab <- cbind(Est = fixef(m), \n#               LL = fixef(m) - 1.96 * se, \n#               UL = fixef(m) + 1.96 * se))\n# paste(\"ODDS RATIOS\")\n# (e <- exp(tab))\n\n\n\n\nTODO Inference\nTODO REDO UPDATE THIS\nWe fit a mixed-effect binomial logistic regression model with random intercepts for subjects and items; to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(4): 11.02, p < 0.001); and the total explanatory power is substantial (conditional R2 = 0.82) and the part related to the fixed effects (marginal R2) is 0.02. Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 2.8 over the control condition \\(e^{\\beta_1}\\) = 2.80, 95% CI [1.54, 5.09], p < 0.001.\n\n\nVisualize\n\n\nCODE\n## | PLOT PARAMETERS \n\n#SJPLOT | MODEL | ODDS RATIO\nplot_model(m, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.95 ) + #manually adjusted for directional test   \n  labs(title = \"Model Estimate | Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\") #why no error bars? problem with model?\n\n\n\n\n\nCODE\n#EASYSTATS | MODEL | ODDS RATIO\nresult <- model_parameters(m, exponentiate = TRUE, component = \"all\")\nplot(result)\n\n\n\n\n\nCODE\n## | PLOT TESTS\n\nresult <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\nplot(result)\n\n\n\n\n\nCODE\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type=\"eff\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Prediction | Probability of Accurate Response\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#PLOT MODEL PREDICTION\nplot_model(m, type = \"pred\")[[1]] +\n  ylim(0,1) \n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nCODE\n  # labs(\n  #   title = \"Model Prediction | Probability of Accurate Response\",\n  #   subtitle = \"Impasse increases Probability of Correct Response\"\n  # )\n\n\n\n\nDiagnostics\n\n\nCODE\nprint(\"SANITY CHECK REPORTING\")\n\n\n[1] \"SANITY CHECK REPORTING\"\n\n\nCODE\n# report(m)\n\nprint(\"DIAGNOSTICS\")\n\n\n[1] \"DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m)\n\n\n\n\n\n\n\nTODO\nModel with random intercept of Question doesn’t want to converge unless I use the optimizer, which also throws warnings. Maybe jsut use random subject? (hint… yes…) Otherwise, try the bayesian version ##### BAYESIAN For some reason there are convergence difficulties. Try again with the brms version\n\n\nCODE\n#BAYESIAN MIXED VERSION\n\ndf <- df_i %>% mutate(\n  accuracy = as.integer(accuracy)\n)\n\n#TODO bernoulli or binomial?\nB.mm.acc.CrSQ <- brm( accuracy ~ pretty_condition + (1|subject) + (1|q),\n                 data = df_i,\n                 # family = \"binomial\",\n                 family = bernoulli(link = \"logit\"),\n                 chains = 4, iter = 2000, warmup = 1000,\n                 cores = 4, seed = 1234,\n                 backend = \"cmdstanr\",\n                 file =\"analysis/SGC4A/models/sgc4a_brms_B.mm.acc.CrSQ.rds\")\n\n\nsummary(B.mm.acc.CrSQ)\n\n\n Family: binomial \n  Links: mu = logit \nFormula: as.integer(accuracy) ~ pretty_condition + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 6240) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.11      0.04     0.05     0.19 1.00     1659     1997\n\n~subject (Number of levels: 480) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.70      0.04     0.63     0.77 1.00     1561     2218\n\nPopulation-Level Effects: \n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                       0.40      0.07     0.26     0.54 1.00     1192\npretty_conditionOrthMSparse     0.09      0.10    -0.11     0.29 1.00     1097\npretty_conditionTriMSparse      0.18      0.09     0.00     0.37 1.00     1070\npretty_conditionOrthMGrid      -0.17      0.10    -0.36     0.02 1.00     1246\n                            Tail_ESS\nIntercept                       1824\npretty_conditionOrthMSparse     2165\npretty_conditionTriMSparse      1844\npretty_conditionOrthMGrid       1992\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\nplot_model(B.mm.acc.CrSQ, show.values = TRUE)\n\n\n\n\n\nCODE\nplot(pd(B.mm.acc.CrSQ))\n\n\n\n\n\nCODE\n# plot(equivalence_test(B.mm.acc.CrSQ))\nplot_model(B.mm.acc.CrSQ, type = \"pred\")\n\n\nUsing the maximum response value as the number of trials.\n\n\nWarning: Using 'binomial' families without specifying 'trials' on the left-hand\nside of the model formula is deprecated.\n\n\nWarning in max(out$Y, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\nNote: uncertainty of error terms are not taken into account. You may want to use `rstantools::posterior_predict()`.\n\n\n$pretty_condition"
  },
  {
    "objectID": "analysis/SGC4A/4_sgc4A_hypotesting.html#h1a-overall-interpretation-state",
    "href": "analysis/SGC4A/4_sgc4A_hypotesting.html#h1a-overall-interpretation-state",
    "title": "14  Hypothesis Testing",
    "section": "H1A | OVERALL INTERPRETATION STATE",
    "text": "H1A | OVERALL INTERPRETATION STATE\n\n“orthogonal” [reference category] includes orthogonal and satisficing responses ==> indicates a primarily orthogonal state of coordinate system understanding\n“other” includes: blank, reference point, responses that can’t be classified (including selecting all datapoints), => indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly not orthogonal nor triangular\n“angular” includes ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n“triangular” includes correct triangular and ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n\n\n\n\n\n\n\n\nResearch Question\nDoes Ss in the impasse condition produce less orthogonal responses across questions?\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of producing unknown and triangle-like response states across all items\n\n\nData\n\ndata: df_items where q nin 6,9 (13 discriminant test phase items)\noutcome: state ( 3 level factor from high_interpretation )\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMIXED Multinomial (Logistic) Regression on state predicted by condition\n\nAlternative:\n\nMIXED Ordinal regression on state (doesn’t meet proportional odds assumption-I think)\nMIXED Multinomial or Ordinal regression on high_interpretation (some cells are 0, produces problems)\n\n\n\n\n\nSetup\n\n\nCODE\n#:::::::: PREP DATA\ndf_i = df_items %>% filter(q %nin% c(6,9)) %>% dplyr::select(q,subject,state,pretty_condition)\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~pretty_mode) + \n   labs(title = \"Interpretation across all Questions\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf_i %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~q) +\n   labs(title = \"Interpretation by Question\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\ntable(df_i$state, df_i$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n            \n             Orth-Full Orth-Sparse Tri-Sparse Orth-Grid    Sum\n  orthogonal    0.6728      0.6442     0.5743    0.7017 0.6439\n  other         0.1215      0.1241     0.1357    0.1664 0.1354\n  tri-like      0.0210      0.0210     0.0342    0.0235 0.0255\n  triangular    0.1846      0.2107     0.2559    0.1083 0.1952\n  Sum           1.0000      1.0000     1.0000    1.0000 1.0000\n\n\nCODE\n(t <- table(df_i$state, df_i$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n            \n             Orth-Full Orth-Sparse Tri-Sparse Orth-Grid  Sum\n  orthogonal      1312         737       1075       894 4018\n  other            237         142        254       212  845\n  tri-like          41          24         64        30  159\n  triangular       360         241        479       138 1218\n  Sum             1950        1144       1872      1274 6240\n\n\n\n\nMIXED MULTINOMIAL REGRESSION\nDoes condition affect the response state of of items across the task?\n\nTODO Fit Model [mblogit]\n\n\nCODE\n# TRY AGAIN... MAYBE HUNG? \n\n# #https://www.elff.eu/software/mclogit/manual/mblogit/\n# #\"baseline category logit\" model matches multinom()\n# \n# #check reference level \n# print(\"Categories (first is reference)\")\n# levels(df_i$state)\n# \n# #FIT EMPTY MODEL\n# # print(\"EMPTY MODEL\")\n# m.mbl0 <- mblogit(state ~ pretty_condition ,  #no random intercepts; fixed only model \n#                   data = df_i)\n# #summary(m.mbl0)\n# \n# #FIT PREDICTOR MODEL\n# # print(\"PREDICTOR MODEL\")\n# m.mbl1 <- mblogit(state ~ pretty_condition , \n#                   random = list( ~ 1|subject, ~1|q), \n#                   data = df_i)\n# # summary(m.mbl1)\n# \n# #COMPARE MODEL FIT\n# paste(\"AIC wth predictor is lower than empty model?\", AIC(m.mbl0) > AIC(m.mbl1))\n# test_lrt(m.mbl0, m.mbl1)\n# \n# #DESCRIBE MODEL\n# summary(m.mbl1)\n# \n# #INTERPRET COEFFICIENTS\n# cint <- confint(m.mbl1, level = 0.95)\n# print(\"ODDS RATIO\")\n# (e <- cbind( exp(coef(m.mbl1)), exp(cint))) #exponentiated, adjusted\n# \n# #PERFORMANCE\n# performance(m.mbl1)\n# \n# #TABLE\n# tab_model(m.mbl1, transform = \"exp\", title = \"Model Predicted Odds Ratio\")\n\n\n\n\nTODO Inference\n\n\nFit Model [brms]\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#BAYESIAN MIXED VERSION\nmm.cat.CrSQ <- brm( state ~ pretty_condition + (1|subject) + (1|q), \n                 data = df_i, \n                 family = \"categorical\",\n                 chains = 4, iter = 2000, warmup = 1000,\n                 cores = 4, seed = 1234,\n                 backend = \"cmdstanr\",\n                 file =\"analysis/SGC4A/models/sgc4a_brms_mixedcat_state.rds\")\n\n\n\n\nCODE\n#set model \nm <- mm.cat.CrSQ\nsummary(m)\n\n\n Family: categorical \n  Links: muother = logit; mutrilike = logit; mutriangular = logit \nFormula: state ~ pretty_condition + (1 | subject) + (1 | q) \n   Data: df_i (Number of observations: 6994) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~q (Number of levels: 13) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          1.65      0.36     1.10     2.51 1.00      936\nsd(mutrilike_Intercept)        2.41      0.70     1.40     4.15 1.00     1257\nsd(mutriangular_Intercept)     1.64      0.41     1.07     2.58 1.01     1172\n                           Tail_ESS\nsd(muother_Intercept)          1638\nsd(mutrilike_Intercept)        2219\nsd(mutriangular_Intercept)     1716\n\n~subject (Number of levels: 538) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(muother_Intercept)          1.83      0.10     1.63     2.03 1.00     1373\nsd(mutrilike_Intercept)        2.99      0.29     2.46     3.61 1.00     1085\nsd(mutriangular_Intercept)     5.27      0.32     4.65     5.92 1.00      928\n                           Tail_ESS\nsd(muother_Intercept)          2313\nsd(mutrilike_Intercept)        2028\nsd(mutriangular_Intercept)     1829\n\nPopulation-Level Effects: \n                                         Estimate Est.Error l-95% CI u-95% CI\nmuother_Intercept                           -2.70      0.51    -3.72    -1.66\nmutrilike_Intercept                         -6.71      0.91    -8.58    -4.97\nmutriangular_Intercept                      -4.46      0.69    -5.85    -3.12\nmuother_pretty_conditionOrthMSparse          0.12      0.28    -0.44     0.67\nmuother_pretty_conditionOrthMGrid            0.48      0.26    -0.03     0.99\nmuother_pretty_conditionTriMSparse           0.69      0.27     0.17     1.22\nmutrilike_pretty_conditionOrthMSparse        0.19      0.59    -0.97     1.32\nmutrilike_pretty_conditionOrthMGrid          0.12      0.57    -1.04     1.22\nmutrilike_pretty_conditionTriMSparse         1.49      0.53     0.48     2.53\nmutriangular_pretty_conditionOrthMSparse     0.21      0.74    -1.20     1.67\nmutriangular_pretty_conditionOrthMGrid      -1.43      0.77    -2.98     0.04\nmutriangular_pretty_conditionTriMSparse      1.54      0.69     0.26     2.97\n                                         Rhat Bulk_ESS Tail_ESS\nmuother_Intercept                        1.00      571     1157\nmutrilike_Intercept                      1.01      735     1277\nmutriangular_Intercept                   1.00      470     1017\nmuother_pretty_conditionOrthMSparse      1.00     1214     2044\nmuother_pretty_conditionOrthMGrid        1.00     1189     2075\nmuother_pretty_conditionTriMSparse       1.00     1237     1993\nmutrilike_pretty_conditionOrthMSparse    1.00     1606     2707\nmutrilike_pretty_conditionOrthMGrid      1.00     1531     2119\nmutrilike_pretty_conditionTriMSparse     1.00     1426     2399\nmutriangular_pretty_conditionOrthMSparse 1.01      440      764\nmutriangular_pretty_conditionOrthMGrid   1.01      497      729\nmutriangular_pretty_conditionTriMSparse  1.01      394      617\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCODE\n## | PLOT PARAMETERS \n\n#SJPLOT | MODEL | ODDS RATIO\n# plot_model(m, vline.color = \"red\", \n#            show.intercept = TRUE, \n#            show.values = TRUE,\n#            p.threshold = 0.1, #manually adjust to account for directional test\n#            ci.lvl = 0.90 ) + #manually adjusted for directional test   \n#   labs(title = \"Model Estimate | Odds Ratio\",\n#        subtitle = \"\",\n#        x = \"Condition\")\n\n\n#EASYSTATS | MODEL | ODDS RATIO\nresult <- model_parameters(m, exponentiate = TRUE, component = \"all\")\nplot(result)\n\n\n\n\n\nCODE\n# result <- simulate_parameters(m)\n# plot(result, stack = FALSE)\n\n#check posterior\npp_check(m, ndraws=1000)\n\n\n\n\n\nCODE\n## | PLOT TESTS\nresult <- equivalence_test(m, rule = \"classic\", ci=0.9) #classic[tost], , bayes\nplot(result)\n\n\nPicking joint bandwidth of 0.0783\n\n\nWarning: Removed 4800 rows containing non-finite values (stat_density_ridges).\n\n\n\n\n\nCODE\nresult <- rope(m)\nplot(result)\n\n\n\n\n\nCODE\nresult <- pd(m)\nplot(result)\n\n\n\n\n\nCODE\n## | PLOT PREDICTIONS\n\n#SJPLOT | MODEL | PROBABILITIES\n# plot_model(m, type=\"eff\",\n#            show.intercept = TRUE,\n#            show.values = TRUE,\n#            title = \"Model Prediction | Probability of Accurate Response\",\n#            axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n# \n# #PLOT MODEL PREDICTION\nplot_model(m, type = \"pred\")\n\n\nNote: uncertainty of error terms are not taken into account. You may want to use `rstantools::posterior_predict()`.\n\n\n$pretty_condition\n\n\n\n\n\nCODE\n#TODO OUTPUT TABLE \n#https://arelbundock.com/posts/modelsummary_multinomial_logit/\n# modelsummary(m)\n\n\n\n\nTODO Inference\n\n\nCOMPARE\n\n\nCODE\n# compare_models(m.mbl1, mm.cat.CrSQ)"
  },
  {
    "objectID": "analysis/SGC4A/4_sgc4A_hypotesting.html#h1b-q1-accuracy",
    "href": "analysis/SGC4A/4_sgc4A_hypotesting.html#h1b-q1-accuracy",
    "title": "14  Hypothesis Testing",
    "section": "H1B | Q1 ACCURACY",
    "text": "H1B | Q1 ACCURACY\nThe graph comprehension task includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their first exposure to the unconventional triangular coordinate system.\n\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\n\nHypothesis\nH1A | Ss in the TRI condition will have a higher likelihood of correctly responding to the first question than those in the CONTROL condition\n\n\nData\n\ndata: df_items where q == 1\noutcome: accuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nLogistic Regression on accuracy predicted by condition\n\naccount for difference in odds of correct score by condition\n\n\nAlternatives:\n\nChi-Square test of independence on outcome accuracy by condition\n\n\n\nNotes\n\nCHIQ SQR is simplest method to examine independence of two categorical factors; LOGISTIC REGRESSION is recommended for binomial ~ continuous; though with regression we can quantify the size of the effect and overall model fit\nindependence assumption : (CHI SQR) as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\ncell frequency : (CHI SQR) expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\n\n\n\nSetup\n\n\nCODE\n#:::::::: PREP DATA\ndf <- df_items %>% filter(q==1)  %>% dplyr::select(accuracy, pretty_condition)\n\n\n\n\nDescribe\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Q1 Accuracy\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#:::::::: BARSTATS\nggbarstats( data = df, y = pretty_condition, x = accuracy)\n\n\n\n\n\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\npaste(\"Proportions of Correct Responses by Condition\")\n\n\n[1] \"Proportions of Correct Responses by Condition\"\n\n\nCODE\ntable(df$accuracy, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            Orth-Full Orth-Sparse Tri-Sparse Orth-Grid    Sum\n  incorrect    0.9000      0.8864     0.8542    0.9592 0.8958\n  correct      0.1000      0.1136     0.1458    0.0408 0.1042\n  Sum          1.0000      1.0000     1.0000    1.0000 1.0000\n\n\nCODE\npaste(\"Number of Correct Responses by Condition\")\n\n\n[1] \"Number of Correct Responses by Condition\"\n\n\nCODE\ntable(df$accuracy, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1) #sanity check sum of columns\n\n\n           \n            Orth-Full Orth-Sparse Tri-Sparse Orth-Grid Sum\n  incorrect       135          78        123        94 430\n  correct          15          10         21         4  50\n  Sum             150          88        144        98 480\n\n\n\n\nLOGISTIC REGRESSION\nFit a logistic regression predicting accuracy (absolute score) (n = 480) by condition (k = 2).\n\n\nParameter estimate: \\(\\beta_{0}\\) = Log Odds of (correct) responses in CONTROL condition\n\\(e^{\\beta_{0}}\\) = ODDS of correct response in CONTROL condition\nParameter estimate: \\(\\beta_{1}\\) = \\(\\beta_{1impasse}\\) Log Odds (Log OR; change in odds for correct response in impasse (vs) control [log scale])\n\\(e^{\\beta_{1}}\\) = ODDS RATIO of correct response in IMPASSE (vs) CONTROL\nNull hypothesis:\\(\\beta_{impasse} \\le 0\\) the odds for a correct response does not change, or decreases\nAlternative hypothesis: \\(\\beta_{impasse} \\gt 0\\) the odds of a correct response increases\n\n\nFit Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\n\nCODE\n# MODEL FITTING ::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\n# print(\"EMPTY MODEL\")\n# summary(m0)\n\n#: 2 CONDITION model\nm1 <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\n# print(\"PREDICTOR MODEL\")\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.561  -0.561  -0.459  -0.289   2.529  \n\nCoefficients:\n                            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                   -2.197      0.272   -8.07  6.9e-16 ***\npretty_conditionOrth-Sparse    0.143      0.432    0.33    0.741    \npretty_conditionTri-Sparse     0.430      0.360    1.19    0.233    \npretty_conditionOrth-Grid     -0.960      0.578   -1.66    0.097 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 320.78  on 479  degrees of freedom\nResidual deviance: 312.90  on 476  degrees of freedom\nAIC: 320.9\n\nNumber of Fisher Scoring iterations: 5\n\n\nCODE\ncar::Anova(m1)\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: accuracy\n                 LR Chisq Df Pr(>Chisq)  \npretty_condition     7.88  3      0.049 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > m1$aic)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName | Model | df | df_diff | Chi2 |     p\n------------------------------------------\nm0   |   glm |  1 |         |      |      \nm1   |   glm |  4 |       3 | 7.88 | 0.049\n\n\nCODE\npaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n\n\n[1] \"Likelihood Ratio test is significant? p =  0.0486364443240912\"\n\n\nThe Condition predictor decreases AIC, but the Likelihood Ratio Test is marginal. We proceed to examine the predictor model, as we plan to do a 1-tailed NHST .\n\n\nDescribe\n\n\nCODE\n# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL [default two-tailed sig test]\")\n\n\n[1] \"PREDICTOR MODEL [default two-tailed sig test]\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.561  -0.561  -0.459  -0.289   2.529  \n\nCoefficients:\n                            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                   -2.197      0.272   -8.07  6.9e-16 ***\npretty_conditionOrth-Sparse    0.143      0.432    0.33    0.741    \npretty_conditionTri-Sparse     0.430      0.360    1.19    0.233    \npretty_conditionOrth-Grid     -0.960      0.578   -1.66    0.097 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 320.78  on 479  degrees of freedom\nResidual deviance: 312.90  on 476  degrees of freedom\nAIC: 320.9\n\nNumber of Fisher Scoring iterations: 5\n\n\nCODE\ncar::Anova(m1)\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: accuracy\n                 LR Chisq Df Pr(>Chisq)  \npretty_condition     7.88  3      0.049 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: \n\n# one-sided (right tail) z test for B COEFFICIENT\n#https://stats.stackexchange.com/questions/330655/strategy-for-a-one-sided-test-of-glms-coefficients\n\n#SANITY CHECK 2-tailed test should match the model output\ntt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"p value for two-tailed test, null B = 0 : \",round(tt,3))\n\n\n[1] \"p value for two-tailed test, null B = 0 :  0.741\"\n\n\nCODE\not <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)\npaste(\"BUT we want a one tailed directional, null: B <= 0: \",round(ot,3))\n\n\n[1] \"BUT we want a one tailed directional, null: B <= 0:  0.37\"\n\n\nCODE\npaste(\"adjusted confint for directional hypothesis\")\n\n\n[1] \"adjusted confint for directional hypothesis\"\n\n\nCODE\n(dcint <- confint(m1, level = 0.90)) # get 90% for right side))\n\n\nWaiting for profiling to be done...\n\n\n                               5 %    95 %\n(Intercept)                 -2.674 -1.7741\npretty_conditionOrth-Sparse -0.587  0.8464\npretty_conditionTri-Sparse  -0.158  1.0329\npretty_conditionOrth-Grid   -2.009 -0.0704\n\n\nCODE\n# https://stats.stackexchange.com/questions/20734/is-a-1-sided-90-prediction-interval-equivalent-to-a-2-sided-95-prediction-inte\n\n#:::::::: INTERPRET COEFFICIENTS\n\n# print(\"Confidence Interval —- LOG ODDS\")\n# confint(m1) #not adjusted for 1-tailed\nprint(\"Coefficients —- ODDS RATIOS\")\n\n\n[1] \"Coefficients —- ODDS RATIOS\"\n\n\nCODE\n# (e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiated, not adjusted\n(e <- cbind( exp(coef(m1)), exp(dcint))) #exponentiated, adjusted\n\n\n                                    5 %  95 %\n(Intercept)                 0.111 0.069 0.170\npretty_conditionOrth-Sparse 1.154 0.556 2.331\npretty_conditionTri-Sparse  1.537 0.853 2.809\npretty_conditionOrth-Grid   0.383 0.134 0.932\n\n\nCODE\nprint(\"MODEL PREDICTIONS\")\n\n\n[1] \"MODEL PREDICTIONS\"\n\n\nCODE\n# Retrieve predictions as probabilities \n# (for each level of the predictor)\n# pred.control <- predict(m1,data.frame(pretty_condition=\"control\"),type=\"response\")\n# #this should match : plogis(intercept coefficient)\n# paste(\"Probability of success in control,\", pred.control)\n# pred.impasse <- predict(m1,data.frame(pretty_condition=\"impasse\"),type=\"response\")\n# #this should match : plogis(intercept coefficient + predictor coeff)\n# paste(\"Probability of success in impasse,\", pred.impasse)\n\n\n\n\nTODO Inference\n\n\nVisualize\n\n\nCODE\n#SET MODEL\nm <- m1\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m, type = \"est\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) +  #manually adjusted for directional test   \n  scale_y_continuous() + #remove to put on log scale x axis \n  # scale_x_discrete(labels=c(\"control\",\"impasse\"))+\n  labs(title = \"MODEL ESTIMATE | Q1 Accuracy ~ condition\",\n       subtitle = \"Impasse increases odds of correct response on Q1\",\n       x = \"Condition\") + theme_clean()\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type=\"pred\")[[1]] +\n  ylim(0,1) + #scale y axis to actual range\n  labs(title = \"MODEL PREDICTION  | Q1 Accuracy ~ condition\",\n       subtitle = \"Impasse increases probability of correct response on Q1\",\n       x = \"Condition\") + theme_clean()\n\n\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.11\n0.06 – 0.18\n<0.001\n\n\npretty condition[Orth-Sparse]\n1.15\n0.48 – 2.67\n0.741\n\n\npretty condition[Tri-Sparse]\n1.54\n0.76 – 3.16\n0.233\n\n\npretty condition[Orth-Grid]\n0.38\n0.11 – 1.09\n0.097\n\n\nObservations\n480\n\n\nR2 Tjur\n0.015\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m1, \"(log odds)\" = m1)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', \n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n\n\n\nDiagnostics\n\n\nCODE\n# print(\"SANITY CHECK REPORTING\")\n# report::report(m)\n\n#print(\"MODEL PERFORMANCE\")\n# performance(m)\n\nprint(\"MODEL DIAGNOSTICS\")\n\n\n[1] \"MODEL DIAGNOSTICS\"\n\n\nCODE\ncheck_model(m)"
  },
  {
    "objectID": "analysis/SGC4A/4_sgc4A_hypotesting.html#h1b-q1-interpretation-state",
    "href": "analysis/SGC4A/4_sgc4A_hypotesting.html#h1b-q1-interpretation-state",
    "title": "14  Hypothesis Testing",
    "section": "H1B | Q1 INTERPRETATION STATE",
    "text": "H1B | Q1 INTERPRETATION STATE\nWhile absolute accuracy score tells us whether a participant successfully interpreted the coordinate system, it doesn’t allow us to differentiate between different kinds of incorrect interpretations. Here we examine the (categorical) interpretation state type based on the nature of subject’s response, and determine if these interpretations differ by experimental condition. State is a 3-category derived response variable that groups the following interpretations:\n\n“orthogonal” [reference category] includes orthogonal and satisficing responses ==> indicates a primarily orthogonal state of coordinate system understanding\n“other” includes: blank, reference point, responses that can’t be classified (including selecting all datapoints), => indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly not orthogonal nor triangular\n“angular” includes ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n“triangular” includes correct triangular and ‘lines connecting’ responses as well as responses that include both orthogonal and triangular answers => indicates some degree of angular/triangular coordinate understanding\n\n\n\n\n\n\n\n\nResearch Question\nDoes Ss in the impasse condition produce less orthogonal responses on the first question?\n\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of producing unknown and and triangle-like response states, relative to orthogonal response states, on the first question\n\n\nData\n\ndata: df_items where q == 1\noutcome: state ( 4 level factor from 5 level high_interpretation )\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMultinomial (Logistic) Regression on state predicted by condition\n\nAlternative:\n\nOrdinal regression on state; but model doesn’t satisfy proportional odds assumption (parallel slopes)\nMultinomial or Ordinal regression on high_interpretation (5 category interpretation state which distinguishes between uncertain (blank, reference) unclassifiable, triangle-like and true triangular.) There are some cells with zeros, however (no uncertain responses in control) which means the model can’t accurately estimate those comparisons\n\n\n\n\n\nSetup\n\n\nCODE\n#:::::::: PREP DATA\ndf <- df_items %>% filter(q==1) %>% dplyr::select(pretty_condition, state)\n\n\n\n\nDescribe\n\n\nCODE\n#:::::::: STACKED BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = state)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  # facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Q1 Interpretation\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"\")\n\n\n\n\n\nCODE\n#:::::::: BARSTATS\nggbarstats( data = df, y = pretty_condition, x = state)\n\n\n\n\n\n\n\nCODE\n#::::::::::::DESCRIPTIVES\n\ntable(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      prop.table(margin=2) %>%  #return proportion (of column)\n      addmargins(1) #sanity check sum of columns\n\n\n            \n             Orth-Full Orth-Sparse Tri-Sparse Orth-Grid     Sum\n  orthogonal   0.86000     0.81818    0.72917   0.91837 0.82500\n  other        0.02667     0.03409    0.03472   0.02041 0.02917\n  tri-like     0.00667     0.03409    0.09028   0.02041 0.03958\n  triangular   0.10667     0.11364    0.14583   0.04082 0.10625\n  Sum          1.00000     1.00000    1.00000   1.00000 1.00000\n\n\nCODE\n(t <- table(df$state, df$pretty_condition) %>% addmargins(2) %>% #display sum for row\n      addmargins(1)) #sanity check sum of columns\n\n\n            \n             Orth-Full Orth-Sparse Tri-Sparse Orth-Grid Sum\n  orthogonal       129          72        105        90 396\n  other              4           3          5         2  14\n  tri-like           1           3         13         2  19\n  triangular        16          10         21         4  51\n  Sum              150          88        144        98 480\n\n\n\n\nMULTINOMIAL REGRESSION\n\nFit Model\n\n\nCODE\n#check reference level \nprint(\"Categories (first is reference)\")\n\n\n[1] \"Categories (first is reference)\"\n\n\nCODE\nlevels(df$state)\n\n\n[1] \"orthogonal\" \"other\"      \"tri-like\"   \"triangular\"\n\n\nCODE\n#FIT EMPTY MODEL\n# print(\"EMPTY MODEL\")\ncatm.0 <- multinom(state ~ 1, data = df)\n\n\n# weights:  8 (3 variable)\ninitial  value 665.421293 \niter  10 value 301.363054\niter  10 value 301.363052\niter  10 value 301.363052\nfinal  value 301.363052 \nconverged\n\n\nCODE\n# summary(catm.0)\n\n#FIT PREDICTOR MODEL\n# print(\"PREDICTOR MODEL\")\ncatm <- multinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\n\n# weights:  20 (12 variable)\ninitial  value 665.421293 \niter  10 value 289.824282\niter  20 value 288.928010\nfinal  value 288.928004 \nconverged\n\n\nCODE\n# summary(catm)\n\n#COMPARE MODEL FIT\npaste(\"AIC wth predictor is lower than empty model?\", catm.0$AIC > catm$AIC)\n\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\n\nCODE\ntest_lrt(catm.0, catm)\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   |    Model | df | df_diff |  Chi2 |     p\n------------------------------------------------\ncatm.0 | multinom |  3 |         |       |      \ncatm   | multinom | 12 |       9 | 24.87 | 0.003\n\n\nCODE\n##compare bayesian version\n#library(brms)\n# b.cat <- brm( state2 ~ pretty_condition, data = df, family = \"categorical\", backend = \"cmdstanr\")\n# summary(b.cat)\n# plot_model(b.cat)\n# report(b.cat)\n# coefficient estimates are very simliar to catm. super cool!\n\n##compare mclogit version\n#\"baseline-category logit model\n# https://www.elff.eu/software/mclogit/manual/mblogit/\n# blm1 <- mblogit(state2 ~ pretty_condition , data = df)\n# summary(blm1)\n#identical to catm. super cool!\n\n\nAIC in predictor model is less than empty model, and likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.\n\n\nDescribe\n\n\nCODE\n#set model\nm <- catm\n\n#::::::::INTERPRETATION\npaste(\"MODEL SUMMARY\")\n\n\n[1] \"MODEL SUMMARY\"\n\n\nCODE\nsummary(m)\n\n\nCall:\nmultinom(formula = state ~ pretty_condition, data = df, model = TRUE)\n\nCoefficients:\n           (Intercept) pretty_conditionOrth-Sparse pretty_conditionTri-Sparse\nother            -3.47                       0.295                      0.429\ntri-like         -4.86                       1.682                      2.771\ntriangular       -2.09                       0.113                      0.478\n           pretty_conditionOrth-Grid\nother                         -0.333\ntri-like                       1.053\ntriangular                    -1.026\n\nStd. Errors:\n           (Intercept) pretty_conditionOrth-Sparse pretty_conditionTri-Sparse\nother            0.508                       0.778                      0.684\ntri-like         1.004                       1.164                      1.046\ntriangular       0.265                       0.429                      0.357\n           pretty_conditionOrth-Grid\nother                          0.877\ntri-like                       1.232\ntriangular                     0.576\n\nResidual Deviance: 578 \nAIC: 602 \n\n\nCODE\ncar::Anova(m)\n\n\n# weights:  8 (3 variable)\ninitial  value 665.421293 \niter  10 value 301.363054\niter  10 value 301.363052\niter  10 value 301.363052\nfinal  value 301.363052 \nconverged\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: state\n                 LR Chisq Df Pr(>Chisq)   \npretty_condition     24.9  9     0.0031 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n# calculate z-statistics of coefficients\nz_stats <- summary(m)$coefficients/summary(m)$standard.errors\n# convert to p-values\np_values <- (1 - pnorm(abs(z_stats)))*2\n# display p-values in transposed data frame\n(p_values <- data.frame(p = (p_values)))\n\n\n           p..Intercept. p.pretty_conditionOrth.Sparse\nother           7.82e-12                         0.704\ntri-like        1.29e-06                         0.149\ntriangular      3.33e-15                         0.792\n           p.pretty_conditionTri.Sparse p.pretty_conditionOrth.Grid\nother                           0.53022                      0.7039\ntri-like                        0.00808                      0.3928\ntriangular                      0.18066                      0.0746\n\n\nCODE\n# display odds ratios in transposed data frame\n\npaste(\"ODDS RATIOS\")\n\n\n[1] \"ODDS RATIOS\"\n\n\nCODE\nodds_ratios <- data.frame(OR = exp(summary(m)$coefficients))\noptions(scipen = 2)\n(results <- cbind(odds_ratios, p_values))\n\n\n           OR..Intercept. OR.pretty_conditionOrth.Sparse\nother             0.03101                           1.34\ntri-like          0.00775                           5.37\ntriangular        0.12403                           1.12\n           OR.pretty_conditionTri.Sparse OR.pretty_conditionOrth.Grid\nother                               1.54                        0.717\ntri-like                           15.97                        2.867\ntriangular                          1.61                        0.358\n           p..Intercept. p.pretty_conditionOrth.Sparse\nother           7.82e-12                         0.704\ntri-like        1.29e-06                         0.149\ntriangular      3.33e-15                         0.792\n           p.pretty_conditionTri.Sparse p.pretty_conditionOrth.Grid\nother                           0.53022                      0.7039\ntri-like                        0.00808                      0.3928\ntriangular                      0.18066                      0.0746\n\n\n\n\nTODO Inference\n\n\nVisualize\n\n\nCODE\n#:::::::: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m, type = \"est\",\n           vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE,\n           p.threshold = 0.1, #manually adjust to account for directional test\n           ci.lvl = 0.90 ) +  #manually adjusted for directional test   \n  # scale_y_continuous() + #remove to put on log scale x axis \n  # scale_x_discrete(labels=c(\"control\",\"impasse\"))+\n  labs(title = \"MODEL ESTIMATE  | Q1 Accuracy ~ condition\",\n       subtitle = \"Impasse increases odds of correct response on Q1\",\n       x = \"Condition\") + theme_clean()\n\n\n\n\n\nCODE\n#SJPLOT | MODEL | PROBABILITIES\nplot_model(m, type=\"eff\", ci.lvl = 0.95)[[1]] +\n  ylim(0,1) +\n  labs(title = \"MODEL PREDICTION  | Q1 State ~ condition\",\n       subtitle = \"Impasse increases probability of more accurate response states Q1\",\n       x = \"Condition\") + theme_clean()\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nCODE\n# #MANUALLY BUILD PREDICTION PLOT FACET BY CONDITION RATHER THAN STATE\n# p <-plot_model(m, type=\"eff\")[[1]]\n# d <- ggplot_build(p)[[1]]  \n# points <- d[[2]]\n# points <- points %>% mutate(\n#   state = recode(PANEL, \"1\" =\"orth\", \"2\"=\"other\", \"3\" = \"trilike\", \"4\"=\"tri\"),\n#   condition = recode(x, \"1\"=\"control\",\"2\"=\"impasse\"),\n#   prob = y\n# )\n# gf_point( prob ~ state, group = ~x, data = points) + \n#   geom_errorbar(aes( x = state, ymin = ymin, ymax = ymax)) + facet_grid(~condition) +ylim(0,1)\n\n\n\n\nCODE\n#SJPLOT | MODEL | TABLE\ntab_model(m)\n\n\n\n\n \nstate\n\n\nPredictors\nOdds Ratios\nCI\np\nResponse\n\n\n(Intercept)\n0.03\n0.01 – 0.08\n<0.001\nother\n\n\npretty condition[Orth-Sparse]\n1.34\n0.29 – 6.20\n0.704\nother\n\n\npretty condition[Tri-Sparse]\n1.54\n0.40 – 5.88\n0.531\nother\n\n\npretty condition[Orth-Grid]\n0.72\n0.13 – 4.01\n0.704\nother\n\n\n(Intercept)\n0.01\n0.00 – 0.06\n<0.001\ntri-like\n\n\npretty condition[Orth-Sparse]\n5.37\n0.55 – 52.94\n0.149\ntri-like\n\n\npretty condition[Tri-Sparse]\n15.97\n2.04 – 124.76\n0.008\ntri-like\n\n\npretty condition[Orth-Grid]\n2.87\n0.25 – 32.30\n0.393\ntri-like\n\n\n(Intercept)\n0.12\n0.07 – 0.21\n<0.001\ntriangular\n\n\npretty condition[Orth-Sparse]\n1.12\n0.48 – 2.60\n0.792\ntriangular\n\n\npretty condition[Tri-Sparse]\n1.61\n0.80 – 3.25\n0.181\ntriangular\n\n\npretty condition[Orth-Grid]\n0.36\n0.12 – 1.11\n0.075\ntriangular\n\n\nObservations\n480\n\n\nR2 / R2 adjusted\n0.041 / 0.038\n\n\n\n\n\n\nCODE\n# #MODEL SUMMARY | save latex table\n# models <- list(\"odds ratios\" = m1, \"(log odds)\" = m1)\n# notes = list(\"* p < 0.05, ** p < 0.01, *** p < 0.001\",\n#              '$sigma^{2}$ = 3.29\" N(subject) = 126 $\\tau_{00}$(subject) = 22.22 N(question) = 13 $\\tau_{00}$(question) = 0.31'\n#                )\n# \n# modelsummary(models,\n#              exponentiate = c(TRUE, FALSE),\n#              shape = term ~ model + statistic,\n#              fmt = 2, #two digits w/ trailing zero\n#              estimate  = \"{estimate} {stars}\",\n#              statistic = \"conf.int\",\n#              gof_map = c(\"AIC\", \"sigma\"),\n#              gof_omit = 'RMSE|ICC|BIC',\n#              coef_rename = c(\"pretty_conditionimpasse\" = \"Condition[impasse]\"),\n#              title = 'Accuracy ~ Condition (Mixed Logistic Regression)', \n#              notes = notes,\n#              output = \"analysis/SGC3A/models/tables/GLMER_OverallAccuracy_lab.tex\")\n# #              # coef_omit = \"Intercept\",\n\n# modelsummary(mixcat.1, s)\n#TODO OUTPUT TABLE \n#https://arelbundock.com/posts/modelsummary_multinomial_logit/\n\n\n\n\nDiagnostics\n\n\nCODE\n#EXAMINE PREDICTIONS\n#create sample data frame\n# test <- data.frame(pretty_condition = c(\"control\", \"impasse\"))\n# pred <- predict(catm, newdata = test, \"probs\")\n# paste(\"Predicted Probability of Being in Each State\")\n# ( x <- cbind(test, pred))\n\nprint(\"MODEL PERFORMANCE\")\n\n\n[1] \"MODEL PERFORMANCE\"\n\n\nCODE\nperformance(catm)\n\n\nCan't calculate log-loss.\nCan't calculate proper scoring rules for ordinal, multinomial or cumulative link models.\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n-----------------------------------------------------\n601.856 | 651.941 | 0.041 |     0.038 | 0.273 | 1.111\n\n\nCODE\nDescTools::PseudoR2(catm, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n\n\n  McFadden   CoxSnell Nagelkerke \n    0.0413     0.0505     0.0706 \n\n\nCODE\n#General Goodness of Fit\n#library(generalhoslem)\n#logitgof(df$state, catm$fitted.values, g = 3)\n#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).\n#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables\n# chisq.test(df$state, predict(catm)) #actual states VS predicted states\n# The chi-square test tests the decrease in unexplained variance from the baseline model to the final model\n\n# print(\"MODEL DIAGNOSTICS\")\n# check_model(m) can't do overall diagnostics, have to do them on individual model equations"
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html",
    "title": "15  Introduction",
    "section": "",
    "text": "In Study 4B we explore the extent to which the design of the marks indicating data points influence how a reader interprets its underlying coordinate system."
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html#methods",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html#methods",
    "title": "15  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 3 levels (Mark: POINT, CROSS, ARROW) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Mark Design: Point, Arrow, Cross )\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 19.1. The list of questions can be found here.\n\n\n\nFigure 15.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nIn each experimental\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a Triangular Model (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items: The Graph Comprehension Task.\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData were collected by convenience sample of a university subject pool during the winter of 2022. Participants accessed the study via a web browser (asynchronously). The stimulus application required the participant stay in full-screen mode for the entirety of the study."
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html#analysis",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html#analysis",
    "title": "15  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nwinter2022_clean_sgc4b.Rmd\n2_sgc4B_scoring.qmd\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#import file\ndf_subjects <- read_rds(\"analysis/SGC4B/data/0-session-level/sgc4b_participants.rds\") #use RDS file as it contains metadata\n\n#NO EXPLANATION COLUMN IN SGC4B DATASET; TRIAL NOT COLLECTED \n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\n# df_q16 <- df_subjects %>% \n#   select(subject, condition, term , mode, explanation) %>% \n#   mutate(\n#     q = 16,\n#     response = explanation\n#   ) %>% select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\n#drop absolute score because we rescore in 2_scoring\ndf_subjects <- df_subjects %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, study, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m,\n                 #exploratory factors\n                 violations, browser, width, height\n                 )\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#set factors\ndf_subjects <- df_subjects %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#read datafiles\ndf_items <- read_rds(\"analysis/SGC4B/data/0-session-level/sgc4b_items.rds\") #use RDS file as it contains metadata\n\n#reduce data collected using new webapp\ndf_items <- df_items %>% \n  select(subject, condition, pretty_condition, study, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  ) %>% \n  mutate(\n    response = str_remove_all(as.character(answer), \",\"),\n    num_o = str_length(response)\n  )%>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4B/data/1-study-level/sgc4b_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4B/data/1-study-level/sgc4b_items.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4B/data/1-study-level/sgc4b_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4B/data/1-study-level/sgc4b_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html#resources",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html#resources",
    "title": "15  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] highr_0.9         httr_1.4.3        pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] munsell_0.5.0     broom_0.8.0       compiler_4.2.1    modelr_0.1.8     \n[29] xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2  \n[33] tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3       viridisLite_0.4.0\n[37] crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1      withr_2.5.0      \n[41] grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1  \n[45] DBI_1.1.3         magrittr_2.0.3    scales_1.2.0      zip_2.2.0        \n[49] cli_3.3.0         stringi_1.7.6     fs_1.5.2          xml2_1.3.3       \n[53] ellipsis_0.3.2    generics_0.1.2    vctrs_0.4.1       openxlsx_4.2.5   \n[57] tools_4.2.1       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n[61] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[65] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html",
    "title": "16  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC4B study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#score-sgc-data",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#score-sgc-data",
    "title": "16  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#backup <- read_rds('analysis/SGC4B/data/1-study-level/sgc4b_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC4B/data/1-study-level/sgc4b_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#summarize-by-subject",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#summarize-by-subject",
    "title": "16  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC4B/data/1-study-level/sgc4b_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#explore-distributions",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#explore-distributions",
    "title": "16  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  pretty_condition = pretty_condition,\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(pretty_condition~q) +\n  labs( x = \"Absolute Score\",\n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>%\ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\",\n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(q~pretty_condition) +\n  labs( x = \"Scaled Score\",\n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) +\n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>%\n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\",\n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#peeking",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#peeking",
    "title": "16  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\nm1 <- lm(s_SCALED ~ pretty_condition, data = df_subjects)\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ pretty_condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8.55  -5.55  -3.72   1.78  20.62 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             -7.621      0.891   -8.55  9.2e-16 ***\npretty_conditioncross    1.344      1.290    1.04    0.299    \npretty_conditionarrow    3.172      1.237    2.56    0.011 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.5 on 269 degrees of freedom\nMultiple R-squared:  0.0241,    Adjusted R-squared:  0.0168 \nF-statistic: 3.32 on 2 and 269 DF,  p-value: 0.0376\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n                  Df Sum Sq Mean Sq F value Pr(>F)  \npretty_condition   2    480   240.0    3.32  0.038 *\nResiduals        269  19434    72.2                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with pretty_condition (formula: s_SCALED ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.02, F(2, 269) = 3.32, p = 0.038, adj. R2 = 0.02). The model's intercept, corresponding to pretty_condition = point, is at -7.62 (95% CI [-9.38, -5.87], t(269) = -8.55, p < .001). Within this model:\n\n  - The effect of pretty condition [cross] is statistically non-significant and positive (beta = 1.34, 95% CI [-1.20, 3.88], t(269) = 1.04, p = 0.299; Std. beta = 0.16, 95% CI [-0.14, 0.45])\n  - The effect of pretty condition [arrow] is statistically significant and positive (beta = 3.17, 95% CI [0.74, 5.61], t(269) = 2.56, p = 0.011; Std. beta = 0.37, 95% CI [0.09, 0.65])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#export",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#export",
    "title": "16  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4B/data/2-scored-data/sgc4b_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4B/data/2-scored-data/sgc4b_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC4B/data/2-scored-data/sgc4b_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC4B/data/2-scored-data/sgc4b_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4B/data/2-scored-data/sgc4b_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4B/data/2-scored-data/sgc4b_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#resources",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#resources",
    "title": "16  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n  [1] TH.data_1.1-1       colorspace_2.0-3    rio_0.5.29         \n  [4] ellipsis_0.3.2      estimability_1.4    htmlTable_2.4.0    \n  [7] parameters_0.18.1   base64enc_0.1-3     fs_1.5.2           \n [10] rstudioapi_0.13     farver_2.1.0        bit64_4.0.5        \n [13] fansi_1.0.3         mvtnorm_1.1-3       lubridate_1.8.0    \n [16] xml2_1.3.3          codetools_0.2-18    splines_4.2.1      \n [19] knitr_1.39          polyclip_1.10-0     jsonlite_1.8.0     \n [22] broom_0.8.0         cluster_2.1.3       dbplyr_2.2.1       \n [25] png_0.1-7           effectsize_0.7.0    ggforce_0.3.3      \n [28] compiler_4.2.1      httr_1.4.3          emmeans_1.7.5      \n [31] backports_1.4.1     assertthat_0.2.1    Matrix_1.4-1       \n [34] fastmap_1.1.0       cli_3.3.0           tweenr_1.0.2       \n [37] htmltools_0.5.2     tools_4.2.1         coda_0.19-4        \n [40] gtable_0.3.0        glue_1.6.2          Rcpp_1.0.8.3       \n [43] cellranger_1.1.0    vctrs_0.4.1         svglite_2.1.0      \n [46] insight_0.17.1      xfun_0.31           openxlsx_4.2.5     \n [49] rvest_1.0.2         lifecycle_1.0.1     mosaicCore_0.9.0   \n [52] zoo_1.8-10          MASS_7.3-57         vroom_1.5.7        \n [55] hms_1.1.1           sandwich_3.0-2      parallel_4.2.1     \n [58] RColorBrewer_1.1-3  curl_4.3.2          yaml_2.3.5         \n [61] gridExtra_2.3       labelled_2.9.1      rpart_4.1.16       \n [64] latticeExtra_0.6-29 stringi_1.7.6       bayestestR_0.12.1  \n [67] checkmate_2.1.0     zip_2.2.0           rlang_1.0.3        \n [70] pkgconfig_2.0.3     systemfonts_1.0.4   evaluate_0.15      \n [73] htmlwidgets_1.5.4   labeling_0.4.2      bit_4.0.4          \n [76] tidyselect_1.1.2    plyr_1.8.7          magrittr_2.0.3     \n [79] R6_2.5.1            generics_0.1.2      multcomp_1.4-19    \n [82] DBI_1.1.3           pillar_1.7.0        haven_2.5.0        \n [85] foreign_0.8-82      withr_2.5.0         datawizard_0.4.1   \n [88] nnet_7.3-17         modelr_0.1.8        crayon_1.5.1       \n [91] utf8_1.2.2          tzdb_0.3.0          rmarkdown_2.14     \n [94] jpeg_0.1-9          grid_4.2.1          readxl_1.4.0       \n [97] data.table_1.14.2   reprex_2.0.1        digest_0.6.29      \n[100] webshot_0.5.3       xtable_1.8-4        munsell_0.5.0      \n[103] viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html",
    "title": "17  Introduction",
    "section": "",
    "text": "TODO UPDATE\nIn Study 4C we explore the extent to which the orientation of the axes in space influence how a reader interprets its underlying coordinate system."
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html#methods",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html#methods",
    "title": "17  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 3 levels (Mark: POINT, CROSS, ARROW) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Mark Design: Point, Arrow, Cross )\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 19.1. The list of questions can be found here.\n\n\n\nFigure 17.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nIn each experimental\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a Triangular Model (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items: The Graph Comprehension Task.\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData were collected by convenience sample of a university subject pool during the winter of 2022. Participants accessed the study via a web browser (asynchronously). The stimulus application required the participant stay in full-screen mode for the entirety of the study."
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html#analysis",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html#analysis",
    "title": "17  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\n\n\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#import file\ndf_subjects <- read_rds(\"analysis/SGC4C/data/0-session-level/sgc4c_participants.rds\") #use RDS file as it contains metadata\n\n#NO EXPLANATION COLUMN IN SGC4c DATASET; TRIAL NOT COLLECTED \n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\n# df_q16 <- df_subjects %>% \n#   select(subject, condition, term , mode, explanation) %>% \n#   mutate(\n#     q = 16,\n#     response = explanation\n#   ) %>% select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\n#drop absolute score because we rescore in 2_scoring\ndf_subjects <- df_subjects %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, study, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m,\n                 #exploratory factors\n                 violations, browser, width, height\n                 )\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#set factors\ndf_subjects <- df_subjects %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#read datafiles\ndf_items <- read_rds(\"analysis/SGC4C/data/0-session-level/sgc4c_items.rds\") #use RDS file as it contains metadata\n\n#reduce data collected using new webapp\ndf_items <- df_items %>% \n  select(subject, condition, pretty_condition, study, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  ) %>% \n  mutate(\n    response = str_remove_all(as.character(answer), \",\"),\n    num_o = str_length(response)\n  )%>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4C/data/1-study-level/sgc4c_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4C/data/1-study-level/sgc4c_items.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4C/data/1-study-level/sgc4c_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4C/data/1-study-level/sgc4c_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html#resources",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html#resources",
    "title": "17  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] highr_0.9         httr_1.4.3        pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] munsell_0.5.0     broom_0.8.0       compiler_4.2.1    modelr_0.1.8     \n[29] xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2  \n[33] tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3       viridisLite_0.4.0\n[37] crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1      withr_2.5.0      \n[41] grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1  \n[45] DBI_1.1.3         magrittr_2.0.3    scales_1.2.0      zip_2.2.0        \n[49] cli_3.3.0         stringi_1.7.6     fs_1.5.2          xml2_1.3.3       \n[53] ellipsis_0.3.2    generics_0.1.2    vctrs_0.4.1       openxlsx_4.2.5   \n[57] tools_4.2.1       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n[61] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[65] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html",
    "title": "18  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC4C study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#score-sgc-data",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#score-sgc-data",
    "title": "18  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#backup <- read_rds('analysis/SGC4C/data/1-study-level/sgc4c_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC4C/data/1-study-level/sgc4c_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#summarize-by-subject",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#summarize-by-subject",
    "title": "18  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC4C/data/1-study-level/sgc4c_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#explore-distributions",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#explore-distributions",
    "title": "18  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  pretty_condition = pretty_condition,\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(pretty_condition~q) +\n  labs( x = \"Absolute Score\",\n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>%\ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\",\n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(q~pretty_condition) +\n  labs( x = \"Scaled Score\",\n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) +\n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>%\n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\",\n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#peeking",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#peeking",
    "title": "18  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\nm1 <- lm(s_SCALED ~ pretty_condition, data = df_subjects)\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ pretty_condition, data = df_subjects)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.933  -6.700   0.183   7.067  16.800 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)  \n(Intercept)                        4.43       2.45    1.81    0.081 .\npretty_conditionORTH-rotate-45    -8.23       3.46   -2.38    0.025 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.48 on 28 degrees of freedom\nMultiple R-squared:  0.168, Adjusted R-squared:  0.138 \nF-statistic: 5.65 on 1 and 28 DF,  p-value: 0.0245\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n                 Df Sum Sq Mean Sq F value Pr(>F)  \npretty_condition  1    508     508    5.65  0.025 *\nResiduals        28   2519      90                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with pretty_condition (formula: s_SCALED ~ pretty_condition). The model explains a statistically significant and moderate proportion of variance (R2 = 0.17, F(1, 28) = 5.65, p = 0.025, adj. R2 = 0.14). The model's intercept, corresponding to pretty_condition = TRI-rotate-45, is at 4.43 (95% CI [-0.58, 9.45], t(28) = 1.81, p = 0.081). Within this model:\n\n  - The effect of pretty condition [ORTH-rotate-45] is statistically significant and negative (beta = -8.23, 95% CI [-15.33, -1.14], t(28) = -2.38, p = 0.025; Std. beta = -0.81, 95% CI [-1.50, -0.11])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#export",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#export",
    "title": "18  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4C/data/2-scored-data/sgc4c_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4C/data/2-scored-data/sgc4c_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC4C/data/2-scored-data/sgc4c_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC4C/data/2-scored-data/sgc4c_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4C/data/2-scored-data/sgc4c_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4C/data/2-scored-data/sgc4c_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4C/2_sgc4C_scoring.html#resources",
    "href": "analysis/SGC4C/2_sgc4C_scoring.html#resources",
    "title": "18  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n  [1] TH.data_1.1-1       colorspace_2.0-3    rio_0.5.29         \n  [4] ellipsis_0.3.2      estimability_1.4    htmlTable_2.4.0    \n  [7] parameters_0.18.1   base64enc_0.1-3     fs_1.5.2           \n [10] rstudioapi_0.13     farver_2.1.0        bit64_4.0.5        \n [13] fansi_1.0.3         mvtnorm_1.1-3       lubridate_1.8.0    \n [16] xml2_1.3.3          codetools_0.2-18    splines_4.2.1      \n [19] knitr_1.39          polyclip_1.10-0     jsonlite_1.8.0     \n [22] broom_0.8.0         cluster_2.1.3       dbplyr_2.2.1       \n [25] png_0.1-7           effectsize_0.7.0    ggforce_0.3.3      \n [28] compiler_4.2.1      httr_1.4.3          emmeans_1.7.5      \n [31] backports_1.4.1     assertthat_0.2.1    Matrix_1.4-1       \n [34] fastmap_1.1.0       cli_3.3.0           tweenr_1.0.2       \n [37] htmltools_0.5.2     tools_4.2.1         coda_0.19-4        \n [40] gtable_0.3.0        glue_1.6.2          Rcpp_1.0.8.3       \n [43] cellranger_1.1.0    vctrs_0.4.1         svglite_2.1.0      \n [46] insight_0.17.1      xfun_0.31           openxlsx_4.2.5     \n [49] rvest_1.0.2         lifecycle_1.0.1     mosaicCore_0.9.0   \n [52] zoo_1.8-10          MASS_7.3-57         vroom_1.5.7        \n [55] hms_1.1.1           sandwich_3.0-2      parallel_4.2.1     \n [58] RColorBrewer_1.1-3  curl_4.3.2          yaml_2.3.5         \n [61] gridExtra_2.3       labelled_2.9.1      rpart_4.1.16       \n [64] latticeExtra_0.6-29 stringi_1.7.6       bayestestR_0.12.1  \n [67] checkmate_2.1.0     zip_2.2.0           rlang_1.0.3        \n [70] pkgconfig_2.0.3     systemfonts_1.0.4   evaluate_0.15      \n [73] htmlwidgets_1.5.4   labeling_0.4.2      bit_4.0.4          \n [76] tidyselect_1.1.2    plyr_1.8.7          magrittr_2.0.3     \n [79] R6_2.5.1            generics_0.1.2      multcomp_1.4-19    \n [82] DBI_1.1.3           pillar_1.7.0        haven_2.5.0        \n [85] foreign_0.8-82      withr_2.5.0         datawizard_0.4.1   \n [88] nnet_7.3-17         modelr_0.1.8        crayon_1.5.1       \n [91] utf8_1.2.2          tzdb_0.3.0          rmarkdown_2.14     \n [94] jpeg_0.1-9          grid_4.2.1          readxl_1.4.0       \n [97] data.table_1.14.2   reprex_2.0.1        digest_0.6.29      \n[100] webshot_0.5.3       xtable_1.8-4        munsell_0.5.0      \n[103] viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html",
    "title": "19  Introduction",
    "section": "",
    "text": "In Study 5A we explore the extent to which requiring mouse-cursor interaction with the graph improves interpretation of the underlying coordinate system."
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html#methods",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html#methods",
    "title": "19  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 19.1. The list of questions can be found here.\n\n\n\nFigure 19.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nNote that across both control and impasse conditions, both the question, response options and graph structure were identical. The experimental manipulation (posing a mental impasse) was accomplished by changing the position of datapoints in the impasse-condition graph, such that for any given question, there was no available response option if the reader were to interpret the graph as cartesian (making an orthogonal rather than diagonal projection from the x-axis.)\nThe green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. We hypothesize that this presents the reader with an obstacle, at which point they are forced to confront their interpretation of the coordinate system and (ideally) develop a new strategy.\n\n\n\nFigure 19.2: Sample Question (Q=1) graphs for each condition\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items : the Graph Comprehension Task\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData was collected by convenience sample …"
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html#analysis",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html#analysis",
    "title": "19  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nwinter2022_clean_sgc5a.Rmd\n2_sgc5_scoring.qmd\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import file\ndf_subjects <- read_rds(\"analysis/SGC5A/data/0-session-level/sgc5_participants.rds\") #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \n# df_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% select(\n#   subject,condition,term,mode,\n#   gender,age,language, schoolyear, country,\n#   effort,difficulty,confidence,enjoyment,other,\n#   totaltime_m,absolute_score\n# )\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_q16 <- df_subjects %>% \n  dplyr::select(subject, condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% dplyr::select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects <- df_subjects %>% \n  # mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 #absolute_score,#drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors\n                 violations, browser, width, height\n                 )\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#set factors\ndf_subjects <- df_subjects %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#read datafiles\ndf_items <- read_rds(\"analysis/SGC5A/data/0-session-level/sgc5_items.rds\") #use RDS file as it contains metadata\n\n#reduce data collected using new webapp\ndf_items <- df_items %>% \n  dplyr::select(subject, condition, pretty_condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  ) %>% \n  mutate(\n    response = str_remove_all(as.character(answer), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC5A/data/1-study-level/sgc5_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC5A/data/1-study-level/sgc5_items.csv\", row.names = FALSE)\nwrite.csv(df_q16,\"analysis/SGC5A/data/1-study-level/sgc5_freeresponse.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC5A/data/1-study-level/sgc5_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC5A/data/1-study-level/sgc5_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html#resources",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html#resources",
    "title": "19  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] highr_0.9         httr_1.4.3        pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] munsell_0.5.0     broom_0.8.0       compiler_4.2.1    modelr_0.1.8     \n[29] xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2  \n[33] tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3       viridisLite_0.4.0\n[37] crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1      withr_2.5.0      \n[41] grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1  \n[45] DBI_1.1.3         magrittr_2.0.3    scales_1.2.0      zip_2.2.0        \n[49] cli_3.3.0         stringi_1.7.6     fs_1.5.2          xml2_1.3.3       \n[53] ellipsis_0.3.2    generics_0.1.2    vctrs_0.4.1       openxlsx_4.2.5   \n[57] tools_4.2.1       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n[61] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[65] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html",
    "title": "20  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC5 study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#score-sgc-data",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#score-sgc-data",
    "title": "20  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#backup <- read_rds('analysis/SGC5A/data/1-study-level/sgc5_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC5A/data/1-study-level/sgc5_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n# extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)\n\n\n\n\nDerive State Score\nThe scaled score represents a 5-category ordering of understanding. We also derive a 3-category ordering that gives higher grained access to “orthogonal-like” vs “uncertain” vs “triangle-like” responses.\nSpecifically, we assign the following values to each interpretation:\n\n(orth-like) : orthogonal, satisfice\n(unknown) : unidentified alternatives, blank, both tri&orth, and reference point responses\n(tri-like) : Tverskian and triangular responses\n\n\n\nCODE\ndf_items <- df_items %>% mutate (\n  score_STATE = recode_factor(df_items$score_SCALED,\n                         \"-1\" = \"orth-like\",\n                         \"-0.5\" = \"unknown\",\n                         \"0\" = \"unknown\",\n                         \"0.5\" = \"tri-like\",\n                         \"1\" = \"tri-like\"),\n  score_STATE = as.ordered(score_STATE))"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#summarize-by-subject",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#summarize-by-subject",
    "title": "20  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC5A/data/1-study-level/sgc5_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#explore-distributions",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#explore-distributions",
    "title": "20  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_condition = recode_factor(condition, \"11115\" = \"point-click\"),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(pretty_condition~q) + \n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, data = df_subjects) %>% \n   gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Total Absolute Score\", \n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(q~pretty_condition) + \n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) + \n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>% \n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\", \n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01)) + \n geom_line(position=position_jitter(w=0.15, h=0.00), size=0.1) +\n facet_wrap( ~ pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01)) + \n geom_line(position=position_jitter(w=0.15, h=0.00), size=0.1) +\n facet_wrap( ~ pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_histogram(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#peeking",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#peeking",
    "title": "20  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\n\nsgc3a <- read_rds(\"analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds\") %>% filter(condition == \"111\") %>% dplyr::select(-pretty_mode)\n\n\ncomb <- rbind(sgc3a, df_subjects)  \n\ngf_histogram(~s_SCALED, data = comb) %>% \n  gf_facet_wrap(~pretty_condition)\n\n\n\n\n\nCODE\nm1 <- lm(s_SCALED ~ pretty_condition, data = comb)\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ pretty_condition, data = comb)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.57  -5.49  -3.57   1.51  20.51 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   -6.427      0.647   -9.93   <2e-16 ***\npretty_conditionpoint-click   -1.086      0.997   -1.09     0.28    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.14 on 271 degrees of freedom\nMultiple R-squared:  0.00436,   Adjusted R-squared:  0.000682 \nF-statistic: 1.19 on 1 and 271 DF,  p-value: 0.277\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n                  Df Sum Sq Mean Sq F value Pr(>F)\npretty_condition   1     78    78.5    1.19   0.28\nResiduals        271  17935    66.2               \n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with pretty_condition (formula: s_SCALED ~ pretty_condition). The model explains a statistically not significant and very weak proportion of variance (R2 = 4.36e-03, F(1, 271) = 1.19, p = 0.277, adj. R2 = 6.82e-04). The model's intercept, corresponding to pretty_condition = control, is at -6.43 (95% CI [-7.70, -5.15], t(271) = -9.93, p < .001). Within this model:\n\n  - The effect of pretty condition [point-click] is statistically non-significant and negative (beta = -1.09, 95% CI [-3.05, 0.88], t(271) = -1.09, p = 0.277; Std. beta = -0.13, 95% CI [-0.37, 0.11])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#export",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#export",
    "title": "20  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC5A/data/2-scored-data/sgc5a_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC5A/data/2-scored-data/sgc5a_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC5A/data/2-scored-data/sgc5a_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC5A/data/2-scored-data/sgc5a_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC5A/data/2-scored-data/sgc5a_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC5A/data/2-scored-data/sgc5a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#resources",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#resources",
    "title": "20  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n  [1] TH.data_1.1-1       colorspace_2.0-3    rio_0.5.29         \n  [4] ellipsis_0.3.2      estimability_1.4    htmlTable_2.4.0    \n  [7] parameters_0.18.1   base64enc_0.1-3     fs_1.5.2           \n [10] rstudioapi_0.13     farver_2.1.0        bit64_4.0.5        \n [13] fansi_1.0.3         mvtnorm_1.1-3       lubridate_1.8.0    \n [16] xml2_1.3.3          codetools_0.2-18    splines_4.2.1      \n [19] knitr_1.39          polyclip_1.10-0     jsonlite_1.8.0     \n [22] broom_0.8.0         cluster_2.1.3       dbplyr_2.2.1       \n [25] png_0.1-7           effectsize_0.7.0    ggforce_0.3.3      \n [28] compiler_4.2.1      httr_1.4.3          emmeans_1.7.5      \n [31] backports_1.4.1     assertthat_0.2.1    Matrix_1.4-1       \n [34] fastmap_1.1.0       cli_3.3.0           tweenr_1.0.2       \n [37] htmltools_0.5.2     tools_4.2.1         coda_0.19-4        \n [40] gtable_0.3.0        glue_1.6.2          Rcpp_1.0.8.3       \n [43] cellranger_1.1.0    vctrs_0.4.1         svglite_2.1.0      \n [46] insight_0.17.1      xfun_0.31           openxlsx_4.2.5     \n [49] rvest_1.0.2         lifecycle_1.0.1     mosaicCore_0.9.0   \n [52] zoo_1.8-10          MASS_7.3-57         vroom_1.5.7        \n [55] hms_1.1.1           sandwich_3.0-2      parallel_4.2.1     \n [58] RColorBrewer_1.1-3  curl_4.3.2          yaml_2.3.5         \n [61] gridExtra_2.3       labelled_2.9.1      rpart_4.1.16       \n [64] latticeExtra_0.6-29 stringi_1.7.6       bayestestR_0.12.1  \n [67] checkmate_2.1.0     zip_2.2.0           rlang_1.0.3        \n [70] pkgconfig_2.0.3     systemfonts_1.0.4   evaluate_0.15      \n [73] htmlwidgets_1.5.4   labeling_0.4.2      bit_4.0.4          \n [76] tidyselect_1.1.2    plyr_1.8.7          magrittr_2.0.3     \n [79] R6_2.5.1            generics_0.1.2      multcomp_1.4-19    \n [82] DBI_1.1.3           pillar_1.7.0        haven_2.5.0        \n [85] foreign_0.8-82      withr_2.5.0         datawizard_0.4.1   \n [88] nnet_7.3-17         modelr_0.1.8        crayon_1.5.1       \n [91] utf8_1.2.2          tzdb_0.3.0          rmarkdown_2.14     \n [94] jpeg_0.1-9          grid_4.2.1          readxl_1.4.0       \n [97] data.table_1.14.2   reprex_2.0.1        digest_0.6.29      \n[100] webshot_0.5.3       xtable_1.8-4        munsell_0.5.0      \n[103] viridisLite_0.4.0"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Schmidt, Dennis, Tobias Raupach, Annette Wiegand, Manfred Herrmann, and\nPhilipp Kanzow. 2021. “Relation Between Examinees’ True Knowledge\nand Examination Scores: Systematic Review and Exemplary Calculations on\nMultiple-True-False\nItems.” Educational Research Review 34 (November):\n100409. https://doi.org/10.1016/j.edurev.2021.100409."
  }
]