[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SGC-X",
    "section": "",
    "text": "6/27 | SGC3A refactor file names/refs to differentiate hypo testing from exploratory modelling\n6/27 | SGC3A chisqr on first question\n6/27 | add sample logistic regression learning file\n… restructure and creation of modelling file\n5/24 | SGC3A SCORING| con’t response exploration + add Q6-Q15 keys\n5/24 | SGC3A EXPLORATION| quick and dirty lms given scaled_score at subject level\n5/18 | SG3A SCORING | refactor scoring to differentiate SATISFICING left vs right\n5/18 | SG3A SCORING | refactor scoring for performance (60mins / dataset to ~25 mins / dataset)\n5/13 - 5/16 | SG3A SCORING | refactor code for robust to condition\n5/11/22 | SGC3A SCORING | cleanup text + continue response strategy analysis 111=Q3\n5/10/22 | SGC3A SCORING | calculate tversky subscores, start derive interpretation from subscores, refactor scoring cell as functions\n5/10/22 | SGC3A SCORING | calculate nice_ABS (dichotomous score ignoring ‘allowed’ false-correct options (such as reference point)\n5/5/22 - 5/8/22 | SGC3A SCORING | explore specific responses on each item\n5/4/22 | SGC3A SCORING | replaced scoring strategy partial[-1/n, +1/n] to partial [-1/q, + 1/p]\n4/29/22 | ported existing .Rmd analysis files to Quarto (.qmd) for sharing status w/ JMH CMW via web"
  },
  {
    "objectID": "analysis/SGC3A/SGC3A.html",
    "href": "analysis/SGC3A/SGC3A.html",
    "title": "SGC3A",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "analysis/SGC3A/SGC3A.html#methods",
    "href": "analysis/SGC3A/SGC3A.html#methods",
    "title": "SGC3A",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Scaffold: control,impasse)\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 2. The list of questions can be found here.\n\n\n\nFigure 2: Sample Question (Q=1) for Graph Comprehension Task\n\n\nNote that across both control and impasse conditions, both the question, response options and graph structure were identical. The experimental manipulation (posing a mental impasse) was accomplished by changing the position of datapoints in the impasse-condition graph, such that for any given question, there was no available response option if the reader were to interpret the graph as cartesian (making an orthogonal rather than diagonal projection from the x-axis.)\nThe green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. We hypothesize that this presents the reader with an obstacle, at which point they are forced to confront their interpretation of the coordinate system and (ideally) develop a new strategy.\n\n\n\nFigure 3: Sample Question (Q=1) graphs for each condition\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items.\n(3A) The first five items in the task are defined as the SCAFFOLDING block. In the IMPASSE condition, the first five questions included an IMPASSE problem state. For participants in the CONTROL condition, the dataset was structure such that there was always an available ‘orthogonal answer’ for the first 5 questions.\n(3B) The remaining 10 items are defined as the TESTING block. In both conditions, these questions were not structured as impasse (i.e. contained an available orthogonal answer)\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData was collected by convenience sample of a university subject pool. Initial data (Fall 2017, Spring 2018) were collected in-person, with large groups of students simultaneously completing the study (independently) in a computer lab. In Fall 2021 and Winter 2022 we collected additional data to replicate results in a remote format (students completing the study asynchronously on their own computers)."
  },
  {
    "objectID": "analysis/SGC3A/SGC3A.html#analysis",
    "href": "analysis/SGC3A/SGC3A.html#analysis",
    "title": "SGC3A",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\nBefore analysis, data files from individual data collection periods are harmonized into a common data format (Section 1).\n\n\nResponse Scoring\nBecause the graph comprehension task utilizes a Multiple-Response (MR) format (rather than simple multiple choice), the raw response data (the combination of answer options selected) for each question first need to be assigned a score. Approaches to scoring MR data and score transformations are derived in Section 2.\n\n\nHypothesis Testing\nExperimental hypotheses are tested in Section 4.\n\n\nExploratory Analysis\nFurther data analyses are documented in Section 5."
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_harmonize.html",
    "href": "analysis/SGC3A/1_sgc3A_harmonize.html",
    "title": "1  Harmonization",
    "section": "",
    "text": "The purpose of this notebook is to harmonize data files collected across different data collection sessions for study SGC_3A."
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_harmonize.html#harmonization",
    "href": "analysis/SGC3A/1_sgc3A_harmonize.html#harmonization",
    "title": "1  Harmonization",
    "section": "1.1 HARMONIZATION",
    "text": "1.1 HARMONIZATION\nData for study SGC_3A were collected across four time periods, interrupted by the Covid-19 pandemic.\n\n\n\nPeriod\nModality\n\n\n\n\nFall 2017\nin person, SONA groups in computer lab\n\n\nSpring 2018\nin person, SONA groups in computer lab\n\n\nFall 2021\nasynchronous, online, SONA\n\n\nWinter 2022\nasynchronous, online, SONA\n\n\n\nData collected in Fall 2017, Spring 2018 constitute the original SGC_3A study, conducted in person. Data collected in Fall 2021, Winter 2022 constitute the web-based replication, conducted online (asynchronously). In all cases, the experiment was administered via a web application.\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\n1.1.1 Participants\nFirst we import participant-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n#set datafiles\nfall17 <- \"data/0-session-level/fall17_sgc3a_participants.csv\"\nspring18 <- \"data/0-session-level/spring18_sgc3a_participants.csv\"\nfall21 <- \"data/0-session-level/fall21_sgc3a_participants.csv\"\nwinter22 <- \"data/0-session-level/winter22_sgc3a_participants.rds\"\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read.csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read.csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_fall21 <- read.csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_subjects_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% select(\n  subject,condition,term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m,absolute_score\n)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, absolute_score)\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_winter22_q16 <- df_subjects_winter22 %>% \n  select(subject, condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects_winter22 <- df_subjects_winter22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, absolute_score)\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_winter22, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#CLEANUP\nrm(df_subjects_fall17,df_subjects_fall21, df_subjects_spring18, df_subjects_winter22,df_subjects_before)\nrm(fall17,fall21,spring18,winter22)\n\n\n\n\n1.1.2 Items\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n#set datafiles\nfall17 <- \"data/0-session-level/fall17_sgc3a_blocks.csv\"\nspring18 <- \"data/0-session-level/spring18_sgc3a_blocks.csv\"\nfall21 <- \"data/0-session-level/fall21_sgc3a_blocks.csv\"\nwinter22 <- \"data/0-session-level/winter22_sgc3a_items.rds\"\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read.csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read.csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_fall21 <- read.csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_items_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- df_items_winter22 %>% group_by(q) %>% select(q,relation) %>% unique()\n\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- df_items_winter22 %>% filter(condition=='X') %>% select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n  \n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18, df_items_fall21) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n  \n#reduce data collected using new webapp\ndf_items_winter22 <- df_items_winter22 %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_winter22,df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  )\n\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% select(-question,-correct,-rt_s,-num_o)\n#add data from wi22 [stored on subject data]\ndf_freeresponse <- rbind(df_freeresponse, df_winter22_q16)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n    question = \"Please describe how to determine what event(s) start at 12pm?\",\n    response = as.character(response) #doesn't need to be factor\n  ) \n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#CLEANUP\nrm(df_items_fall17,df_items_fall21, df_items_spring18, df_items_winter22, df_items_before, df_winter22_q16)\nrm(fall17,fall21,spring18,winter22, map_relations)\n\n\n\n1.1.2.1 Validation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE"
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_harmonize.html#export",
    "href": "analysis/SGC3A/1_sgc3A_harmonize.html#export",
    "title": "1  Harmonization",
    "section": "1.2 EXPORT",
    "text": "1.2 EXPORT\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n#SAVE FILES\nwrite.csv(df_subjects,\"data/1-study-level/sgc3a_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"data/1-study-level/sgc3a_items.csv\", row.names = FALSE)\nwrite.csv(df_freeresponse,\"data/1-study-level/sgc3a_items.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"data/1-study-level/sgc3a_participants.rds\") # to R data structure file\nrio::export(df_items, \"data/1-study-level/sgc3a_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_harmonize.html#resources",
    "href": "analysis/SGC3A/1_sgc3A_harmonize.html#resources",
    "title": "1  Harmonization",
    "section": "1.3 RESOURCES",
    "text": "1.3 RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.0.2 (2020-06-22)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS  10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] codebook_0.9.2  forcats_0.5.0   stringr_1.4.0   dplyr_1.0.2    \n [5] purrr_0.3.4     readr_1.4.0     tidyr_1.1.2     tibble_3.1.2   \n [9] ggplot2_3.3.5   tidyverse_1.3.0\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.5        lubridate_1.7.9   assertthat_0.2.1  digest_0.6.27    \n [5] utf8_1.2.1        R6_2.5.0          cellranger_1.1.0  backports_1.2.1  \n [9] reprex_0.3.0      labelled_2.8.0    evaluate_0.14     httr_1.4.2       \n[13] pillar_1.6.1      rlang_0.4.11      curl_4.3          readxl_1.3.1     \n[17] rstudioapi_0.13   data.table_1.13.2 blob_1.2.1        rmarkdown_2.11   \n[21] foreign_0.8-80    htmlwidgets_1.5.2 munsell_0.5.0     broom_0.7.12     \n[25] compiler_4.0.2    modelr_0.1.8      xfun_0.29         pkgconfig_2.0.3  \n[29] htmltools_0.5.2   tidyselect_1.1.0  rio_0.5.16        fansi_0.5.0      \n[33] crayon_1.4.1      dbplyr_1.4.4      withr_2.4.2       grid_4.0.2       \n[37] jsonlite_1.7.1    gtable_0.3.0      lifecycle_1.0.0   DBI_1.1.0        \n[41] magrittr_2.0.1    scales_1.1.1      zip_2.1.1         cli_3.3.0        \n[45] stringi_1.7.3     fs_1.5.0          xml2_1.3.2        ellipsis_0.3.2   \n[49] generics_0.0.2    vctrs_0.3.8       openxlsx_4.2.3    tools_4.0.2      \n[53] glue_1.6.2        hms_0.5.3         fastmap_1.1.0     yaml_2.2.1       \n[57] colorspace_2.0-2  rvest_0.3.6       knitr_1.37        haven_2.3.1"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html",
    "title": "2  Response Scoring",
    "section": "",
    "text": "TODO\nThe purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC_3A study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.)"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#multiple-response-scoring",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#multiple-response-scoring",
    "title": "2  Response Scoring",
    "section": "2.1 MULTIPLE RESPONSE SCORING",
    "text": "2.1 MULTIPLE RESPONSE SCORING\nThe graph comprehension task of study SGC 3A presents readers with a graph, a question, and a series of checkboxes. Participants are instructed to use the graph to answer the question, and respond by selecting all the checkboxes that apply, where each checkbox corresponds to a datapoint in the graph.\n\n\n\nFigure 1. Sample Graph Comprehension (Question # 6)\n\n\nIn the psychology and education literatures on Tests & Measures, the format of this type of question is referred to as Multiple Response (MR), (also: Multiple Choice Multiple Answer (MCMA) and Multiple Answer Multiple Choice (MAMC)). It has a number of properties that make it different from traditional Single Answer Multiple Choice (SAMC) questions, where the respondent marks a single response from a number of options. In particular, there are a number of very different ways that MAMC questions can be scored.\nIn tranditional SAMC format questions, one point is given for selecting the option designated as correct, and zero points given for marking any of the alternative (i.e. distractor) options. Individual response options on MAMC questions, however might be partially correct (\\(i\\)), while responses on other answer options within the same item might be incorrect (\\(n – i\\)). In MR, it is not obvious how to allocate points when the respondent marks a true-correct option (i.e. options that should be selected, denoted \\(p\\)), as well as one or more false-correct options (i.e. options that should not be selected, denoted \\(q\\)). Should partial credit be awarded? If so, are options that respondents false-selected and false-unselected items equally penalized?\nSchmidt et al. (2021) performed a systematic literature review of publications proposing MAMC (or equivalent) scoring schemes, ultimately synthesizing over 80 sources into 27 distinct scoring approaches. Upon reviewing the benefits of trade-offs of each approach, for this study we choose utilize two of the schemes: dichotomous scoring ( Schmidt et al. (2021) scheme #1), and partial scoring \\([-1/q,0, +1/p]\\) ( Schmidt et al. (2021) scheme #26), as well as a scaled discriminant score that leverages partial scoring to discriminate between strategy-specific patterns of response.\n\n2.1.1 Response Encoding\nFirst, we note that the question type evaluated by Schmidt et al. (2021) is referred to as Multiple True-False (MTF), a variant of MAMC where respondents are presented with a question (stem) and series of response options with True/False (e.g. radio buttons) for each. Depending on the implementation of the underlying instrument, it may or may not be possible for respondents to not respond to a particular option (i.e. leave the item ‘blank’). Although MTF questions have a different underlying implementation (and potentially different psychometric properties) they are identical in their mathematical properties; that is, responses to a MAMC question of ‘select all that apply’ can be coded as a series of T/F responses to each response option\n\n\n\nFigure 2.1: Figure 2. SAMC (vs) MAMC (vs) MTF\n\n\nIn this example (Figure 2.1), we see an example of a question with four response options (\\(n=4\\)) in each question type. In the SAMC approach (at left), there are four possible responses, given explicitly by the response options (respondent can select only one) \\((\\text{number of possible responses} = n)\\). With only four possible responses, we cannot entirely discriminate between all combinations of the underlying response variants we might be interested in, and must always choose an ‘ideal subset’ of possible distractors to present as response options. In the MAMC (middle) and MTF (at right), the same number of response options (\\(n=4\\)) yield a much greater number \\((\\text{number of possible responses} = 2^{n})\\). We can also see the equivalence between a MAMC and MTF format questions with the same response options. Options the respondent selects in MAMC are can be coded as T, and options they leave unselected can be coded as F. Thus, for response options (ABCD), a response of [AB] can also be encoded as [TTFF].\n\n\n2.1.2 Scoring Schemes\nIn the sections that follow, we use the terminology:\nProperties of the Stimulus-Question\n\\[\\begin{align}\nn &= \\text{number of response options} \\\\  \n  &= p + q \\\\\n  p &= \\text{number of true-select options (i.e. should be selected)} \\\\\n  q &= \\text{number of true-unselect options (i.e. should not be selected)}\n\\end{align}\\]\nProperties of the Subject’s Response\n\\[\\begin{align}\ni &= \\text{number of options in correct state}, (0 ≤ i ≤ n) \\\\\nf &= \\text{resulting score}\n\\end{align}\\]\n\n2.1.2.1 Dichotomous Scoring\nDichotomous Scoring is the strictest scoring scheme, where a response only receives points if it is exactly correct, meaning the respondent includes only correct-select options, and does select any additional (i.e. incorrect-select) options that should not be selected. This is also known as all or nothing scoring, and importantly, it ignores any partial knowledge that a participant may be expressing through their choice of options. They may select some but not all of the correct-select options, and one or more but not all of the correct-unselect items, but receive the same score as a respondent selects none of the correct-select options, or all of the correct-unselect options. In this sense, dichotomous scoring tells us only about perfect knowledge, and ignores any indication of partial knowledge the respondent may be indicating through their selection of response options.\nIn Dichotomous Scoring\n\nscore for the question is either 0 or 1\nfull credit is only given if all responses are correct; otherwise no credit\ndoes not account for partial knowledge. - with increasing number of response options, scoring becomes stricter as each statement must be marked correctly.\n\nThe algorithm for dichotomous scoring is given by:\n\\[\\begin{gather*}\nf =\n\\begin{cases}\n  1, \\text{if } i = n \\\\    \n  0, \\text{otherwise}    \n\\end{cases}\n\\end{gather*}\\] 0 i n\n\n\nCODE\nf_dichom <- function(i, n) {\n \n  # print(paste(\"i is :\",i,\" n is:\",n)) \n  \n  #if (n == 0 ) return error \n  ifelse( (n == 0), print(\"ERROR n can't be 0\"), \"\")\n  \n  #if (i > n ) return error \n  ifelse( (i > n), print(\"i n can't > n\"), \"\")\n  \n  #if (i==n) return 1, else 0\n  return (ifelse( (i==n), 1 , 0))\n \n}\n\n\n\n\n2.1.2.2 Partial Scoring [-1/n, +1/n]\nPartial Scoring refers to a class or scoring schemes that award the respondent partial credit depending on pattern of options they select. Schmidt et al. (2021) identify twenty-six different partial credit scoring schemes in the literature, varying in the range of possible scores, and the relative weighting of incorrectly selected (vs) incorrectly unselected options.\nA particularly elegant approach to partial scoring is referred to as the \\([-1/n, +1/n]\\) approach ( Schmidt et al. (2021) #17). This approach is appealing in the context of SGC3A, because it: (1) takes into account all information provided by the respondent: the pattern of what the select, and choose not to select.\nIn Partial Scoring \\([-1/n, +1/n]\\):\n\nScores range from [-1, +1]\nOne point is awarded if all options are correct\nOne point point is subtracted if all options are incorrect.\nIntermediate results are credited as fractions accordingly (\\(+1/n\\) for each correct, \\(-1/n\\) for each incorrect)\nThis results in at chance performance (i.e. half of the given options marked correctly), being awarded 0 points are awarded\n\nThis scoring is more consistent with the motivating theory that Triangular Graph readers start out with an incorrect (i.e. orthogonal, cartesian) interpretation of the coordinate system, and transition to a correct (i.e. triangular) interpretation. But the first step in making this transition is realizing the cartesian interpretation is incorrect, which may yield blank responses where the respondent is essentially saying, ‘there is no correct answer to this question’.\nSchmidt et al. (2021) describe the Partial \\({[-1/n, +1/n]}\\) scoring scheme as the only scoring method (of the 27 described) where respondents’ scoring results can be interpreted as a percentage of their true knowledge. One important drawback of this method is that a respondent may receive credit (a great deal of credit, depending on the number of answer options n) even if she did not select any options. In the case (such as ours) where there are many more response options \\(n\\) than there are options meant to be selected \\(p\\), this partial scoring algorithm poses a challenge because the respondent can achieve an almost completely perfect score by selecting a small number of options that should not be selected.\nThe algorithm for partial scoring\\([-1/n, +1/n]\\) is given by:\n\\[\\begin{align}\nf &= (1/n * i) - (1/n * (n-i)) \\\\\n&= (2i - n)/{n}\n\\end{align}\\]\n\n\nCODE\nf_partialN <- function(i, n) {\n\n# print(paste(\"i is :\",i,\" n is:\",n))\n\n#if(n==0) return error\nifelse((n==0),print(\"ERROR: n should not be 0\"),\"\")\n\n#if(i >n ) return error\nifelse((i > n),print(\"ERROR: i CANNOT BE GREATER THAN n\"),\"\")\n\nreturn ((2*i - n) / n) \n}\n\n\n\n\n2.1.2.3 Partial Scoring [-1/q, +1/p]\nOne drawback of the Partial Scoring \\([-1/n, +1/n]\\) approach is that treats the choice to select, and choice to not select options as equally indicative of the respondent’s understanding. That is to say, incorrectly selecting one particular option is no more or less informative than incorrectly not-selecting a different item. This represents an important difference between MAMC (i.e. “select all correct options”) vs MTF (i.e. “Mark each option as true or false”) questions.\nIn our study, the selection of any particular option (remember options represent data points on the stimulus graph) is indicative of a particular interpretation of the stimulus. Incorrectly selecting an option indicates an interpretation of the graph with respect to that particular option. Alternatively, failing to select a correct option might mean the individual has a different interpretation, or that they failed to find all the data points consistent with the interpretation.\nFor this reason, we consider another alternative Partial Scoring scheme that takes into consideration only the selected statements, without penalizing statements incorrectly not selected. (See Schmidt et al. (2021) method #26; also referred to as the Morgan-Method) This partial scoring scheme takes into consideration that the most effort-free (or ‘default’) response for any given item is the null, or blank response. Blank responses indicate no understanding, perhaps confusion, or refusal to answer. These lack of responses are awarded zero credit. Whereas taking the action to select an incorrect option is effortful, and is indicative of incorrect understanding.\nPartial Scoring \\([-1/q, +1/p]\\):\n\nawards +1/p for each correctly selected option (\\(p_s\\)), and subtracts \\(1/(n-p) = 1/q\\) for each incorrectly selected option (\\(q_s\\))\nonly considers selected options; does not penalize nor reward unselected options\n\nProperties of Item\n\\[\\begin{align}\np &= \\text{number of true-select options (i.e. should be selected)} \\\\\nq &= \\text{number of true-unselect options (i.e. should not be selected)} \\\\\nn &= \\text{number of options} \\: ( n = p + q)\n\\end{align}\\]\nProperties of Response\n\\[\\begin{align}\np_s &= \\text{number of true-select options selected (i.e. number of correctly checked options)}\\\\\nq_s &= \\text{number of true-unselect options selected (i.e. number of incorrectly checked options }\n\\end{align}\\]\nThe algorithm for partial scoring \\([-1/q, +1/p]\\) is given by:\n\\[\\begin{align}\nf &= (p_s / p) - ({q_s}/{q}) \\\\\n\\end{align}\\]\n\n\nCODE\nf_partialP <- function(t,p,f,q) {\n\n  #t = number of correct-selected options\n  #p = number of true options\n  #f = number of incorrect-selected options\n  #q = number of false options\n  #n = number of options + p + q\n  \n  ifelse( (p == 0), return(NA), \"\") #handle empty response set gracefully by returning nothing rather than 0\n  ifelse( (p != 0), return( (t / p) - (f/q)), \"\")\n}\n\n\n\n\n\n2.1.3 Comparison of Schemes\nWhich scoring scheme is most appropriate for the goals of the graph comprehension task?\nConsider the following example:\nFor a question with \\(n = 5\\) response options (data points A, B, C, D and E) with a correct response of A, the schemes under consideration yield the following scores:\n\n\nCODE\ntitle <- \"Comparison of Scoring Schemes for n = 5 options [ A,B,C,D,E ]\"\n\ncorrect <- c( \"A____\",  \n              \"A____\",      \n              \"A____\",        \n              \"A____\",        \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\" ) \n\nresponse <- c(\"A____\",  \n              \"AB___\",      \n              \"A___E\",      \n              \"AB__E\",        \n              \"____E\",\n              \"___DE\",\n              \"_BCDE\",      \n              \"ABCDE\",      \n              \"_____\" )\n\ni <- c(        5,       \n               4,              \n               4,              \n               3,               \n               \n               3,\n               2,\n               0,\n               1,\n               4)\n\nabs <- c(f_dichom(5,5), \n         f_dichom(4,5), \n         f_dichom(4,5), \n         f_dichom(3,5), \n         \n         f_dichom(3,5), \n         f_dichom(2,5),\n         f_dichom(0,5),\n         f_dichom(1,5),\n         f_dichom(4,5))\n\npartial1 <- c(f_partialN(5,5), \n              f_partialN(4,5), \n              f_partialN(4,5), \n              f_partialN(3,5), \n              \n              f_partialN(3,5), \n              f_partialN(2,5),\n              f_partialN(0,5),\n              f_partialN(1,5),\n              f_partialN(4,5))\n\npartial2 <- c(f_partialP(1,1,0,4), \n              f_partialP(1,1,1,4), \n              f_partialP(1,1,1,4), \n              f_partialP(1,1,2,4), \n              \n              f_partialP(0,1,1,4),\n              f_partialP(0,1,2,4),\n              f_partialP(0,1,4,4),\n              f_partialP(1,1,4,4), \n              f_partialP(0,1,0,4))\n\nnames = c(    \"Correct Answer\",\n              \"Response\",\n              \"i \",\n              \"Dichotomous\",\n              \"Partial [-1/n, +1/n]\",\n              \"Partial[-1/q, +1/p]\")\n\ndt <- data.frame(correct, response, i, abs, partial1 , partial2)\n\nkbl(dt, col.names = names, caption = title, digits=3) %>%\n  kable_classic() %>%\n    add_header_above(c(\"Response Scenario \" = 3, \"Scores\" = 3)) %>% \n    pack_rows(\"Perfect Response\", 1, 1) %>%\n    pack_rows(\"Correct + Extra Incorrect Selections\", 2, 4) %>%\n    pack_rows(\"Only Incorrect Selections\", 5, 6) %>%\n    pack_rows(\"Completely Inverse Response \", 7, 7) %>%\n    pack_rows(\"Selected ALL or NONE\", 8, 9) %>%\n    footnote(general = paste(\"i = number of options in correct state; _ indicates option not selected\"),\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nComparison of Scoring Schemes for n = 5 options [ A,B,C,D,E ]\n \n\nResponse Scenario \nScores\n\n  \n    Correct Answer \n    Response \n    i  \n    Dichotomous \n    Partial [-1/n, +1/n] \n    Partial[-1/q, +1/p] \n  \n \n\n  Perfect Response\n\n    A____ \n    A____ \n    5 \n    1 \n    1.0 \n    1.00 \n  \n  Correct + Extra Incorrect Selections\n\n    A____ \n    AB___ \n    4 \n    0 \n    0.6 \n    0.75 \n  \n  \n    A____ \n    A___E \n    4 \n    0 \n    0.6 \n    0.75 \n  \n  \n    A____ \n    AB__E \n    3 \n    0 \n    0.2 \n    0.50 \n  \n  Only Incorrect Selections\n\n    A____ \n    ____E \n    3 \n    0 \n    0.2 \n    -0.25 \n  \n  \n    A____ \n    ___DE \n    2 \n    0 \n    -0.2 \n    -0.50 \n  \n  Completely Inverse Response \n\n    A____ \n    _BCDE \n    0 \n    0 \n    -1.0 \n    -1.00 \n  \n  Selected ALL or NONE\n\n    A____ \n    ABCDE \n    1 \n    0 \n    -0.6 \n    0.00 \n  \n  \n    A____ \n    _____ \n    4 \n    0 \n    0.6 \n    0.00 \n  \n\n\nNote:   i = number of options in correct state; _ indicates option not selected\n\n\n\n\nCODE\n#cleanup\nrm(dt, abs, correct,i,names,partial1,partial2,response,title)\n\n\n\nWe see that in the Dichotomous scheme, only the precisely correct response (row 1) yields a score other than zero. This scheme does now allow us to differentiate between different response patters.\nThe Partial \\([-1/n, +1/n]\\) scheme yields a range from \\([-1,1]\\), differentiating between responses. However, a blank response (bottom row) receives the same score (0.6) as the selection of the correct option + 1 incorrect option (row 2), which is problematic with for the goals of this study, where we need to differentiate between states of confusion or uncertainty yielding blank responses and the intentional selection of incorrect items.\nThe Partial \\([-1/q, +1/p]\\) scheme also yields a range of scores from \\([-1,1]\\). A blank response (bottom row) yields the same score (\\(0\\)) as the selection of all answer options (row 7); both are patterns of behavior we would expect to see if a respondent is confused or uncertain that there is a correct answer to the question.\n\nNext we consider an example from our study, with \\(n = 15\\) options and \\(p = 1\\) correct option to be selected.\n\n\nCODE\ntitle <- \"Comparison of Scoring Schemes for SGC3 with n=15 and p=1 options [A,B...N,O]  \"\n\ncorrect <- c( \"A____\",  \n              \"A____\",      \n              \"A____\",      \n              \"A____\",        \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\" ) \n\nresponse <- c(\"A__...__\",  \n              \"AB_...__\",      \n              \"A__..._O\",      \n              \"AB_..._O\",        \n              \"___..._O\",      \n              \"___...NO\",      \n              \"_BC...NO\",\n              \"ABC...NO\",      \n              \"___...__\" )\n\ni <- c(        15,       \n               14,              \n               14,              \n               13,\n               13,               \n               12,          \n               0,\n               1,\n               14)\n\nabs <- c(f_dichom(15,15), \n         f_dichom(14,15), \n         f_dichom(14,15), \n         f_dichom(13,15), \n         f_dichom(13,15),\n         f_dichom(12,15),\n         f_dichom(0,15),\n         f_dichom(1,15),\n         f_dichom(14,15))\n\npartial1 <- c(f_partialN(15,15), \n              f_partialN(14,15), \n              f_partialN(14,15), \n              f_partialN(13,15), \n              f_partialN(13,15),\n              f_partialN(12,15),\n              f_partialN(0,15),\n              f_partialN(1,15),\n              f_partialN(14,15))\n\npartial2 <- c(f_partialP(1,1,0,14), \n              f_partialP(1,1,1,14), \n              f_partialP(1,1,1,14), \n              f_partialP(1,1,2,14), \n              f_partialP(0,1,1,14),\n              f_partialP(0,1,2,14),\n              f_partialP(0,1,14,14),\n              f_partialP(1,1,14,14), \n              f_partialP(0,1,0,14))\n\nnames = c(    \"Correct Answer\",\n              \"Response\",\n              \"$i$ \",\n              \"Dichotomous\",\n              \"Partial [-1/n, +1/n]\",\n              \"Partial [-1/q, +1/p]\")\n\ndt <- data.frame(correct, response, i, abs, partial1 , partial2)\n\nkbl(dt, col.names = names, caption = title, digits=3) %>%\n  kable_classic() %>%\n    add_header_above(c(\"Response Scenario \" = 3, \"Scores\" = 3)) %>% \n    pack_rows(\"Perfect Response\", 1, 1) %>%\n    pack_rows(\"Correct + Extra Incorrect Selections\", 2, 4) %>%\n    pack_rows(\"Only Incorrect Selections\", 5, 6) %>%\n    pack_rows(\"Completely Inverse Response \", 7, 7) %>%\n    pack_rows(\"Selected ALL or NONE\", 8, 9) %>%\n    footnote(general = paste(\"i = number of options in correct state; _ indicates option not selected\"),\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nComparison of Scoring Schemes for SGC3 with n=15 and p=1 options [A,B...N,O]  \n \n\nResponse Scenario \nScores\n\n  \n    Correct Answer \n    Response \n    $i$  \n    Dichotomous \n    Partial [-1/n, +1/n] \n    Partial [-1/q, +1/p] \n  \n \n\n  Perfect Response\n\n    A____ \n    A__...__ \n    15 \n    1 \n    1.000 \n    1.000 \n  \n  Correct + Extra Incorrect Selections\n\n    A____ \n    AB_...__ \n    14 \n    0 \n    0.867 \n    0.929 \n  \n  \n    A____ \n    A__..._O \n    14 \n    0 \n    0.867 \n    0.929 \n  \n  \n    A____ \n    AB_..._O \n    13 \n    0 \n    0.733 \n    0.857 \n  \n  Only Incorrect Selections\n\n    A____ \n    ___..._O \n    13 \n    0 \n    0.733 \n    -0.071 \n  \n  \n    A____ \n    ___...NO \n    12 \n    0 \n    0.600 \n    -0.143 \n  \n  Completely Inverse Response \n\n    A____ \n    _BC...NO \n    0 \n    0 \n    -1.000 \n    -1.000 \n  \n  Selected ALL or NONE\n\n    A____ \n    ABC...NO \n    1 \n    0 \n    -0.867 \n    0.000 \n  \n  \n    A____ \n    ___...__ \n    14 \n    0 \n    0.867 \n    0.000 \n  \n\n\nNote:   i = number of options in correct state; _ indicates option not selected\n\n\n\n\nCODE\n#cleanup\nrm(dt, abs, correct,i,names,partial1,partial2,response,title)\n\n\nHere again we see that the Partial \\([-1/q, +1/p]\\) scheme allows us differentiate between patterns of responses, in a way that is more sensible for the goals of the SGC3 graph comprehension task.\n\n\n\n\n\n\nDecision\n\n\n\nThe Partial \\([-1/q, +1/p]\\) scheme is more appropriate for scoring the graph comprehension task than the Partial \\([-1/n, +1/n]\\) scheme because it allows us to differentially penalize incorrectly selected and incorrectly not selected answer options."
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#score-sgc-data",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#score-sgc-data",
    "title": "2  Response Scoring",
    "section": "2.2 SCORE SGC DATA",
    "text": "2.2 SCORE SGC DATA\nIn SGC_3A we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nTODO UPDATE SCORE DEFS\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key.\n5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing stategy) answer key.\n\n2.2.1 Prepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\nkey_111_raw <- read_csv('data/keys/SGC3A_scaffold_111_key.csv') %>% mutate(condition = 111, phase = \"scaffold\")\nkey_121_raw <- read_csv('data/keys/SGC3A_scaffold_121_key.csv')%>% mutate(condition = 121, phase = \"scaffold\")\ncs = rep('c', 23) %>% str_c(collapse=\"\") #create column spec \nkey_test_raw <- read_csv('data/keys/SGC3A_test_key.csv', col_types = cs)%>% mutate(condition = \"\", phase = \"test\") \n\nkeys_raw <- rbind(key_111_raw, key_121_raw, key_test_raw )\nrm(key_111_raw, key_121_raw, key_test_raw)\n\n\nIn order to calculate scores using the \\([-1/q, +1/p]\\) algorithm, we need to define the subset of all response options (set N) that should be selected (set P) and should not be selected (set Q). In order to calculate subscores for each graph interpretation (i.e. triangular, orthogonal, tversky) we must define these sets independently for each interpretation. For each question, the keys_raw dataframe gives us set N (all response options), and a set P (options that should be selected) for each interpretation. From these we must derive set Q for each interpretation.\n\nSET \\(N\\), all response options (superset) . This set is the same across all interpretations (a property of the question) and is given in the answer key.\nSET \\(P\\), \\(P \\subset N\\) , the subset of options that should be selected (rewarded as +1/p) . This set differs by interpretation, and is given in the answer key.\nSET \\(A, A \\subset N, A \\sqcup P\\) , the subset of options that should not be selected, but if they are, aren’t penalized (i.e. these options are ignored. Not rewarded, nor penalized). These include any options referenced in the question (i.e. select shifts that start at the same time as X; don’t penalize if they also select ‘X’), as well as options within 0.5hr offset from the data point to accommodate reasonable visual errors. This set differs by interpretation, and is given in the answer key (columns REF_POINT and _also).\nSET \\(Q\\), the subset of options that should not be selected and are penalized (as -1/q). This set differs by interpretation and is not given in the answer key. We can derive set Q for each interpretation by \\(Q = N - (P \\cup A)\\)\n\nThe next step in scoring is preparing interpretation-specific answer keys that specify sets N, P, A and Q for each question.\n\n2.2.1.1 Triangular Key\nFirst we construct a key set based on the ‘Triangular’ interpretation (i.e. the actually correct answers).\n\n\nCODE\nverify_tri = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TRIANGULAR KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tri <- keys_raw %>% \n  select(Q, condition, OPTIONS, TRIANGULAR, TRI_allow, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    TRI_allow = str_replace_na(TRI_allow,\"\"),\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TRIANGULAR,\n    set_p = str_replace_na(set_p,\"\"),#replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TRI_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"),#replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DEFINE SETS N, P, A\nfor (x in 1:nrow(keys_tri)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tri[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tri[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tri[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tri[x,'set_q'] = Q\n  keys_tri[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tri[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tri <- keys_tri %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#cleanup \nrm(N,A,P,Q,n_q,s,x,tempunion)\n\n\nThis leaves us a dataframe keys_tri that define the sets of response options consistent with the triangular graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each option in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\n2.2.1.2 Orthogonal Key\nNext we construct a key set based on the ‘Orthogonal’ interpretation.\n\n\nCODE\nverify_orth = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT ORTHOGONAL KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_orth <- keys_raw %>% \n  select(Q, condition, OPTIONS, ORTHOGONAL, ORTH_allow, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    ORTH_allow = str_replace_na(ORTH_allow,\"\"),\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = ORTHOGONAL,\n    set_p = str_replace_na(set_p,\"\"),#replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(ORTH_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answer options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_orth)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_orth[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_orth[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_orth[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_orth[x,'set_q'] = Q\n  keys_orth[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  # print(tempunion)\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n\n  verify_orth[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_orth <- keys_orth %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x, cs)\n\n\nThis leaves us a dataframe keys_orth that define the sets of response options consistent with the orthogonal graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\n2.2.1.3 Satisficing Key\nNext we construct two keys based on the ‘Satisficing’ strategy. Satisficing involves selecting any data points within 0.5hr visual offset of the orthogonal interpretation of the graph (because no orthogonal response option is available). One key represents selecting a point slightly to the left of the orthogonal, and the other key represents selecting a point slightly to the right of the orthogonal.\n\n\nCODE\nverify_satisfice_right = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT SATISFICE RIGHT KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_satisfice_right <- keys_raw %>% \n  select(Q, condition, OPTIONS, SATISFICE_right, REF_POINT) %>% \n  mutate(\n    #replace NAs\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n\n    #P options that SHOULD be selected (rewarded)\n    set_p = SATISFICE_right,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n\n    #A options that are ignored if selected\n    set_a = str_c(REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n\n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_satisfice_right)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_satisfice_right[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_satisfice_right[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_satisfice_right[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to data frame\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_satisfice_right[x,'set_q'] = Q\n  keys_satisfice_right[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_satisfice_right[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_satisfice_right <- keys_satisfice_right %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q)%>% mutate(verify = n_p + n_a + n_q)\n\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\n\nverify_satisfice_left = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT SATISFICE left KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_satisfice_left <- keys_raw %>% \n  select(Q, condition, OPTIONS, SATISFICE_left, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = SATISFICE_left,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_satisfice_left)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_satisfice_left[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_satisfice_left[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_satisfice_left[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to data frame\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_satisfice_left[x,'set_q'] = Q\n  keys_satisfice_left[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_satisfice_left[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_satisfice_left <- keys_satisfice_left %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q)%>% mutate(verify = n_p + n_a + n_q)\n\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\nThis leaves us a dataframe keys_satisfice that define the sets of response options consistent with the orthogonal graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\n2.2.1.4 Tversky Keys\nNext we construct the key set based on a partial-understanding strategy we refer to as ‘Tversky’. We use the label Tversky as shorthand for a partial interpretation of the coordinate system where subjects select a set of responses that lay along a connecting line from the referenced data point or referenced time for that item. The term is named for Barbara Tversky based on her work on graphical primitives (e.g. “lines connect, arrows direct, boxes contain”).\n\n\nCODE\nverify_max = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-MAX\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_max <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_max, TV_max_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_max = str_replace_na(TV_max,\"\"),\n    TV_max_allow = str_replace_na(TV_max_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_max,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_max_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_max)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_max[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_max[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_max[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_max[x,'set_q'] = Q\n  keys_tversky_max[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_max[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_max <- keys_tversky_max %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\nverify_tversky_start = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-START\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_start <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_start, TV_start_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_start = str_replace_na(TV_start,\"\"),\n    TV_start_allow = str_replace_na(TV_start_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_start,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_start_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_start)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_start[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_start[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_start[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_start[x,'set_q'] = Q\n  keys_tversky_start[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_start[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_start <- keys_tversky_start %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n\nverify_tversky_end = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-END\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_end <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_end, TV_end_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_end = str_replace_na(TV_end,\"\"),\n    TV_end_allow = str_replace_na(TV_end_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_end,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_end_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_end)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_end[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_end[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_end[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_end[x,'set_q'] = Q\n  keys_tversky_end[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_end[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_end <- keys_tversky_end %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\nverify_tversky_duration = c()\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-DURATION\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_duration <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_dur, TV_dur_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_dur = str_replace_na(TV_dur,\"\"),\n    TV_dur_allow = str_replace_na(TV_dur_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_dur,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_dur_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_duration)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_duration[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_duration[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_duration[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_duration[x,'set_q'] = Q\n  keys_tversky_duration[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_duration[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_duration <- keys_tversky_duration %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\nThis leaves us four dataframes, each corresponding to a different variant of a ‘lines connecting to reference point’ strategy.\n- keys_tversky_max : the superset of lines connecting options - keys_tversky_start : lines connecting to the rightward diagonal (start time) of the reference point - keys_tversky_end: lines connecting to the leftward diagonal (end time) of the reference point - keys_tversky_duration: lines connecting to the horizontal y-intercept (duration) of the reference point\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nCODE\n#cleanup\nrm(verify_tri, verify_orth, verify_max, verify_tversky_duration, verify_tversky_end, verify_tversky_start, verify_satisfice_right, verify_satisfice_left)\n\n\n\n\n\n2.2.2 Score Items\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tverskyi nterpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points)\n\n\n\nCODE\nbackup <- read_rds('data/1-study-level/sgc3a_items.rds')\ndf_items <- read_rds('data/1-study-level/sgc3a_items.rds')\n\n\n\n\nCODE\ncalc_sub_score2 <- function(question, cond, response, keyframe){\n\n  # print(paste(question, cond, response))\n\n    #STEP 1 GET KEY\n  if (question < 6) #for q1 - q5 find key for question by condition\n  {\n    # print(keyframe)\n    #GET KEY FOR THIS SCORE TYPE, QUESTION AND CONDITION\n    p =  keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n    q =  keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% unlist()\n    pn = keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(n_p)\n    qn = keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(n_q)\n\n    # print(p)\n    # print(q)\n    # print(paste(\"pn \",pn))\n    # print(paste(\"qn \",qn))\n    \n  } else {\n    #GET KEY FOR THIS SCORE TYPE, QUESTION\n    p =  keyframe %>% filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n    q =  keyframe %>% filter(Q == question) %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% unlist()\n    pn = keyframe %>% filter(Q == question) %>% select(n_p)\n    qn = keyframe %>% filter(Q == question) %>% select(n_q)\n  }\n\n  #STEP 2 CALC INTERSECTIONS BETWEEN RESPONSE AND KEY\n  \n  #if response is not empty, split apart response for set comparison\n    if(response != \"\")\n    { response = response %>% str_split(\"\") %>% unlist()}\n  \n  #set comparisons \n  ps = length(intersect(response,p))\n  # print(paste(\"correct selected\" ,ps))\n  qs = length(intersect(response,q))\n  # print(paste(\"incorrect selected\", qs))\n  # df_items[x,'tri_ps'] = tri_ps\n  # df_items[x,'tri_qs'] = tri_qs\n\n  #STEP 3 CALC f_partialP schema SCORE FOR THIS INTERSECTION\n  x = f_partialP(ps,pn,qs,qn) %>% unlist() %>% as.numeric()\n  # print(x)\n  #cleanup\n  rm(p,q,pn,qn,ps,qs)\n\n  return(x)\n}\n\n#CALCULATE THE REFERENCE SCORES\ncalc_ref_score2 <- function(question, cond, response){\n  \n  #1. GET reference point from REF_POINT column in raw keys\n  if (question < 6) {\n    ref_p = keys_raw %>% filter(Q == question) %>% filter(condition == cond) %>% select(REF_POINT) %>% pull(REF_POINT) %>% str_split(\"\") %>% unlist()\n  } else {\n    ref_p = keys_raw %>% filter(Q == question) %>%select(REF_POINT) %>% \n      pull(REF_POINT) %>% str_split(\"\") %>% unlist()\n  }\n    \n    #2. Is the response PRECISELY the REFERENCE POINT?\n    x = identical(ref_p,response)\n    x = as.numeric(x)  \n  \n    # paste(\"ref: \",ref_p)\n    # paste(\"response: \",response)\n    # paste(\"x: \",ref_p == response)\n    \n    #cleanup\n    rm(ref_p, response, question, cond)   \n    return(x) #1 = match, 0 = not match\n}\n\n#CALCULATE SCORE BASED ON UNION OF ORTH & TRI (SUBJECT SELECTS BOTH ANSWERS )\ncalc_both_score2 <- function(question, cond, response){\n  \n  \n  #TRAPDOOR \n  #since no orth responses exist for impasse condition q1 - q5, set to 0\n  if (question < 6 & cond == 121) {x = NA}\n  \n  #ELSE \n  #calculate union of ORTH and TRI\n  else {\n    if (question < 6 & cond == 111) #for q1 - q5 find key for question by condition\n  {\n     #grab the tri and orth keys for this question as well as N option set\n     tri_p =  keys_tri %>%  filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n     orth_p = keys_orth %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n     set_n =  keys_tri %>%  filter(Q == question) %>% filter(condition == cond) %>% select(set_n) %>% pull(set_n) %>% str_split(\"\") %>% unlist() \n     #1. calc answer that is both tri and orth and only these --> union of tri_p and orth_p\n     both_p = union(tri_p, orth_p) #the selection of tri and p\n     #2. calc answers that should't be selected as diffrence between N [same for all keys] and both_p\n     both_q = setdiff(set_n,both_p)\n     both_pn = length(both_p)\n     both_qn = length(both_q)\n  } else{\n    \n     #grab the tri and orth keys for this question as well as N option set\n     tri_p =  keys_tri %>%  filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n     orth_p = keys_orth %>% filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n     set_n =  keys_tri %>%  filter(Q == question) %>% select(set_n) %>% pull(set_n) %>% str_split(\"\") %>% unlist() \n     #1. calc answer that is both tri and orth and only these --> union of tri_p and orth_p\n     both_p = union(tri_p, orth_p) #the selection of tri and p\n     #2. calc answers that shouldn't be selected as difference between N [same for all keys] and both_p\n     both_q = setdiff(set_n,both_p)\n     both_pn = length(both_p)\n     both_qn = length(both_q)\n  }\n    \n  #STEP 2 CALC INTERSECTIONS BETWEEN RESPONSE AND KEY\n  \n  #if response is not empty, split apart response for set comparison\n    if(response != \"\")\n    { response = response %>% str_split(\"\") %>% unlist()}\n  \n    both_ps = length(intersect(response,both_p))\n    both_qs = length(intersect(response,both_q))\n  \n \n  #STEP 3 CALC f_partialP schema SCORE FOR THIS INTERSECTION \n  x = f_partialP(both_ps,both_pn,both_qs,both_qn)%>% unlist() %>% as.numeric()\n  \n  #cleanup\n  rm(both_p,both_q,both_pn,both_qn,both_ps,both_qs, question, cond, response )   \n  }\n  \n  return(x) #true correct, trues, false correct, false\n}\n\n\n#REORDER THE CHARACTERS IN A STRING\nreorder_inplace <- function(x)\n{\n  y =  x %>% str_split(\"\") %>% unlist() %>% sort() %>% str_c(collapse=\"\")\n  return (y)\n}\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n# #STRATEGY SUBSCORES\ndf_items$score_TRI = pbmapply(calc_sub_score2, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_sub_score2, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\n\ndf_items$score_SAT_left = pbmapply(calc_sub_score2, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_sub_score2, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\n\ndf_items$score_TV_max = pbmapply(calc_sub_score2, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_sub_score2, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_sub_score2, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_sub_score2, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#TODO SPECIAL SUBSCORES\ndf_items$score_REF = pbmapply(calc_ref_score2, df_items$q, df_items$condition, df_items$response)\ndf_items$score_BOTH = pbmapply(calc_both_score2, df_items$q, df_items$condition, df_items$response)\n  \n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1))\n\n# alt_scored = df_items\n\n\nTODO: generate heat maps of Q9. Same answer but very different optimal operation paths!\n\n\n2.2.3 Derive Interpretation\nFinally, we compare the interpretation subscores and decide which is highest. This indicates the interpretation that the individual’s response most closely approximates. This algorithm makes assigns one of the following values to each item response:\n\nOrthogonal\nTriangular\nTversky\nboth Tri + Orth\nreference\nblank\nfrenzy\n? TODO ADJUST ‘both’ to select for both tri/satisfice or both tri/orth? See Q1 121 ‘CO’\n\n\n\nCODE\nthreshold_range = 0.5 #set required variance in subscores to be discriminant\nthreshold_frenzy = 4\n\nfor (x in 1:nrow(df_items)) {\n  \n  #CALCULATE MAX TVERSKY SUBSCORE\n  t = df_items[x,] %>% select(score_TV_max, score_TV_start, score_TV_end, score_TV_duration) #reshape\n  t.long = gather(t,score, value, 1:4)\n  t.long[t.long == \"\"] = NA #replace empty scores with NA so we can ignore them\n  if(length(unique(t.long$value)) == 1 ){\n    if(is.na(unique(t.long$value))){\n      df_items[x,'score_TVERSKY'] = NA\n      df_items[x,'tv_type'] = NA   \n    }\n  } else {\n    df_items[x,'score_TVERSKY'] = as.numeric(max(t.long$value,na.rm = TRUE))\n    df_items[x,'tv_type'] = t.long[which.max(t.long$value),'score']\n  }\n  \n  #CALCULATE MAX SATISFICING SUBSCORE\n  t = df_items[x,] %>% select(score_SAT_left, score_SAT_right)\n  t.long = gather(t,score, value, 1:2)\n  t.long[t.long == \"\"] = NA #replace empty scores\n  if(length(unique(t.long$value)) == 1 ){\n    if(is.na(unique(t.long$value))){\n      df_items[x,'score_SATISFICE'] = NA\n      df_items[x,'sat_type'] = NA   \n    }\n  } else {\n    df_items[x,'score_SATISFICE'] = as.numeric(max(t.long$value,na.rm = TRUE))\n    df_items[x,'sat_type'] = t.long[which.max(t.long$value),'score']  \n  }\n  \n  #NOW CALCULATE RANGE AMONG SUBSCORES\n  #order of this selection matters in breaking ties! \n  t = df_items[x,] %>% select(score_TRI, score_TVERSKY, score_SATISFICE, score_ORTH)\n  t.long = gather(t,score, value, 1:4)\n  t.long[t.long == \"\"] = NA\n  \n  df_items[x,'top_score'] = as.numeric(max(t.long$value,na.rm = TRUE))\n  df_items[x,'top_type'] = t.long[which.max(t.long$value),'score']\n  \n  #calculate the range between highest and lowest scores \n  r = as.numeric(range(t.long$value,na.rm = TRUE))\n  r = diff(r)\n  df_items[x,'range'] = r\n  \n  #DISCRIMINANT BETWEEN SUBSCORES TO PREDICT BEST FIT INTERPRETATION\n  \n  if (r < threshold_range) {\n      #then we can't predict the interpretation, leave it as \"?\"\n    df_items[x,'best'] = \"?\"\n  } else {\n      p =  df_items[x,'top_type']\n      if (p == \"score_TRI\") {df_items[x,'best'] = \"Triangular\"\n      } else if(p == \"score_ORTH\") {df_items[x,'best'] = \"Orthogonal\"\n      } else if(p == \"score_TVERSKY\") {df_items[x,'best'] = \"Tversky\"\n      } else if(p == \"score_SATISFICE\") {df_items[x,'best'] = \"Satisfice\"}\n  }\n  \n  #CHECK SPECIAL SITUATIONS\n\n  #BOTH TRI AND ORTH?  \n  if (!is.na(df_items[x,'score_BOTH'])) { #only check if both is not null\n      if( df_items[x,'score_BOTH'] == 1) {\n        df_items[x,'best'] = \"both tri + orth\"}\n  }\n  \n  #IS BLANK?\n  if( df_items[x,'num_o'] == 0) {  \n    df_items[x,'best'] = \"blank\"\n  }\n  \n  #IS FRENZY?\n  if( df_items[x,'num_o'] > threshold_frenzy) { \n      df_items[x,'best'] = \"frenzy\"\n  }\n\n  #IS REF POINT?\n  if (!is.na(df_items[x,'score_REF'])) { #only check if the score is NOT null\n      if( df_items[x,'score_REF'] == 1) {\n          df_items[x,'best'] = \"reference\"\n      }\n  }\n\n}#end loop\n\n#cleanup \nrm(t, t.long, x, r,p)\nrm(threshold_frenzy, threshold_range)\n\n#set order of levels\ndf_items$interpretation <- factor(df_items$best,\n                                  levels = c(\"Triangular\", \"Tversky\",\n                                             \"Satisfice\", \"Orthogonal\", \"reference\", \"both tri + orth\", \"blank\",\"frenzy\",\"?\"))\n\n\ndf_items$sorted_interpretation <- factor(df_items$best,\n                                  levels = c(\"Triangular\", \"Tversky\", \"both tri + orth\",\n                                             \"reference\",\"frenzy\",\"blank\",\"?\",\n                                             \"Satisfice\", \"Orthogonal\"))\n\n#recode as numeric inase they are char \n# df_items$score_TV_duration <- df_items$score_TV_duration %>% as.numeric()\n# df_items$score_SATISFICE <- df_items$score_SATISFICE %>% as.numeric()\n\n\n\n\n2.2.4 Derive Discriminant Score\n“Orthogonal” = -1, “Satisfice” = -1, “Triangular” = 1, “Tversky” = 0.5, “both tri + orth” = 0.5, “reference” = 0, “blank” = 0, “frenzy” = 0, “?” = 0)\nTODO should ‘both’ be coded as 0 or 1 or 0.5?\nTODO write description TODO recode with satisfice as -1\n\n\nCODE\ndf_items$score_SCALED <- recode(df_items$interpretation,\n                          \"Orthogonal\" = -1,\n                          \"Satisfice\" = -1,\n                          \"Triangular\" = 1,\n                          \"Tversky\" = 0.5,\n                          \"both tri + orth\" = 0.5,\n                          \"reference\" = 0,\n                          \"blank\" = 0, \n                          \"frenzy\" = 0,\n                          \"?\" = 0)"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#explore-responses",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#explore-responses",
    "title": "2  Response Scoring",
    "section": "2.5 EXPLORE RESPONSES",
    "text": "2.5 EXPLORE RESPONSES\nIn this section we explore responses given by participants to each particular item in the graph comprehension task, indicate how each response was scored, and what interpretation of the graph is indicated by different responses.\n\n2.5.1 Scaffold Phase\nThe first five questions constitute the ‘scaffold’ (or learning) phase, where participants see a different version of the stimulus (specifically a different dataset is visualized) invoking a different experimental condition.\n\n2.5.1.1 Question #1\n\n2.5.1.1.1 Q1. Control Condition\nWe start by exploring the range of response options checked by participants on Question 1, for those assigned to the control (non-impasse) condition (condition = 111).\n\n\n\nFigure 2.2: Question 1 — Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 111) %>% filter(Q==1)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q1 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q1 Control Condition :  Which shift(s) start at 11 am?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n    Z \n  \n  \n    Orthgonal \n    A \n    OI \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    CF \n    Z \n  \n  \n    Tversky [start diagonal] \n    F \n    Z \n  \n  \n    Tversky [end diagonal] \n    C \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nHere we summarize the distinct response options given by participants on this item. Each letter in response indicates a checkbox selected by the participant (See Figure 2.2 ). n indicates the number of participants who gave this response, while interpretation indicates the graph interpretation most consistent with that response. At the right of this table are the Absolute, followed by Partial Credit subscores for each response. NA indicates that there is no score calculated (occurs when there is no subset of response options that accord with that interpretation for this question).\nNotice that for this Question, the Triangular answer is the same as the Tversky [start diagonal] answer. In fact, for most questions, one of the Tversky sub-types will match the correct response.\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #1 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 1 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 1) %>% \n  pack_rows(\"Lines-Connect\", 2, 2) %>% \n  pack_rows(\"Orthogonal\", 3, 3) %>% \n  pack_rows(\"Other\", 4, 4)  %>% \n  pack_rows(\"Unknown\", 5, 7)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #1 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    22 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.083 \n    1.0 \n  \n  Lines-Connect\n\n    CF \n    3 \n    Tversky \n    0 \n    0.923 \n    1.000 \n    NA \n    -0.167 \n    0.5 \n  \n  Orthogonal\n\n    A \n    129 \n    Orthogonal \n    0 \n    -0.077 \n    -0.071 \n    NA \n    1.000 \n    -1.0 \n  \n  Other\n\n    AF \n    1 \n    both tri + orth \n    0 \n    0.923 \n    0.923 \n    NA \n    0.917 \n    0.5 \n  \n  Unknown\n\n    DIJ \n    1 \n    ? \n    0 \n    -0.231 \n    -0.214 \n    NA \n    -0.167 \n    0.0 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.077 \n    -0.071 \n    NA \n    -0.083 \n    0.0 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    0.000 \n    0.000 \n    NA \n    -0.083 \n    0.0 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nWe see that nearly all of the subjects selected a response consistent with one of the identified interpretations. Responses that do not accord with any interpretation are indicated as ? .\n\n\n\n\n\n\n\nWhich shifts start at 11am?\n\n\n\n\n\n\nResponse: A\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, projecting an invisible orthogonal line upward, and locating data point A.\n\n\n\n\nResponse: F\n\nindicates the triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following the right-diagonal gridline, identifying data point F.\n\n\n\n\nResponse: C, F\n\nindicates a maximal-Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following both the right-diagonal and left-diagonal gridlines, identifying both datapoints F and C.\n\n\n\n\nResponse: A , F\n\nThe reader selects both triangular and orthogonal-consistent data points\nPossibly indicates uncertainty or confusion\n\n\n\n\nThree responses were given that were not consistent with any of the identified interpretations. Note that options highlighted in light grey are considered within the range of ‘visual error’, defined by 0.5hr offset from the interpretation-specific projection.\n\n\n\n\n\n\n\n\nD I J\nX\nZ\nTODO find this person, did they subsequently give tri answers?\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.1.1.2 Q1. Impasse Condition\nNext we explore the range of response options checked by participants on Question 1, for those assigned to the control (non-impasse) condition (condition = 111).\n\n\n\nFigure 2.3: Question 1 — Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==1)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q1 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q1 Impasse Condition :  Which shift(s) start at 11 am?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    O \n     \n  \n  \n    Satisficing [right] \n    AI \n     \n  \n  \n    Tversky [maximal] \n    CF \n     \n  \n  \n    Tversky [start diagonal] \n    F \n     \n  \n  \n    Tversky [end diagonal] \n    C \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nNotice that there is no orthogonal answer for this question. This is the purpose of the impasse condition, to remove the possibility of selecting the orthogonal answer, we expect learners will be more likely to restructure their understanding of the coordinate system, and arrive at a correct (triangular) interpretation.\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #1 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 1 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 1) %>% \n  pack_rows(\"Lines-Connect\", 2, 4) %>% \n  pack_rows(\"Satisfice\", 5, 9) %>% \n  pack_rows(\"Other\", 10, 10) %>% \n  pack_rows(\"Unknown\", 11, 12) %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #1 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    49 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.071 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    CF \n    14 \n    Tversky \n    0 \n    0.929 \n    1.000 \n    -0.143 \n    NA \n    0.5 \n  \n  \n    C \n    3 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    -0.071 \n    NA \n    0.5 \n  \n  \n    CO \n    1 \n    Tversky \n    0 \n    -0.143 \n    0.929 \n    0.929 \n    NA \n    0.5 \n  \n  Satisfice\n\n    O \n    28 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AI \n    9 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    A \n    4 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    AO \n    2 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    0.929 \n    NA \n    -1.0 \n  \n  \n    I \n    2 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  Other\n\n     \n    57 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  Unknown\n\n    E \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.071 \n    NA \n    0.0 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.071 \n    NA \n    0.0 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nWe see that nearly all of the subjects selected a response consistent with one of the identified interpretations. Responses that do not accord with any interpretation are indicated as ? .\nTODO ADJUST ‘both’ to select for both tri/satisfice or both tri/orth\n\n\n\n\n\n\n\nWhich shifts start at 11am?\n\n\n\n\n\n\nResponse: F\n\nindicates the triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following the right-diagonal gridline, identifying data point F.\n\n\n\n\nResponse: [C, F]\n\nindicates a maximal-Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following both the right-diagonal and left-diagonal gridlines, identifying both datapoints F and C gridline.\n\n\n\n\nResponses: [AOI]\n\nindicates a satisficing strategy\nConsistent with the reader identifying the datapoints nearest to the orthogonal projection from the reference point point\n\n\n\n\nTwo responses were given that were not consistent with any of the identified interpretations.\n\n\n\n\n\n\n[E],[X]\n\n\n\n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==1)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q1 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q ==1)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q1 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\n2.5.1.2 Question #2\n\n2.5.1.2.1 Q2. Control Condition\n\n\n\nFigure 2.4: Q2—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 111) %>% filter(Q==2)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q2 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q2 Control Condition :  Which shift(s) start at the same time as D?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    K \n    Z \n  \n  \n    Orthgonal \n    E \n    G \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    AKJX \n    Z \n  \n  \n    Tversky [start diagonal] \n    AK \n    Z \n  \n  \n    Tversky [end diagonal] \n    X \n     \n  \n  \n    Tversky [duration line] \n    J \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #2 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 2 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 4) %>% \n  pack_rows(\"Orthogonal\", 5, 7) %>%\n  pack_rows(\"Other\", 8, 8)  %>% \n  pack_rows(\"Unknown\", 9, 10)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #2 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    K \n    24 \n    Triangular \n    1 \n    1.000 \n    0.500 \n    NA \n    -0.083 \n    1.0 \n  \n  \n    DK \n    1 \n    Triangular \n    1 \n    1.000 \n    0.500 \n    NA \n    -0.083 \n    1.0 \n  \n  Lines-Connect\n\n    J \n    4 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    NA \n    -0.083 \n    0.5 \n  \n  \n    AK \n    1 \n    Tversky \n    0 \n    0.917 \n    1.000 \n    NA \n    -0.167 \n    0.5 \n  \n  Orthogonal\n\n    E \n    121 \n    Orthogonal \n    0 \n    -0.083 \n    -0.077 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    DE \n    3 \n    Orthogonal \n    0 \n    -0.083 \n    -0.077 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EG \n    1 \n    Orthogonal \n    0 \n    -0.167 \n    -0.154 \n    NA \n    1.000 \n    -1.0 \n  \n  Other\n\n    D \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    B \n    1 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    NA \n    -0.083 \n    0.0 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    NA \n    -0.083 \n    0.0 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nAgain, we see that most subjects selected a response consistent with one of the identified interpretations. (note, when the question stem includes a data point rather than time as reference, we do not penalize respondents for selecting the reference data point in addition to an interpretation consistent response. For example, in this question, we do not penalize respondents for selecting option D, the reference point in the question. )\n\n\n\n\n\n\n\nWhich shift(s) start at the same time as D?\n\n\n\n\nReponse: E (also EG, DE)\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (D) on the graph, projecting an invisible orthogonal line through it, and locating data point E.\n\n\n\n\nResponse: K (also KD)\n\nindicates an triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (D) on the graph, and following its descending-leftward diagonal gridline, and locating data point K.\n\n\n\n\nResponse: AK\n\nindicates an Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (D) on the graph, and following its descending-leftward diagonal gridline, and locating data point K then continuing along the connecting ascending leftward diagonal locating data point A.\n\n\n\n\nResponse: J\n\nindicates an Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (D) on the graph, and following its horizontal gridline to the y-axis, locating data point J.\n\n\n\n\nResponse: D\n\nthe reader selected only the reference point\nConsistent with the reader identifying the reference point (D) on the graph\nPossibly indicates uncertainty or confusion\n\n\n\n\n\n\n\n\n\n\n\nB\nC\n\n\n\n\n\n\n\n\n\n\n\n2.5.1.2.2 Q2. Impasse Condition\n\n\n\nFigure 2.5: Q2—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==2)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q2 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q2 Impasse Condition :  Which shift(s) start at the same time as D?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    K \n    Z \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n    G \n     \n  \n  \n    Tversky [maximal] \n    JKE \n    Z \n  \n  \n    Tversky [start diagonal] \n    K \n    Z \n  \n  \n    Tversky [end diagonal] \n    E \n     \n  \n  \n    Tversky [duration line] \n    J \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #2 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 2 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 10) %>% \n  pack_rows(\"Satisfice\", 11, 12) %>%\n  pack_rows(\"Other\", 13, 16)  %>% \n  pack_rows(\"Unknown\", 17, 18)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #2 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    K \n    69 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    DK \n    1 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    J \n    12 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    EK \n    3 \n    Tversky \n    0 \n    0.917 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    EX \n    2 \n    Tversky \n    0 \n    -0.167 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BEG \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.846 \n    0.846 \n    NA \n    0.5 \n  \n  \n    E \n    1 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    EKX \n    1 \n    Tversky \n    0 \n    0.833 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    HJZ \n    1 \n    Tversky \n    0 \n    -0.167 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    JK \n    1 \n    Tversky \n    0 \n    0.917 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  Satisfice\n\n    G \n    19 \n    Satisfice \n    0 \n    -0.083 \n    -0.077 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    BG \n    2 \n    Satisfice \n    0 \n    -0.167 \n    -0.154 \n    0.923 \n    NA \n    -1.0 \n  \n  Other\n\n    D \n    7 \n    reference \n    0 \n    0.000 \n    NA \n    0.000 \n    NA \n    0.0 \n  \n  \n     \n    43 \n    blank \n    0 \n    0.000 \n    NA \n    0.000 \n    NA \n    0.0 \n  \n  \n    ACDFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.250 \n    0.250 \n    -0.846 \n    NA \n    0.0 \n  \n  \n    BEGKUZ \n    1 \n    frenzy \n    0 \n    0.667 \n    0.667 \n    0.615 \n    NA \n    0.0 \n  \n  Unknown\n\n    C \n    6 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    FO \n    1 \n    ? \n    0 \n    -0.167 \n    -0.154 \n    -0.154 \n    NA \n    0.0 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==2)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q2 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q ==2)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q2 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\n2.5.1.3 Question #3\n\n2.5.1.3.1 Q3. Control Condition\n\n\n\nFigure 2.6: Q3—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 111) %>% filter(Q==3)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q3 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q3 Control Condition :  Which shift(s) begin when C ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n    Z \n  \n  \n    Orthgonal \n    Z \n    FIO \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    AUBFOJ \n     \n  \n  \n    Tversky [start diagonal] \n    OJ \n     \n  \n  \n    Tversky [end diagonal] \n    F \n    Z \n  \n  \n    Tversky [duration line] \n    AUB \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #3 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 3 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 7) %>% \n  pack_rows(\"Orthogonal\", 8, 8) %>% \n  pack_rows(\"Other\", 9, 10) %>% \n  pack_rows(\"Unknown\", 11, 17)  \n\n\n\nFrequency of Selected Response Options for Question #3 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    24 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    0.0 \n    1.0 \n  \n  \n    EFK \n    1 \n    Triangular \n    0 \n    0.833 \n    0.833 \n    NA \n    -0.2 \n    1.0 \n  \n  Lines-Connect\n\n    ABU \n    4 \n    Tversky \n    0 \n    -0.250 \n    1.000 \n    NA \n    -0.3 \n    0.5 \n  \n  \n    O \n    3 \n    Tversky \n    0 \n    -0.083 \n    0.500 \n    NA \n    0.0 \n    0.5 \n  \n  \n    JO \n    2 \n    Tversky \n    0 \n    -0.167 \n    1.000 \n    NA \n    -0.1 \n    0.5 \n  \n  \n    DJO \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.917 \n    NA \n    -0.2 \n    0.5 \n  \n  \n    KO \n    1 \n    Tversky \n    0 \n    -0.167 \n    0.417 \n    NA \n    -0.1 \n    0.5 \n  \n  Orthogonal\n\n    Z \n    94 \n    Orthogonal \n    0 \n    0.000 \n    0.000 \n    NA \n    1.0 \n    -1.0 \n  \n  Other\n\n    C \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.0 \n    0.0 \n  \n  \n    ABDEFGHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.000 \n    NA \n    NA \n    0.0 \n    0.0 \n  \n  Unknown\n\n    A \n    18 \n    ? \n    0 \n    -0.083 \n    0.333 \n    NA \n    -0.1 \n    0.0 \n  \n  \n    K \n    3 \n    ? \n    0 \n    -0.083 \n    -0.083 \n    NA \n    -0.1 \n    0.0 \n  \n  \n    AH \n    1 \n    ? \n    0 \n    -0.167 \n    0.242 \n    NA \n    -0.2 \n    0.0 \n  \n  \n    DE \n    1 \n    ? \n    0 \n    -0.167 \n    -0.167 \n    NA \n    -0.2 \n    0.0 \n  \n  \n    E \n    1 \n    ? \n    0 \n    -0.083 \n    -0.083 \n    NA \n    -0.1 \n    0.0 \n  \n  \n    EU \n    1 \n    ? \n    0 \n    -0.167 \n    0.242 \n    NA \n    -0.2 \n    0.0 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.083 \n    0.333 \n    NA \n    -0.1 \n    0.0 \n  \n\n\n\n\n\nTODO\n\naddress RESPONSE FKE which is classified as Triangular but doesn’t seem to fit this interpretation?\nShould O,K be considered Tvresky ?\nconsider adding trapdoor on n_q, such that score is penalized (OR interpretation is not predicted?) if the Ss selects more than 1 extra options, or is missing more than 2 options?\nLEFT OFF HERE\n\n\n\n\n\n\n\n\nWhat shift(s) begin when C ends?\n\n\n\n\n\n\nResponse: Z\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (C) then using the duration encoded on the y-axis (2) , project along the horizontal gridline by two hours, and then project an invisible orthogonal line through that time (12PM) locating data point Z.\n\n\n\n\nResponse: F\n\nindicates a (correct) triangular interpretation of the coordinate system\nConsistent with the reader identifying the reference point (C) on the graph, and following the descending gridline to the x-axis to identify the end-time (11AM) and then following the ascending gridline to identify datapoints starting at 11AM and locating data point F.\n\n\n\n\nResponse: AUB (also A)\n\nindicates a Tversky strategy following connecting lines (duration)\nConsistent with the reader identifying the reference point (C) on the graph, and following the horizontal y-axis gridline and locating data points A U B.\n\n\n\n\nResponse: OJ\n\nindicates a Tversky strategy following connecting lines (start-time)\nConsistent with the reader identifying the reference point (C) on the graph, and following the ascending diagonal gridline and locating data points O J.\n\n\n\n\nResponse: C\n\nthe participant selected the point referenced in the question\npossibly indicates confusion or uncertainty\n\n\n\n\nResponse: AIOZFHJXKUDEGB\n\nthe participant selects all (or nearly all) the data points\npossibly indicates confusion or uncertainty\n\n\n\n\nSix responses (from 9 participants) appear inconsistent with any interpretation.\n\n\n\n\n\n\n\n\nK (n=3)\nAH (n=1)\nDE (n=1)\n\n\n\n\n\n\n\n\n\nUE (n=1)\nU (n=1)\nE (n=1)\n\n\n\n\n\n\n\n\n\n\n2.5.1.3.2 Q3. Impasse Condition\n\n\n\nFigure 2.7: Q3—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==3)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q3 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q3 Impasse Condition :  Which shift(s) begin when C ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    AI \n     \n  \n  \n    Satisficing [right] \n    F \n     \n  \n  \n    Tversky [maximal] \n    BJ \n     \n  \n  \n    Tversky [start diagonal] \n    J \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n    B \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nTODO investigate these responses 17 at O?\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #3 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 3 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 5) %>% \n  pack_rows(\"Lines-Connect\", 3, 5) %>% \n  pack_rows(\"Satisfice\", 6, 15) %>% \n  pack_rows(\"Other\", 16, 21) %>% \n  pack_rows(\"Unknown\", 22, 29) \n\n\n\nFrequency of Selected Response Options for Question #3 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    61 \n    Triangular \n    1 \n    1.000 \n    -0.077 \n    1.000 \n    NA \n    1.0 \n  \n  \n    AF \n    5 \n    Triangular \n    0 \n    0.923 \n    -0.154 \n    0.923 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    AFG \n    1 \n    Triangular \n    0 \n    0.846 \n    -0.231 \n    0.846 \n    NA \n    1.0 \n  \n  \n    B \n    8 \n    Tversky \n    0 \n    -0.077 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    J \n    3 \n    Tversky \n    0 \n    -0.077 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  Satisfice\n\n    BE \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BJ \n    1 \n    Tversky \n    0 \n    -0.154 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    HJZ \n    1 \n    Tversky \n    0 \n    -0.231 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    A \n    7 \n    Satisfice \n    0 \n    -0.077 \n    -0.077 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    AH \n    5 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    0.417 \n    NA \n    -1.0 \n  \n  \n    AI \n    3 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AOU \n    3 \n    Satisfice \n    0 \n    -0.231 \n    -0.231 \n    0.333 \n    NA \n    -1.0 \n  \n  \n    AFI \n    2 \n    Satisfice \n    0 \n    0.846 \n    -0.231 \n    0.917 \n    NA \n    -1.0 \n  \n  \n    AIO \n    2 \n    Satisfice \n    0 \n    -0.231 \n    -0.231 \n    0.917 \n    NA \n    -1.0 \n  \n  \n    AO \n    2 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    0.417 \n    NA \n    -1.0 \n  \n  Other\n\n    C \n    2 \n    reference \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  \n     \n    36 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  \n    ABDEFGHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  \n    ABDEFGHJKUZ \n    1 \n    frenzy \n    0 \n    0.231 \n    0.250 \n    0.231 \n    NA \n    0.0 \n  \n  \n    BDEFGHJKU \n    1 \n    frenzy \n    0 \n    0.385 \n    0.417 \n    0.385 \n    NA \n    0.0 \n  \n  \n    BDEFGHJKUXZ \n    1 \n    frenzy \n    0 \n    0.231 \n    0.250 \n    0.231 \n    NA \n    0.0 \n  \n  Unknown\n\n    O \n    17 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    DK \n    2 \n    ? \n    0 \n    -0.154 \n    -0.154 \n    -0.154 \n    NA \n    0.0 \n  \n  \n    FJZ \n    1 \n    ? \n    0 \n    0.846 \n    0.846 \n    0.846 \n    NA \n    0.0 \n  \n  \n    K \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    KO \n    1 \n    ? \n    0 \n    -0.154 \n    -0.154 \n    -0.154 \n    NA \n    0.0 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    0.0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==3)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q3 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q ==3)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q3 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\n2.5.1.4 Question #4\n[PLACEHOLDER — NOT YET CONSIDERED THIS QUESTION]\n\n2.5.1.4.0.1 Q4. Control Condition\n\n\n\nFigure 2.8: Q4—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 111) %>% filter(Q==4)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q4 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q4 Control Condition :  Which shift(s) end at 4 pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    H \n     \n  \n  \n    Orthgonal \n    U \n    OF \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    BH \n     \n  \n  \n    Tversky [start diagonal] \n    B \n     \n  \n  \n    Tversky [end diagonal] \n    H \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #4 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 4 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 3) %>% \n  pack_rows(\"Orthogonal\", 4, 8) %>% \n  pack_rows(\"Other\", 9, 10) %>% \n  pack_rows(\"Unknown\", 11, 16) \n\n\n\nFrequency of Selected Response Options for Question #4 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    H \n    29 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.083 \n    1.0 \n  \n  \n    AH \n    1 \n    Triangular \n    0 \n    0.929 \n    0.929 \n    NA \n    -0.167 \n    1.0 \n  \n  Lines-Connect\n\n    B \n    3 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    NA \n    -0.083 \n    0.5 \n  \n  Orthogonal\n\n    U \n    87 \n    Orthogonal \n    0 \n    -0.071 \n    -0.071 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    FU \n    2 \n    Orthogonal \n    0 \n    -0.143 \n    -0.143 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    DEOU \n    1 \n    Orthogonal \n    0 \n    -0.286 \n    -0.286 \n    NA \n    0.833 \n    -1.0 \n  \n  \n    DEU \n    1 \n    Orthogonal \n    0 \n    -0.214 \n    -0.214 \n    NA \n    0.833 \n    -1.0 \n  \n  \n    KU \n    1 \n    Orthogonal \n    0 \n    -0.143 \n    -0.143 \n    NA \n    0.917 \n    -1.0 \n  \n  Other\n\n     \n    6 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n    ACFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.286 \n    0.286 \n    NA \n    0.333 \n    0.0 \n  \n  Unknown\n\n    DE \n    14 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    NA \n    -0.167 \n    0.0 \n  \n  \n    E \n    6 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    0.0 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    0.0 \n  \n  \n    O \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    0.000 \n    0.0 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    0.0 \n  \n  \n    G \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    0.0 \n  \n\n\n\n\n\n\nTBL4 test\n\n\n\n\n\n\nOrthogonal\nOrthogonal-LinesConnecting\n\n\n\n\n |\n\n\n\nIf the subject calculates end time for each data point (using duration on the y axis), they find that an (incorrect) projection of point U ‘end time’ intersects with the (incorrect) orthogonal projection of 4:00PM.\nAlternatively, some subjects selected points E and D which intersect with an orthogonal projection from 4:00pm. We call this an ’orthogonal-lines connect” strategy, because it (incorrectly) adapts the orthogonal procedure for finding events that start at 4:00pm in order to find those that end at 4:00pm, thus selecting any data point with an orthogonal intersection with 4:00pm.\n\n\n\n\n\n2.5.1.4.0.2 Q4. Impasse Condition\n\n\n\nFigure 2.9: Q4—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==4)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q4 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q4 Impasse Condition :  Which shift(s) end at 4 pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    H \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    FO \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    BH \n     \n  \n  \n    Tversky [start diagonal] \n    B \n     \n  \n  \n    Tversky [end diagonal] \n    H \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nTODO investigate D? add to tversky or orth?\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #4 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 4 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 6) %>% \n  pack_rows(\"Satisfice\", 7, 10) %>% \n  pack_rows(\"Other\", 11, 12) %>% \n  pack_rows(\"Unknown\", 13, 19) \n\n\n\nFrequency of Selected Response Options for Question #4 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    H \n    64 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    DH \n    1 \n    Triangular \n    0 \n    0.929 \n    0.929 \n    -0.154 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    B \n    6 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    BD \n    2 \n    Tversky \n    0 \n    -0.143 \n    0.929 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BH \n    2 \n    Tversky \n    0 \n    0.929 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BDEG \n    1 \n    Tversky \n    0 \n    -0.286 \n    0.786 \n    -0.308 \n    NA \n    0.5 \n  \n  Satisfice\n\n    O \n    11 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    F \n    8 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    FO \n    7 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AFG \n    1 \n    Satisfice \n    0 \n    -0.214 \n    -0.214 \n    0.346 \n    NA \n    -1.0 \n  \n  Other\n\n     \n    20 \n    blank \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n    ACFHIJKOUZ \n    1 \n    frenzy \n    0 \n    0.357 \n    0.357 \n    0.385 \n    NA \n    0.0 \n  \n  Unknown\n\n    D \n    35 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    A \n    5 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    K \n    3 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    G \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    AI \n    1 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    -0.154 \n    NA \n    0.0 \n  \n  \n    DK \n    1 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    -0.154 \n    NA \n    0.0 \n  \n  \n    J \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    0.0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==4)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q4 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q ==4)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q4 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\n2.5.1.5 Question #5\n\n2.5.1.5.0.1 Q5. Control Condition\n\n\n\nFigure 2.10: Q5—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 111) %>% filter(Q==5)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q5 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q5 Control Condition :  Coffee breaks happen halfway through a shift. Which shift(s) share a break with I?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    O \n    AZ \n  \n  \n    Orthgonal \n    U \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    UGX \n    AZKD \n  \n  \n    Tversky [start diagonal] \n    X \n     \n  \n  \n    Tversky [end diagonal] \n    UG \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #5 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 5 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>% \n  pack_rows(\"Lines-Connect\", 5, 7) %>% \n  pack_rows(\"Orthogonal\", 8, 9) %>% \n  pack_rows(\"Other\", 10, 11) %>% \n  pack_rows(\"Unknown\", 12, 22) \n\n\n\nFrequency of Selected Response Options for Question #5 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    O \n    50 \n    Triangular \n    1 \n    1.000 \n    -0.077 \n    NA \n    -0.077 \n    1.0 \n  \n  \n    FO \n    3 \n    Triangular \n    0 \n    0.909 \n    -0.154 \n    NA \n    -0.154 \n    1.0 \n  \n  \n    HO \n    1 \n    Triangular \n    0 \n    0.909 \n    -0.154 \n    NA \n    -0.154 \n    1.0 \n  \n  \n    KO \n    1 \n    Triangular \n    0 \n    0.909 \n    -0.143 \n    NA \n    -0.154 \n    1.0 \n  \n  Lines-Connect\n\n    FG \n    2 \n    Tversky \n    0 \n    -0.182 \n    0.417 \n    NA \n    -0.154 \n    0.5 \n  \n  \n    G \n    1 \n    Tversky \n    0 \n    -0.091 \n    0.500 \n    NA \n    -0.077 \n    0.5 \n  \n  \n    X \n    1 \n    Tversky \n    0 \n    -0.091 \n    1.000 \n    NA \n    -0.077 \n    0.5 \n  \n  Orthogonal\n\n    U \n    64 \n    Orthogonal \n    0 \n    -0.091 \n    0.500 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    HU \n    1 \n    Orthogonal \n    0 \n    -0.182 \n    0.417 \n    NA \n    0.923 \n    -1.0 \n  \n  Other\n\n    I \n    1 \n    reference \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n     \n    6 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    F \n    10 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    0.0 \n  \n  \n    H \n    3 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    0.0 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    0.0 \n  \n  \n    B \n    2 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    0.0 \n  \n  \n    DJ \n    2 \n    ? \n    0 \n    -0.182 \n    -0.143 \n    NA \n    -0.154 \n    0.0 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.091 \n    0.000 \n    NA \n    -0.077 \n    0.0 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    0.0 \n  \n  \n    DEHJ \n    1 \n    ? \n    0 \n    -0.364 \n    -0.308 \n    NA \n    -0.308 \n    0.0 \n  \n  \n    FK \n    1 \n    ? \n    0 \n    -0.182 \n    -0.143 \n    NA \n    -0.154 \n    0.0 \n  \n  \n    HJ \n    1 \n    ? \n    0 \n    -0.182 \n    -0.154 \n    NA \n    -0.154 \n    0.0 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    0.000 \n    0.000 \n    NA \n    -0.077 \n    0.0 \n  \n\n\n\n\n\nTODO note the compelling cases of internal inconsistency (HJDE)\n\n\n2.5.1.5.1 Q5. Impasse Condition\n\n\n\nFigure 2.11: Q5—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==5)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q5 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q5 Impasse Condition :  Coffee breaks happen halfway through a shift. Which shift(s) share a break with I?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    A \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    K \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    OX \n     \n  \n  \n    Tversky [start diagonal] \n    OX \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #5 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 5 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 7) %>% \n  pack_rows(\"Lines-Connect\", 8, 13) %>% \n  pack_rows(\"Orthogonal\", 14, 16) %>% \n  pack_rows(\"Other\", 17, 21) %>% \n  pack_rows(\"Unknown\", 22, 31) \n\n\n\nFrequency of Selected Response Options for Question #5 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    A \n    83 \n    Triangular \n    1 \n    1.000 \n    -0.083 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    AFG \n    5 \n    Triangular \n    0 \n    0.846 \n    -0.250 \n    -0.231 \n    NA \n    1.0 \n  \n  \n    AF \n    4 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AO \n    2 \n    Triangular \n    0 \n    0.923 \n    0.417 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AI \n    1 \n    Triangular \n    1 \n    1.000 \n    -0.083 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    AU \n    1 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AZ \n    1 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    O \n    6 \n    Tversky \n    0 \n    -0.077 \n    0.500 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    CO \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.417 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    JO \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.417 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    OX \n    1 \n    Tversky \n    0 \n    -0.154 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    UXZ \n    1 \n    Tversky \n    0 \n    -0.231 \n    0.333 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    X \n    1 \n    Tversky \n    0 \n    -0.077 \n    0.500 \n    -0.077 \n    NA \n    0.5 \n  \n  Orthogonal\n\n    K \n    5 \n    Satisfice \n    0 \n    -0.077 \n    -0.083 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    HK \n    3 \n    Satisfice \n    0 \n    -0.154 \n    -0.167 \n    0.923 \n    NA \n    -1.0 \n  \n  \n    HKUZ \n    1 \n    Satisfice \n    0 \n    -0.308 \n    -0.333 \n    0.769 \n    NA \n    -1.0 \n  \n  Other\n\n    I \n    2 \n    reference \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n     \n    24 \n    blank \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n    ABCFGUZ \n    1 \n    frenzy \n    0 \n    0.538 \n    -0.583 \n    -0.538 \n    NA \n    0.0 \n  \n  \n    ACDEFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.154 \n    0.167 \n    0.154 \n    NA \n    0.0 \n  \n  \n    FHJKX \n    1 \n    frenzy \n    0 \n    -0.385 \n    0.167 \n    0.692 \n    NA \n    0.0 \n  \n  Unknown\n\n    H \n    11 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    C \n    2 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    DJ \n    2 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    0.0 \n  \n  \n    F \n    2 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    FU \n    2 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    0.0 \n  \n  \n    DG \n    1 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    0.0 \n  \n  \n    FHZ \n    1 \n    ? \n    0 \n    -0.231 \n    -0.250 \n    -0.231 \n    NA \n    0.0 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    0.0 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    0.0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==5)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q5 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q ==5)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q5 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\n\n2.5.2 Testing Phase\nThe following 10 questions were the same for both conditions.\n\n2.5.2.1 Question #6 NONDISCRIM\n\n\n\nFigure 2.12: Q6-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==6)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) are six hours long?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    EG \n     \n  \n  \n    Orthgonal \n    EG \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\nTODO discuss non discriminant\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #6\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 6) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) \n\n\n\nFrequency of Selected Response Options for Question #6\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  \n    EG \n    330 \n    both tri + orth \n    1 \n    1 \n    NA \n    NA \n    1 \n    0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==6)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q6 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q ==6)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q6 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n2.5.2.2 Question #7\n\n\n\nFigure 2.13: Q7-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==7)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which 2 shifts less than 5 hours long start at the same time?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    OX \n     \n  \n  \n    Orthgonal \n    FB \n    M \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    IJZNCHOX \n     \n  \n  \n    Tversky [start diagonal] \n    OX \n     \n  \n  \n    Tversky [end diagonal] \n    IJZN \n     \n  \n  \n    Tversky [duration line] \n    CH \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #7\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 7) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 5) %>%\n  pack_rows(\"Lines-Connect\", 6, 9) %>%\n  pack_rows(\"Orthogonal\", 10, 13) %>%\n  pack_rows(\"Other\", 14, 14) %>%\n  pack_rows(\"Unknown\", 15, 17)\n\n\n\nFrequency of Selected Response Options for Question #7\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    OX \n    93 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MO \n    2 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    AX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MOX \n    1 \n    Triangular \n    0 \n    0.938 \n    0.938 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.067 \n    1.0 \n  \n  Lines-Connect\n\n    IJ \n    3 \n    Tversky \n    0 \n    -0.125 \n    0.500 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    CH \n    1 \n    Tversky \n    0 \n    -0.125 \n    1.000 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    DJNX \n    1 \n    Tversky \n    0 \n    0.312 \n    0.357 \n    NA \n    -0.267 \n    0.5 \n  \n  \n    HK \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.438 \n    NA \n    -0.133 \n    0.5 \n  \n  Orthogonal\n\n    BF \n    203 \n    Orthogonal \n    0 \n    -0.125 \n    -0.125 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    FZ \n    16 \n    Orthogonal \n    0 \n    -0.125 \n    0.179 \n    NA \n    0.433 \n    -1.0 \n  \n  \n    B \n    1 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    1 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    0.500 \n    -1.0 \n  \n  Other\n\n     \n    2 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    GK \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.133 \n    0.0 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.125 \n    0.179 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    KM \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.067 \n    0.0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 7)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q7 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q == 7)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q7 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n2.5.2.3 Question #8\n\n\n\nFigure 2.14: Q8-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==8)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q: \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q:  Which shift(s) under 7 hours long starts before B starts, and ends after X ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    G \n     \n  \n  \n    Orthgonal \n    E \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #8\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 8) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 10) %>%\n  pack_rows(\"Orthogonal\", 11, 16) %>%\n  pack_rows(\"Other\", 17, 21) %>%\n  pack_rows(\"Unknown\", 22, 45)\n\n\n\nFrequency of Selected Response Options for Question #8\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    G \n    64 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    -0.067 \n    1 \n  \n  \n    AGK \n    4 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1 \n  \n  \n    CG \n    3 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1 \n  \n  \n    FG \n    3 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1 \n  \n  \n    AG \n    2 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1 \n  \n  \n    CFGO \n    2 \n    Triangular \n    0 \n    0.800 \n    NA \n    NA \n    -0.267 \n    1 \n  \n  \n    ACGP \n    1 \n    Triangular \n    0 \n    0.800 \n    NA \n    NA \n    -0.267 \n    1 \n  \n  \n    CFG \n    1 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1 \n  \n  \n    CGM \n    1 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1 \n  \n  \n    GM \n    1 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1 \n  \n  Orthogonal\n\n    E \n    157 \n    Orthogonal \n    0 \n    -0.067 \n    NA \n    NA \n    1.000 \n    -1 \n  \n  \n    EIJ \n    5 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.867 \n    -1 \n  \n  \n    EFIJ \n    3 \n    Orthogonal \n    0 \n    -0.267 \n    NA \n    NA \n    0.800 \n    -1 \n  \n  \n    EF \n    2 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.933 \n    -1 \n  \n  \n    EI \n    2 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.933 \n    -1 \n  \n  \n    EFI \n    1 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.867 \n    -1 \n  \n  Other\n\n     \n    12 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0 \n  \n  \n    DEHIJNOZ \n    2 \n    frenzy \n    0 \n    -0.533 \n    NA \n    NA \n    0.533 \n    0 \n  \n  \n    EFGIJ \n    2 \n    frenzy \n    0 \n    0.733 \n    NA \n    NA \n    0.733 \n    0 \n  \n  \n    CDGHLNOXZ \n    1 \n    frenzy \n    0 \n    0.533 \n    NA \n    NA \n    -0.533 \n    0 \n  \n  \n    DEIJN \n    1 \n    frenzy \n    0 \n    -0.333 \n    NA \n    NA \n    0.733 \n    0 \n  \n  Unknown\n\n    IJ \n    17 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    I \n    7 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    EFG \n    3 \n    ? \n    0 \n    0.867 \n    NA \n    NA \n    0.867 \n    0 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    O \n    3 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    A \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    AK \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    C \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    DN \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    F \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    IJM \n    2 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    0 \n  \n  \n    L \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    M \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    CM \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    CX \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    DHNZ \n    1 \n    ? \n    0 \n    -0.267 \n    NA \n    NA \n    -0.267 \n    0 \n  \n  \n    DIJN \n    1 \n    ? \n    0 \n    -0.267 \n    NA \n    NA \n    -0.267 \n    0 \n  \n  \n    EFGI \n    1 \n    ? \n    0 \n    0.800 \n    NA \n    NA \n    0.800 \n    0 \n  \n  \n    HLO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    0 \n  \n  \n    IO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    KL \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 8)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q8 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q == 8)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q8 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n2.5.2.4 Question #9 NONDISCRIM\n\n\n\nFigure 2.15: Q9-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==9)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) begins before J begins and ends during B?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    I \n     \n  \n  \n    Orthgonal \n    I \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #9\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q ==9) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Other\", 1, 2) %>%\n  pack_rows(\"Unknown\", 3, 19)\n\n\n\nFrequency of Selected Response Options for Question #9\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Other\n\n    I \n    247 \n    both tri + orth \n    1 \n    1.000 \n    NA \n    NA \n    1.000 \n    0.5 \n  \n  \n     \n    23 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    E \n    29 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0.0 \n  \n  \n    F \n    6 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0.0 \n  \n  \n    M \n    4 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0.0 \n  \n  \n    EI \n    3 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    0.0 \n  \n  \n    FI \n    3 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    0.0 \n  \n  \n    J \n    3 \n    ? \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0.0 \n  \n  \n    AGN \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    0.0 \n  \n  \n    B \n    1 \n    ? \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0.0 \n  \n  \n    CHO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    0.0 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0.0 \n  \n  \n    DK \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0.0 \n  \n  \n    IJ \n    1 \n    ? \n    1 \n    1.000 \n    NA \n    NA \n    1.000 \n    0.0 \n  \n  \n    IM \n    1 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    0.0 \n  \n  \n    IO \n    1 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    0.0 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0.0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 9)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q9 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q == 9)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q9 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n2.5.2.5 Question #10\n\n\n\nFigure 2.16: Q10-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==10)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) end at the same time as F?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    E \n     \n  \n  \n    Orthgonal \n    X \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    EGZ \n     \n  \n  \n    Tversky [start diagonal] \n    G \n     \n  \n  \n    Tversky [end diagonal] \n    E \n     \n  \n  \n    Tversky [duration line] \n    Z \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #10\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 10) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 7) %>%\n  pack_rows(\"Orthogonal\", 8, 11) %>%\n  pack_rows(\"Other\", 12, 14) %>%\n  pack_rows(\"Unknown\", 15, 27)\n\n\n\nFrequency of Selected Response Options for Question #10\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    E \n    103 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    EF \n    1 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  Lines-Connect\n\n    Z \n    23 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    XZ \n    2 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    0.938 \n    0.5 \n  \n  \n    CG \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    -0.125 \n    0.5 \n  \n  \n    G \n    1 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    HLPZ \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.812 \n    NA \n    -0.250 \n    0.5 \n  \n  Orthogonal\n\n    X \n    139 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BX \n    2 \n    Orthogonal \n    0 \n    -0.125 \n    -0.125 \n    NA \n    0.938 \n    -1.0 \n  \n  \n    FX \n    2 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    AMX \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    -0.188 \n    NA \n    0.875 \n    -1.0 \n  \n  Other\n\n    F \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n     \n    6 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    CEGIO \n    1 \n    frenzy \n    0 \n    0.750 \n    0.750 \n    NA \n    -0.312 \n    0.0 \n  \n  Unknown\n\n    B \n    27 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    J \n    6 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    IJ \n    2 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    0.0 \n  \n  \n    P \n    2 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    BO \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    0.0 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    HLP \n    1 \n    ? \n    0 \n    -0.188 \n    -0.188 \n    NA \n    -0.188 \n    0.0 \n  \n  \n    I \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    0.0 \n  \n  \n    K \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    L \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    O \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 10)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q10 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q == 10)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q10 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n2.5.2.6 Question #11\n\n\n\nFigure 2.17: Q11-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==11)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) start at 12pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    ML \n     \n  \n  \n    Orthgonal \n    FB \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #11\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 11) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>%\n  pack_rows(\"Orthogonal\", 5, 9) %>%\n  pack_rows(\"Other\", 10, 12) %>%\n  pack_rows(\"Unknown\", 13, 17)\n\n\n\nFrequency of Selected Response Options for Question #11\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    LM \n    99 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    -0.125 \n    1 \n  \n  \n    M \n    7 \n    Triangular \n    0 \n    0.500 \n    NA \n    NA \n    -0.062 \n    1 \n  \n  \n    BLM \n    2 \n    Triangular \n    0 \n    0.938 \n    NA \n    NA \n    0.375 \n    1 \n  \n  \n    EKM \n    1 \n    Triangular \n    0 \n    0.375 \n    NA \n    NA \n    -0.188 \n    1 \n  \n  Orthogonal\n\n    BF \n    201 \n    Orthogonal \n    0 \n    -0.125 \n    NA \n    NA \n    1.000 \n    -1 \n  \n  \n    B \n    4 \n    Orthogonal \n    0 \n    -0.062 \n    NA \n    NA \n    0.500 \n    -1 \n  \n  \n    F \n    2 \n    Orthogonal \n    0 \n    -0.062 \n    NA \n    NA \n    0.500 \n    -1 \n  \n  \n    BFXZ \n    1 \n    Orthogonal \n    0 \n    -0.250 \n    NA \n    NA \n    0.875 \n    -1 \n  \n  \n    BH \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    NA \n    NA \n    0.438 \n    -1 \n  \n  Other\n\n     \n    4 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0 \n  \n  \n    ACDGHKLMNOPXZ \n    1 \n    frenzy \n    0 \n    0.312 \n    NA \n    NA \n    -0.812 \n    0 \n  \n  \n    DHLMNOXZ \n    1 \n    frenzy \n    0 \n    0.625 \n    NA \n    NA \n    -0.500 \n    0 \n  \n  Unknown\n\n    J \n    2 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    0 \n  \n  \n    CX \n    1 \n    ? \n    0 \n    -0.125 \n    NA \n    NA \n    -0.125 \n    0 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    0 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    0 \n  \n  \n    XZ \n    1 \n    ? \n    0 \n    -0.125 \n    NA \n    NA \n    -0.125 \n    0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 11)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q11 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q == 11)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q11 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n2.5.2.7 Question #12\n\n\n\nFigure 2.18: Q12-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==12)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) start at the same time as F?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    G \n     \n  \n  \n    Orthgonal \n    B \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    GZ \n     \n  \n  \n    Tversky [start diagonal] \n    G \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n    Z \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #12\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 12) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 3) %>%\n  pack_rows(\"Lines-Connect\", 4, 6) %>%\n  pack_rows(\"Orthogonal\", 7, 8) %>%\n  pack_rows(\"Other\", 9, 10) %>%\n  pack_rows(\"Unknown\", 11, 14)\n\n\n\nFrequency of Selected Response Options for Question #12\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    G \n    98 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    FG \n    3 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    GP \n    1 \n    Triangular \n    0 \n    0.938 \n    0.938 \n    NA \n    -0.125 \n    1.0 \n  \n  Lines-Connect\n\n    Z \n    4 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    BZ \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    0.938 \n    0.5 \n  \n  \n    B \n    206 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  Orthogonal\n\n    BF \n    5 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n     \n    3 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Other\n\n    CEGIO \n    1 \n    frenzy \n    0 \n    0.750 \n    0.750 \n    NA \n    -0.312 \n    0.0 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  Unknown\n\n    E \n    2 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    FM \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    0.0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 12)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q12 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q == 12)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q12 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n2.5.2.8 Question #13\n\n\n\nFigure 2.19: Q13-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==13)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which 2 shifts end when Z begins?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    EF \n     \n  \n  \n    Orthgonal \n    FX \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #13\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 13) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 3) %>%\n  pack_rows(\"Orthogonal\", 4, 13) %>%\n  pack_rows(\"Other\", 14, 14) %>%\n  pack_rows(\"Unknown\", 15, 36)\n\n\n\nFrequency of Selected Response Options for Question #13\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    EF \n    91 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    0.433 \n    1 \n  \n  \n    CE \n    1 \n    Triangular \n    0 \n    0.433 \n    NA \n    NA \n    -0.133 \n    1 \n  \n  \n    E \n    1 \n    Triangular \n    0 \n    0.500 \n    NA \n    NA \n    -0.067 \n    1 \n  \n  Orthogonal\n\n    FX \n    141 \n    Orthogonal \n    0 \n    0.433 \n    NA \n    NA \n    1.000 \n    -1 \n  \n  \n    X \n    9 \n    Orthogonal \n    0 \n    -0.067 \n    NA \n    NA \n    0.500 \n    -1 \n  \n  \n    OX \n    4 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1 \n  \n  \n    KX \n    3 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1 \n  \n  \n    ACX \n    1 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.367 \n    -1 \n  \n  \n    BX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1 \n  \n  \n    CX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1 \n  \n  \n    DJNX \n    1 \n    Orthogonal \n    0 \n    -0.267 \n    NA \n    NA \n    0.300 \n    -1 \n  \n  \n    GX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1 \n  \n  \n    JX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1 \n  \n  Other\n\n     \n    5 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0 \n  \n  Unknown\n\n    HN \n    13 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    BF \n    11 \n    ? \n    0 \n    0.433 \n    NA \n    NA \n    0.433 \n    0 \n  \n  \n    F \n    10 \n    ? \n    0 \n    0.500 \n    NA \n    NA \n    0.500 \n    0 \n  \n  \n    EX \n    6 \n    ? \n    0 \n    0.433 \n    NA \n    NA \n    0.433 \n    0 \n  \n  \n    HL \n    5 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    HLP \n    5 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    0 \n  \n  \n    BM \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    CO \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    DN \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    AG \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    BO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    CG \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    CGO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    0 \n  \n  \n    CH \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    DKM \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    0 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    HZ \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n  \n    LP \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    NO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    0 \n  \n  \n    NZ \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 13)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q13 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q == 13)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q13 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n2.5.2.9 Question #14\n\n\n\nFigure 2.20: Q14-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==14)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) end at 3pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    X \n     \n  \n  \n    Orthgonal \n    B \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    XJND \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n    X \n     \n  \n  \n    Tversky [duration line] \n    JND \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #14\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 14) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>%\n  pack_rows(\"Orthogonal\", 5, 7) %>%\n  pack_rows(\"Other\", 8, 9) %>%\n  pack_rows(\"Unknown\", 10, 22)\n\n\n\nFrequency of Selected Response Options for Question #14\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    X \n    107 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.059 \n    1.0 \n  \n  \n    FX \n    2 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  \n    EX \n    1 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  \n    OX \n    1 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  Orthogonal\n\n    B \n    150 \n    Orthogonal \n    0 \n    -0.059 \n    -0.059 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BF \n    12 \n    Orthogonal \n    0 \n    -0.118 \n    -0.118 \n    NA \n    0.941 \n    -1.0 \n  \n  \n    BIO \n    2 \n    Orthogonal \n    0 \n    -0.176 \n    -0.176 \n    NA \n    0.882 \n    -1.0 \n  \n  Other\n\n    BX \n    2 \n    both tri + orth \n    0 \n    0.941 \n    0.941 \n    NA \n    0.941 \n    0.5 \n  \n  \n     \n    29 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    O \n    5 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    0.0 \n  \n  \n    F \n    3 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    0.0 \n  \n  \n    G \n    3 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    0.0 \n  \n  \n    A \n    2 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    0.0 \n  \n  \n    HLP \n    2 \n    ? \n    0 \n    -0.176 \n    -0.176 \n    NA \n    -0.176 \n    0.0 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    0.0 \n  \n  \n    AH \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    0.0 \n  \n  \n    DHO \n    1 \n    ? \n    0 \n    -0.176 \n    0.200 \n    NA \n    -0.176 \n    0.0 \n  \n  \n    FG \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    0.0 \n  \n  \n    HL \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    0.0 \n  \n  \n    IJ \n    1 \n    ? \n    0 \n    -0.118 \n    0.267 \n    NA \n    -0.118 \n    0.0 \n  \n  \n    M \n    1 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    0.0 \n  \n  \n    P \n    1 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    0.0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 14)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q14 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q == 14)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q14 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n2.5.2.10 Question #15\n\n\n\nFigure 2.21: Q15-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==15)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Coffee breaks happen halfway through a shift. Which shifts share a break at 2pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    XK \n     \n  \n  \n    Orthgonal \n    EF \n    B \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    XKZ \n     \n  \n  \n    Tversky [start diagonal] \n    Z \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #15\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 15) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(interpretation),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 10) %>%\n  pack_rows(\"Lines-Connect\", 11, 13) %>%\n  pack_rows(\"Orthogonal\", 14, 22) %>%\n  pack_rows(\"Other\", 23, 23) %>%\n  pack_rows(\"Unknown\", 24, 44)\n\n\n\nFrequency of Selected Response Options for Question #15\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    KX \n    100 \n    Triangular \n    1 \n    1.000 \n    0.667 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    X \n    6 \n    Triangular \n    0 \n    0.500 \n    0.333 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    CX \n    2 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    DJNX \n    2 \n    Triangular \n    0 \n    0.312 \n    0.133 \n    NA \n    -0.267 \n    1.0 \n  \n  \n    AKPX \n    1 \n    Triangular \n    0 \n    0.875 \n    0.533 \n    NA \n    -0.267 \n    1.0 \n  \n  \n    CK \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    GK \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    JX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    K \n    1 \n    Triangular \n    0 \n    0.500 \n    0.333 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    LX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  Lines-Connect\n\n    FZ \n    3 \n    Tversky \n    0 \n    -0.125 \n    0.941 \n    NA \n    0.433 \n    0.5 \n  \n  \n    OZ \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.941 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    Z \n    1 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.067 \n    0.5 \n  \n  Orthogonal\n\n    EF \n    118 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BF \n    17 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    13 \n    Orthogonal \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    E \n    8 \n    Orthogonal \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BE \n    4 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BEF \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    -0.176 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EFZ \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    0.882 \n    NA \n    0.933 \n    -1.0 \n  \n  \n    EI \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.433 \n    -1.0 \n  \n  \n    FI \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.433 \n    -1.0 \n  \n  Other\n\n     \n    11 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    G \n    4 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    B \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.000 \n    0.0 \n  \n  \n    C \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    O \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    AG \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    0.0 \n  \n  \n    BM \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    CG \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    0.0 \n  \n  \n    M \n    2 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    A \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    BG \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    DN \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    0.0 \n  \n  \n    FK \n    1 \n    ? \n    0 \n    0.438 \n    0.267 \n    NA \n    0.433 \n    0.0 \n  \n  \n    FX \n    1 \n    ? \n    0 \n    0.438 \n    0.267 \n    NA \n    0.433 \n    0.0 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    HN \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    0.0 \n  \n  \n    HO \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    0.0 \n  \n  \n    I \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    IJ \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    0.0 \n  \n  \n    J \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    L \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    0.0 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    0.0 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 15)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q15 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~sorted_interpretation, fill = ~condition, data = df_items %>% filter(q == 15)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q15 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nTODO TRAPDOOR?\nSET T = options that result in -1 score (trapdoor)\nTODO: TEST first with NO trapdoor, and see what the scores yield, then if the responses can be scaled based on comparing subscores\nf_partialP <- function(t,p,f,q) {\nt = number of correct-selected options p = number of true options f = number of incorrect-selected options q = number of false options n = number of options + p + q return( (t / p) - (f/q)) }\nTo prepare for partial scoring [-1/q, + 1/p], we first need to define the parameters t, p , f, q\nOur approach to calculating an ORTHOGONAL PARTIAL score is as follows:\n\n1/p for each correct-selected\n-1/q for each incorrect-selected (except ‘allowed’ based on reference point and visual error)\ntrapdoor : default to -1 if select triangular"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#todo-export",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#todo-export",
    "title": "2  Response Scoring",
    "section": "\n2.4 TODO EXPORT",
    "text": "2.4 TODO EXPORT\nSummarize by subject Export subjects Export items\n\ndoes response time predict interpretation vs. non interpretation?\n\nFinally, we export the scored items and summarized subjects.\n\nCODE#prep items\ndf_items <- df_items %>% select(-best) %>% mutate(\n  tv_type = as.factor(tv_type)\n)\n\n#SAVE FILES\n# write.csv(df_subjects,\"data/1-study-level/sgc3a_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"data/2-scored-data/sgc3a_scored_items.csv\", row.names = FALSE)\n\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\n# rio::export(df_subjects, \"data/1-study-level/sgc3a_participants.rds\") # to R data structure file\nrio::export(df_items, \"data/2-scored-data/sgc3a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#resources",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#resources",
    "title": "2  Response Scoring",
    "section": "2.7 RESOURCES",
    "text": "2.7 RESOURCES\nset operations\nhttps://stat.ethz.ch/R-manual/R-devel/library/base/html/sets.html\nkableExtra tables\nhttps://haozhu233.github.io/kableExtra/awesome_table_in_html.html#grouped_columns__rows\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] Hmisc_4.7-0      Formula_1.2-4    survival_3.3-1   lattice_0.20-45 \n [5] pbapply_1.5-0    ggpubr_0.4.0     ggformula_0.10.1 ggridges_0.5.3  \n [9] scales_1.2.0     ggstance_0.3.5   kableExtra_1.3.4 forcats_0.5.1   \n[13] stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4      readr_2.1.2     \n[17] tidyr_1.2.0      tibble_3.1.7     ggplot2_3.3.6    tidyverse_1.3.1 \n\nloaded via a namespace (and not attached):\n [1] colorspace_2.0-3    ggsignif_0.6.3      ellipsis_0.3.2     \n [4] rio_0.5.29          htmlTable_2.4.0     base64enc_0.1-3    \n [7] fs_1.5.2            rstudioapi_0.13     farver_2.1.0       \n[10] bit64_4.0.5         fansi_1.0.3         lubridate_1.8.0    \n[13] xml2_1.3.3          codetools_0.2-18    splines_4.2.1      \n[16] knitr_1.39          polyclip_1.10-0     jsonlite_1.8.0     \n[19] broom_0.8.0         cluster_2.1.3       dbplyr_2.2.1       \n[22] png_0.1-7           ggforce_0.3.3       compiler_4.2.1     \n[25] httr_1.4.3          backports_1.4.1     assertthat_0.2.1   \n[28] Matrix_1.4-1        fastmap_1.1.0       cli_3.3.0          \n[31] tweenr_1.0.2        htmltools_0.5.2     tools_4.2.1        \n[34] gtable_0.3.0        glue_1.6.2          Rcpp_1.0.8.3       \n[37] carData_3.0-5       cellranger_1.1.0    vctrs_0.4.1        \n[40] svglite_2.1.0       xfun_0.31           openxlsx_4.2.5     \n[43] rvest_1.0.2         lifecycle_1.0.1     mosaicCore_0.9.0   \n[46] rstatix_0.7.0       MASS_7.3-57         vroom_1.5.7        \n[49] hms_1.1.1           parallel_4.2.1      RColorBrewer_1.1-3 \n[52] curl_4.3.2          yaml_2.3.5          gridExtra_2.3      \n[55] labelled_2.9.1      rpart_4.1.16        latticeExtra_0.6-29\n[58] stringi_1.7.6       highr_0.9           checkmate_2.1.0    \n[61] zip_2.2.0           rlang_1.0.3         pkgconfig_2.0.3    \n[64] systemfonts_1.0.4   evaluate_0.15       htmlwidgets_1.5.4  \n[67] labeling_0.4.2      bit_4.0.4           tidyselect_1.1.2   \n[70] plyr_1.8.7          magrittr_2.0.3      R6_2.5.1           \n[73] generics_0.1.2      DBI_1.1.3           pillar_1.7.0       \n[76] haven_2.5.0         foreign_0.8-82      withr_2.5.0        \n[79] abind_1.4-5         nnet_7.3-17         modelr_0.1.8       \n[82] crayon_1.5.1        car_3.1-0           utf8_1.2.2         \n[85] tzdb_0.3.0          rmarkdown_2.14      jpeg_0.1-9         \n[88] grid_4.2.1          readxl_1.4.0        data.table_1.14.2  \n[91] reprex_2.0.1        digest_0.6.29       webshot_0.5.3      \n[94] munsell_0.5.0       viridisLite_0.4.0  \n\n\n\n2.7.1 ARCHIVE\nfunctions for for-loop version of scoring\n\n\nCODE\n# #CALCULATE THE TRIANGULAR, ORTHOGONAL OR TVERSKIAN SUBSCORES FROM KEYFRAME\n# calc_sub_score <- function(question, cond, response,keyframe){\n# \n#   #STEP 1 GET KEY\n#   if (question < 6) #for q1 - q5 find key for question by condition\n#   {\n#     # print(keyframe)\n#     #GET KEY FOR THIS SCORE TYPE, QUESTION AND CONDITION\n#     p =  keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#     q =  keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% unlist()\n#     pn = keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(n_p)\n#     qn = keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(n_q)\n# \n#   } else {\n#     #GET KEY FOR THIS SCORE TYPE, QUESTION\n#     p =  keyframe %>% filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#     q =  keyframe %>% filter(Q == question) %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% unlist()\n#     pn = keyframe %>% filter(Q == question) %>% select(n_p)\n#     qn = keyframe %>% filter(Q == question) %>% select(n_q)\n#   }\n# \n#   #STEP 2 CALC INTERSECTIONS BETWEEN RESPONSE AND KEY\n#   \n#   #if response is not empty, split apart response for set comparison\n#     if(response != \"\")\n#     { response = response %>% str_split(\"\") %>% unlist()}\n#     \n#   ps = length(intersect(response,p))\n#   qs = length(intersect(response,q))\n#   # df_items[x,'tri_ps'] = tri_ps\n#   # df_items[x,'tri_qs'] = tri_qs\n# \n#   #STEP 3 CALC f_partialP schema SCORE FOR THIS INTERSECTION\n#   x = f_partialP(ps,pn,qs,qn) %>% unlist() %>% as.numeric()\n#   \n#   #cleanup\n#   rm(p,q,pn,qn,ps,qs)\n#   return(x)\n# \n# }\n# \n# #CALCULATE THE REFERENCE SCORES\n# calc_ref_score <- function(question, cond, response){\n#   \n#     #1. GET reference point from REF_POINT column in raw keys\n#     ref_p = keys_raw %>% filter(Q == question) %>% filter(condition == cond) %>% select(REF_POINT) %>% pull(REF_POINT) %>% str_split(\"\") %>% unlist()\n#      \n#     #2. if response has more than one character, it can't be correct\n#     #there is only ever 1 reference character\n#     n = nchar(response)\n#     if (n == 0) {x = 0}\n#     else if(n>1) {x = 0}\n#     else {\n#       #3 is the response PRECISELY the REFERENCE POINT?\n#       x = ref_p == response\n#       x = as.numeric(x)  \n#     }\n#     \n#     #cleanup\n#     rm(ref_p, response, question, cond)   \n#     return(x) #1 = match, 0 = not match\n# }\n# \n# \n# #CALCULATE SCORE BASED ON UNION OF ORTH & TRI (SUBJECT SELECTS BOTH ANSWERS )\n# calc_both_score <- function(question, cond, response){\n#   \n#TRAPDOOR \n#   #since no orth responses exist for impasse condition q1 - q5, set to 0\n#   if (question < 6 & cond == 121) {x = NA}\n#   \n#   #ELSE \n#   #calculate union of ORTH and TRI\n#   else {\n#     if (question < 6 & cond == 111) #for q1 - q5 find key for question by condition\n#   {\n#      #grab the tri and orth keys for this question as well as N option set\n#      tri_p =  keys_tri %>%  filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      orth_p = keys_orth %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      set_n =  keys_tri %>%  filter(Q == question) %>% filter(condition == cond) %>% select(set_n) %>% pull(set_n) %>% str_split(\"\") %>% unlist() \n#      #1. calc answer that is both tri and orth and only these --> union of tri_p and orth_p\n#      both_p = union(tri_p, orth_p) #the selection of tri and p\n#      #2. calc answers that should't be selected as diffrence between N [same for all keys] and both_p\n#      both_q = setdiff(set_n,both_p)\n#      both_pn = length(both_p)\n#      both_qn = length(both_q)\n#   } else{\n#     \n#      #grab the tri and orth keys for this question as well as N option set\n#      tri_p =  keys_tri %>%  filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      orth_p = keys_orth %>% filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      set_n =  keys_tri %>%  filter(Q == question) %>% select(set_n) %>% pull(set_n) %>% str_split(\"\") %>% unlist() \n#      #1. calc answer that is both tri and orth and only these --> union of tri_p and orth_p\n#      both_p = union(tri_p, orth_p) #the selection of tri and p\n#      #2. calc answers that shouldn't be selected as difference between N [same for all keys] and both_p\n#      both_q = setdiff(set_n,both_p)\n#      both_pn = length(both_p)\n#      both_qn = length(both_q)\n#   }\n#     \n#   #STEP 2 CALC INTERSECTIONS BETWEEN RESPONSE AND KEY\n#   \n#   #if response is not empty, split apart response for set comparison\n#     if(response != \"\")\n#     { response = response %>% str_split(\"\") %>% unlist()}\n#   \n#     both_ps = length(intersect(response,both_p))\n#     both_qs = length(intersect(response,both_q))\n#   \n#  \n#   #STEP 3 CALC f_partialP schema SCORE FOR THIS INTERSECTION \n#   x = f_partialP(both_ps,both_pn,both_qs,both_qn)%>% unlist() %>% as.numeric()\n#   \n#   #cleanup\n#   rm(both_p,both_q,both_pn,both_qn,both_ps,both_qs, question, cond, response )   \n#   }\n#   \n#   return(x) #true correct, trues, false correct, falses\n# }\n\n\nlooping to do the scoring\n\n\nCODE\n#RUN THIS OR THE CALCULATE-SCORES-MAPPLY\n# df_items = trad \n# \n# pb <- timerProgressBar() \n# on.exit(close(pb)) \n#  \n# #CALCULATE SUBSCORES (in loop)\n# \n# for (x in 1:nrow(df_items)) {\n#   \n#   #show progress bar \n#   setTimerProgressBar(pb, x) \n#   \n#   #PREPARE ITEMS FOR SCORING\n#   #sort response vectors alphabetically\n#   #doesn't impact scoring, but does impact response display tables\n#    df_items[x,'response'] <-  df_items[x,'response'] %>% str_split(\"\") %>% unlist() %>% sort() %>% str_c(collapse=\"\")\n# \n#   #get properties of the RESPONSE ITEM\n#   qu = df_items[x,'q'] %>% as.numeric()\n#   cond = as.character(df_items[x,'condition']) %>% as.numeric()\n#   r = df_items[x,'response'] \n# \n#   #calculate the main subscores\n#   df_items[x,'score_TRI'] = calc_sub_score(qu, cond, r,keys_tri)\n#   df_items[x,'score_ORTH'] = calc_sub_score(qu, cond, r,keys_orth)\n#   df_items[x,'score_SATISFICE'] = calc_sub_score(qu, cond, r,keys_satisfice)\n#   df_items[x,'score_TV_max'] = calc_sub_score(qu, cond, r,keys_tversky_max)\n#   df_items[x,'score_TV_start'] = calc_sub_score(qu, cond, r,keys_tversky_start)\n#   df_items[x,'score_TV_end'] = calc_sub_score(qu, cond, r,keys_tversky_end)\n#   df_items[x,'score_TV_duration'] = calc_sub_score(qu, cond, r, keys_tversky_duration)\n#   \n#   #calculate special subscores\n#   df_items[x,'score_REF'] = calc_ref_score(qu, cond, r)\n#   df_items[x,'score_BOTH'] = calc_both_score(qu, cond, r)\n# }\n# \n# #CALCULATE ABSOLUTE SCORES\n# #calculate absolute scores dichotomous\n# df_items$score_ABS = as.integer(df_items$correct)\n# #niceABS indicates if the response is correct without penalizing the allowable triangular options(ie. the ref point)\n# df_items$score_niceABS  <- as.integer((df_items$score_TRI == 1))\n#  \n# #cleanup\n# rm(qu,cond,r, x)\n\n# trad_scored = df_items\n\n\nsanity check equivalence of for-loop and mapply scoring\n\n\nCODE\n#CHECK EQUIVALENCE OF LOOP AND MAPPLY SCORING \n# tests = data.frame (\n#   alt_tri = alt_scored$score_TRI,\n#   trad_tri = trad_scored$score_TRI,\n#   alt_orth = alt_scored$score_ORTH,\n#   trad_orth = trad_scored$score_ORTH,\n#   alt_ref = alt_scored$score_REF,\n#   trad_ref = trad_scored$score_REF,\n#   alt_tv_max = alt_scored$score_TV_max,\n#   trad_tv_max = trad_scored$score_TV_max,\n#   alt_tv_dur = alt_scored$score_TV_duration,\n#   trad_tv_dur = trad_scored$score_TV_duration,\n#   alt_tv_start = alt_scored$score_TV_start,\n#   trad_tv_start = trad_scored$score_TV_start,\n#   alt_tv_end = alt_scored$score_TV_end,\n#   trad_tv_end = trad_scored$score_TV_end,\n#   alt_both = alt_scored$score_BOTH,\n#   trad_both = trad_scored$score_BOTH,\n#   trad_response = trad_scored$response,\n#   alt_response = alt_scored$response,\n#   q_match = trad_scored$q == alt_scored$q,\n#   q = trad_scored$q,\n#   c_match = trad_scored$condition == alt_scored$condition,\n#   condition = trad_scored$condition\n# )\n# \n# tests$tri = tests$alt_tri == tests$trad_tri\n# tests$orth = tests$alt_orth == tests$trad_orth\n# tests$ref = tests$alt_ref == tests$trad_ref\n# tests$tvdur = tests$alt_tv_dur == tests$trad_tv_dur\n# tests$tvstart = tests$alt_tv_start == tests$trad_tv_start\n# tests$tvend = tests$alt_tv_end == tests$trad_tv_end\n# tests$both = tests$alt_both == tests$trad_both\n# \n# #CHECKS \n# unique(tests$tri)\n# unique(tests$orth)\n# unique(tests$ref)\n# unique(tests$tvdur)\n# unique(tests$tvstart)\n# unique(tests$tvend)\n# unique(tests$both)\n# \n# unique(alt_scored$score_ABS == trad_scored$score_ABS)\n# unique(alt_scored$score_niceABS == trad_scored$score_niceABS)\n\n\n\n\n2.7.2 TODO A Discriminant Score\n[TODO old text from creating discriminant score; reconsider this]\nThough it appears the Partial \\([-1/q, +1/p]\\) scheme is a more appropriate scoring method for differentiating between meaningful patterns of responses, it does have one significant limitation: it treats all incorrectly selected answer options in the same way.\nIn the case of the SGC study paradigm, the choice of which options the subject selects reveals important information about their understanding of the graph stimulus. Specifically, there are certain patterns of options that correspond to a triangular versus orthogonal interpretations. In the previous example, rows 2 and 3 both indicate a subject has selected the correct option (A) plus one additional option (B or O). Both responses receive the same score under \\([-1/q, +1/q]\\). These two responses may be very meaningfully different, however if one of the alternate options chosen indicates a different interpretation of the stimulus. The inclusion of data-point B might be reasonable under some degree of visual tracing error, while the inclusion of data-point O might indicate a cartesian misinterpretation of the coordinate system. Essentially, some answer options are more incorrect than others. We want to be able to differentiate between these responses.\nTo accomplish this task, we need to sets of interpretation-consistent options for each item. Based on our prior work, we define four interpretations for which to define options. We can think of these as four different answer keys, each defining the set of ‘correct’ options (those that should be selected) under each interpretation of the graph.\n\nTriangular : the (true, actually correct) triangular interpretation of the coordinate system; indicated by data points intersecting the appropriate diagonal projection from the x-axis.\nOrthogonal: the (incorrect but most common) cartesian interpretation of the coordinate system; indicated by data points intersecting an (invisible) orthogonal projection from the x-axis.\nLines-Connect: an (incorrect) interpretation that is partially-triangular, insofar as it is indicated by data points intersecting any diagonal projection from the x-axis (not necessarily the correct projection).\nSatisficing: an (incorrect) interpretation that indicated by selecting the data points nearest to the (invisible) orthogonal projection from the x-axis; indicates an orthogonal interpretation in absence of an available orthogonal intersect.\n\nTODO FIX THIS SECTION ONWARDS … RESPONSE SETS NEED NOT BE MUTUTALLY EXCLUSIVE\nThus for each item, in the graph comprehension task, we will define five sets, whose members constitute a partition of the set of answer options.\n\\[\\begin{align}\nQ &= \\text{the (universal) set of all answer options } \\\\\n\\\\\nT &=  \\{o:o \\:is\\:triangular  \\}, |T| \\ne 0  \\\\\nR &=  \\{o:o \\:is\\:orthogonal \\} \\\\\nL &=  \\{o:o \\:is\\:line-connecting \\} \\\\\nS &=  \\{o:o \\:is\\:satisficing \\} \\\\\n\\\\\n\\emptyset &= T \\cap R \\cap L \\cap S  \\: \\text{(the interpretation sets are disjoint)}\\\\\nD &= Q - \\{T \\cup R \\cup L \\cup S \\} , \\text{(distractors; the remaining options not consistent with any interpretation})\n\\\\\n\\end{align}\\]\nFor example, for the following sample stimuli, TODO IMAGE\n\\[\\begin{align}\nQ &= \\{a, b, c, d, e, f, g \\} \\\\\n  &= \\{ \\{a \\}\\}, \\{b\\}, \\{c,d\\}, \\{e\\},  \\{f,g\\}\\} \\\\\n\\\\\nT &= \\{ a \\} \\\\\nR &= \\{ b \\} \\\\\nL &= \\{ c,d \\} \\\\\nS &= \\{ e \\} \\\\\nD &= \\{ f, g\\} \\\\\n\\end{align}\\]\nTo capture this important source of variation, we can apply the idea behind the partial scoring \\([-1/q, +1/p]\\) scheme in combination with the response option subgroups given by the graph interpretations to produce a single score (scaled from \\(-1\\) to \\(+1\\)) that captures the nature of the respondent’s partial knowledge.\nA Discriminant Score will offer:\n\n+1 for a complete triangular response\n-1 for a complete orthogonal response\n+/- fraction of complete triangular or orthogonal response\n0 for non-strategy responses\n0 for blank (empty) responses\n\n\\[\\begin{align}\n\\text{Score}_{discriminant} &= 0 + 1*(t_s / t) - 1 * (r_s / r) + 0*(d_s / d) \\\\\n&= (t_s / t) - (r_s / r)\n\\end{align}\\]\nWhere:\n\n\\(t = |T|\\) number of triangular-correct options (consistent with triangular interpretation)\n\\(r = |R| =\\) number of orthogonal-correct options (consistent with orthogonal interpretation)\n\\(d\\) = number of non-strategy options (not consistent with either interpretation)\n\\(n\\) = total number of response options (equals the number of checkboxes in the question; equals the number of data points in the stimulus graph)\n\\(n = t + r + d\\) (15 for scaffold phase, 18 for test phase)\n\\(t_s \\ge 0\\) number of triangular-correct options selected\n\\(r_s \\ge 0\\) = number of orthogonal-correct options selected\n\\(d_s \\ge 0\\) = number of non-strategy options options selected\n\\(s \\ge 0\\) = number of options selected\n\\(s = t_s + r_s + d_s\\)\n\nTODO:: add 0.5(tversky) - 0.5(satisficing)\n\n\nCODE\nf_discrim <- function(t_s,t,r_s,r){\n  return((t_s / t) - (r_s / r))\n}\n\n\n\n\n\n\n\n\nSchmidt, Dennis, Tobias Raupach, Annette Wiegand, Manfred Herrmann, and Philipp Kanzow. 2021. “Relation Between Examinees’ True Knowledge and Examination Scores: Systematic Review and Exemplary Calculations on Multiple-True-False Items.” Educational Research Review 34 (November): 100409. https://doi.org/10.1016/j.edurev.2021.100409."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_exploration.html",
    "href": "analysis/SGC3A/3_sgc3A_exploration.html",
    "title": "3  Exploration",
    "section": "",
    "text": "THIS NOTEBOOK IS INCOMPLETE\nThe purpose of this notebook is explore the distribution of dependent variables for Study SGC3A."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_exploration.html#sample",
    "href": "analysis/SGC3A/3_sgc3A_exploration.html#sample",
    "title": "3  Exploration",
    "section": "3.1 Sample",
    "text": "3.1 Sample\n\n3.1.1 Data Collection\nData was initially collected (in person, SONA groups in computer lab) in Fall 2017. In Spring 2018, additional data were collected after small modifications were made to the experimental platform to increase the size of multiple-choice input buttons, and to add an additional free-response question following the main task block. In Fall 2021, the study was replicated using asynchronous, online SONA pool, with additional participants collected in Winter 2022.\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_subjects$term, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    fall17 \n    27 \n    27 \n    54 \n  \n  \n    spring18 \n    35 \n    37 \n    72 \n  \n  \n    fall21 \n    68 \n    71 \n    139 \n  \n  \n    winter22 \n    28 \n    37 \n    65 \n  \n  \n    Sum \n    158 \n    172 \n    330 \n  \n\n\n\n\n\n\n\n3.1.2 Participants\n\n\nCODE\n#Describe participants\nsubject.stats <- rbind(\n  \"lab\"= df_subjects %>% filter(mode == 'lab-synch') %>% select(age) %>% unlist() %>% favstats(),\n  \"online\" = df_subjects %>% filter(mode == \"asynch\") %>% select(age) %>% unlist() %>% favstats()\n) \nsubject.stats$female <- c(\n  (df_subjects %>% filter(mode == 'lab-synch') %>%  filter(gender==\"Female\") %>% count())$n,\n  (df_subjects %>% filter(mode == \"asynch\") %>% filter(gender==\"Female\") %>% count())$n\n)\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    female \n  \n \n\n  \n    lab \n    18 \n    19 \n    20 \n    21 \n    33 \n    20.4 \n    2.12 \n    126 \n    0 \n    78 \n  \n  \n    online \n    18 \n    20 \n    20 \n    21 \n    31 \n    20.6 \n    2.00 \n    204 \n    0 \n    137 \n  \n\n\n\n\n\nFor in-person collection, 126 participants (60 % female ) undergraduate STEM majors at a public American University participated in person in exchange for course credit (age: 18 - 33 years). Participants were randomly assigned to one of two experimental groups.\nFor online replication 204 participants (70 % female ) undergraduate STEM majors at a public American University participated online, asynchronously in exchange for course credit (age: 18 - 31 years). Participants were randomly assigned to one of two experimental groups.\n\n\nCODE\nreport_participants(df_subjects, education = \"schoolyear\", sex = \"gender\", group = \"mode\")\n\n\n[1] \"For the 'mode - asynch' group: 204 participants (Mean age = 20.6, SD = 2.0, range: [18, 31]; Sex: 67.2% females, 30.4% males, 2.5% other; Education: First, 11.76%; Second, 16.67%; Third, 33.82%; Fourth, 34.31%; Fifth, 2.94%; Other, 0.49%) and for the 'mode - lab-synch' group: 126 participants (Mean age = 20.4, SD = 2.1, range: [18, 33]; Sex: 61.9% females, 37.3% males, 0.8% other; Education: First, 21.43%; Second, 20.63%; Third, 26.98%; Fourth, 19.05%; Fifth, 9.52%; Other, 2.38%)\""
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_exploration.html#response-accuracy",
    "href": "analysis/SGC3A/3_sgc3A_exploration.html#response-accuracy",
    "title": "3  Exploration",
    "section": "3.2 Response Accuracy",
    "text": "3.2 Response Accuracy\n\n3.2.1 Cumulative Scores\nCumulative scores indicate the response accuracy by a particular participant across all discriminant items (n=13) in the graph comprehension task.\n\n3.2.1.1 Cumulative Absolute Score\nRecall from Section 2.1.2.1 that the absolute score (following the dichotomous scoring approach) s_ABS indicates if the subject’s response for a particular item was perfectly correct: whether they selected all correct answer options and no others (excluding certain allowed exceptions, such as also selecting the data point referenced in the question). The absolute score for an individual item is either 0 or 1. When summarized across the entire set of discriminant items, the cumulative absolute score for an individual subject ranges from [0,13].\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Cumulative Item Absolute Score)\"\nabs.stats <- rbind(\n  \"lab\"= df_subjects %>% filter(mode == 'lab-synch') %>% select(s_ABS) %>% unlist() %>% favstats(),\n  \"online\" = df_subjects %>% filter(mode == \"asynch\") %>% select(s_ABS) %>% unlist() %>% favstats()\n) \nabs.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Cumulative Item Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    0 \n    0 \n    1 \n    9 \n    13 \n    4.08 \n    5.05 \n    126 \n    0 \n  \n  \n    online \n    0 \n    0 \n    1 \n    8 \n    13 \n    3.51 \n    4.87 \n    204 \n    0 \n  \n\n\n\n\n\nFor in person collection, cumulative absolute scores (n = 126) range from 0 to 13 with a mean score of (M = 4.08, SD = 5.05).\nFor online replication, (online) cumulative accuracy scores (n = 204) range from 0 to 13 with a slighly lower mean score of (M = 3.51, SD = 4.87).\n\n\nCODE\n#VISUALIZE distribution of response accuracy across SUBJECTS\n\n#HISTOGRAM\nstats = df_subjects %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(s_ABS))\ngf_density(~s_ABS, data = df_subjects) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_lims(x = c(0, 13)) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Cumulative Absolute Score\",\n       y = \"proportion of subjects\",\n       title = \"Subject Cumulative Score (Absolute)\",\n       subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n  theme_minimal()\n\n\n\n\n\nCODE\n#RIDGEPLOT\n# ggplot(data = df_subjects, aes(x = s_NABS, y = mode)) +\n#   geom_density_ridges() + xlim(0,13)+\n#   facet_wrap(~condition, labeller = label_both) +\n#  labs(x = \"Cumulative Absolute Score\",\n#       y = \"proportion of subjects\",\n#        title = \"Subject Cumulative Score (Absolute)\",\n#        subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n#   theme_minimal()\n\n\n\n\n\n\n\n\nDecision\n\n\n\nCondition appears to exert a positive influence on Cumulative Absolute Score acrosss data collection modalities.\n\n\n\n\n3.2.1.2 Cumulative Scaled Scores\nThe Cumulative Scaled score s_SCALED summarizes the scaled score on the 13 strategy-discriminant questions, for each subject The range is from -13 (all orthogonal) to 13 (all triangular). Recall that the s_SCALED score for an item is a numeric representation of the strategy-consistent response, scaled from -1 to +1, where “Orthogonal” = -1, “Satisfice” = -1, “Triangular” = 1, “Tversky” = 0.5, “both tri + orth” = 0.5, “reference” = 0, “blank” = 0, “frenzy” = 0, “?” = 0.\nMost importantly, the Scaled score gives us a way of quantitatively examining how correctly a participant interpreted the coordinate system across the entire set of items. It offers a more nuanced look into performance than absolute score\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Cumulative Scaled Score)\"\nscaled.stats <- rbind(\n  \"lab\"= df_subjects %>% filter(mode == 'lab-synch') %>% select(s_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_subjects %>% filter(mode == \"asynch\") %>% select(s_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Cumulative Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -13 \n    -11.9 \n    -6 \n    9.75 \n    13 \n    -2.13 \n    10.05 \n    126 \n    0 \n  \n  \n    online \n    -13 \n    -9.0 \n    -6 \n    8.00 \n    13 \n    -2.03 \n    9.12 \n    204 \n    0 \n  \n\n\n\n\n\nFor in person collection, cumulative absolute scores (n = 126) range from -13 to 13 with a mean score of (M = -2.13, SD = 10.05).\nFor online replication, (online) cumulative accuracy scores (n = 204) range from -13 to 13 with a slighly lower mean score of (M = -2.03, SD = 9.12).\n\n\nCODE\n#VISUALIZE distribution of response accuracy across SUBJECTS\n\n#HISTOGRAM\nstats = df_subjects %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(s_SCALED))\ngf_density(~s_SCALED, data = df_subjects, binwidth = 1) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_lims(x = c(-13, 13)) %>%\n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n   labs(x = \"Cumulative Scaled Score\",\n       y = \"proportion of subjects\",\n       title = \"Subject Cumulative Score (Scaled)\",\n       subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nDecision\n\n\n\nCondition appears to exert a positive influence on Scaled Score across data collection modalities.\n\n\n\n\n\n3.2.2 Item Scores\nItem scores indicate the response accuracy by a participant on each individual question discriminant question (n=13) in the graph comprehension task.\n\n3.2.2.1 Item Absolute Score\n\n\nCODE\nx <- df_items %>% mutate(score = as.logical(score_ABS))\n\ntitle = \"Proportion of Correct Items By Condition (Lab)\"\n\nitem.contingency <- df_items %>% filter(mode == \"lab-synch\") %>% select(score_ABS, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Items By Condition (Lab)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    0 \n    0.344 \n    0.268 \n    0.613 \n  \n  \n    1 \n    0.148 \n    0.240 \n    0.387 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Items By Condition (Online)\"\nitem.contingency <- df_items %>% filter(mode == \"asynch\") %>% select(score_ABS, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Items By Condition (Online)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    0 \n    0.342 \n    0.307 \n    0.649 \n  \n  \n    1 \n    0.128 \n    0.223 \n    0.351 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\nAcross both data collection modalities, the proportion of correct answers is greater in the impasse vs. control condition.\n\n\nCODE\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM\nstats = df_items %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(score_niceABS))\ngf_props(~score_niceABS, data = df_items) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) +\n  labs(x = \"Item Absolute Score\",\n       title = \"Item Absolute Score\",\n       subtitle=\"Across modalities, the impasse condition yielded more correct responses\")+\n  theme_minimal()\n\n\n\n\n\n\n\n3.2.2.2 Item Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1.\n\n\nCODE\ntitle = \"Descriptive Statistics of Item Response Accuracy (Scaled Score)\"\nscaled.stats.items <- rbind(\n  \"lab\"= df_items %>% filter(mode == 'lab-synch') %>% select(score_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_items %>% filter(mode == \"asynch\") %>% select(score_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats.items %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Item Response Accuracy (Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    -0.084 \n    0.870 \n    1890 \n    0 \n  \n  \n    online \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    -0.077 \n    0.832 \n    3060 \n    0 \n  \n\n\n\n\n\n\n\nCODE\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM\nstats = df_items %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(score_SCALED))\ngf_density(~score_SCALED, data = df_items) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Scaled Score for Item\",\n       y = \"Proportion of Items\",\n       title = \"Distribution of Accuracy per Item (Scale Score)\",\n       subtitle=\"The impasse condition shifts density toward the positive score\")+\n  theme_minimal()\n\n\n\n\n\n\n\n3.2.2.3 Item Interpretation Scores\n\n\nCODE\n#VISUALIZE distribution of interpretations across all ITEMS\n\n\n#REORDER INTERPRETATION LEVELS\ndf_items$interpretation <- factor(df_items$interpretation, levels = c(\"Triangular\", \"Tversky\", \"both tri + orth\", \"blank\", \"?\", \"frenzy\",\"reference\",\"Satisfice\", \"Orthogonal\"))            \n\n\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Lab)\"\nitem.contingency <- df_items %>% filter(mode == \"lab-synch\") %>% select(interpretation, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Lab)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    Triangular \n    0.094 \n    0.195 \n    0.288 \n  \n  \n    Tversky \n    0.004 \n    0.017 \n    0.021 \n  \n  \n    both tri + orth \n    0.061 \n    0.056 \n    0.116 \n  \n  \n    blank \n    0.008 \n    0.034 \n    0.042 \n  \n  \n    ? \n    0.025 \n    0.054 \n    0.079 \n  \n  \n    frenzy \n    0.002 \n    0.005 \n    0.007 \n  \n  \n    reference \n    0.001 \n    0.004 \n    0.005 \n  \n  \n    Satisfice \n    0.000 \n    0.028 \n    0.028 \n  \n  \n    Orthogonal \n    0.297 \n    0.116 \n    0.414 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Online)\"\nitem.contingency <- df_items %>% filter(mode == \"asynch\") %>% select(interpretation, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Online)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    Triangular \n    0.078 \n    0.175 \n    0.253 \n  \n  \n    Tversky \n    0.011 \n    0.023 \n    0.035 \n  \n  \n    both tri + orth \n    0.056 \n    0.061 \n    0.118 \n  \n  \n    blank \n    0.013 \n    0.055 \n    0.068 \n  \n  \n    ? \n    0.050 \n    0.066 \n    0.116 \n  \n  \n    frenzy \n    0.002 \n    0.001 \n    0.003 \n  \n  \n    reference \n    0.000 \n    0.002 \n    0.002 \n  \n  \n    Satisfice \n    0.000 \n    0.024 \n    0.024 \n  \n  \n    Orthogonal \n    0.260 \n    0.122 \n    0.382 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_propsh(~interpretation, data = df_items, fill = ~mode) %>% \n  gf_facet_grid(mode~condition, labeller = label_both) +\n  labs(x = \"Interpretation for Item\",\n       title = \"Proportion of Interpretations Across Items\",\n       subtitle=\"Impasse Condition yields shift from Orthogonal to alternative interpretations\")+\n  theme_minimal()+ theme(legend.position = \"none\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Proportion of Item Interpretation across Conditions\",\n            data = df_items, condition ~ interpretation, rot_labels=c(0,90,0,0), \n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\")))"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_exploration.html#response-latency",
    "href": "analysis/SGC3A/3_sgc3A_exploration.html#response-latency",
    "title": "3  Exploration",
    "section": "3.3 Response Latency",
    "text": "3.3 Response Latency\n\nTODO: Investigate super high and super low response times..\nTODO: Investigate appropriate models for response time data. (see: https://lindeloev.github.io/shiny-rt/).\nEspecially see https://lindeloev.github.io/shiny-rt/ for ideas on modelling reaction time data\n\n\n3.3.1 Time on Study\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_subjects %>% filter(mode == 'lab-synch') %>% select(totaltime_m) %>% unlist() %>% favstats(),\n  \"online\"= df_subjects %>% filter(mode == 'asynch') %>% select(totaltime_m) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of Response Latency (Time on Study)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Latency (Time on Study)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    6.01 \n    10.50 \n    12.2 \n    14.4 \n    23.9 \n    12.8 \n    3.37 \n    126 \n    0 \n  \n  \n    online \n    2.91 \n    9.18 \n    11.5 \n    15.0 \n    111.0 \n    13.4 \n    9.21 \n    204 \n    0 \n  \n\n\n\n\n\nTotal time on study for in person subjects (n = 126) ranged from 6.01 to 23.86 minutes with a mean duration of (M = 12.8, SD = 3.37).\nTotal time on study for online replication subjects (n = 204) ranged from 2.91 to 111.02 minutes with a mean duration of (M = 13.37, SD = 9.21).\n\n\nCODE\n#VISUALIZE distribution of response time\nplab <- gf_dhistogram(~totaltime_m, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Lab\", x = \"Total Time (mins)\", y = \"% subjects\") +  theme_minimal()\n\nponline <- gf_dhistogram(~totaltime_m, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"online\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist = \"gamma\", color=\"red\")+\n  labs(title=\"Online\", x = \"Total Time (mins)\", y = \"% subjects\") +  theme_minimal()\n\nplot <-ggarrange(plab, ponline, common.legend = TRUE, nrow = 1, ncol =2)\n\nannotate_figure(plot, \n                top = text_grob(\"Total Time by Study Mode\",color = \"black\", face = \"bold\", size = 14),\n                bottom = text_grob(\"fit by Gamma distribution\", face = \"italic\", size = 10))\n\n\n\n\n\nTODO consider log transform of response latency data see archive sgc3A_participants.Rmd\n\n\n3.3.2 Time on Question\nTODO time on question"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_exploration.html#resources",
    "href": "analysis/SGC3A/3_sgc3A_exploration.html#resources",
    "title": "3  Exploration",
    "section": "3.5 Resources",
    "text": "3.5 Resources\n\nhttps://rpkgs.datanovia.com/ggpubr/reference/index.html"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Schmidt, Dennis, Tobias Raupach, Annette Wiegand, Manfred Herrmann, and\nPhilipp Kanzow. 2021. “Relation Between Examinees’ True Knowledge\nand Examination Scores: Systematic Review and Exemplary Calculations on\nMultiple-True-False\nItems.” Educational Research Review 34 (November):\n100409. https://doi.org/10.1016/j.edurev.2021.100409."
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#export",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#export",
    "title": "2  Response Scoring",
    "section": "2.6 EXPORT",
    "text": "2.6 EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects)\n\n\nCODE\n#SAVE FILES\nwrite.csv(df_subjects,\"data/2-scored-data/sgc3a_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"data/2-scored-data/sgc3a_scored_items.csv\", row.names = FALSE)\n\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"data/2-scored-data/sgc3a_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"data/2-scored-data/sgc3a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_exploration.html#model-peeking",
    "href": "analysis/SGC3A/3_sgc3A_exploration.html#model-peeking",
    "title": "3  Exploration",
    "section": "3.4 Model Peeking",
    "text": "3.4 Model Peeking\nTODO\n\nmultiple regression with condition and response time\n\n\n\nCODE\nlibrary(supernova)\nlibrary(report)\nlibrary(lmerTest)\n\n\nLoading required package: lme4\n\n\nRegistered S3 methods overwritten by 'lme4':\n  method                          from\n  cooks.distance.influence.merMod car \n  influence.merMod                car \n  dfbeta.influence.merMod         car \n  dfbetas.influence.merMod        car \n\n\n\nAttaching package: 'lme4'\n\n\nThe following object is masked from 'package:mosaic':\n\n    factorize\n\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:mosaic':\n\n    rand\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nCODE\nlibrary(lme4)\nm1 <- lm( s_SCALED ~ condition, data = df_subjects %>% filter(mode=='asynch'))\nm1\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nCoefficients:\n (Intercept)  condition121  \n       -5.62          6.78  \n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13.16  -7.16  -3.02   7.12  18.62 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.620      0.866   -6.49  6.5e-10 ***\ncondition121    6.777      1.190    5.70  4.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.48 on 202 degrees of freedom\nMultiple R-squared:  0.138, Adjusted R-squared:  0.134 \nF-statistic: 32.4 on 1 and 202 DF,  p-value: 4.3e-08\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   2334    2334    32.4 4.3e-08 ***\nResiduals 202  14536      72                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nsuperanova(m1)\n\n\n Analysis of Variance Table (Type III SS)\n Model: s_SCALED ~ condition\n\n                                SS  df       MS      F    PRE     p\n ----- --------------- | --------- --- -------- ------ ------ -----\n Model (error reduced) |  2334.346   1 2334.346 32.439 0.1384 .0000\n Error (from model)    | 14536.196 202   71.961                    \n ----- --------------- | --------- --- -------- ------ ------ -----\n Total (empty model)   | 16870.543 203   83.106                    \n\n\nCODE\nplot(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCODE\ngf_histogram(~s_SCALED, data = df_subjects)\n\n\n\n\n\nCODE\ngf_histogram(~m1$residuals)\n\n\n\n\n\nCODE\n#Assess assumption of independence of errors\n#DW statistic should be close to 2\nlibrary(car)\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following objects are masked from 'package:mosaic':\n\n    deltaMethod, logit\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCODE\ndurbinWatsonTest(m1)\n\n\n lag Autocorrelation D-W Statistic p-value\n   1         -0.0134          2.01   0.954\n Alternative hypothesis: rho != 0\n\n\nCODE\n#Test for equality of variance\n#H0 is equality; p > 0.05 infer you can't reject null\nleveneTest(m1)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)  \ngroup   1     5.4  0.021 *\n      202                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA simple linear regression model predicting cumulative scaled score (at subject level) by condition explains 13% of the total variance, F(1,329) = 47.8, p < 0.001. The model predicts that participants in the impasse condition will score on average 6.38 points higher than those in the control condition, 95% CI [4.56, 8.19].\n\n\nCODE\nt.test(s_SCALED ~ condition, data = df_subjects)\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by condition\nt = -7, df = 322, p-value = 1e-12\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9.10 -5.29\nsample estimates:\nmean in group 111 mean in group 121 \n            -5.82              1.38 \n\n\nCODE\n#%>% report()\n\n\n\n\nCODE\n# report_participants(df_subjects)\nm1 %>% report()\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with condition (formula: s_SCALED ~ condition). The model explains a statistically significant and moderate proportion of variance (R2 = 0.14, F(1, 202) = 32.44, p < .001, adj. R2 = 0.13). The model's intercept, corresponding to condition = 111, is at -5.62 (95% CI [-7.33, -3.91], t(202) = -6.49, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 6.78, 95% CI [4.43, 9.12], t(202) = 5.70, p < .001; Std. beta = 0.74, 95% CI [0.49, 1.00])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\nanova(m1) %>% report()\n\n\nFor one-way between subjects designs, partial eta squared is equivalent to eta squared.\nReturning eta squared.\n\n\nThe ANOVA suggests that:\n\n  - The main effect of condition is statistically significant and medium (F(1, 202) = 32.44, p < .001; Eta2 = 0.14, 95% CI [0.07, 1.00])\n\n\nCODE\n#significant intercept means that group is significantly different than zero\n\n\n\n\nCODE\n#logistic regression on on scaled df_subjects because residuals not normal in lm?\nmlog <- glm(s_SCALED ~ condition , data = df_subjects, family = gaussian)\nsummary(mlog)\n\n\n\nCall:\nglm(formula = s_SCALED ~ condition, family = gaussian, data = df_subjects)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-13.38   -7.18   -3.28    8.50   18.82  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.820      0.698   -8.34  2.1e-15 ***\ncondition121    7.198      0.967    7.45  8.6e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 77)\n\n    Null deviance: 29508  on 329  degrees of freedom\nResidual deviance: 25242  on 328  degrees of freedom\nAIC: 2374\n\nNumber of Fisher Scoring iterations: 2\n\n\nCODE\nreport(mlog)\n\n\nWe fitted a linear model (estimated using ML) to predict s_SCALED with condition (formula: s_SCALED ~ condition). The model's explanatory power is moderate (R2 = 0.14). The model's intercept, corresponding to condition = 111, is at -5.82 (95% CI [-7.19, -4.45], t(328) = -8.34, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 7.20, 95% CI [5.30, 9.09], t(328) = 7.45, p < .001; Std. beta = 0.76, 95% CI [0.56, 0.96])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\n#logistic regression on niceABS by condition\n#pretends that questions are independent and not from same subjects INVALID\nmlog <- glm(score_niceABS ~ condition , data = df_items %>% filter(q<6), family = binomial())\nsummary(mlog)\n\n\n\nCall:\nglm(formula = score_niceABS ~ condition, family = binomial(), \n    data = df_items %>% filter(q < 6))\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.980  -0.980  -0.649   1.389   1.823  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.4508     0.0907  -15.99   <2e-16 ***\ncondition121   0.9672     0.1147    8.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1986.2  on 1649  degrees of freedom\nResidual deviance: 1911.3  on 1648  degrees of freedom\nAIC: 1915\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\nreport(mlog)\n\n\nWe fitted a logistic model (estimated using ML) to predict score_niceABS with condition (formula: score_niceABS ~ condition). The model's explanatory power is weak (Tjur's R2 = 0.04). The model's intercept, corresponding to condition = 111, is at -1.45 (95% CI [-1.63, -1.28], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.97, 95% CI [0.74, 1.19], p < .001; Std. beta = 0.97, 95% CI [0.74, 1.19])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm2 <- lm( s_NABS ~ condition, data = df_subjects)\nm2\n\n\n\nCall:\nlm(formula = s_NABS ~ condition, data = df_subjects)\n\nCoefficients:\n (Intercept)  condition121  \n        2.47          2.46  \n\n\nCODE\nsummary(m2)\n\n\n\nCall:\nlm(formula = s_NABS ~ condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.92  -3.67  -2.47   4.08  10.53 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.468      0.384    6.43  4.4e-10 ***\ncondition121    2.456      0.531    4.62  5.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.82 on 328 degrees of freedom\nMultiple R-squared:  0.0611,    Adjusted R-squared:  0.0583 \nF-statistic: 21.4 on 1 and 328 DF,  p-value: 5.49e-06\n\n\nCODE\nanova(m2)\n\n\nAnalysis of Variance Table\n\nResponse: s_NABS\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1    497     497    21.4 5.5e-06 ***\nResiduals 328   7629      23                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nsupernova(m2)\n\n\n Analysis of Variance Table (Type III SS)\n Model: s_NABS ~ condition\n\n                               SS  df      MS      F    PRE     p\n ----- --------------- | -------- --- ------- ------ ------ -----\n Model (error reduced) |  496.765   1 496.765 21.357 0.0611 .0000\n Error (from model)    | 7629.359 328  23.260                    \n ----- --------------- | -------- --- ------- ------ ------ -----\n Total (empty model)   | 8126.124 329  24.699                    \n\n\nA simple linear regression model predicting cumulative absolute score by condition explains 5% of variance, F(1,328) = 16.36, p < 0.001. The model predicts that subjects in the impasse condition will score on average 2 points higher than those in the control condition (Beta = 2.02, 95% CI [1.04, 3.00])\n\n\nCODE\nreport(m2)\n\n\nWe fitted a linear model (estimated using OLS) to predict s_NABS with condition (formula: s_NABS ~ condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.06, F(1, 328) = 21.36, p < .001, adj. R2 = 0.06). The model's intercept, corresponding to condition = 111, is at 2.47 (95% CI [1.71, 3.22], t(328) = 6.43, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 2.46, 95% CI [1.41, 3.50], t(328) = 4.62, p < .001; Std. beta = 0.49, 95% CI [0.28, 0.70])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\n\n\nCODE\nm.m1 <- lmer( score_SCALED ~ (1 + condition|subject), data = df_items)\n\n\nWarning: Model failed to converge with 1 negative eigenvalue: -8.1e-04\n\n\nCODE\nm.m1\n\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: score_SCALED ~ (1 + condition | subject)\n   Data: df_items\nREML criterion at convergence: 9773\nRandom effects:\n Groups   Name         Std.Dev. Corr \n subject  (Intercept)  0.632         \n          condition121 0.827    -0.71\n Residual              0.591         \nNumber of obs: 4950, groups:  subject, 330\nFixed Effects:\n(Intercept)  \n    -0.0617  \n\n\nCODE\nsummary(m.m1)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | subject)\n   Data: df_items\n\nREML criterion at convergence: 9773\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9664 -0.6738 -0.0461  0.5889  2.7646 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n subject  (Intercept)  0.400    0.632         \n          condition121 0.685    0.827    -0.71\n Residual              0.349    0.591         \nNumber of obs: 4950, groups:  subject, 330\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(>|t|)  \n(Intercept)  -0.0617     0.0344 326.4131   -1.79    0.074 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m.m1)\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition and subject as random effects (formula: ~1 + condition | subject). . The model's intercept is at -0.06 (95% CI [-0.13, 5.71e-03], t(4945) = -1.79, p = 0.073). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm.m2 <- lmer( score_SCALED ~ (1+ condition|q), data = df_items)\nm.m2\n\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: score_SCALED ~ (1 + condition | q)\n   Data: df_items\nREML criterion at convergence: 11500\nRandom effects:\n Groups   Name         Std.Dev. Corr \n q        (Intercept)  0.602         \n          condition121 0.530    -0.93\n Residual              0.765         \nNumber of obs: 4950, groups:  q, 15\nFixed Effects:\n(Intercept)  \n      0.172  \n\n\nCODE\nsummary(m.m2)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | q)\n   Data: df_items\n\nREML criterion at convergence: 11500\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-1.972 -0.794 -0.093  0.887  2.164 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n q        (Intercept)  0.363    0.602         \n          condition121 0.281    0.530    -0.93\n Residual              0.585    0.765         \nNumber of obs: 4950, groups:  q, 15\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)   \n(Intercept)   0.1717     0.0575 14.0014    2.98   0.0099 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m.m2)\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition and q as random effects (formula: ~1 + condition | q). . The model's intercept is at 0.17 (95% CI [0.06, 0.28], t(4945) = 2.98, p = 0.003). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm.m3 <- lmer( score_SCALED ~ (1+ condition|q) + (1+condition|subject), data = df_items)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00208782 (tol = 0.002, component 1)\n\n\nCODE\nm.m3 %>% summary() \n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | q) + (1 + condition | subject)\n   Data: df_items\n\nREML criterion at convergence: 8593\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.724 -0.644 -0.016  0.697  3.599 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n subject  (Intercept)  0.339    0.583         \n          condition121 0.218    0.467    -0.46\n q        (Intercept)  0.268    0.517         \n          condition121 0.197    0.443    -0.91\n Residual              0.266    0.516         \nNumber of obs: 4950, groups:  subject, 330; q, 15\n\nFixed effects:\n            Estimate Std. Error     df t value Pr(>|t|)\n(Intercept)    0.117      0.071 24.208    1.65     0.11\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00208782 (tol = 0.002, component 1)\n\n\nCODE\nm.m3 %>% report()\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition, q and subject as random effects (formula: list(~1 + condition | q, ~1 + condition | subject)). . The model's intercept is at 0.12 (95% CI [-0.02, 0.26], t(4942) = 1.65, p = 0.098). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nanova(m.m1, m.m2, m.m3)\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: df_items\nModels:\nm.m1: score_SCALED ~ (1 + condition | subject)\nm.m2: score_SCALED ~ (1 + condition | q)\nm.m3: score_SCALED ~ (1 + condition | q) + (1 + condition | subject)\n     npar   AIC   BIC logLik deviance Chisq Df Pr(>Chisq)    \nm.m1    5  9778  9811  -4884     9768                        \nm.m2    5 11506 11539  -5748    11496     0  0               \nm.m3    8  8605  8657  -4295     8589  2907  3     <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_exploration.html#due-diligence",
    "href": "analysis/SGC3A/3_sgc3A_exploration.html#due-diligence",
    "title": "3  Exploration",
    "section": "3.4 Due Diligence",
    "text": "3.4 Due Diligence\n\n3.4.1 Data Collection Mode on Absolute Score\nDoes Mode Change Effect of Condition on Score?\nTo verify that the data collected in the lab and remotely online are comparable, we perform a t-test on group means of ABSOLUTE SCORE for each condition, and examine whether data collection modality is a significant predictor of variance in absolute score\n\n\nCODE\npaste(\"Two Sample T-Test for S_ABS LAB vs. ONLINE control condition\")\n\n\n[1] \"Two Sample T-Test for S_ABS LAB vs. ONLINE control condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 111), s_ABS ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_ABS by mode\nt = -0.5, df = 120, p-value = 0.6\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.84  1.09\nsample estimates:\n   mean in group asynch mean in group lab-synch \n                   2.30                    2.68 \n\n\nCODE\npaste(\"Two Sample T-Test for S_ABS LAB vs. ONLINE impasse condition\")\n\n\n[1] \"Two Sample T-Test for S_ABS LAB vs. ONLINE impasse condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 121), s_ABS ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_ABS by mode\nt = -1, df = 135, p-value = 0.3\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.435  0.727\nsample estimates:\n   mean in group asynch mean in group lab-synch \n                   4.58                    5.44 \n\n\nCODE\npaste(\"OLS Linear Regression Predicting Absolute Score by Data Collection Mode\")\n\n\n[1] \"OLS Linear Regression Predicting Absolute Score by Data Collection Mode\"\n\n\nCODE\nsummary(lm(data = df_subjects, formula = s_ABS ~ mode ))\n\n\n\nCall:\nlm(formula = s_ABS ~ mode, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.08  -3.51  -2.51   4.49   9.49 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      3.510      0.346   10.14   <2e-16 ***\nmodelab-synch    0.570      0.560    1.02     0.31    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.94 on 328 degrees of freedom\nMultiple R-squared:  0.00314,   Adjusted R-squared:  0.000105 \nF-statistic: 1.03 on 1 and 328 DF,  p-value: 0.31\n\n\nBoth t-tests are non-significant with 95% confidence intervals including 0. Further, an OLS linear regression model predicting cumulative absolute score indicates that data collection mode is not a significant predictor, explaining only 0.01% of variance in absolute score, F(1,328) = 1.03, p > 0.05.\n\n\n\n\n\n\nDecision\n\n\n\nIt is reasonable to infer that data from both in-person and remote studies arise from the same data generating process.\n\n\n\n\n3.4.2 Data Collection Mode on Cumulative Score\nAre the by-condition group means significantly different by data collection modality?\nTo verify that the data collected in the lab and remotely online are comparable, we perform a t-test on group means of SCALED SCORE for each condition.\n\n\nCODE\npaste(\"Two Sample T-Test for s_SCALED LAB vs. ONLINE control condition\")\n\n\n[1] \"Two Sample T-Test for s_SCALED LAB vs. ONLINE control condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 111), s_SCALED ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by mode\nt = 0.3, df = 116, p-value = 0.7\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.50  3.52\nsample estimates:\n   mean in group asynch mean in group lab-synch \n                  -5.62                   -6.13 \n\n\nCODE\npaste(\"Two Sample T-Test for s_SCALED LAB vs. ONLINE impasse condition\")\n\n\n[1] \"Two Sample T-Test for s_SCALED LAB vs. ONLINE impasse condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 121), s_SCALED ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by mode\nt = -0.4, df = 130, p-value = 0.7\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.29  2.10\nsample estimates:\n   mean in group asynch mean in group lab-synch \n                   1.16                    1.75 \n\n\nCODE\npaste(\"OLS Linear Regression Predicting Scaled Score by Data Collection Mode\")\n\n\n[1] \"OLS Linear Regression Predicting Scaled Score by Data Collection Mode\"\n\n\nCODE\nsummary(lm(data = df_subjects, formula = s_SCALED ~ mode ))\n\n\n\nCall:\nlm(formula = s_SCALED ~ mode, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10.97  -7.97  -3.92  10.03  15.13 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    -2.0319     0.6641   -3.06   0.0024 **\nmodelab-synch  -0.0951     1.0747   -0.09   0.9295   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.48 on 328 degrees of freedom\nMultiple R-squared:  2.39e-05,  Adjusted R-squared:  -0.00302 \nF-statistic: 0.00783 on 1 and 328 DF,  p-value: 0.93\n\n\nBoth t-tests are non-significant with 95% confidence intervals including 0. Further, an OLS linear regression model predicting cumulative scaled score indicates that data collection mode is not a significant predictor, explaining less than 0.001% of variance in absolute score, F(1,328) = 0.0078, p > 0.05.\n\n\n\n\n\n\nDecision\n\n\n\nIt is reasonable to infer that data from both in-person and remote studies arise from the same data generating process."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_modelling.html",
    "href": "analysis/SGC3A/4_sgc3A_modelling.html",
    "title": "4  Modelling",
    "section": "",
    "text": "TODO - port modelling from 3_exploration into 4_modelling - clarify core questions being asked - review models already created in ARCHIVE?\nThe purpose of this notebook is explore the distribution of dependent variables for Study SGC3A."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_modelling.html#effect-of-condition",
    "href": "analysis/SGC3A/4_sgc3A_modelling.html#effect-of-condition",
    "title": "4  Modelling",
    "section": "4.1 Effect of Condition",
    "text": "4.1 Effect of Condition\n\n4.1.1 Cumulative Score By Condition\nCumulative scores indicate the response accuracy by a particular participant across all discriminant items (n=13) in the graph comprehension task.\n\n\nCODE\n#VISUALIZE distribution of response accuracy across SUBJECTS\n\n#HISTOGRAM\nstats = df_subjects %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(s_ABS))\ngf_density(~s_ABS, data = df_subjects) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_lims(x = c(0, 13)) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Cumulative Absolute Score\",\n       y = \"proportion of subjects\",\n       title = \"Subject Cumulative Score (Absolute)\",\n       subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n  theme_minimal()\n\n\n\n\n\nDoes the IMPASSE condition yield higher scores?\n\n4.1.1.1 Linear Regression\nTo address this question we assess how much variance in cumulative (absolute) score is explained by experimental condition by utilizing OLS linear regression, predicting absolute score from condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_ABS ~ condition, data = df_subjects %>% filter(mode == \n    \"lab-synch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -5.44  -2.68  -2.68   4.31  10.32 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.677      0.620    4.32 0.000031 ***\ncondition121    2.760      0.869    3.17   0.0019 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.88 on 124 degrees of freedom\nMultiple R-squared:  0.0752,    Adjusted R-squared:  0.0677 \nF-statistic: 10.1 on 1 and 124 DF,  p-value: 0.00189\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_ABS\n           Df Sum Sq Mean Sq F value Pr(>F)   \ncondition   1    240   239.9    10.1 0.0019 **\nResiduals 124   2951    23.8                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)   1.45   3.90\ncondition121  1.04   4.48\n\n\nCODE\n# report(m1) #sanity check\n\n\nFor in-lab data collection an OLS linear regression predicting cumulative absolute score by experimental condition explains a statistically significant though small 7.5% variance in score (F(1,124) = 10.1, p < 0.01). The estimated beta coefficient (\\(/beta\\) = 2.76, 95% CI [1.04, 4.48]) predicts that participants in the impasse condition will on average score 2.76 points (21%) higher than those in the control condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"asynch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_ABS ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.58  -3.58  -2.30   3.42  10.70 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.302      0.485    4.75  3.8e-06 ***\ncondition121    2.281      0.666    3.43  0.00074 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.75 on 202 degrees of freedom\nMultiple R-squared:  0.0549,    Adjusted R-squared:  0.0502 \nF-statistic: 11.7 on 1 and 202 DF,  p-value: 0.000745\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_ABS\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1    264   264.5    11.7 0.00074 ***\nResiduals 202   4554    22.5                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  1.347   3.26\ncondition121 0.968   3.59\n\n\nCODE\n#report(m1) #sanity check\n\n\nFor the online replication, an OLS linear regression model predicting cumulative absolute score by condition explains a statistically significant though small 5.5% of variance in absolute score (F(1,202) = 11.73, p < 0.001). The beta coefficient for condition indicates that on average, participants in the IMPASSE group scored 2.3 points higher on the task than those in the control condition (CI[0.97, 3.59]).\n\n\n\n\n\n\nNote\n\n\n\nFrom these models we can reasonably conclude that the impasse condition yielded a small but reliable improvement in performance across items in the graph reading task.\n\n\n\n\nCODE\ncheck_model(m1)\n\n\n\n\n\n\n\n4.1.1.2 Poisson Regression\nThe outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can consider it a type of count, (ie. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function).\n\n\nCODE\n# library('fitdistrplus')\n# plot(fitdist(df_subjects$s_ABS,\"pois\"))\n# plot(fitdist(df_subjects$s_ABS,\"norm\"))\n# plot(fitdist(df_subjects$s_ABS,\"beta\"))\n# \n# \n# plotdist(df_subjects$s_ABS, histo = TRUE, demp = TRUE)\n# descdist(df_subjects$s_ABS, discrete=FALSE, boot=500)\n\n\n\n\nCODE\n#SCORE predicted by CONDITION --> POISSON DISTRIBUTION\nmp1 <- glm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"), family = \"poisson\")\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(mp1)\n\n\n\nCall:\nglm(formula = s_ABS ~ condition, family = \"poisson\", data = df_subjects %>% \n    filter(mode == \"lab-synch\"))\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -3.30   -2.31   -2.31    1.75    4.52  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    0.9849     0.0776   12.69  < 2e-16 ***\ncondition121   0.7085     0.0943    7.51  5.9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 855.68  on 125  degrees of freedom\nResidual deviance: 795.46  on 124  degrees of freedom\nAIC: 1045\n\nNumber of Fisher Scoring iterations: 6\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(mp1)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: s_ABS\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev\nNULL                        125        856\ncondition  1     60.2       124        795\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(mp1)\n\n\nWaiting for profiling to be done...\n\n\n             2.5 % 97.5 %\n(Intercept)  0.829  1.133\ncondition121 0.525  0.896\n\n\nCODE\nreport(mp1) #sanity check\n\n\nWe fitted a poisson model (estimated using ML) to predict s_ABS with condition (formula: s_ABS ~ condition). The model's explanatory power is substantial (Nagelkerke's R2 = 0.38). The model's intercept, corresponding to condition = 111, is at 0.98 (95% CI [0.83, 1.13], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.71, 95% CI [0.53, 0.90], p < .001; Std. beta = 0.71, 95% CI [0.53, 0.90])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\ncheck_model(mp1)\n\n\n\n\n\n\n\nCODE\n#Which is a better fit? linear or poisson?\n\ncompare_performance(m1,mp1)\n\n\nWarning: When comparing models, please note that probably not all models were fit from\n  same data.\n\n\n# Comparison of Model Performance Indices\n\nName | Model |      AIC | AIC weights |      BIC | BIC weights |  RMSE | Sigma |    R2 | R2 (adj.) | Nagelkerke's R2 | Score_log | Score_spherical\n--------------------------------------------------------------------------------------------------------------------------------------------------\nm1   |    lm | 1218.500 |     < 0.001 | 1228.454 |     < 0.001 | 4.725 | 4.748 | 0.055 |     0.050 |                 |           |                \nmp1  |   glm | 1044.751 |        1.00 | 1050.423 |        1.00 | 4.840 | 2.533 |       |           |           0.380 |    -4.130 |           0.063\n\n\n\n\n4.1.1.3 COPIED FROM 3\nDoes the IMPASSE condition more accurate interpretation?\nTo address this question we assess how much variance in cumulative (absolute) score is explained by experimental condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_SCALED ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"lab-synch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12.75  -6.87  -4.87   9.03  19.13 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -6.13       1.18   -5.20  8.0e-07 ***\ncondition121     7.88       1.65    4.76  5.2e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.28 on 124 degrees of freedom\nMultiple R-squared:  0.155, Adjusted R-squared:  0.148 \nF-statistic: 22.7 on 1 and 124 DF,  p-value: 5.21e-06\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   1955    1955    22.7 5.2e-06 ***\nResiduals 124  10681      86                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  -8.46   -3.8\ncondition121  4.61   11.2\n\n\nCODE\n# report(m1) #sanity check\n\n\nFor in-lab data collection an OLS linear regression predicting scaled score by experimental condition explains a statistically significant and moderate 15% variance in score (F(1,124) = 22.7, p < 0.001). The estimated beta coefficient (\\(/beta\\) = 7.88, 95% CI [4.61, 11.2]) predicts that participants in the impasse condition will on average score around 8 points (31%) higher than those in the control condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_SCALED ~ condition, data = df_subjects %>% filter(mode==\"asynch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13.16  -7.16  -3.02   7.12  18.62 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.620      0.866   -6.49  6.5e-10 ***\ncondition121    6.777      1.190    5.70  4.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.48 on 202 degrees of freedom\nMultiple R-squared:  0.138, Adjusted R-squared:  0.134 \nF-statistic: 32.4 on 1 and 202 DF,  p-value: 4.3e-08\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   2334    2334    32.4 4.3e-08 ***\nResiduals 202  14536      72                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  -7.33  -3.91\ncondition121  4.43   9.12\n\n\nCODE\n#report(m1) #sanity check\n\n\nFor the online replication, an OLS linear regression model predicting scaled score by condition explains a statistically significant and moderate 14% of variance in absolute score (F(1,202) = 32.4, p < 0.001). The beta coefficient for condition indicates that on average, participants in the IMPASSE group scored 6.8 points higher on the task than those in the control condition (CI[4.43, 9.12]).\n\n\n\n\n\n\nNote\n\n\n\nFrom these models we can reasonably conclude that the impasse condition yields a reliable, moderate sized effect of improved interpretation on the graph reading tasks."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_modelling.html#resources",
    "href": "analysis/SGC3A/4_sgc3A_modelling.html#resources",
    "title": "4  Modelling",
    "section": "4.5 Resources",
    "text": "4.5 Resources\n\nhttps://rpkgs.datanovia.com/ggpubr/reference/index.html"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_modelling.html#model-peeking",
    "href": "analysis/SGC3A/4_sgc3A_modelling.html#model-peeking",
    "title": "4  Modelling",
    "section": "4.4 Model Peeking",
    "text": "4.4 Model Peeking\nTODO\n\nmultiple regression with condition and response time\n\n\n\nCODE\nlibrary(supernova)\n\n\n\nAttaching package: 'supernova'\n\n\nThe following object is masked from 'package:scales':\n\n    number\n\n\nCODE\nlibrary(report)\nlibrary(lmerTest)\n\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nRegistered S3 methods overwritten by 'lme4':\n  method                          from\n  cooks.distance.influence.merMod car \n  influence.merMod                car \n  dfbeta.influence.merMod         car \n  dfbetas.influence.merMod        car \n\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nCODE\nlibrary(lme4)\nm1 <- lm( s_SCALED ~ condition, data = df_subjects %>% filter(mode=='asynch'))\nm1\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nCoefficients:\n (Intercept)  condition121  \n       -5.62          6.78  \n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13.16  -7.16  -3.02   7.12  18.62 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.620      0.866   -6.49  6.5e-10 ***\ncondition121    6.777      1.190    5.70  4.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.48 on 202 degrees of freedom\nMultiple R-squared:  0.138, Adjusted R-squared:  0.134 \nF-statistic: 32.4 on 1 and 202 DF,  p-value: 4.3e-08\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   2334    2334    32.4 4.3e-08 ***\nResiduals 202  14536      72                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nsuperanova(m1)\n\n\n Analysis of Variance Table (Type III SS)\n Model: s_SCALED ~ condition\n\n                                SS  df       MS      F    PRE     p\n ----- --------------- | --------- --- -------- ------ ------ -----\n Model (error reduced) |  2334.346   1 2334.346 32.439 0.1384 .0000\n Error (from model)    | 14536.196 202   71.961                    \n ----- --------------- | --------- --- -------- ------ ------ -----\n Total (empty model)   | 16870.543 203   83.106                    \n\n\nCODE\nplot(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCODE\ngf_histogram(~s_SCALED, data = df_subjects)\n\n\n\n\n\nCODE\ngf_histogram(~m1$residuals)\n\n\n\n\n\nCODE\n#Assess assumption of independence of errors\n#DW statistic should be close to 2\nlibrary(car)\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'carData'\n\n\nThe following object is masked from 'package:vcdExtra':\n\n    Burt\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCODE\ndurbinWatsonTest(m1)\n\n\n lag Autocorrelation D-W Statistic p-value\n   1         -0.0134          2.01   0.916\n Alternative hypothesis: rho != 0\n\n\nCODE\n#Test for equality of variance\n#H0 is equality; p > 0.05 infer you can't reject null\nleveneTest(m1)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)  \ngroup   1     5.4  0.021 *\n      202                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA simple linear regression model predicting cumulative scaled score (at subject level) by condition explains 13% of the total variance, F(1,329) = 47.8, p < 0.001. The model predicts that participants in the impasse condition will score on average 6.38 points higher than those in the control condition, 95% CI [4.56, 8.19].\n\n\nCODE\nt.test(s_SCALED ~ condition, data = df_subjects)\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by condition\nt = -7, df = 322, p-value = 1e-12\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9.10 -5.29\nsample estimates:\nmean in group 111 mean in group 121 \n            -5.82              1.38 \n\n\nCODE\n#%>% report()\n\n\n\n\nCODE\n# report_participants(df_subjects)\nm1 %>% report()\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with condition (formula: s_SCALED ~ condition). The model explains a statistically significant and moderate proportion of variance (R2 = 0.14, F(1, 202) = 32.44, p < .001, adj. R2 = 0.13). The model's intercept, corresponding to condition = 111, is at -5.62 (95% CI [-7.33, -3.91], t(202) = -6.49, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 6.78, 95% CI [4.43, 9.12], t(202) = 5.70, p < .001; Std. beta = 0.74, 95% CI [0.49, 1.00])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\nanova(m1) %>% report()\n\n\nFor one-way between subjects designs, partial eta squared is equivalent to eta squared.\nReturning eta squared.\n\n\nThe ANOVA suggests that:\n\n  - The main effect of condition is statistically significant and medium (F(1, 202) = 32.44, p < .001; Eta2 = 0.14, 95% CI [0.07, 1.00])\n\n\nCODE\n#significant intercept means that group is significantly different than zero\n\n\n\n\nCODE\n#logistic regression on on scaled df_subjects because residuals not normal in lm?\nmlog <- glm(s_SCALED ~ condition , data = df_subjects, family = gaussian)\nsummary(mlog)\n\n\n\nCall:\nglm(formula = s_SCALED ~ condition, family = gaussian, data = df_subjects)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-13.38   -7.18   -3.28    8.50   18.82  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.820      0.698   -8.34  2.1e-15 ***\ncondition121    7.198      0.967    7.45  8.6e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 77)\n\n    Null deviance: 29508  on 329  degrees of freedom\nResidual deviance: 25242  on 328  degrees of freedom\nAIC: 2374\n\nNumber of Fisher Scoring iterations: 2\n\n\nCODE\nreport(mlog)\n\n\nWe fitted a linear model (estimated using ML) to predict s_SCALED with condition (formula: s_SCALED ~ condition). The model's explanatory power is moderate (R2 = 0.14). The model's intercept, corresponding to condition = 111, is at -5.82 (95% CI [-7.19, -4.45], t(328) = -8.34, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 7.20, 95% CI [5.30, 9.09], t(328) = 7.45, p < .001; Std. beta = 0.76, 95% CI [0.56, 0.96])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\n#logistic regression on niceABS by condition\n#pretends that questions are independent and not from same subjects INVALID\nmlog <- glm(score_niceABS ~ condition , data = df_items %>% filter(q<6), family = binomial())\nsummary(mlog)\n\n\n\nCall:\nglm(formula = score_niceABS ~ condition, family = binomial(), \n    data = df_items %>% filter(q < 6))\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.980  -0.980  -0.649   1.389   1.823  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.4508     0.0907  -15.99   <2e-16 ***\ncondition121   0.9672     0.1147    8.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1986.2  on 1649  degrees of freedom\nResidual deviance: 1911.3  on 1648  degrees of freedom\nAIC: 1915\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\nreport(mlog)\n\n\nWe fitted a logistic model (estimated using ML) to predict score_niceABS with condition (formula: score_niceABS ~ condition). The model's explanatory power is weak (Tjur's R2 = 0.04). The model's intercept, corresponding to condition = 111, is at -1.45 (95% CI [-1.63, -1.28], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.97, 95% CI [0.74, 1.19], p < .001; Std. beta = 0.97, 95% CI [0.74, 1.19])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm2 <- lm( s_NABS ~ condition, data = df_subjects)\nm2\n\n\n\nCall:\nlm(formula = s_NABS ~ condition, data = df_subjects)\n\nCoefficients:\n (Intercept)  condition121  \n        2.47          2.46  \n\n\nCODE\nsummary(m2)\n\n\n\nCall:\nlm(formula = s_NABS ~ condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.92  -3.67  -2.47   4.08  10.53 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.468      0.384    6.43  4.4e-10 ***\ncondition121    2.456      0.531    4.62  5.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.82 on 328 degrees of freedom\nMultiple R-squared:  0.0611,    Adjusted R-squared:  0.0583 \nF-statistic: 21.4 on 1 and 328 DF,  p-value: 5.49e-06\n\n\nCODE\nanova(m2)\n\n\nAnalysis of Variance Table\n\nResponse: s_NABS\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1    497     497    21.4 5.5e-06 ***\nResiduals 328   7629      23                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nsupernova(m2)\n\n\n Analysis of Variance Table (Type III SS)\n Model: s_NABS ~ condition\n\n                               SS  df      MS      F    PRE     p\n ----- --------------- | -------- --- ------- ------ ------ -----\n Model (error reduced) |  496.765   1 496.765 21.357 0.0611 .0000\n Error (from model)    | 7629.359 328  23.260                    \n ----- --------------- | -------- --- ------- ------ ------ -----\n Total (empty model)   | 8126.124 329  24.699                    \n\n\nA simple linear regression model predicting cumulative absolute score by condition explains 5% of variance, F(1,328) = 16.36, p < 0.001. The model predicts that subjects in the impasse condition will score on average 2 points higher than those in the control condition (Beta = 2.02, 95% CI [1.04, 3.00])\n\n\nCODE\nreport(m2)\n\n\nWe fitted a linear model (estimated using OLS) to predict s_NABS with condition (formula: s_NABS ~ condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.06, F(1, 328) = 21.36, p < .001, adj. R2 = 0.06). The model's intercept, corresponding to condition = 111, is at 2.47 (95% CI [1.71, 3.22], t(328) = 6.43, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 2.46, 95% CI [1.41, 3.50], t(328) = 4.62, p < .001; Std. beta = 0.49, 95% CI [0.28, 0.70])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\n\n\nCODE\nm.m1 <- lmer( score_SCALED ~ (1 + condition|subject), data = df_items)\n\n\nWarning: Model failed to converge with 1 negative eigenvalue: -8.1e-04\n\n\nCODE\nm.m1\n\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: score_SCALED ~ (1 + condition | subject)\n   Data: df_items\nREML criterion at convergence: 9773\nRandom effects:\n Groups   Name         Std.Dev. Corr \n subject  (Intercept)  0.632         \n          condition121 0.827    -0.71\n Residual              0.591         \nNumber of obs: 4950, groups:  subject, 330\nFixed Effects:\n(Intercept)  \n    -0.0617  \n\n\nCODE\nsummary(m.m1)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | subject)\n   Data: df_items\n\nREML criterion at convergence: 9773\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9664 -0.6738 -0.0461  0.5889  2.7646 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n subject  (Intercept)  0.400    0.632         \n          condition121 0.685    0.827    -0.71\n Residual              0.349    0.591         \nNumber of obs: 4950, groups:  subject, 330\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(>|t|)  \n(Intercept)  -0.0617     0.0344 326.4131   -1.79    0.074 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m.m1)\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition and subject as random effects (formula: ~1 + condition | subject). . The model's intercept is at -0.06 (95% CI [-0.13, 5.71e-03], t(4945) = -1.79, p = 0.073). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm.m2 <- lmer( score_SCALED ~ (1+ condition|q), data = df_items)\nm.m2\n\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: score_SCALED ~ (1 + condition | q)\n   Data: df_items\nREML criterion at convergence: 11500\nRandom effects:\n Groups   Name         Std.Dev. Corr \n q        (Intercept)  0.602         \n          condition121 0.530    -0.93\n Residual              0.765         \nNumber of obs: 4950, groups:  q, 15\nFixed Effects:\n(Intercept)  \n      0.172  \n\n\nCODE\nsummary(m.m2)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | q)\n   Data: df_items\n\nREML criterion at convergence: 11500\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-1.972 -0.794 -0.093  0.887  2.164 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n q        (Intercept)  0.363    0.602         \n          condition121 0.281    0.530    -0.93\n Residual              0.585    0.765         \nNumber of obs: 4950, groups:  q, 15\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)   \n(Intercept)   0.1717     0.0575 14.0014    2.98   0.0099 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m.m2)\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition and q as random effects (formula: ~1 + condition | q). . The model's intercept is at 0.17 (95% CI [0.06, 0.28], t(4945) = 2.98, p = 0.003). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm.m3 <- lmer( score_SCALED ~ (1+ condition|q) + (1+condition|subject), data = df_items)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00208782 (tol = 0.002, component 1)\n\n\nCODE\nm.m3 %>% summary() \n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | q) + (1 + condition | subject)\n   Data: df_items\n\nREML criterion at convergence: 8593\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.724 -0.644 -0.016  0.697  3.599 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n subject  (Intercept)  0.339    0.583         \n          condition121 0.218    0.467    -0.46\n q        (Intercept)  0.268    0.517         \n          condition121 0.197    0.443    -0.91\n Residual              0.266    0.516         \nNumber of obs: 4950, groups:  subject, 330; q, 15\n\nFixed effects:\n            Estimate Std. Error     df t value Pr(>|t|)\n(Intercept)    0.117      0.071 24.208    1.65     0.11\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00208782 (tol = 0.002, component 1)\n\n\nCODE\nm.m3 %>% report()\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition, q and subject as random effects (formula: list(~1 + condition | q, ~1 + condition | subject)). . The model's intercept is at 0.12 (95% CI [-0.02, 0.26], t(4942) = 1.65, p = 0.098). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nanova(m.m1, m.m2, m.m3)\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: df_items\nModels:\nm.m1: score_SCALED ~ (1 + condition | subject)\nm.m2: score_SCALED ~ (1 + condition | q)\nm.m3: score_SCALED ~ (1 + condition | q) + (1 + condition | subject)\n     npar   AIC   BIC logLik deviance Chisq Df Pr(>Chisq)    \nm.m1    5  9778  9811  -4884     9768                        \nm.m2    5 11506 11539  -5748    11496     0  0               \nm.m3    8  8605  8657  -4295     8589  2907  3     <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_modelling.html#cumulative-performance",
    "href": "analysis/SGC3A/4_sgc3A_modelling.html#cumulative-performance",
    "title": "4  Modelling",
    "section": "4.2 Cumulative Performance",
    "text": "4.2 Cumulative Performance\nOverall does the impasse condition affect performance on the graph comprehension task?\n\n4.2.1 Cumulative Score By Condition\nCumulative scores indicate the response accuracy by a particular participant across all discriminant items (n=13) in the graph comprehension task.\n\n\nCODE\n#VISUALIZE distribution of response accuracy across SUBJECTS\n\n#HISTOGRAM\nstats = df_subjects %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(s_ABS))\ngf_density(~s_ABS, data = df_subjects) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_lims(x = c(0, 13)) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Cumulative Absolute Score\",\n       y = \"proportion of subjects\",\n       title = \"Subject Cumulative Score (Absolute)\",\n       subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n  theme_minimal()\n\n\n\n\n\nDoes the IMPASSE condition yield higher scores?\n\n4.2.1.1 Linear Regression\nTo address this question we assess how much variance in cumulative (absolute) score is explained by experimental condition by utilizing OLS linear regression, predicting absolute score from condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_ABS ~ condition, data = df_subjects %>% filter(mode == \n    \"lab-synch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -5.44  -2.68  -2.68   4.31  10.32 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.677      0.620    4.32 0.000031 ***\ncondition121    2.760      0.869    3.17   0.0019 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.88 on 124 degrees of freedom\nMultiple R-squared:  0.0752,    Adjusted R-squared:  0.0677 \nF-statistic: 10.1 on 1 and 124 DF,  p-value: 0.00189\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_ABS\n           Df Sum Sq Mean Sq F value Pr(>F)   \ncondition   1    240   239.9    10.1 0.0019 **\nResiduals 124   2951    23.8                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)   1.45   3.90\ncondition121  1.04   4.48\n\n\nCODE\n# report(m1) #sanity check\n\n\nFor in-lab data collection an OLS linear regression predicting cumulative absolute score by experimental condition explains a statistically significant though small 7.5% variance in score (F(1,124) = 10.1, p < 0.01). The estimated beta coefficient (\\(/beta\\) = 2.76, 95% CI [1.04, 4.48]) predicts that participants in the impasse condition will on average score 2.76 points (21%) higher than those in the control condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"asynch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_ABS ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.58  -3.58  -2.30   3.42  10.70 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.302      0.485    4.75  3.8e-06 ***\ncondition121    2.281      0.666    3.43  0.00074 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.75 on 202 degrees of freedom\nMultiple R-squared:  0.0549,    Adjusted R-squared:  0.0502 \nF-statistic: 11.7 on 1 and 202 DF,  p-value: 0.000745\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_ABS\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1    264   264.5    11.7 0.00074 ***\nResiduals 202   4554    22.5                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  1.347   3.26\ncondition121 0.968   3.59\n\n\nCODE\n#report(m1) #sanity check\n\n\nFor the online replication, an OLS linear regression model predicting cumulative absolute score by condition explains a statistically significant though small 5.5% of variance in absolute score (F(1,202) = 11.73, p < 0.001). The beta coefficient for condition indicates that on average, participants in the IMPASSE group scored 2.3 points higher on the task than those in the control condition (CI[0.97, 3.59]).\n\n\n\n\n\n\nNote\n\n\n\nFrom these models we can reasonably conclude that the impasse condition yielded a small but reliable improvement in performance across items in the graph reading task.\n\n\nModel Diagnostics\n\n\nCODE\ncheck_model(m1)\n\n\n\n\n\n\n\n4.2.1.2 Poisson Regression\nThe outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can consider it a type of count, (ie. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function).\n\n\nCODE\n# library('fitdistrplus')\n# plot(fitdist(df_subjects$s_ABS,\"pois\"))\n# plot(fitdist(df_subjects$s_ABS,\"norm\"))\n# plot(fitdist(df_subjects$s_ABS,\"beta\"))\n# \n# \n# plotdist(df_subjects$s_ABS, histo = TRUE, demp = TRUE)\n# descdist(df_subjects$s_ABS, discrete=FALSE, boot=500)\n\n\n\n\nCODE\n#SCORE predicted by CONDITION --> POISSON DISTRIBUTION\nmp1 <- glm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"), family = \"poisson\")\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(mp1)\n\n\n\nCall:\nglm(formula = s_ABS ~ condition, family = \"poisson\", data = df_subjects %>% \n    filter(mode == \"lab-synch\"))\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -3.30   -2.31   -2.31    1.75    4.52  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    0.9849     0.0776   12.69  < 2e-16 ***\ncondition121   0.7085     0.0943    7.51  5.9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 855.68  on 125  degrees of freedom\nResidual deviance: 795.46  on 124  degrees of freedom\nAIC: 1045\n\nNumber of Fisher Scoring iterations: 6\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(mp1)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: s_ABS\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev\nNULL                        125        856\ncondition  1     60.2       124        795\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(mp1)\n\n\nWaiting for profiling to be done...\n\n\n             2.5 % 97.5 %\n(Intercept)  0.829  1.133\ncondition121 0.525  0.896\n\n\nCODE\nreport(mp1) #sanity check\n\n\nWe fitted a poisson model (estimated using ML) to predict s_ABS with condition (formula: s_ABS ~ condition). The model's explanatory power is substantial (Nagelkerke's R2 = 0.38). The model's intercept, corresponding to condition = 111, is at 0.98 (95% CI [0.83, 1.13], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.71, 95% CI [0.53, 0.90], p < .001; Std. beta = 0.71, 95% CI [0.53, 0.90])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\ncheck_model(mp1)\n\n\n\n\n\n\n\nCODE\n#Which is a better fit? linear or poisson?\n\ncompare_performance(m1,mp1)\n\n\nWarning: When comparing models, please note that probably not all models were fit from\n  same data.\n\n\n# Comparison of Model Performance Indices\n\nName | Model |      AIC | AIC weights |      BIC | BIC weights |  RMSE | Sigma |    R2 | R2 (adj.) | Nagelkerke's R2 | Score_log | Score_spherical\n--------------------------------------------------------------------------------------------------------------------------------------------------\nm1   |    lm | 1218.500 |     < 0.001 | 1228.454 |     < 0.001 | 4.725 | 4.748 | 0.055 |     0.050 |                 |           |                \nmp1  |   glm | 1044.751 |        1.00 | 1050.423 |        1.00 | 4.840 | 2.533 |       |           |           0.380 |    -4.130 |           0.063\n\n\n\n\n4.2.1.3 COPIED FROM 3\nDoes the IMPASSE condition more accurate interpretation?\nTo address this question we assess how much variance in cumulative (absolute) score is explained by experimental condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_SCALED ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"lab-synch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12.75  -6.87  -4.87   9.03  19.13 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -6.13       1.18   -5.20  8.0e-07 ***\ncondition121     7.88       1.65    4.76  5.2e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.28 on 124 degrees of freedom\nMultiple R-squared:  0.155, Adjusted R-squared:  0.148 \nF-statistic: 22.7 on 1 and 124 DF,  p-value: 5.21e-06\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   1955    1955    22.7 5.2e-06 ***\nResiduals 124  10681      86                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  -8.46   -3.8\ncondition121  4.61   11.2\n\n\nCODE\n# report(m1) #sanity check\n\n\nFor in-lab data collection an OLS linear regression predicting scaled score by experimental condition explains a statistically significant and moderate 15% variance in score (F(1,124) = 22.7, p < 0.001). The estimated beta coefficient (\\(/beta\\) = 7.88, 95% CI [4.61, 11.2]) predicts that participants in the impasse condition will on average score around 8 points (31%) higher than those in the control condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_SCALED ~ condition, data = df_subjects %>% filter(mode==\"asynch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13.16  -7.16  -3.02   7.12  18.62 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.620      0.866   -6.49  6.5e-10 ***\ncondition121    6.777      1.190    5.70  4.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.48 on 202 degrees of freedom\nMultiple R-squared:  0.138, Adjusted R-squared:  0.134 \nF-statistic: 32.4 on 1 and 202 DF,  p-value: 4.3e-08\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   2334    2334    32.4 4.3e-08 ***\nResiduals 202  14536      72                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  -7.33  -3.91\ncondition121  4.43   9.12\n\n\nCODE\n#report(m1) #sanity check\n\n\nFor the online replication, an OLS linear regression model predicting scaled score by condition explains a statistically significant and moderate 14% of variance in absolute score (F(1,202) = 32.4, p < 0.001). The beta coefficient for condition indicates that on average, participants in the IMPASSE group scored 6.8 points higher on the task than those in the control condition (CI[4.43, 9.12]).\n\n\n\n\n\n\nNote\n\n\n\nFrom these models we can reasonably conclude that the impasse condition yields a reliable, moderate sized effect of improved interpretation on the graph reading tasks."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_modelling.html#first-question",
    "href": "analysis/SGC3A/4_sgc3A_modelling.html#first-question",
    "title": "4  Modelling",
    "section": "4.2 First Question",
    "text": "4.2 First Question\nDoes the impasse condition affect performance on the first item of the graph comprehension task?"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_modelling.html#item-level-performance",
    "href": "analysis/SGC3A/4_sgc3A_modelling.html#item-level-performance",
    "title": "4  Modelling",
    "section": "4.3 Item-Level Performance",
    "text": "4.3 Item-Level Performance\nIndividual differences with a mixed model."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_modelling.html#initial-performance",
    "href": "analysis/SGC3A/4_sgc3A_modelling.html#initial-performance",
    "title": "4  Modelling",
    "section": "4.1 Initial Performance",
    "text": "4.1 Initial Performance\nThe graph comprehension tasks includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their first exposure to the unconventional triangular coordinate system.\nTODO: - does impasse yield different exploration behavior? (characterize mouse) - does impasse yield more time on task? (characterize response time ? number of answers then de-selected?)\nTODO: Think about characterizing how variable the interpretations are across a participant. Do they form an interpretation and hold it constant? Or do they change question to question.\n\n4.1.1 Response Accuracy of First Question by Condition\n\n4.1.1.1 ChiSquare | Accuracy ~ Condition\n\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\n\nAnalysis Strategy\nChi-Square test of independence on outcome score_niceABS by condition for df_items where q == 1\n\n\nJustification\n(0) simplest method to examine independence of two categorical factors; logistic regression is recommended for binomial ~ continuous\n(1) independence assumption : as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\n(2) frequency size assumption : expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\nSteps\n(1) Express raw data as contingency table & visualize\n(2) Calculate Chi-Squared Statistic and p-value\n(3) Interpret Odds-Ratio as effect size\n\n\nInference\nLab A Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition approaching statistical significance, \\(\\chi^2\\) (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. In this particular data sample, the sample odds ratio 2.18 indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than if the control condition (Odds Ratio = 2.18, p = 0.055, 95% CI [0.982, +Inf]).\nOnline A Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. The sample odds ratio = 2.68 indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition (Odds Ratio = 2.68, p = 0.005, 95% CI [1.37, +Inf]).\n\n\n\n\n\nCODE\n#FITER THE DATASET\ndf = df_items %>% filter(q==1) \n\n#PROPORTIONAL BAR CHART\ngf_props(~score_niceABS, data = df, fill = ~mode) %>% \n  gf_facet_grid(mode~condition, labeller = label_both) +\n  labs(x = \"Correct Response on Q 1\",\n       title = \"Accuracy on First Question by Condition (Both Modalities)\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses than control \")+\n  theme_minimal()+ theme(legend.position = \"none\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Accuracy on First Question by Condition (Both Modalities)\",\n            data = df, score_niceABS ~ condition, rot_labels=c(0,90,0,0),\n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\")))\n\n\n\n\n\nCODE\n#PRINT CONTINGENCY TABLE\ntitle = \"Proportion of Correct Responses On First Item (Both Modalities)\"\nitem.contingency <-  df %>% dplyr::select(condition, score_niceABS) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Responses On First Item (Both Modalities)\n \n  \n      \n    0 \n    1 \n    Sum \n  \n \n\n  \n    111 \n    0.412 \n    0.067 \n    0.479 \n  \n  \n    121 \n    0.373 \n    0.148 \n    0.521 \n  \n  \n    Sum \n    0.785 \n    0.215 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\ndf = df_items %>% filter(q==1) %>% filter(mode == \"lab-synch\")\nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  126 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |        52 |        10 |        62 | \n             |    47.730 |    14.270 |           | \n             |     0.382 |     1.278 |           | \n             |     0.839 |     0.161 |     0.492 | \n             |     0.536 |     0.345 |           | \n             |     0.413 |     0.079 |           | \n-------------|-----------|-----------|-----------|\n         121 |        45 |        19 |        64 | \n             |    49.270 |    14.730 |           | \n             |     0.370 |     1.238 |           | \n             |     0.703 |     0.297 |     0.508 | \n             |     0.464 |     0.655 |           | \n             |     0.357 |     0.151 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |        97 |        29 |       126 | \n             |     0.770 |     0.230 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  3.27     d.f. =  1     p =  0.0707 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  2.55     d.f. =  1     p =  0.111 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.18 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.0909 \n95% confidence interval:  0.86 5.84 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  0.979 \n95% confidence interval:  0 5.03 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.0547 \n95% confidence interval:  0.982 Inf \n\n\n \n\n\nInspecting the output of the Chi-Squared test, we first see that we meet the assumption of expected frequency in each cell (indicated by the second row in each box, ‘Expected N’. The model predicts more than 5 observations in each cell.) The Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition approaching statistical significance, \\(\\chi^2\\) (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is not equal to 1. In this particular data sample, the odds ratio 2.18 indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition (Odds Ratio = 2.18, p = 0.055, 95% CI [0.982, +Inf]).\n\n\nCODE\ndf = df_items %>% filter(q==1) %>% filter(mode == \"asynch\")\nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  204 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |        84 |        12 |        96 | \n             |    76.235 |    19.765 |           | \n             |     0.791 |     3.050 |           | \n             |     0.875 |     0.125 |     0.471 | \n             |     0.519 |     0.286 |           | \n             |     0.412 |     0.059 |           | \n-------------|-----------|-----------|-----------|\n         121 |        78 |        30 |       108 | \n             |    85.765 |    22.235 |           | \n             |     0.703 |     2.711 |           | \n             |     0.722 |     0.278 |     0.529 | \n             |     0.481 |     0.714 |           | \n             |     0.382 |     0.147 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |       162 |        42 |       204 | \n             |     0.794 |     0.206 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  7.26     d.f. =  1     p =  0.00707 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  6.35     d.f. =  1     p =  0.0117 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.68 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.00894 \n95% confidence interval:  1.23 6.17 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  0.998 \n95% confidence interval:  0 5.42 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.00539 \n95% confidence interval:  1.37 Inf \n\n\n \n\n\nInspecting the output of the Chi-Squared test, we first see that we meet the assumption of expected frequency in each cell (indicated by the second row in each box, ‘Expected N’. The model predicts more than 5 observations in each cell.) The Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is not equal to 1. The odds ratio = 2.68 indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition (Odds Ratio = 2.68, p = 0.005, 95% CI [1.37, +Inf])."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html",
    "title": "4  Hypothesis Testing",
    "section": "",
    "text": "TODO\nThe purpose of this notebook is test the hypotheses that determined the design of the SGC3A study.\nResearch Questions\nIn SGC3A we set out to answer the following question: Does posing a mental impasse improve performance on the graph comprehension task?\nSpecifically, we will test the following experimental hypotheses:\nExperimental Hypothesis: Learners posed with impasse-inducing questions will be more likely to correct interpret the graph.\nThe Null Hypothesis asserts that no significant differences in performance will exist between learners in the impasse and control conditions."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#initial-performance",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#initial-performance",
    "title": "4  Hypothesis Testing",
    "section": "4.2 INITIAL PERFORMANCE",
    "text": "4.2 INITIAL PERFORMANCE\nThe graph comprehension tasks includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their first exposure to the unconventional triangular coordinate system.\nTODO: - does impasse yield different exploration behavior? (characterize mouse) - does impasse yield more time on task? (characterize response time ? number of answers then de-selected?)\nTODO: Think about characterizing how variable the interpretations are across a participant. Do they form an interpretation and hold it constant? Or do they change question to question.\n\n4.2.1 Response Accuracy of First Question by Condition\n\n4.2.1.1 Chi Square | Accuracy ~ Condition\n\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\n\nAnalysis Strategy\nChi-Square test of independence on outcome score_niceABS by condition for df_items where q == 1\n\n\nJustification\n(0) simplest method to examine independence of two categorical factors; logistic regression is recommended for binomial ~ continuous\n(1) independence assumption : as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\n(2) frequency size assumption : expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\nSteps\n(1) Express raw data as contingency table & visualize\n(2) Calculate Chi-Squared Statistic and p-value\n(3) Interpret Odds-Ratio as effect size\n\n\nInference\nLab For the in-lab data collection (n=126) the Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition approaching statistical significance, \\(\\chi^2\\) (1) = 10.3, p = 0.07. In this particular data sample, the odds ratio (2.18, p = 0.055, 95% CI [0.982, +Inf]) indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition.\nOnline For online data collection (n=204), a Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. The odds ratio (2.68, p = 0.005, 95% CI [1.37, +Inf]) indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition.\n\n\n\n\n\nCODE\n#FITER THE DATASET\ndf = df_items %>% filter(q==1) \n\n#PROPORTIONAL BAR CHART\ngf_props(~score_niceABS, data = df, fill = ~mode) %>% \n  gf_facet_grid(mode~condition, labeller = label_both) +\n  labs(x = \"Correct Response on Q 1\",\n       title = \"Accuracy on First Question by Condition (Both Modalities)\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses than control \")+\n  theme_minimal()+ theme(legend.position = \"none\")\n\n\n\n\n\nA proportional bar chart visualizing the proportion of incorrect (x =0) vs correct (x = 1) responses in each condition (right/left facet) for each data collection modality (top/bottom) reveal that the pattern of responses appear the same regardless of the data collection modality. In both data collection sessions, the proportion of incorrect responses is much greater than the proportion of correct responses, regardless of condition. In the impasse condition, the difference in proportions is smaller than the control condition (i.e. There are more correct responses in the impasse condition than the control condition).\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Accuracy on First Question by Condition (Both Modalities)\",\n            data = df, score_niceABS ~ condition, rot_labels=c(0,90,0,0),\n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\")))\n\n\n\n\n\nCODE\n#PRINT CONTINGENCY TABLE\ntitle = \"Proportion of Correct Responses On First Item (Both Modalities)\"\nitem.contingency <-  df %>% dplyr::select(condition, score_niceABS) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Responses On First Item (Both Modalities)\n \n  \n      \n    0 \n    1 \n    Sum \n  \n \n\n  \n    111 \n    0.412 \n    0.067 \n    0.479 \n  \n  \n    121 \n    0.373 \n    0.148 \n    0.521 \n  \n  \n    Sum \n    0.785 \n    0.215 \n    1.000 \n  \n\n\n\n\n\nA mosaic plot condition by response accuracy on the first question (across both data collection modalities) reveals the same pattern (the mosaic plot is an alternative visualization technique to the proportional bar chart). The relative size of condition boxes (111 vs 121) reflects that the sample is roughly evenly split across experimental conditions. The difference in size between 0 (incorrect) and 1 (correct) reflects that the proportion of correct responses (1) is greater in the impasse condition (121).\nNext, we compute a contingency table and Pearson’s Chi-Squared test for each data collection modality.\n\n\nCODE\ndf = df_items %>% filter(q==1) %>% filter(mode == \"lab-synch\")\nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  126 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |        52 |        10 |        62 | \n             |    47.730 |    14.270 |           | \n             |     0.382 |     1.278 |           | \n             |     0.839 |     0.161 |     0.492 | \n             |     0.536 |     0.345 |           | \n             |     0.413 |     0.079 |           | \n-------------|-----------|-----------|-----------|\n         121 |        45 |        19 |        64 | \n             |    49.270 |    14.730 |           | \n             |     0.370 |     1.238 |           | \n             |     0.703 |     0.297 |     0.508 | \n             |     0.464 |     0.655 |           | \n             |     0.357 |     0.151 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |        97 |        29 |       126 | \n             |     0.770 |     0.230 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  3.27     d.f. =  1     p =  0.0707 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  2.55     d.f. =  1     p =  0.111 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.18 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.0909 \n95% confidence interval:  0.86 5.84 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  0.979 \n95% confidence interval:  0 5.03 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.0547 \n95% confidence interval:  0.982 Inf \n\n\n \n\n\nFor the in-lab data collection (n=126) the Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition approaching statistical significance, \\(\\chi^2\\) (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. In this particular data sample, the odds ratio (Odds Ratio = 2.18, p = 0.055, 95% CI [0.982, +Inf]) indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition .\n\n\nCODE\ndf = df_items %>% filter(q==1) %>% filter(mode == \"asynch\")\nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  204 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |        84 |        12 |        96 | \n             |    76.235 |    19.765 |           | \n             |     0.791 |     3.050 |           | \n             |     0.875 |     0.125 |     0.471 | \n             |     0.519 |     0.286 |           | \n             |     0.412 |     0.059 |           | \n-------------|-----------|-----------|-----------|\n         121 |        78 |        30 |       108 | \n             |    85.765 |    22.235 |           | \n             |     0.703 |     2.711 |           | \n             |     0.722 |     0.278 |     0.529 | \n             |     0.481 |     0.714 |           | \n             |     0.382 |     0.147 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |       162 |        42 |       204 | \n             |     0.794 |     0.206 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  7.26     d.f. =  1     p =  0.00707 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  6.35     d.f. =  1     p =  0.0117 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.68 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.00894 \n95% confidence interval:  1.23 6.17 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  0.998 \n95% confidence interval:  0 5.42 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.00539 \n95% confidence interval:  1.37 Inf \n\n\n \n\n\nFor online data collection (n=204), a Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. The odds ratio (Odds Ratio = 2.68, p = 0.005, 95% CI [1.37, +Inf]) indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition .\n\n\nCODE\ndf = df_items %>% filter(q==1) \nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  330 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |       136 |        22 |       158 | \n             |   124.006 |    33.994 |           | \n             |     1.160 |     4.232 |           | \n             |     0.861 |     0.139 |     0.479 | \n             |     0.525 |     0.310 |           | \n             |     0.412 |     0.067 |           | \n-------------|-----------|-----------|-----------|\n         121 |       123 |        49 |       172 | \n             |   134.994 |    37.006 |           | \n             |     1.066 |     3.887 |           | \n             |     0.715 |     0.285 |     0.521 | \n             |     0.475 |     0.690 |           | \n             |     0.373 |     0.148 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |       259 |        71 |       330 | \n             |     0.785 |     0.215 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  10.3     d.f. =  1     p =  0.0013 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  9.5     d.f. =  1     p =  0.00205 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.46 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.00131 \n95% confidence interval:  1.37 4.53 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  1 \n95% confidence interval:  0 4.12 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.000928 \n95% confidence interval:  1.49 Inf \n\n\n \n\n\nCombining data across both sessions (n=330), a Pearson’s Chi-squared test suggests a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi_2\\) (1) = 10.3, p = 0.001. The sample odds ratio (2.46, p = 0.001, 95% CI [1.37, 4.53]) indicates that the odds of providing a correct response to the first question are 2.46 higher for subjects in the impasse condition than those in the control condition."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#cumulative-performance",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#cumulative-performance",
    "title": "4  Hypothesis Testing",
    "section": "4.2 CUMULATIVE PERFORMANCE",
    "text": "4.2 CUMULATIVE PERFORMANCE\nOverall does the impasse condition affect performance on the graph comprehension task?\n\n4.2.1 Cumulative Score By Condition\nCumulative scores indicate the response accuracy by a particular participant across all discriminant items (n=13) in the graph comprehension task.\n\n\nCODE\n#VISUALIZE distribution of response accuracy across SUBJECTS\n\n#HISTOGRAM\nstats = df_subjects %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(s_ABS))\ngf_density(~s_ABS, data = df_subjects) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_lims(x = c(0, 13)) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Cumulative Absolute Score\",\n       y = \"proportion of subjects\",\n       title = \"Subject Cumulative Score (Absolute)\",\n       subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n  theme_minimal()\n\n\n\n\n\nDoes the IMPASSE condition yield higher scores?\n\n4.2.1.1 Linear Regression\nTo address this question we assess how much variance in cumulative (absolute) score is explained by experimental condition by utilizing OLS linear regression, predicting absolute score from condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_ABS ~ condition, data = df_subjects %>% filter(mode == \n    \"lab-synch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -5.44  -2.68  -2.68   4.31  10.32 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.677      0.620    4.32 0.000031 ***\ncondition121    2.760      0.869    3.17   0.0019 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.88 on 124 degrees of freedom\nMultiple R-squared:  0.0752,    Adjusted R-squared:  0.0677 \nF-statistic: 10.1 on 1 and 124 DF,  p-value: 0.00189\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_ABS\n           Df Sum Sq Mean Sq F value Pr(>F)   \ncondition   1    240   239.9    10.1 0.0019 **\nResiduals 124   2951    23.8                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)   1.45   3.90\ncondition121  1.04   4.48\n\n\nCODE\n# report(m1) #sanity check\n\n\nFor in-lab data collection an OLS linear regression predicting cumulative absolute score by experimental condition explains a statistically significant though small 7.5% variance in score (F(1,124) = 10.1, p < 0.01). The estimated beta coefficient (\\(/beta\\) = 2.76, 95% CI [1.04, 4.48]) predicts that participants in the impasse condition will on average score 2.76 points (21%) higher than those in the control condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"asynch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_ABS ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.58  -3.58  -2.30   3.42  10.70 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.302      0.485    4.75  3.8e-06 ***\ncondition121    2.281      0.666    3.43  0.00074 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.75 on 202 degrees of freedom\nMultiple R-squared:  0.0549,    Adjusted R-squared:  0.0502 \nF-statistic: 11.7 on 1 and 202 DF,  p-value: 0.000745\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_ABS\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1    264   264.5    11.7 0.00074 ***\nResiduals 202   4554    22.5                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  1.347   3.26\ncondition121 0.968   3.59\n\n\nCODE\n#report(m1) #sanity check\n\n\nFor the online replication, an OLS linear regression model predicting cumulative absolute score by condition explains a statistically significant though small 5.5% of variance in absolute score (F(1,202) = 11.73, p < 0.001). The beta coefficient for condition indicates that on average, participants in the IMPASSE group scored 2.3 points higher on the task than those in the control condition (CI[0.97, 3.59]).\n\n\n\n\n\n\nNote\n\n\n\nFrom these models we can reasonably conclude that the impasse condition yielded a small but reliable improvement in performance across items in the graph reading task.\n\n\nModel Diagnostics\n\n\nCODE\ncheck_model(m1)\n\n\n\n\n\n\n\n4.2.1.2 Poisson Regression\nThe outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can consider it a type of count, (ie. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function).\n\n\nCODE\n# library('fitdistrplus')\n# plot(fitdist(df_subjects$s_ABS,\"pois\"))\n# plot(fitdist(df_subjects$s_ABS,\"norm\"))\n# plot(fitdist(df_subjects$s_ABS,\"beta\"))\n# \n# \n# plotdist(df_subjects$s_ABS, histo = TRUE, demp = TRUE)\n# descdist(df_subjects$s_ABS, discrete=FALSE, boot=500)\n\n\n\n\nCODE\n#SCORE predicted by CONDITION --> POISSON DISTRIBUTION\nmp1 <- glm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"), family = \"poisson\")\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(mp1)\n\n\n\nCall:\nglm(formula = s_ABS ~ condition, family = \"poisson\", data = df_subjects %>% \n    filter(mode == \"lab-synch\"))\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -3.30   -2.31   -2.31    1.75    4.52  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    0.9849     0.0776   12.69  < 2e-16 ***\ncondition121   0.7085     0.0943    7.51  5.9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 855.68  on 125  degrees of freedom\nResidual deviance: 795.46  on 124  degrees of freedom\nAIC: 1045\n\nNumber of Fisher Scoring iterations: 6\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(mp1)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: s_ABS\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev\nNULL                        125        856\ncondition  1     60.2       124        795\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(mp1)\n\n\nWaiting for profiling to be done...\n\n\n             2.5 % 97.5 %\n(Intercept)  0.829  1.133\ncondition121 0.525  0.896\n\n\nCODE\nreport(mp1) #sanity check\n\n\nWe fitted a poisson model (estimated using ML) to predict s_ABS with condition (formula: s_ABS ~ condition). The model's explanatory power is substantial (Nagelkerke's R2 = 0.38). The model's intercept, corresponding to condition = 111, is at 0.98 (95% CI [0.83, 1.13], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.71, 95% CI [0.53, 0.90], p < .001; Std. beta = 0.71, 95% CI [0.53, 0.90])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\ncheck_model(mp1)\n\n\n\n\n\n\n\nCODE\n#Which is a better fit? linear or poisson?\n\ncompare_performance(m1,mp1)\n\n\nWarning: When comparing models, please note that probably not all models were fit from\n  same data.\n\n\n# Comparison of Model Performance Indices\n\nName | Model |      AIC | AIC weights |      BIC | BIC weights |  RMSE | Sigma |    R2 | R2 (adj.) | Nagelkerke's R2 | Score_log | Score_spherical\n--------------------------------------------------------------------------------------------------------------------------------------------------\nm1   |    lm | 1218.500 |     < 0.001 | 1228.454 |     < 0.001 | 4.725 | 4.748 | 0.055 |     0.050 |                 |           |                \nmp1  |   glm | 1044.751 |        1.00 | 1050.423 |        1.00 | 4.840 | 2.533 |       |           |           0.380 |    -4.130 |           0.063"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#item-level-performance",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#item-level-performance",
    "title": "4  Hypothesis Testing",
    "section": "4.3 Item-Level Performance",
    "text": "4.3 Item-Level Performance\nIndividual differences with a mixed model."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#model-peeking",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#model-peeking",
    "title": "4  Hypothesis Testing",
    "section": "4.4 Model Peeking",
    "text": "4.4 Model Peeking\nTODO\n\nmultiple regression with condition and response time\n\n\n\nCODE\nlibrary(supernova)\n\n\n\nAttaching package: 'supernova'\n\n\nThe following object is masked from 'package:scales':\n\n    number\n\n\nCODE\nlibrary(report)\nlibrary(lmerTest)\n\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nRegistered S3 methods overwritten by 'lme4':\n  method                          from\n  cooks.distance.influence.merMod car \n  influence.merMod                car \n  dfbeta.influence.merMod         car \n  dfbetas.influence.merMod        car \n\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nCODE\nlibrary(lme4)\nm1 <- lm( s_SCALED ~ condition, data = df_subjects %>% filter(mode=='asynch'))\nm1\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nCoefficients:\n (Intercept)  condition121  \n       -5.62          6.78  \n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13.16  -7.16  -3.02   7.12  18.62 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.620      0.866   -6.49  6.5e-10 ***\ncondition121    6.777      1.190    5.70  4.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.48 on 202 degrees of freedom\nMultiple R-squared:  0.138, Adjusted R-squared:  0.134 \nF-statistic: 32.4 on 1 and 202 DF,  p-value: 4.3e-08\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   2334    2334    32.4 4.3e-08 ***\nResiduals 202  14536      72                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nsuperanova(m1)\n\n\n Analysis of Variance Table (Type III SS)\n Model: s_SCALED ~ condition\n\n                                SS  df       MS      F    PRE     p\n ----- --------------- | --------- --- -------- ------ ------ -----\n Model (error reduced) |  2334.346   1 2334.346 32.439 0.1384 .0000\n Error (from model)    | 14536.196 202   71.961                    \n ----- --------------- | --------- --- -------- ------ ------ -----\n Total (empty model)   | 16870.543 203   83.106                    \n\n\nCODE\nplot(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCODE\ngf_histogram(~s_SCALED, data = df_subjects)\n\n\n\n\n\nCODE\ngf_histogram(~m1$residuals)\n\n\n\n\n\nCODE\n#Assess assumption of independence of errors\n#DW statistic should be close to 2\nlibrary(car)\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'carData'\n\n\nThe following object is masked from 'package:vcdExtra':\n\n    Burt\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCODE\ndurbinWatsonTest(m1)\n\n\n lag Autocorrelation D-W Statistic p-value\n   1         -0.0134          2.01   0.942\n Alternative hypothesis: rho != 0\n\n\nCODE\n#Test for equality of variance\n#H0 is equality; p > 0.05 infer you can't reject null\nleveneTest(m1)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)  \ngroup   1     5.4  0.021 *\n      202                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA simple linear regression model predicting cumulative scaled score (at subject level) by condition explains 13% of the total variance, F(1,329) = 47.8, p < 0.001. The model predicts that participants in the impasse condition will score on average 6.38 points higher than those in the control condition, 95% CI [4.56, 8.19].\n\n\nCODE\nt.test(s_SCALED ~ condition, data = df_subjects)\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by condition\nt = -7, df = 322, p-value = 1e-12\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9.10 -5.29\nsample estimates:\nmean in group 111 mean in group 121 \n            -5.82              1.38 \n\n\nCODE\n#%>% report()\n\n\n\n\nCODE\n# report_participants(df_subjects)\nm1 %>% report()\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with condition (formula: s_SCALED ~ condition). The model explains a statistically significant and moderate proportion of variance (R2 = 0.14, F(1, 202) = 32.44, p < .001, adj. R2 = 0.13). The model's intercept, corresponding to condition = 111, is at -5.62 (95% CI [-7.33, -3.91], t(202) = -6.49, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 6.78, 95% CI [4.43, 9.12], t(202) = 5.70, p < .001; Std. beta = 0.74, 95% CI [0.49, 1.00])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\nanova(m1) %>% report()\n\n\nFor one-way between subjects designs, partial eta squared is equivalent to eta squared.\nReturning eta squared.\n\n\nThe ANOVA suggests that:\n\n  - The main effect of condition is statistically significant and medium (F(1, 202) = 32.44, p < .001; Eta2 = 0.14, 95% CI [0.07, 1.00])\n\n\nCODE\n#significant intercept means that group is significantly different than zero\n\n\n\n\nCODE\n#logistic regression on on scaled df_subjects because residuals not normal in lm?\nmlog <- glm(s_SCALED ~ condition , data = df_subjects, family = gaussian)\nsummary(mlog)\n\n\n\nCall:\nglm(formula = s_SCALED ~ condition, family = gaussian, data = df_subjects)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-13.38   -7.18   -3.28    8.50   18.82  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.820      0.698   -8.34  2.1e-15 ***\ncondition121    7.198      0.967    7.45  8.6e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 77)\n\n    Null deviance: 29508  on 329  degrees of freedom\nResidual deviance: 25242  on 328  degrees of freedom\nAIC: 2374\n\nNumber of Fisher Scoring iterations: 2\n\n\nCODE\nreport(mlog)\n\n\nWe fitted a linear model (estimated using ML) to predict s_SCALED with condition (formula: s_SCALED ~ condition). The model's explanatory power is moderate (R2 = 0.14). The model's intercept, corresponding to condition = 111, is at -5.82 (95% CI [-7.19, -4.45], t(328) = -8.34, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 7.20, 95% CI [5.30, 9.09], t(328) = 7.45, p < .001; Std. beta = 0.76, 95% CI [0.56, 0.96])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\n#logistic regression on niceABS by condition\n#pretends that questions are independent and not from same subjects INVALID\nmlog <- glm(score_niceABS ~ condition , data = df_items %>% filter(q<6), family = binomial())\nsummary(mlog)\n\n\n\nCall:\nglm(formula = score_niceABS ~ condition, family = binomial(), \n    data = df_items %>% filter(q < 6))\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.980  -0.980  -0.649   1.389   1.823  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.4508     0.0907  -15.99   <2e-16 ***\ncondition121   0.9672     0.1147    8.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1986.2  on 1649  degrees of freedom\nResidual deviance: 1911.3  on 1648  degrees of freedom\nAIC: 1915\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\nreport(mlog)\n\n\nWe fitted a logistic model (estimated using ML) to predict score_niceABS with condition (formula: score_niceABS ~ condition). The model's explanatory power is weak (Tjur's R2 = 0.04). The model's intercept, corresponding to condition = 111, is at -1.45 (95% CI [-1.63, -1.28], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.97, 95% CI [0.74, 1.19], p < .001; Std. beta = 0.97, 95% CI [0.74, 1.19])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm2 <- lm( s_NABS ~ condition, data = df_subjects)\nm2\n\n\n\nCall:\nlm(formula = s_NABS ~ condition, data = df_subjects)\n\nCoefficients:\n (Intercept)  condition121  \n        2.47          2.46  \n\n\nCODE\nsummary(m2)\n\n\n\nCall:\nlm(formula = s_NABS ~ condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.92  -3.67  -2.47   4.08  10.53 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.468      0.384    6.43  4.4e-10 ***\ncondition121    2.456      0.531    4.62  5.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.82 on 328 degrees of freedom\nMultiple R-squared:  0.0611,    Adjusted R-squared:  0.0583 \nF-statistic: 21.4 on 1 and 328 DF,  p-value: 5.49e-06\n\n\nCODE\nanova(m2)\n\n\nAnalysis of Variance Table\n\nResponse: s_NABS\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1    497     497    21.4 5.5e-06 ***\nResiduals 328   7629      23                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nsupernova(m2)\n\n\n Analysis of Variance Table (Type III SS)\n Model: s_NABS ~ condition\n\n                               SS  df      MS      F    PRE     p\n ----- --------------- | -------- --- ------- ------ ------ -----\n Model (error reduced) |  496.765   1 496.765 21.357 0.0611 .0000\n Error (from model)    | 7629.359 328  23.260                    \n ----- --------------- | -------- --- ------- ------ ------ -----\n Total (empty model)   | 8126.124 329  24.699                    \n\n\nA simple linear regression model predicting cumulative absolute score by condition explains 5% of variance, F(1,328) = 16.36, p < 0.001. The model predicts that subjects in the impasse condition will score on average 2 points higher than those in the control condition (Beta = 2.02, 95% CI [1.04, 3.00])\n\n\nCODE\nreport(m2)\n\n\nWe fitted a linear model (estimated using OLS) to predict s_NABS with condition (formula: s_NABS ~ condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.06, F(1, 328) = 21.36, p < .001, adj. R2 = 0.06). The model's intercept, corresponding to condition = 111, is at 2.47 (95% CI [1.71, 3.22], t(328) = 6.43, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 2.46, 95% CI [1.41, 3.50], t(328) = 4.62, p < .001; Std. beta = 0.49, 95% CI [0.28, 0.70])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\n\n\nCODE\nm.m1 <- lmer( score_SCALED ~ (1 + condition|subject), data = df_items)\n\n\nWarning: Model failed to converge with 1 negative eigenvalue: -8.1e-04\n\n\nCODE\nm.m1\n\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: score_SCALED ~ (1 + condition | subject)\n   Data: df_items\nREML criterion at convergence: 9773\nRandom effects:\n Groups   Name         Std.Dev. Corr \n subject  (Intercept)  0.632         \n          condition121 0.827    -0.71\n Residual              0.591         \nNumber of obs: 4950, groups:  subject, 330\nFixed Effects:\n(Intercept)  \n    -0.0617  \n\n\nCODE\nsummary(m.m1)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | subject)\n   Data: df_items\n\nREML criterion at convergence: 9773\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9664 -0.6738 -0.0461  0.5889  2.7646 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n subject  (Intercept)  0.400    0.632         \n          condition121 0.685    0.827    -0.71\n Residual              0.349    0.591         \nNumber of obs: 4950, groups:  subject, 330\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(>|t|)  \n(Intercept)  -0.0617     0.0344 326.4131   -1.79    0.074 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m.m1)\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition and subject as random effects (formula: ~1 + condition | subject). . The model's intercept is at -0.06 (95% CI [-0.13, 5.71e-03], t(4945) = -1.79, p = 0.073). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm.m2 <- lmer( score_SCALED ~ (1+ condition|q), data = df_items)\nm.m2\n\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: score_SCALED ~ (1 + condition | q)\n   Data: df_items\nREML criterion at convergence: 11500\nRandom effects:\n Groups   Name         Std.Dev. Corr \n q        (Intercept)  0.602         \n          condition121 0.530    -0.93\n Residual              0.765         \nNumber of obs: 4950, groups:  q, 15\nFixed Effects:\n(Intercept)  \n      0.172  \n\n\nCODE\nsummary(m.m2)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | q)\n   Data: df_items\n\nREML criterion at convergence: 11500\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-1.972 -0.794 -0.093  0.887  2.164 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n q        (Intercept)  0.363    0.602         \n          condition121 0.281    0.530    -0.93\n Residual              0.585    0.765         \nNumber of obs: 4950, groups:  q, 15\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)   \n(Intercept)   0.1717     0.0575 14.0014    2.98   0.0099 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m.m2)\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition and q as random effects (formula: ~1 + condition | q). . The model's intercept is at 0.17 (95% CI [0.06, 0.28], t(4945) = 2.98, p = 0.003). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm.m3 <- lmer( score_SCALED ~ (1+ condition|q) + (1+condition|subject), data = df_items)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00208782 (tol = 0.002, component 1)\n\n\nCODE\nm.m3 %>% summary() \n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | q) + (1 + condition | subject)\n   Data: df_items\n\nREML criterion at convergence: 8593\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.724 -0.644 -0.016  0.697  3.599 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n subject  (Intercept)  0.339    0.583         \n          condition121 0.218    0.467    -0.46\n q        (Intercept)  0.268    0.517         \n          condition121 0.197    0.443    -0.91\n Residual              0.266    0.516         \nNumber of obs: 4950, groups:  subject, 330; q, 15\n\nFixed effects:\n            Estimate Std. Error     df t value Pr(>|t|)\n(Intercept)    0.117      0.071 24.208    1.65     0.11\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00208782 (tol = 0.002, component 1)\n\n\nCODE\nm.m3 %>% report()\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition, q and subject as random effects (formula: list(~1 + condition | q, ~1 + condition | subject)). . The model's intercept is at 0.12 (95% CI [-0.02, 0.26], t(4942) = 1.65, p = 0.098). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nanova(m.m1, m.m2, m.m3)\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: df_items\nModels:\nm.m1: score_SCALED ~ (1 + condition | subject)\nm.m2: score_SCALED ~ (1 + condition | q)\nm.m3: score_SCALED ~ (1 + condition | q) + (1 + condition | subject)\n     npar   AIC   BIC logLik deviance Chisq Df Pr(>Chisq)    \nm.m1    5  9778  9811  -4884     9768                        \nm.m2    5 11506 11539  -5748    11496     0  0               \nm.m3    8  8605  8657  -4295     8589  2907  3     <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#resources",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#resources",
    "title": "4  Hypothesis Testing",
    "section": "4.3 RESOURCES",
    "text": "4.3 RESOURCES\n\nhttps://rpkgs.datanovia.com/ggpubr/reference/index.html"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html",
    "href": "analysis/SGC3A/3_sgc3A_description.html",
    "title": "3  Description",
    "section": "",
    "text": "TODO\nThe purpose of this notebook is describe the distributions of dependent variables for Study SGC3A."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#sample",
    "href": "analysis/SGC3A/3_sgc3A_description.html#sample",
    "title": "3  Description",
    "section": "3.1 SAMPLE",
    "text": "3.1 SAMPLE\n\n3.1.1 Data Collection\nData was initially collected (in person, SONA groups in computer lab) in Fall 2017. In Spring 2018, additional data were collected after small modifications were made to the experimental platform to increase the size of multiple-choice input buttons, and to add an additional free-response question following the main task block. In Fall 2021, the study was replicated using asynchronous, online SONA pool, with additional participants collected in Winter 2022.\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_subjects$term, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    fall17 \n    27 \n    27 \n    54 \n  \n  \n    spring18 \n    35 \n    37 \n    72 \n  \n  \n    fall21 \n    68 \n    71 \n    139 \n  \n  \n    winter22 \n    28 \n    37 \n    65 \n  \n  \n    Sum \n    158 \n    172 \n    330 \n  \n\n\n\n\n\n\n\n3.1.2 Participants\n\n\nCODE\n#Describe participants\nsubject.stats <- rbind(\n  \"lab\"= df_subjects %>% filter(mode == 'lab-synch') %>% select(age) %>% unlist() %>% favstats(),\n  \"online\" = df_subjects %>% filter(mode == \"asynch\") %>% select(age) %>% unlist() %>% favstats()\n) \nsubject.stats$female <- c(\n  (df_subjects %>% filter(mode == 'lab-synch') %>%  filter(gender==\"Female\") %>% count())$n,\n  (df_subjects %>% filter(mode == \"asynch\") %>% filter(gender==\"Female\") %>% count())$n\n)\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    female \n  \n \n\n  \n    lab \n    18 \n    19 \n    20 \n    21 \n    33 \n    20.4 \n    2.12 \n    126 \n    0 \n    78 \n  \n  \n    online \n    18 \n    20 \n    20 \n    21 \n    31 \n    20.6 \n    2.00 \n    204 \n    0 \n    137 \n  \n\n\n\n\n\nFor in-person collection, 126 participants (60 % female ) undergraduate STEM majors at a public American University participated in person in exchange for course credit (age: 18 - 33 years). Participants were randomly assigned to one of two experimental groups.\nFor online replication 204 participants (70 % female ) undergraduate STEM majors at a public American University participated online, asynchronously in exchange for course credit (age: 18 - 31 years). Participants were randomly assigned to one of two experimental groups.\n\n\nCODE\nreport_participants(df_subjects, education = \"schoolyear\", sex = \"gender\", group = \"mode\")\n\n\n[1] \"For the 'mode - asynch' group: 204 participants (Mean age = 20.6, SD = 2.0, range: [18, 31]; Sex: 67.2% females, 30.4% males, 2.5% other; Education: First, 11.76%; Second, 16.67%; Third, 33.82%; Fourth, 34.31%; Fifth, 2.94%; Other, 0.49%) and for the 'mode - lab-synch' group: 126 participants (Mean age = 20.4, SD = 2.1, range: [18, 33]; Sex: 61.9% females, 37.3% males, 0.8% other; Education: First, 21.43%; Second, 20.63%; Third, 26.98%; Fourth, 19.05%; Fifth, 9.52%; Other, 2.38%)\""
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#response-accuracy",
    "href": "analysis/SGC3A/3_sgc3A_description.html#response-accuracy",
    "title": "3  Description",
    "section": "3.2 RESPONSE ACCURACY",
    "text": "3.2 RESPONSE ACCURACY\n\n3.2.1 Cumulative Scores\nCumulative scores indicate the response accuracy by a particular participant across all discriminant items (n=13) in the graph comprehension task.\n\n3.2.1.1 Cumulative Absolute Score\nRecall from Section 2.1.2.1 that the absolute score (following the dichotomous scoring approach) s_ABS indicates if the subject’s response for a particular item was perfectly correct: whether they selected all correct answer options and no others (excluding certain allowed exceptions, such as also selecting the data point referenced in the question). The absolute score for an individual item is either 0 or 1. When summarized across the entire set of discriminant items, the cumulative absolute score for an individual subject ranges from [0,13].\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Cumulative Item Absolute Score)\"\nabs.stats <- rbind(\n  \"lab\"= df_subjects %>% filter(mode == 'lab-synch') %>% select(s_ABS) %>% unlist() %>% favstats(),\n  \"online\" = df_subjects %>% filter(mode == \"asynch\") %>% select(s_ABS) %>% unlist() %>% favstats()\n) \nabs.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Cumulative Item Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    0 \n    0 \n    1 \n    9 \n    13 \n    4.08 \n    5.05 \n    126 \n    0 \n  \n  \n    online \n    0 \n    0 \n    1 \n    8 \n    13 \n    3.51 \n    4.87 \n    204 \n    0 \n  \n\n\n\n\n\nFor in person collection, cumulative absolute scores (n = 126) range from 0 to 13 with a mean score of (M = 4.08, SD = 5.05).\nFor online replication, (online) cumulative accuracy scores (n = 204) range from 0 to 13 with a slighly lower mean score of (M = 3.51, SD = 4.87).\n\n\nCODE\n#VISUALIZE distribution of response accuracy across SUBJECTS\n\n#HISTOGRAM\nstats = df_subjects %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(s_ABS))\ngf_density(~s_ABS, data = df_subjects) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_lims(x = c(0, 13)) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Cumulative Absolute Score\",\n       y = \"proportion of subjects\",\n       title = \"Subject Cumulative Score (Absolute)\",\n       subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n  theme_minimal()\n\n\n\n\n\nCODE\n#RIDGEPLOT\n# ggplot(data = df_subjects, aes(x = s_NABS, y = mode)) +\n#   geom_density_ridges() + xlim(0,13)+\n#   facet_wrap(~condition, labeller = label_both) +\n#  labs(x = \"Cumulative Absolute Score\",\n#       y = \"proportion of subjects\",\n#        title = \"Subject Cumulative Score (Absolute)\",\n#        subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n#   theme_minimal()\n\n\n\n\n\n\n\n\nDecision\n\n\n\nCondition appears to exert a positive influence on Cumulative Absolute Score across data collection modalities.\n\n\n\n\n3.2.1.2 Cumulative Scaled Scores\nThe Cumulative Scaled score s_SCALED summarizes the scaled score on the 13 strategy-discriminant questions, for each subject The range is from -13 (all orthogonal) to 13 (all triangular). Recall that the s_SCALED score for an item is a numeric representation of the strategy-consistent response, scaled from -1 to +1, where “Orthogonal” = -1, “Satisfice” = -1, “Triangular” = 1, “Tversky” = 0.5, “both tri + orth” = 0.5, “reference” = 0, “blank” = 0, “frenzy” = 0, “?” = 0.\nMost importantly, the Scaled score gives us a way of quantitatively examining how correctly a participant interpreted the coordinate system across the entire set of items. It offers a more nuanced look into performance than absolute score\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Cumulative Scaled Score)\"\nscaled.stats <- rbind(\n  \"lab\"= df_subjects %>% filter(mode == 'lab-synch') %>% select(s_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_subjects %>% filter(mode == \"asynch\") %>% select(s_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Cumulative Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -13 \n    -11.9 \n    -6 \n    9.75 \n    13 \n    -2.13 \n    10.05 \n    126 \n    0 \n  \n  \n    online \n    -13 \n    -9.0 \n    -6 \n    8.00 \n    13 \n    -2.03 \n    9.12 \n    204 \n    0 \n  \n\n\n\n\n\nFor in person collection, cumulative absolute scores (n = 126) range from -13 to 13 with a mean score of (M = -2.13, SD = 10.05).\nFor online replication, (online) cumulative accuracy scores (n = 204) range from -13 to 13 with a slighly lower mean score of (M = -2.03, SD = 9.12).\n\n\nCODE\n#VISUALIZE distribution of response accuracy across SUBJECTS\n\n#HISTOGRAM\nstats = df_subjects %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(s_SCALED))\ngf_density(~s_SCALED, data = df_subjects, binwidth = 1) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_lims(x = c(-13, 13)) %>%\n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n   labs(x = \"Cumulative Scaled Score\",\n       y = \"proportion of subjects\",\n       title = \"Subject Cumulative Score (Scaled)\",\n       subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nDecision\n\n\n\nCondition appears to exert a positive influence on Scaled Score across data collection modalities.\n\n\n\n\n\n3.2.2 Item Scores\nItem scores indicate the response accuracy by a participant on each individual question discriminant question (n=13) in the graph comprehension task.\n\n3.2.2.1 Item Absolute Score\n\n\nCODE\nx <- df_items %>% mutate(score = as.logical(score_ABS))\n\ntitle = \"Proportion of Correct Items By Condition (Lab)\"\n\nitem.contingency <- df_items %>% filter(mode == \"lab-synch\") %>% select(score_ABS, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Items By Condition (Lab)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    0 \n    0.344 \n    0.268 \n    0.613 \n  \n  \n    1 \n    0.148 \n    0.240 \n    0.387 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Items By Condition (Online)\"\nitem.contingency <- df_items %>% filter(mode == \"asynch\") %>% select(score_ABS, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Items By Condition (Online)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    0 \n    0.342 \n    0.307 \n    0.649 \n  \n  \n    1 \n    0.128 \n    0.223 \n    0.351 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\nAcross both data collection modalities, the proportion of correct answers is greater in the impasse vs. control condition.\n\n\nCODE\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM\nstats = df_items %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(score_niceABS))\ngf_props(~score_niceABS, data = df_items) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) +\n  labs(x = \"Item Absolute Score\",\n       title = \"Item Absolute Score\",\n       subtitle=\"Across modalities, the impasse condition yielded more correct responses\")+\n  theme_minimal()\n\n\n\n\n\n\n\n3.2.2.2 Item Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1.\n\n\nCODE\ntitle = \"Descriptive Statistics of Item Response Accuracy (Scaled Score)\"\nscaled.stats.items <- rbind(\n  \"lab\"= df_items %>% filter(mode == 'lab-synch') %>% select(score_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_items %>% filter(mode == \"asynch\") %>% select(score_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats.items %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Item Response Accuracy (Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    -0.084 \n    0.870 \n    1890 \n    0 \n  \n  \n    online \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    -0.077 \n    0.832 \n    3060 \n    0 \n  \n\n\n\n\n\n\n\nCODE\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM\nstats = df_items %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(score_SCALED))\ngf_density(~score_SCALED, data = df_items) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Scaled Score for Item\",\n       y = \"Proportion of Items\",\n       title = \"Distribution of Accuracy per Item (Scale Score)\",\n       subtitle=\"The impasse condition shifts density toward the positive score\")+\n  theme_minimal()\n\n\n\n\n\n\n\n3.2.2.3 Item Interpretation Scores\n\n\nCODE\n#VISUALIZE distribution of interpretations across all ITEMS\n\n\n#REORDER INTERPRETATION LEVELS\ndf_items$interpretation <- factor(df_items$interpretation, levels = c(\"Triangular\", \"Tversky\", \"both tri + orth\", \"blank\", \"?\", \"frenzy\",\"reference\",\"Satisfice\", \"Orthogonal\"))            \n\n\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Lab)\"\nitem.contingency <- df_items %>% filter(mode == \"lab-synch\") %>% select(interpretation, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Lab)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    Triangular \n    0.094 \n    0.195 \n    0.288 \n  \n  \n    Tversky \n    0.004 \n    0.017 \n    0.021 \n  \n  \n    both tri + orth \n    0.061 \n    0.056 \n    0.116 \n  \n  \n    blank \n    0.008 \n    0.034 \n    0.042 \n  \n  \n    ? \n    0.025 \n    0.054 \n    0.079 \n  \n  \n    frenzy \n    0.002 \n    0.005 \n    0.007 \n  \n  \n    reference \n    0.001 \n    0.004 \n    0.005 \n  \n  \n    Satisfice \n    0.000 \n    0.028 \n    0.028 \n  \n  \n    Orthogonal \n    0.297 \n    0.116 \n    0.414 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Online)\"\nitem.contingency <- df_items %>% filter(mode == \"asynch\") %>% select(interpretation, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Online)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    Triangular \n    0.078 \n    0.175 \n    0.253 \n  \n  \n    Tversky \n    0.011 \n    0.023 \n    0.035 \n  \n  \n    both tri + orth \n    0.056 \n    0.061 \n    0.118 \n  \n  \n    blank \n    0.013 \n    0.055 \n    0.068 \n  \n  \n    ? \n    0.050 \n    0.066 \n    0.116 \n  \n  \n    frenzy \n    0.002 \n    0.001 \n    0.003 \n  \n  \n    reference \n    0.000 \n    0.002 \n    0.002 \n  \n  \n    Satisfice \n    0.000 \n    0.024 \n    0.024 \n  \n  \n    Orthogonal \n    0.260 \n    0.122 \n    0.382 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_propsh(~interpretation, data = df_items, fill = ~mode) %>% \n  gf_facet_grid(mode~condition, labeller = label_both) +\n  labs(x = \"Interpretation for Item\",\n       title = \"Proportion of Interpretations Across Items\",\n       subtitle=\"Impasse Condition yields shift from Orthogonal to alternative interpretations\")+\n  theme_minimal()+ theme(legend.position = \"none\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Proportion of Item Interpretation across Conditions\",\n            data = df_items, condition ~ interpretation, rot_labels=c(0,90,0,0), \n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\")))"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#response-latency",
    "href": "analysis/SGC3A/3_sgc3A_description.html#response-latency",
    "title": "3  Description",
    "section": "3.3 RESPONSE LATENCY",
    "text": "3.3 RESPONSE LATENCY\n\nTODO: Investigate super high and super low response times..\nTODO: Investigate appropriate models for response time data. (see: https://lindeloev.github.io/shiny-rt/).\nEspecially see https://lindeloev.github.io/shiny-rt/ for ideas on modelling reaction time data\n\n\n3.3.1 Time on Study\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_subjects %>% filter(mode == 'lab-synch') %>% select(totaltime_m) %>% unlist() %>% favstats(),\n  \"online\"= df_subjects %>% filter(mode == 'asynch') %>% select(totaltime_m) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of Response Latency (Time on Study)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Latency (Time on Study)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    6.01 \n    10.50 \n    12.2 \n    14.4 \n    23.9 \n    12.8 \n    3.37 \n    126 \n    0 \n  \n  \n    online \n    2.91 \n    9.18 \n    11.5 \n    15.0 \n    111.0 \n    13.4 \n    9.21 \n    204 \n    0 \n  \n\n\n\n\n\nTotal time on study for in person subjects (n = 126) ranged from 6.01 to 23.86 minutes with a mean duration of (M = 12.8, SD = 3.37).\nTotal time on study for online replication subjects (n = 204) ranged from 2.91 to 111.02 minutes with a mean duration of (M = 13.37, SD = 9.21).\n\n\nCODE\n#VISUALIZE distribution of response time\nplab <- gf_dhistogram(~totaltime_m, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Lab\", x = \"Total Time (mins)\", y = \"% subjects\") +  theme_minimal()\n\nponline <- gf_dhistogram(~totaltime_m, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"online\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist = \"gamma\", color=\"red\")+\n  labs(title=\"Online\", x = \"Total Time (mins)\", y = \"% subjects\") +  theme_minimal()\n\nplot <-ggarrange(plab, ponline, common.legend = TRUE, nrow = 1, ncol =2)\n\nannotate_figure(plot, \n                top = text_grob(\"Total Time by Study Mode\",color = \"black\", face = \"bold\", size = 14),\n                bottom = text_grob(\"fit by Gamma distribution\", face = \"italic\", size = 10))\n\n\n\n\n\nTODO consider log transform of response latency data see archive sgc3A_participants.Rmd\n\n\n3.3.2 Time on Question\nTODO time on question"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#due-diligence",
    "href": "analysis/SGC3A/3_sgc3A_description.html#due-diligence",
    "title": "3  Description",
    "section": "3.5 DUE DILIGENCE",
    "text": "3.5 DUE DILIGENCE\n\n3.5.1 Data Collection Mode on Absolute Score\nDoes Mode Change Effect of Condition on Score?\nTo verify that the data collected in the lab and remotely online are comparable, we perform a t-test on group means of ABSOLUTE SCORE for each condition, and examine whether data collection modality is a significant predictor of variance in absolute score\n\n\nCODE\npaste(\"Two Sample T-Test for S_ABS LAB vs. ONLINE control condition\")\n\n\n[1] \"Two Sample T-Test for S_ABS LAB vs. ONLINE control condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 111), s_ABS ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_ABS by mode\nt = -0.5, df = 120, p-value = 0.6\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.84  1.09\nsample estimates:\n   mean in group asynch mean in group lab-synch \n                   2.30                    2.68 \n\n\nCODE\npaste(\"Two Sample T-Test for S_ABS LAB vs. ONLINE impasse condition\")\n\n\n[1] \"Two Sample T-Test for S_ABS LAB vs. ONLINE impasse condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 121), s_ABS ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_ABS by mode\nt = -1, df = 135, p-value = 0.3\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.435  0.727\nsample estimates:\n   mean in group asynch mean in group lab-synch \n                   4.58                    5.44 \n\n\nCODE\npaste(\"OLS Linear Regression Predicting Absolute Score by Data Collection Mode\")\n\n\n[1] \"OLS Linear Regression Predicting Absolute Score by Data Collection Mode\"\n\n\nCODE\nsummary(lm(data = df_subjects, formula = s_ABS ~ mode ))\n\n\n\nCall:\nlm(formula = s_ABS ~ mode, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.08  -3.51  -2.51   4.49   9.49 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      3.510      0.346   10.14   <2e-16 ***\nmodelab-synch    0.570      0.560    1.02     0.31    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.94 on 328 degrees of freedom\nMultiple R-squared:  0.00314,   Adjusted R-squared:  0.000105 \nF-statistic: 1.03 on 1 and 328 DF,  p-value: 0.31\n\n\nBoth t-tests are non-significant with 95% confidence intervals including 0. Further, an OLS linear regression model predicting cumulative absolute score indicates that data collection mode is not a significant predictor, explaining only 0.01% of variance in absolute score, F(1,328) = 1.03, p > 0.05.\n\n\n\n\n\n\nDecision\n\n\n\nIt is reasonable to infer that data from both in-person and remote studies arise from the same data generating process.\n\n\n\n\n3.5.2 Data Collection Mode on Cumulative Score\nAre the by-condition group means significantly different by data collection modality?\nTo verify that the data collected in the lab and remotely online are comparable, we perform a t-test on group means of SCALED SCORE for each condition.\n\n\nCODE\npaste(\"Two Sample T-Test for s_SCALED LAB vs. ONLINE control condition\")\n\n\n[1] \"Two Sample T-Test for s_SCALED LAB vs. ONLINE control condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 111), s_SCALED ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by mode\nt = 0.3, df = 116, p-value = 0.7\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.50  3.52\nsample estimates:\n   mean in group asynch mean in group lab-synch \n                  -5.62                   -6.13 \n\n\nCODE\npaste(\"Two Sample T-Test for s_SCALED LAB vs. ONLINE impasse condition\")\n\n\n[1] \"Two Sample T-Test for s_SCALED LAB vs. ONLINE impasse condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 121), s_SCALED ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by mode\nt = -0.4, df = 130, p-value = 0.7\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.29  2.10\nsample estimates:\n   mean in group asynch mean in group lab-synch \n                   1.16                    1.75 \n\n\nCODE\npaste(\"OLS Linear Regression Predicting Scaled Score by Data Collection Mode\")\n\n\n[1] \"OLS Linear Regression Predicting Scaled Score by Data Collection Mode\"\n\n\nCODE\nsummary(lm(data = df_subjects, formula = s_SCALED ~ mode ))\n\n\n\nCall:\nlm(formula = s_SCALED ~ mode, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10.97  -7.97  -3.92  10.03  15.13 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    -2.0319     0.6641   -3.06   0.0024 **\nmodelab-synch  -0.0951     1.0747   -0.09   0.9295   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.48 on 328 degrees of freedom\nMultiple R-squared:  2.39e-05,  Adjusted R-squared:  -0.00302 \nF-statistic: 0.00783 on 1 and 328 DF,  p-value: 0.93\n\n\nBoth t-tests are non-significant with 95% confidence intervals including 0. Further, an OLS linear regression model predicting cumulative scaled score indicates that data collection mode is not a significant predictor, explaining less than 0.001% of variance in absolute score, F(1,328) = 0.0078, p > 0.05.\n\n\n\n\n\n\nDecision\n\n\n\nIt is reasonable to infer that data from both in-person and remote studies arise from the same data generating process."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#resources",
    "href": "analysis/SGC3A/3_sgc3A_description.html#resources",
    "title": "3  Description",
    "section": "3.6 RESOURCES",
    "text": "3.6 RESOURCES\n\nhttps://rpkgs.datanovia.com/ggpubr/reference/index.html"
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_harmonize.html#harmonization-strategy",
    "href": "analysis/SGC3A/1_sgc3A_harmonize.html#harmonization-strategy",
    "title": "1  Harmonization",
    "section": "1.1 HARMONIZATION STRATEGY",
    "text": "1.1 HARMONIZATION STRATEGY\nData for study SGC_3A were collected across four time periods, interrupted by the Covid-19 pandemic.\n\n\n\nPeriod\nModality\n\n\n\n\nFall 2017\nin person, SONA groups in computer lab\n\n\nSpring 2018\nin person, SONA groups in computer lab\n\n\nFall 2021\nasynchronous, online, SONA\n\n\nWinter 2022\nasynchronous, online, SONA\n\n\n\nData collected in Fall 2017, Spring 2018 constitute the original SGC_3A study, conducted in person. Data collected in Fall 2021, Winter 2022 constitute the web-based replication, conducted online (asynchronously). In all cases, the experiment was administered via a web application.\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\n1.1.1 Participants\nFirst we import participant-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n#set datafiles\nfall17 <- \"data/0-session-level/fall17_sgc3a_participants.csv\"\nspring18 <- \"data/0-session-level/spring18_sgc3a_participants.csv\"\nfall21 <- \"data/0-session-level/fall21_sgc3a_participants.csv\"\nwinter22 <- \"data/0-session-level/winter22_sgc3a_participants.rds\"\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read.csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read.csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_fall21 <- read.csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_subjects_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% select(\n  subject,condition,term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m,absolute_score\n)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, absolute_score)\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_winter22_q16 <- df_subjects_winter22 %>% \n  select(subject, condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects_winter22 <- df_subjects_winter22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, absolute_score)\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_winter22, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#CLEANUP\nrm(df_subjects_fall17,df_subjects_fall21, df_subjects_spring18, df_subjects_winter22,df_subjects_before)\nrm(fall17,fall21,spring18,winter22)\n\n\n\n\n1.1.2 Items\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n#set datafiles\nfall17 <- \"data/0-session-level/fall17_sgc3a_blocks.csv\"\nspring18 <- \"data/0-session-level/spring18_sgc3a_blocks.csv\"\nfall21 <- \"data/0-session-level/fall21_sgc3a_blocks.csv\"\nwinter22 <- \"data/0-session-level/winter22_sgc3a_items.rds\"\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read.csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read.csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_fall21 <- read.csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_items_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- df_items_winter22 %>% group_by(q) %>% select(q,relation) %>% unique()\n\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- df_items_winter22 %>% filter(condition=='X') %>% select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n  \n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18, df_items_fall21) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n  \n#reduce data collected using new webapp\ndf_items_winter22 <- df_items_winter22 %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_winter22,df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  )\n\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% select(-question,-correct,-rt_s,-num_o)\n#add data from wi22 [stored on subject data]\ndf_freeresponse <- rbind(df_freeresponse, df_winter22_q16)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n    question = \"Please describe how to determine what event(s) start at 12pm?\",\n    response = as.character(response) #doesn't need to be factor\n  ) \n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#CLEANUP\nrm(df_items_fall17,df_items_fall21, df_items_spring18, df_items_winter22, df_items_before, df_winter22_q16)\nrm(fall17,fall21,spring18,winter22, map_relations)\n\n\n\n\n1.1.3 Validation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE"
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_harmonize.html#participants",
    "href": "analysis/SGC3A/1_sgc3A_harmonize.html#participants",
    "title": "1  Harmonization",
    "section": "1.2 Participants",
    "text": "1.2 Participants\nFirst we import participant-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n#set datafiles\nfall17 <- \"data/0-session-level/fall17_sgc3a_participants.csv\"\nspring18 <- \"data/0-session-level/spring18_sgc3a_participants.csv\"\nfall21 <- \"data/0-session-level/fall21_sgc3a_participants.csv\"\nwinter22 <- \"data/0-session-level/winter22_sgc3a_participants.rds\"\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read.csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read.csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_fall21 <- read.csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_subjects_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% select(\n  subject,condition,term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m,absolute_score\n)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, absolute_score)\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_winter22_q16 <- df_subjects_winter22 %>% \n  select(subject, condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects_winter22 <- df_subjects_winter22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, absolute_score)\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_winter22, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#CLEANUP\nrm(df_subjects_fall17,df_subjects_fall21, df_subjects_spring18, df_subjects_winter22,df_subjects_before)\nrm(fall17,fall21,spring18,winter22)\n\n\n\n1.2.1 Items\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n#set datafiles\nfall17 <- \"data/0-session-level/fall17_sgc3a_blocks.csv\"\nspring18 <- \"data/0-session-level/spring18_sgc3a_blocks.csv\"\nfall21 <- \"data/0-session-level/fall21_sgc3a_blocks.csv\"\nwinter22 <- \"data/0-session-level/winter22_sgc3a_items.rds\"\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read.csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read.csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_fall21 <- read.csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_items_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- df_items_winter22 %>% group_by(q) %>% select(q,relation) %>% unique()\n\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- df_items_winter22 %>% filter(condition=='X') %>% select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n  \n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18, df_items_fall21) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n  \n#reduce data collected using new webapp\ndf_items_winter22 <- df_items_winter22 %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_winter22,df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  )\n\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% select(-question,-correct,-rt_s,-num_o)\n#add data from wi22 [stored on subject data]\ndf_freeresponse <- rbind(df_freeresponse, df_winter22_q16)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n    question = \"Please describe how to determine what event(s) start at 12pm?\",\n    response = as.character(response) #doesn't need to be factor\n  ) \n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#CLEANUP\nrm(df_items_fall17,df_items_fall21, df_items_spring18, df_items_winter22, df_items_before, df_winter22_q16)\nrm(fall17,fall21,spring18,winter22, map_relations)\n\n\n\n1.2.1.1 Validation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#response-consistency",
    "href": "analysis/SGC3A/3_sgc3A_description.html#response-consistency",
    "title": "3  Description",
    "section": "3.4 RESPONSE CONSISTENCY",
    "text": "3.4 RESPONSE CONSISTENCY\nTODO"
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html",
    "title": "5  Exploratory Analyses",
    "section": "",
    "text": "TODO\n- clarify core questions being asked\n- review models already created in ARCHIVE?\n- explore response consistency - fix references\nThe purpose of this notebook is exploratory analyses of data collected for study SGC3A.\nExploratory Questions\nConsistency | How consistent are learners in their interpretation of the graph? Do they adopt an interpretation on the first question and hold constant? Or do they change interpretations from question to question? Are there any interpretations that serve as ‘absorbing states’ (i.e. once encountered, the learner does not exist this state).\nTime Course of Exploration | What is the relationship between response accuracy (and interpretation) and time spent on each item?\nCan exploration strategies be derived from mouse cursor activity?"
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#xyz",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#xyz",
    "title": "5  Exploratory Analyses",
    "section": "5.1 XYZ",
    "text": "5.1 XYZ\n\n5.1.1 XYZ\n\n5.1.1.1 XYZ\n\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\n\nAnalysis Strategy\nChi-Square test of independence on outcome score_niceABS by condition for df_items where q == 1\n\n\nJustification\n(0) simplest method to examine independence of two categorical factors; logistic regression is recommended for binomial ~ continuous\n(1) independence assumption : as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\n(2) frequency size assumption : expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\nSteps\n(1) Express raw data as contingency table & visualize\n(2) Calculate Chi-Squared Statistic and p-value\n(3) Interpret Odds-Ratio as effect size\n\n\nInference\nLab A Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition approaching statistical significance, \\(\\chi^2\\) (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. In this particular data sample, the sample odds ratio 2.18 indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than if the control condition (Odds Ratio = 2.18, p = 0.055, 95% CI [0.982, +Inf]).\nOnline A Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. The sample odds ratio = 2.68 indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition (Odds Ratio = 2.68, p = 0.005, 95% CI [1.37, +Inf]).\n\n\n\n\n\nCODE\n#FITER THE DATASET\ndf = df_items %>% filter(q==1) \n\n#PROPORTIONAL BAR CHART\ngf_props(~score_niceABS, data = df, fill = ~mode) %>% \n  gf_facet_grid(mode~condition, labeller = label_both) +\n  labs(x = \"Correct Response on Q 1\",\n       title = \"Accuracy on First Question by Condition (Both Modalities)\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses than control \")+\n  theme_minimal()+ theme(legend.position = \"none\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Accuracy on First Question by Condition (Both Modalities)\",\n            data = df, score_niceABS ~ condition, rot_labels=c(0,90,0,0),\n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\")))\n\n\n\n\n\nCODE\n#PRINT CONTINGENCY TABLE\ntitle = \"Proportion of Correct Responses On First Item (Both Modalities)\"\nitem.contingency <-  df %>% dplyr::select(condition, score_niceABS) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Responses On First Item (Both Modalities)\n \n  \n      \n    0 \n    1 \n    Sum \n  \n \n\n  \n    111 \n    0.412 \n    0.067 \n    0.479 \n  \n  \n    121 \n    0.373 \n    0.148 \n    0.521 \n  \n  \n    Sum \n    0.785 \n    0.215 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\ndf = df_items %>% filter(q==1) %>% filter(mode == \"lab-synch\")\nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  126 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |        52 |        10 |        62 | \n             |    47.730 |    14.270 |           | \n             |     0.382 |     1.278 |           | \n             |     0.839 |     0.161 |     0.492 | \n             |     0.536 |     0.345 |           | \n             |     0.413 |     0.079 |           | \n-------------|-----------|-----------|-----------|\n         121 |        45 |        19 |        64 | \n             |    49.270 |    14.730 |           | \n             |     0.370 |     1.238 |           | \n             |     0.703 |     0.297 |     0.508 | \n             |     0.464 |     0.655 |           | \n             |     0.357 |     0.151 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |        97 |        29 |       126 | \n             |     0.770 |     0.230 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  3.27     d.f. =  1     p =  0.0707 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  2.55     d.f. =  1     p =  0.111 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.18 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.0909 \n95% confidence interval:  0.86 5.84 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  0.979 \n95% confidence interval:  0 5.03 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.0547 \n95% confidence interval:  0.982 Inf \n\n\n \n\n\nInspecting the output of the Chi-Squared test, we first see that we meet the assumption of expected frequency in each cell (indicated by the second row in each box, ‘Expected N’. The model predicts more than 5 observations in each cell.) The Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition approaching statistical significance, \\(\\chi^2\\) (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is not equal to 1. In this particular data sample, the odds ratio 2.18 indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition (Odds Ratio = 2.18, p = 0.055, 95% CI [0.982, +Inf]).\n\n\nCODE\ndf = df_items %>% filter(q==1) %>% filter(mode == \"asynch\")\nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  204 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |        84 |        12 |        96 | \n             |    76.235 |    19.765 |           | \n             |     0.791 |     3.050 |           | \n             |     0.875 |     0.125 |     0.471 | \n             |     0.519 |     0.286 |           | \n             |     0.412 |     0.059 |           | \n-------------|-----------|-----------|-----------|\n         121 |        78 |        30 |       108 | \n             |    85.765 |    22.235 |           | \n             |     0.703 |     2.711 |           | \n             |     0.722 |     0.278 |     0.529 | \n             |     0.481 |     0.714 |           | \n             |     0.382 |     0.147 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |       162 |        42 |       204 | \n             |     0.794 |     0.206 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  7.26     d.f. =  1     p =  0.00707 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  6.35     d.f. =  1     p =  0.0117 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.68 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.00894 \n95% confidence interval:  1.23 6.17 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  0.998 \n95% confidence interval:  0 5.42 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.00539 \n95% confidence interval:  1.37 Inf \n\n\n \n\n\nInspecting the output of the Chi-Squared test, we first see that we meet the assumption of expected frequency in each cell (indicated by the second row in each box, ‘Expected N’. The model predicts more than 5 observations in each cell.) The Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is not equal to 1. The odds ratio = 2.68 indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition (Odds Ratio = 2.68, p = 0.005, 95% CI [1.37, +Inf])."
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#copied-from-3",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#copied-from-3",
    "title": "5  Exploratory Analyses",
    "section": "5.2 COPIED FROM 3",
    "text": "5.2 COPIED FROM 3\nDoes the IMPASSE condition more accurate interpretation?\nTo address this question we assess how much variance in cumulative (absolute) score is explained by experimental condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_SCALED ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"lab-synch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12.75  -6.87  -4.87   9.03  19.13 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -6.13       1.18   -5.20  8.0e-07 ***\ncondition121     7.88       1.65    4.76  5.2e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.28 on 124 degrees of freedom\nMultiple R-squared:  0.155, Adjusted R-squared:  0.148 \nF-statistic: 22.7 on 1 and 124 DF,  p-value: 5.21e-06\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   1955    1955    22.7 5.2e-06 ***\nResiduals 124  10681      86                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  -8.46   -3.8\ncondition121  4.61   11.2\n\n\nCODE\n# report(m1) #sanity check\n\n\nFor in-lab data collection an OLS linear regression predicting scaled score by experimental condition explains a statistically significant and moderate 15% variance in score (F(1,124) = 22.7, p < 0.001). The estimated beta coefficient (\\(/beta\\) = 7.88, 95% CI [4.61, 11.2]) predicts that participants in the impasse condition will on average score around 8 points (31%) higher than those in the control condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_SCALED ~ condition, data = df_subjects %>% filter(mode==\"asynch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13.16  -7.16  -3.02   7.12  18.62 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.620      0.866   -6.49  6.5e-10 ***\ncondition121    6.777      1.190    5.70  4.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.48 on 202 degrees of freedom\nMultiple R-squared:  0.138, Adjusted R-squared:  0.134 \nF-statistic: 32.4 on 1 and 202 DF,  p-value: 4.3e-08\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   2334    2334    32.4 4.3e-08 ***\nResiduals 202  14536      72                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  -7.33  -3.91\ncondition121  4.43   9.12\n\n\nCODE\n#report(m1) #sanity check\n\n\nFor the online replication, an OLS linear regression model predicting scaled score by condition explains a statistically significant and moderate 14% of variance in absolute score (F(1,202) = 32.4, p < 0.001). The beta coefficient for condition indicates that on average, participants in the IMPASSE group scored 6.8 points higher on the task than those in the control condition (CI[4.43, 9.12]).\n\n\n\n\n\n\nNote\n\n\n\nFrom these models we can reasonably conclude that the impasse condition yields a reliable, moderate sized effect of improved interpretation on the graph reading tasks."
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#item-level-performance",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#item-level-performance",
    "title": "5  Exploratory Analyses",
    "section": "5.3 Item-Level Performance",
    "text": "5.3 Item-Level Performance\nIndividual differences with a mixed model."
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#model-peeking",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#model-peeking",
    "title": "5  Exploratory Analyses",
    "section": "5.4 Model Peeking",
    "text": "5.4 Model Peeking\nTODO\n\nmultiple regression with condition and response time\n\n\n\nCODE\nlibrary(supernova)\n\n\n\nAttaching package: 'supernova'\n\n\nThe following object is masked from 'package:scales':\n\n    number\n\n\nCODE\nlibrary(report)\nlibrary(lmerTest)\n\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nRegistered S3 methods overwritten by 'lme4':\n  method                          from\n  cooks.distance.influence.merMod car \n  influence.merMod                car \n  dfbeta.influence.merMod         car \n  dfbetas.influence.merMod        car \n\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nCODE\nlibrary(lme4)\nm1 <- lm( s_SCALED ~ condition, data = df_subjects %>% filter(mode=='asynch'))\nm1\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nCoefficients:\n (Intercept)  condition121  \n       -5.62          6.78  \n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13.16  -7.16  -3.02   7.12  18.62 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.620      0.866   -6.49  6.5e-10 ***\ncondition121    6.777      1.190    5.70  4.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.48 on 202 degrees of freedom\nMultiple R-squared:  0.138, Adjusted R-squared:  0.134 \nF-statistic: 32.4 on 1 and 202 DF,  p-value: 4.3e-08\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   2334    2334    32.4 4.3e-08 ***\nResiduals 202  14536      72                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nsuperanova(m1)\n\n\n Analysis of Variance Table (Type III SS)\n Model: s_SCALED ~ condition\n\n                                SS  df       MS      F    PRE     p\n ----- --------------- | --------- --- -------- ------ ------ -----\n Model (error reduced) |  2334.346   1 2334.346 32.439 0.1384 .0000\n Error (from model)    | 14536.196 202   71.961                    \n ----- --------------- | --------- --- -------- ------ ------ -----\n Total (empty model)   | 16870.543 203   83.106                    \n\n\nCODE\nplot(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCODE\ngf_histogram(~s_SCALED, data = df_subjects)\n\n\n\n\n\nCODE\ngf_histogram(~m1$residuals)\n\n\n\n\n\nCODE\n#Assess assumption of independence of errors\n#DW statistic should be close to 2\nlibrary(car)\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'carData'\n\n\nThe following object is masked from 'package:vcdExtra':\n\n    Burt\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCODE\ndurbinWatsonTest(m1)\n\n\n lag Autocorrelation D-W Statistic p-value\n   1         -0.0134          2.01   0.942\n Alternative hypothesis: rho != 0\n\n\nCODE\n#Test for equality of variance\n#H0 is equality; p > 0.05 infer you can't reject null\nleveneTest(m1)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)  \ngroup   1     5.4  0.021 *\n      202                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA simple linear regression model predicting cumulative scaled score (at subject level) by condition explains 13% of the total variance, F(1,329) = 47.8, p < 0.001. The model predicts that participants in the impasse condition will score on average 6.38 points higher than those in the control condition, 95% CI [4.56, 8.19].\n\n\nCODE\nt.test(s_SCALED ~ condition, data = df_subjects)\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by condition\nt = -7, df = 322, p-value = 1e-12\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9.10 -5.29\nsample estimates:\nmean in group 111 mean in group 121 \n            -5.82              1.38 \n\n\nCODE\n#%>% report()\n\n\n\n\nCODE\n# report_participants(df_subjects)\nm1 %>% report()\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with condition (formula: s_SCALED ~ condition). The model explains a statistically significant and moderate proportion of variance (R2 = 0.14, F(1, 202) = 32.44, p < .001, adj. R2 = 0.13). The model's intercept, corresponding to condition = 111, is at -5.62 (95% CI [-7.33, -3.91], t(202) = -6.49, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 6.78, 95% CI [4.43, 9.12], t(202) = 5.70, p < .001; Std. beta = 0.74, 95% CI [0.49, 1.00])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\nanova(m1) %>% report()\n\n\nFor one-way between subjects designs, partial eta squared is equivalent to eta squared.\nReturning eta squared.\n\n\nThe ANOVA suggests that:\n\n  - The main effect of condition is statistically significant and medium (F(1, 202) = 32.44, p < .001; Eta2 = 0.14, 95% CI [0.07, 1.00])\n\n\nCODE\n#significant intercept means that group is significantly different than zero\n\n\n\n\nCODE\n#logistic regression on on scaled df_subjects because residuals not normal in lm?\nmlog <- glm(s_SCALED ~ condition , data = df_subjects, family = gaussian)\nsummary(mlog)\n\n\n\nCall:\nglm(formula = s_SCALED ~ condition, family = gaussian, data = df_subjects)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-13.38   -7.18   -3.28    8.50   18.82  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.820      0.698   -8.34  2.1e-15 ***\ncondition121    7.198      0.967    7.45  8.6e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 77)\n\n    Null deviance: 29508  on 329  degrees of freedom\nResidual deviance: 25242  on 328  degrees of freedom\nAIC: 2374\n\nNumber of Fisher Scoring iterations: 2\n\n\nCODE\nreport(mlog)\n\n\nWe fitted a linear model (estimated using ML) to predict s_SCALED with condition (formula: s_SCALED ~ condition). The model's explanatory power is moderate (R2 = 0.14). The model's intercept, corresponding to condition = 111, is at -5.82 (95% CI [-7.19, -4.45], t(328) = -8.34, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 7.20, 95% CI [5.30, 9.09], t(328) = 7.45, p < .001; Std. beta = 0.76, 95% CI [0.56, 0.96])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\n#logistic regression on niceABS by condition\n#pretends that questions are independent and not from same subjects INVALID\nmlog <- glm(score_niceABS ~ condition , data = df_items %>% filter(q<6), family = binomial())\nsummary(mlog)\n\n\n\nCall:\nglm(formula = score_niceABS ~ condition, family = binomial(), \n    data = df_items %>% filter(q < 6))\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.980  -0.980  -0.649   1.389   1.823  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.4508     0.0907  -15.99   <2e-16 ***\ncondition121   0.9672     0.1147    8.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1986.2  on 1649  degrees of freedom\nResidual deviance: 1911.3  on 1648  degrees of freedom\nAIC: 1915\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\nreport(mlog)\n\n\nWe fitted a logistic model (estimated using ML) to predict score_niceABS with condition (formula: score_niceABS ~ condition). The model's explanatory power is weak (Tjur's R2 = 0.04). The model's intercept, corresponding to condition = 111, is at -1.45 (95% CI [-1.63, -1.28], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.97, 95% CI [0.74, 1.19], p < .001; Std. beta = 0.97, 95% CI [0.74, 1.19])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm2 <- lm( s_NABS ~ condition, data = df_subjects)\nm2\n\n\n\nCall:\nlm(formula = s_NABS ~ condition, data = df_subjects)\n\nCoefficients:\n (Intercept)  condition121  \n        2.47          2.46  \n\n\nCODE\nsummary(m2)\n\n\n\nCall:\nlm(formula = s_NABS ~ condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.92  -3.67  -2.47   4.08  10.53 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.468      0.384    6.43  4.4e-10 ***\ncondition121    2.456      0.531    4.62  5.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.82 on 328 degrees of freedom\nMultiple R-squared:  0.0611,    Adjusted R-squared:  0.0583 \nF-statistic: 21.4 on 1 and 328 DF,  p-value: 5.49e-06\n\n\nCODE\nanova(m2)\n\n\nAnalysis of Variance Table\n\nResponse: s_NABS\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1    497     497    21.4 5.5e-06 ***\nResiduals 328   7629      23                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nsupernova(m2)\n\n\n Analysis of Variance Table (Type III SS)\n Model: s_NABS ~ condition\n\n                               SS  df      MS      F    PRE     p\n ----- --------------- | -------- --- ------- ------ ------ -----\n Model (error reduced) |  496.765   1 496.765 21.357 0.0611 .0000\n Error (from model)    | 7629.359 328  23.260                    \n ----- --------------- | -------- --- ------- ------ ------ -----\n Total (empty model)   | 8126.124 329  24.699                    \n\n\nA simple linear regression model predicting cumulative absolute score by condition explains 5% of variance, F(1,328) = 16.36, p < 0.001. The model predicts that subjects in the impasse condition will score on average 2 points higher than those in the control condition (Beta = 2.02, 95% CI [1.04, 3.00])\n\n\nCODE\nreport(m2)\n\n\nWe fitted a linear model (estimated using OLS) to predict s_NABS with condition (formula: s_NABS ~ condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.06, F(1, 328) = 21.36, p < .001, adj. R2 = 0.06). The model's intercept, corresponding to condition = 111, is at 2.47 (95% CI [1.71, 3.22], t(328) = 6.43, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 2.46, 95% CI [1.41, 3.50], t(328) = 4.62, p < .001; Std. beta = 0.49, 95% CI [0.28, 0.70])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\n\n\nCODE\nm.m1 <- lmer( score_SCALED ~ (1 + condition|subject), data = df_items)\n\n\nWarning: Model failed to converge with 1 negative eigenvalue: -8.1e-04\n\n\nCODE\nm.m1\n\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: score_SCALED ~ (1 + condition | subject)\n   Data: df_items\nREML criterion at convergence: 9773\nRandom effects:\n Groups   Name         Std.Dev. Corr \n subject  (Intercept)  0.632         \n          condition121 0.827    -0.71\n Residual              0.591         \nNumber of obs: 4950, groups:  subject, 330\nFixed Effects:\n(Intercept)  \n    -0.0617  \n\n\nCODE\nsummary(m.m1)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | subject)\n   Data: df_items\n\nREML criterion at convergence: 9773\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9664 -0.6738 -0.0461  0.5889  2.7646 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n subject  (Intercept)  0.400    0.632         \n          condition121 0.685    0.827    -0.71\n Residual              0.349    0.591         \nNumber of obs: 4950, groups:  subject, 330\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(>|t|)  \n(Intercept)  -0.0617     0.0344 326.4131   -1.79    0.074 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m.m1)\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition and subject as random effects (formula: ~1 + condition | subject). . The model's intercept is at -0.06 (95% CI [-0.13, 5.71e-03], t(4945) = -1.79, p = 0.073). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm.m2 <- lmer( score_SCALED ~ (1+ condition|q), data = df_items)\nm.m2\n\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: score_SCALED ~ (1 + condition | q)\n   Data: df_items\nREML criterion at convergence: 11500\nRandom effects:\n Groups   Name         Std.Dev. Corr \n q        (Intercept)  0.602         \n          condition121 0.530    -0.93\n Residual              0.765         \nNumber of obs: 4950, groups:  q, 15\nFixed Effects:\n(Intercept)  \n      0.172  \n\n\nCODE\nsummary(m.m2)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | q)\n   Data: df_items\n\nREML criterion at convergence: 11500\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-1.972 -0.794 -0.093  0.887  2.164 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n q        (Intercept)  0.363    0.602         \n          condition121 0.281    0.530    -0.93\n Residual              0.585    0.765         \nNumber of obs: 4950, groups:  q, 15\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)   \n(Intercept)   0.1717     0.0575 14.0014    2.98   0.0099 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m.m2)\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition and q as random effects (formula: ~1 + condition | q). . The model's intercept is at 0.17 (95% CI [0.06, 0.28], t(4945) = 2.98, p = 0.003). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm.m3 <- lmer( score_SCALED ~ (1+ condition|q) + (1+condition|subject), data = df_items)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00208782 (tol = 0.002, component 1)\n\n\nCODE\nm.m3 %>% summary() \n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | q) + (1 + condition | subject)\n   Data: df_items\n\nREML criterion at convergence: 8593\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.724 -0.644 -0.016  0.697  3.599 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n subject  (Intercept)  0.339    0.583         \n          condition121 0.218    0.467    -0.46\n q        (Intercept)  0.268    0.517         \n          condition121 0.197    0.443    -0.91\n Residual              0.266    0.516         \nNumber of obs: 4950, groups:  subject, 330; q, 15\n\nFixed effects:\n            Estimate Std. Error     df t value Pr(>|t|)\n(Intercept)    0.117      0.071 24.208    1.65     0.11\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00208782 (tol = 0.002, component 1)\n\n\nCODE\nm.m3 %>% report()\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition, q and subject as random effects (formula: list(~1 + condition | q, ~1 + condition | subject)). . The model's intercept is at 0.12 (95% CI [-0.02, 0.26], t(4942) = 1.65, p = 0.098). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nanova(m.m1, m.m2, m.m3)\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: df_items\nModels:\nm.m1: score_SCALED ~ (1 + condition | subject)\nm.m2: score_SCALED ~ (1 + condition | q)\nm.m3: score_SCALED ~ (1 + condition | q) + (1 + condition | subject)\n     npar   AIC   BIC logLik deviance Chisq Df Pr(>Chisq)    \nm.m1    5  9778  9811  -4884     9768                        \nm.m2    5 11506 11539  -5748    11496     0  0               \nm.m3    8  8605  8657  -4295     8589  2907  3     <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#resources",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#resources",
    "title": "5  Exploratory Analyses",
    "section": "5.5 RESOURCES",
    "text": "5.5 RESOURCES"
  },
  {
    "objectID": "analysis/SGC3A/SGC3A.html#introduction",
    "href": "analysis/SGC3A/SGC3A.html#introduction",
    "title": "SGC3A",
    "section": "INTRODUCTION",
    "text": "INTRODUCTION\nIn Study 3A we explore a hypothesis that emerged from analysis of Study 2, that presenting a learner with a situation that induces a state of impasse will increase the probability that learners experience a moment of insight, and in turn restructure their interpretation of the coordinate system.\nIn the context of Study 2, an impasse state was (unintentionally) induced when the combination of question + data set yielded no available answer in the incorrect (cartesian) interpretation of the graph. In Study 3A, we test this hypothesis by comparing performance between a (treatment) group receiving impasse-inducing questions followed by normal questions, and a non-impasse control.\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 1: Posing a mental impasse\n\n\nTo try the study yourself:\n\ncontrol condition\nimpasse condition\n\n\nHypotheses\nExperimental Hypothesis: Learners posed with impasse-inducing questions will be more likely to correct interpret the graph.\n\nH1A | Learners in the impasse condition will have better cumulative accuracy (get more questions correctly overall)\nH1B | Learners in the impasse condition will be more likely to correctly answer the first question\nH1C | Learners in the impasse condition will spend more time on the first question.\n\nNull Hypothesis: No significant differences in performance will exist between learners in the impasse and control conditions.\nExploratory Questions\n\nConsistency | How consistent are learners in their interpretation of the graph? Do they adopt an interpretation on the first question and hold constant? Or do they change interpretations from question to question? Are there any interpretations that serve as ‘absorbing states’ (i.e. once encountered, the learner does not exist this state).\nTime Course of Exploration | What is the relationship between response accuracy (and interpretation) and time spent on each item?\nCan exploration strategies be derived from mouse cursor activity?"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#summarize-by-subject",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#summarize-by-subject",
    "title": "2  Response Scoring",
    "section": "2.3 SUMMARIZE BY SUBJECT",
    "text": "2.3 SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n#prep items\ndf_items <- df_items %>% mutate(\n  tv_type = as.factor(tv_type),\n  top_type = as.factor(top_type),\n  condition_pretty = recode_factor(condition, \"111\" = \"Control\", \"121\" =  \"Impasse\")\n)\n\n#summarize SCORES and TIME by subject\nsubjects <- df_items %>% filter(q %nin% c(6,9)) %>% group_by(subject) %>% dplyr::summarise (\n  subject = as.character(subject),\n  s_TRI = sum(score_TRI),\n  s_ORTH = sum(score_ORTH),\n  s_TVERSKY = sum(score_TVERSKY),\n  s_SATISFICE = sum(score_SATISFICE),\n  s_REF = sum(score_REF),\n  s_ABS = sum(score_ABS),\n  s_NABS = sum(score_niceABS),\n  s_SCALED = sum(score_SCALED),\n  rt_m = sum(rt_s)/60,\n  condition_pretty = recode_factor(condition, \"111\" = \"Control\", \"121\" =  \"Impasse\")\n) %>% arrange(subject) %>% slice(1L)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\nCODE\n#import subjects\ndf_subjects <- read_rds('data/1-study-level/sgc3a_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#TODO DOUBLE CHECK THIS!  \nx = merge(subjects, df_subjects)\ndf_subjects <- x %>% select(-absolute_score) #drop absolute score from webapp that includes Q6 and Q9"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#explore-distributions",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#explore-distributions",
    "title": "2  Response Scoring",
    "section": "2.4 EXPLORE DISTRIBUTIONS",
    "text": "2.4 EXPLORE DISTRIBUTIONS\n\n\nCODE\n# treat score nice abs as factor\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(score_niceABS = as.factor(score_niceABS))\n\n\n\n2.4.1 Absolute Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_bar(~score_niceABS, fill = ~condition_pretty, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (All Items)\",\n        subtitle = \"The Impasse Condition (blue) yields more correct responses across the entire task\",\n        y = \"Number of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_bar(~score_niceABS, fill = ~condition_pretty, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(condition_pretty~q) + \n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score by Item\",\n        subtitle = \"The Impasse Condition (blue) yields more correct responses on each item\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_bar(~s_NABS, fill = ~condition_pretty, position = position_dodge(), data = df_subjects) %>% \ngf_facet_grid(condition_pretty ~. )+\n  labs( x = \"Cumulative Absolute Score\", \n        title = \"Distribution of Cumulative Absolute Score (by Subject)\",\n        subtitle = \"The Impasse Condition (blue) yields higher cumulative absolute scores\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\n\n2.4.2 Scaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_bar(~score_SCALED, fill = ~condition_pretty, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (All Items)\",\n        subtitle = \"The Impasse Condition (blue) yields higher scaled scores across the entire task\",\n        y = \"Number of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_bar(~score_SCALED, fill = ~condition_pretty, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(condition_pretty~q) + \n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score by Item\",\n        subtitle = \"The Impasse Condition (blue) yields higher scaled scores on each item\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_histogram(~s_SCALED, fill = ~condition_pretty, data = df_subjects)  %>% \n  gf_facet_grid(condition_pretty ~. )+\n  labs( x = \"Cumulative Scaled Score\", \n        title = \"Distribution of Cumulative Absolute Score (by Subject)\",\n        subtitle = \"The Impasse Condition (blue) yields higher cumulative scaled scores\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nTODO: INVESTIGATE if some of the scores assigned to 0 should be assigned to -0.5 to balance TODO: INVESTIGATE DISTRIBUTIONS of each subscore type\n\n\n2.4.3 Interpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~ sorted_interpretation, fill = ~condition_pretty, data = df) %>% \n  gf_facet_grid( condition_pretty ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"The Impasse condition (blue) yields more Triangular and intermediary responses\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_barh(~ sorted_interpretation, fill = ~condition_pretty, data = df) %>% \n  gf_facet_grid( condition_pretty~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"The impasse condition (blue) yields more Triangular responses on each question\",\n        y = \"Interpretation\", x = \"Number of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n2.4.4 Interpretation Subscores\n…TODO INSPECT AND DO REMAINING\n\n\nCODE\ngf_histogram(~ s_NABS, fill = ~condition_pretty, data = df_subjects) %>% \n  gf_facet_wrap( ~ condition_pretty, labeller = label_both) + \n  labs( x = \"Item ABSOLUTE Score\", title = \"Distribution of Absolute Score\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SCALED, fill = ~condition_pretty, data = df_subjects) %>% \n  gf_facet_wrap( ~ condition_pretty, labeller = label_both) + \n  labs( x = \"Item SCALED Score\", title = \"Distribution of Scaled Score\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_TRI, fill = ~condition_pretty, data = df_subjects) %>% \n  gf_facet_wrap( ~ condition_pretty, labeller = label_both) + \n  labs( x = \"Item TRIANGULAR Score\", title = \"Distribution of Triangular Score\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_ORTH, fill = ~condition_pretty, data = df_subjects) %>% \n  gf_facet_wrap( ~ condition_pretty, labeller = label_both) + \n  labs( x = \"Item ORTHOGONAL Score\", title = \"Distribution of Triangular Score\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\nWarning: Removed 172 rows containing non-finite values (stat_bin).\n\n\n\n\n\n\n\n2.4.5 Score Relationships\nTODO investigate relationships between different subscores\n\n\nCODE\ngf_jitter( score_TRI ~ score_ORTH, fill = ~condition_pretty, alpha = 0.5, data = df_items) %>% \n  gf_facet_wrap( ~ condition_pretty, labeller = label_both) + \n  labs( x = \"ORTH score\", y = \"TRI score\", \n        title = \"Relationship between ORTH and TRI scores\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\nWarning: Removed 860 rows containing missing values (geom_point)."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1a-cumulative-performance",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1a-cumulative-performance",
    "title": "4  Hypothesis Testing",
    "section": "4.1 H1A | CUMULATIVE PERFORMANCE",
    "text": "4.1 H1A | CUMULATIVE PERFORMANCE\nOver the course of the entire graph comprehension task does the impasse condition affect performance on the graph comprehension task?\n\n4.1.1 Cumulative Absolute Score\nCumulative absolute score gives us a measure of the exact triangular-correctness of each response given by a participant across all discriminant items (n=13) in the graph comprehension task.\n\n4.1.1.1 Linear Regression\n\n\n\n\n\n\n\nResearch Question\nDoes posing a mental impasse improve performance?\n\n\n\n\nHypothesis\n(H1A) Participants in the IMPASSE condition will have significantly higher cumulative performance than those in the CONTROL condition.\n\n\nAnalysis Strategy\nOLS Linear Regression s_NABS ~ condition\n\n\nJustification\n(0) goal is to understand how much variance in absolute score is accounted for by condition\n(1) linearity assumption TODO\n(2) homoscedasticity assumption: TODO\n(3) independence assumption : observations correspond to individual participants and are thus independent\n(2) normality assumption : TODO\n\n\nSteps\n(1) Calculate regression model using lmer\n(2) Interpret coefficients\n(3) Interpret ANOVA and \\(R^2\\) value\n(4) Model diagnostics\n\n\nInference\nFor in-lab data collection an OLS linear regression predicting cumulative absolute score by experimental condition explains a statistically significant though small 7.5% variance in score (F(1,124) = 10.1, p < 0.01). The estimated beta coefficient (\\(\\beta\\) = 2.76, 95% CI [1.04, 4.48]) predicts that participants in the impasse condition will on average score 2.76 points (21%) higher than those in the control condition.\nFor the online replication, an OLS linear regression model predicting cumulative absolute score by condition explains a statistically significant though small 5.5% of variance in absolute score (F(1,202) = 11.73, p < 0.001). The beta coefficient for condition indicates that on average, participants in the IMPASSE group scored 2.3 points higher on the task than those in the control condition (\\(\\beta\\) = 2.28, CI[0.97, 3.59]).\n\n\n\n\n\nCODE\n#VISUALIZE distribution of response accuracy across SUBJECTS\n\n#HISTOGRAM\nstats = df_subjects %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(s_ABS))\ngf_density(~s_ABS, data = df_subjects) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_lims(x = c(0, 13)) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Cumulative Absolute Score\",\n       y = \"proportion of subjects\",\n       title = \"Subject Cumulative Score (Absolute)\",\n       subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n  theme_minimal()\n\n\n\n\n\n\n4.1.1.1.1 In-Lab Data Collection\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_ABS ~ condition, data = df_subjects %>% filter(mode == \n    \"lab-synch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -5.44  -2.68  -2.68   4.31  10.32 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.677      0.620    4.32 0.000031 ***\ncondition121    2.760      0.869    3.17   0.0019 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.88 on 124 degrees of freedom\nMultiple R-squared:  0.0752,    Adjusted R-squared:  0.0677 \nF-statistic: 10.1 on 1 and 124 DF,  p-value: 0.00189\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_ABS\n           Df Sum Sq Mean Sq F value Pr(>F)   \ncondition   1    240   239.9    10.1 0.0019 **\nResiduals 124   2951    23.8                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)   1.45   3.90\ncondition121  1.04   4.48\n\n\nCODE\n# report(m1) #sanity check\n\n\nFor in-lab data collection an OLS linear regression predicting cumulative absolute score by experimental condition explains a statistically significant though small 7.5% variance in score (F(1,124) = 10.1, p < 0.01). The estimated beta coefficient (\\(\\beta\\) = 2.76, 95% CI [1.04, 4.48]) predicts that participants in the impasse condition will on average score 2.76 points (21%) higher than those in the control condition.\n\n\nCODE\n#HISTOGRAM\nstats = df_subjects %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(s_ABS))\ngf_density(~s_ABS, data = df_subjects) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_lims(x = c(0, 13)) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Cumulative Absolute Score\",\n       y = \"proportion of subjects\",\n       title = \"Subject Cumulative Score (Absolute)\",\n       subtitle = \"Score distributions are comparable across modalities and different across conditions\") + \n  theme_minimal()\n\n\n\n\n\n\n\n4.1.1.1.2 Online Replication\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"asynch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_ABS ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.58  -3.58  -2.30   3.42  10.70 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.302      0.485    4.75  3.8e-06 ***\ncondition121    2.281      0.666    3.43  0.00074 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.75 on 202 degrees of freedom\nMultiple R-squared:  0.0549,    Adjusted R-squared:  0.0502 \nF-statistic: 11.7 on 1 and 202 DF,  p-value: 0.000745\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_ABS\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1    264   264.5    11.7 0.00074 ***\nResiduals 202   4554    22.5                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  1.347   3.26\ncondition121 0.968   3.59\n\n\nCODE\n#report(m1) #sanity check\n\n\nFor the online replication, an OLS linear regression model predicting cumulative absolute score by condition explains a statistically significant though small 5.5% of variance in absolute score (F(1,202) = 11.73, p < 0.001). The beta coefficient for condition indicates that on average, participants in the IMPASSE group scored 2.3 points higher on the task than those in the control condition (CI[0.97, 3.59]).\n\n\n\n\n\n\nNote\n\n\n\nFrom these models we can reasonably conclude that the impasse condition yielded a small but reliable improvement in performance across items in the graph reading task.\n\n\n\n\n4.1.1.1.3 Model Diagnostics\n\n\nCODE\ncheck_model(m1)\n\n\n\n\n\nCODE\nplot(check_normality(m1), type = \"qq\")\n\n\nWarning: Non-normality of residuals detected (p < .001).\n\n\n\n\n\nCODE\nplot(check_heteroscedasticity(m1))\n\n\nOK: Error variance appears to be homoscedastic (p = 0.054).\n\n\n\n\n\nCODE\nplot(result <- check_homogeneity(m1))\n\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.053).\n\n\n\n\n\nTODO\n\n\n\n4.1.1.2 Poisson Regression\nThe outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can consider it a type of count, (ie. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function).\n\n\nCODE\n# library('fitdistrplus')\n# plot(fitdist(df_subjects$s_ABS,\"pois\"))\n# plot(fitdist(df_subjects$s_ABS,\"norm\"))\n# plot(fitdist(df_subjects$s_ABS,\"beta\"))\n# \n# \n# plotdist(df_subjects$s_ABS, histo = TRUE, demp = TRUE)\n# descdist(df_subjects$s_ABS, discrete=FALSE, boot=500)\n\n\n\n\nCODE\n#SCORE predicted by CONDITION --> POISSON DISTRIBUTION\nmp1 <- glm(s_ABS ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"), family = \"poisson\")\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(mp1)\n\n\n\nCall:\nglm(formula = s_ABS ~ condition, family = \"poisson\", data = df_subjects %>% \n    filter(mode == \"lab-synch\"))\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -3.30   -2.31   -2.31    1.75    4.52  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    0.9849     0.0776   12.69  < 2e-16 ***\ncondition121   0.7085     0.0943    7.51  5.9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 855.68  on 125  degrees of freedom\nResidual deviance: 795.46  on 124  degrees of freedom\nAIC: 1045\n\nNumber of Fisher Scoring iterations: 6\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(mp1)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: s_ABS\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev\nNULL                        125        856\ncondition  1     60.2       124        795\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(mp1)\n\n\nWaiting for profiling to be done...\n\n\n             2.5 % 97.5 %\n(Intercept)  0.829  1.133\ncondition121 0.525  0.896\n\n\nCODE\nreport(mp1) #sanity check\n\n\nWe fitted a poisson model (estimated using ML) to predict s_ABS with condition (formula: s_ABS ~ condition). The model's explanatory power is substantial (Nagelkerke's R2 = 0.38). The model's intercept, corresponding to condition = 111, is at 0.98 (95% CI [0.83, 1.13], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.71, 95% CI [0.53, 0.90], p < .001; Std. beta = 0.71, 95% CI [0.53, 0.90])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\ncheck_model(mp1)\n\n\n\n\n\n\n\nCODE\n#Which is a better fit? linear or poisson?\n\ncompare_performance(m1,mp1)\n\n\nWarning: When comparing models, please note that probably not all models were fit from\n  same data.\n\n\n# Comparison of Model Performance Indices\n\nName | Model |      AIC | AIC weights |      BIC | BIC weights |  RMSE | Sigma |    R2 | R2 (adj.) | Nagelkerke's R2 | Score_log | Score_spherical\n--------------------------------------------------------------------------------------------------------------------------------------------------\nm1   |    lm | 1218.500 |     < 0.001 | 1228.454 |     < 0.001 | 4.725 | 4.748 | 0.055 |     0.050 |                 |           |                \nmp1  |   glm | 1044.751 |        1.00 | 1050.423 |        1.00 | 4.840 | 2.533 |       |           |           0.380 |    -4.130 |           0.063"
  }
]