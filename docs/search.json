[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SGC-X",
    "section": "",
    "text": "Study SGC2 | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC2 | 4 Hypothesis Testing\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC2 | Description\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nSGCX | Modelling Reference\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC3A | 1 Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC3A | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC3A | 3 Description\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC3A | 4 Hypothesis Testing\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC3A | 5 Exploratory Analyses\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC4A | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4A | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC4B | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC4B | 2 Response Scoring\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy SGC5A | 1 Introduction\n\n\n\n\n\n\n\n\n\n \n\n\n\nStudy SGC5A | 2 Response Scoring\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "analysis/utils/scoring.html",
    "href": "analysis/utils/scoring.html",
    "title": "Scoring Strategy",
    "section": "",
    "text": "The purpose of this notebook is to describe the strategy for assigning a score ( a measure of accuracy) to response data for the SGC studies. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.)"
  },
  {
    "objectID": "analysis/utils/scoring.html#multiple-response-scoring",
    "href": "analysis/utils/scoring.html#multiple-response-scoring",
    "title": "Scoring Strategy",
    "section": "MULTIPLE RESPONSE SCORING",
    "text": "MULTIPLE RESPONSE SCORING\nThe graph comprehension task of the SGC studies presents readers with a graph, a question, and a series of checkboxes. Participants are instructed to use the graph to answer the question, and respond by selecting all the checkboxes that apply, where each checkbox corresponds to a datapoint in the graph.\n\n\n\nFigure 1. Sample Graph Comprehension (Question # 6)\n\n\nIn the psychology and education literatures on Tests & Measures, the format of this type of question is referred to as Multiple Response (MR), (also: Multiple Choice Multiple Answer (MCMA) and Multiple Answer Multiple Choice (MAMC)). It has a number of properties that make it different from traditional Single Answer Multiple Choice (SAMC) questions, where the respondent marks a single response from a number of options. In particular, there are a number of very different ways that MAMC questions can be scored.\nIn tranditional SAMC format questions, one point is given for selecting the option designated as correct, and zero points given for marking any of the alternative (i.e. distractor) options. Individual response options on MAMC questions, however might be partially correct (\\(i\\)), while responses on other answer options within the same item might be incorrect (\\(n – i\\)). In MR, it is not obvious how to allocate points when the respondent marks a true-correct option (i.e. options that should be selected, denoted \\(p\\)), as well as one or more false-correct options (i.e. options that should not be selected, denoted \\(q\\)). Should partial credit be awarded? If so, are options that respondents false-selected and false-unselected items equally penalized?\nSchmidt et al. (2021) performed a systematic literature review of publications proposing MAMC (or equivalent) scoring schemes, ultimately synthesizing over 80 sources into 27 distinct scoring approaches. Upon reviewing the benefits of trade-offs of each approach, for this study we choose utilize two of the schemes: dichotomous scoring ( Schmidt et al. (2021) scheme #1), and partial scoring \\([-1/q,0, +1/p]\\) ( Schmidt et al. (2021) scheme #26), as well as a scaled discriminant score that leverages partial scoring to discriminate between strategy-specific patterns of response.\n\nResponse Encoding\nFirst, we note that the question type evaluated by Schmidt et al. (2021) is referred to as Multiple True-False (MTF), a variant of MAMC where respondents are presented with a question (stem) and series of response options with True/False (e.g. radio buttons) for each. Depending on the implementation of the underlying instrument, it may or may not be possible for respondents to not respond to a particular option (i.e. leave the item ‘blank’). Although MTF questions have a different underlying implementation (and potentially different psychometric properties) they are identical in their mathematical properties; that is, responses to a MAMC question of ‘select all that apply’ can be coded as a series of T/F responses to each response option\n\n\n\nFigure 3.1: Figure 2. SAMC (vs) MAMC (vs) MTF\n\n\nIn this example (Figure 3.1), we see an example of a question with four response options (\\(n=4\\)) in each question type. In the SAMC approach (at left), there are four possible responses, given explicitly by the response options (respondent can select only one) \\((\\text{number of possible responses} = n)\\). With only four possible responses, we cannot entirely discriminate between all combinations of the underlying response variants we might be interested in, and must always choose an ‘ideal subset’ of possible distractors to present as response options. In the MAMC (middle) and MTF (at right), the same number of response options (\\(n=4\\)) yield a much greater number \\((\\text{number of possible responses} = 2^{n})\\). We can also see the equivalence between a MAMC and MTF format questions with the same response options. Options the respondent selects in MAMC are can be coded as T, and options they leave unselected can be coded as F. Thus, for response options (ABCD), a response of [AB] can also be encoded as [TTFF].\n\n\nScoring Schemes\nIn the sections that follow, we use the terminology:\nProperties of the Stimulus-Question\n\\[\\begin{align}\nn &= \\text{number of response options} \\\\  \n  &= p + q \\\\\n  p &= \\text{number of true-select options (i.e. should be selected)} \\\\\n  q &= \\text{number of true-unselect options (i.e. should not be selected)}\n\\end{align}\\]\nProperties of the Subject’s Response\n\\[\\begin{align}\ni &= \\text{number of options in correct state}, (0 ≤ i ≤ n) \\\\\nf &= \\text{resulting score}\n\\end{align}\\]\n\nDichotomous Scoring\nDichotomous Scoring is the strictest scoring scheme, where a response only receives points if it is exactly correct, meaning the respondent includes only correct-select options, and does select any additional (i.e. incorrect-select) options that should not be selected. This is also known as all or nothing scoring, and importantly, it ignores any partial knowledge that a participant may be expressing through their choice of options. They may select some but not all of the correct-select options, and one or more but not all of the correct-unselect items, but receive the same score as a respondent selects none of the correct-select options, or all of the correct-unselect options. In this sense, dichotomous scoring tells us only about perfect knowledge, and ignores any indication of partial knowledge the respondent may be indicating through their selection of response options.\nIn Dichotomous Scoring\n\nscore for the question is either 0 or 1\nfull credit is only given if all responses are correct; otherwise no credit\ndoes not account for partial knowledge. - with increasing number of response options, scoring becomes stricter as each statement must be marked correctly.\n\nThe algorithm for dichotomous scoring is given by:\n\\[\\begin{gather*}\nf =\n\\begin{cases}\n  1, \\text{if } i = n \\\\    \n  0, \\text{otherwise}    \n\\end{cases}\n\\end{gather*}\\] 0 i n\n\n\nCODE\nf_dichom <- function(i, n) {\n \n  # print(paste(\"i is :\",i,\" n is:\",n)) \n  \n  #if (n == 0 ) return error \n  ifelse( (n == 0), print(\"ERROR n can't be 0\"), \"\")\n  \n  #if (i > n ) return error \n  ifelse( (i > n), print(\"i n can't > n\"), \"\")\n  \n  #if (i==n) return 1, else 0\n  return (ifelse( (i==n), 1 , 0))\n \n}\n\n\n\n\nPartial Scoring [-1/n, +1/n]\nPartial Scoring refers to a class or scoring schemes that award the respondent partial credit depending on pattern of options they select. Schmidt et al. (2021) identify twenty-six different partial credit scoring schemes in the literature, varying in the range of possible scores, and the relative weighting of incorrectly selected (vs) incorrectly unselected options.\nA particularly elegant approach to partial scoring is referred to as the \\([-1/n, +1/n]\\) approach ( Schmidt et al. (2021) #17). This approach is appealing in the context of SGC3A, because it: (1) takes into account all information provided by the respondent: the pattern of what the select, and choose not to select.\nIn Partial Scoring \\([-1/n, +1/n]\\):\n\nScores range from [-1, +1]\nOne point is awarded if all options are correct\nOne point point is subtracted if all options are incorrect.\nIntermediate results are credited as fractions accordingly (\\(+1/n\\) for each correct, \\(-1/n\\) for each incorrect)\nThis results in at chance performance (i.e. half of the given options marked correctly), being awarded 0 points are awarded\n\nThis scoring is more consistent with the motivating theory that Triangular Graph readers start out with an incorrect (i.e. orthogonal, cartesian) interpretation of the coordinate system, and transition to a correct (i.e. triangular) interpretation. But the first step in making this transition is realizing the cartesian interpretation is incorrect, which may yield blank responses where the respondent is essentially saying, ‘there is no correct answer to this question’.\nSchmidt et al. (2021) describe the Partial \\({[-1/n, +1/n]}\\) scoring scheme as the only scoring method (of the 27 described) where respondents’ scoring results can be interpreted as a percentage of their true knowledge. One important drawback of this method is that a respondent may receive credit (a great deal of credit, depending on the number of answer options n) even if she did not select any options. In the case (such as ours) where there are many more response options \\(n\\) than there are options meant to be selected \\(p\\), this partial scoring algorithm poses a challenge because the respondent can achieve an almost completely perfect score by selecting a small number of options that should not be selected.\nThe algorithm for partial scoring\\([-1/n, +1/n]\\) is given by:\n\\[\\begin{align}\nf &= (1/n * i) - (1/n * (n-i)) \\\\\n&= (2i - n)/{n}\n\\end{align}\\]\n\n\nCODE\nf_partialN <- function(i, n) {\n\n# print(paste(\"i is :\",i,\" n is:\",n))\n\n#if(n==0) return error\nifelse((n==0),print(\"ERROR: n should not be 0\"),\"\")\n\n#if(i >n ) return error\nifelse((i > n),print(\"ERROR: i CANNOT BE GREATER THAN n\"),\"\")\n\nreturn ((2*i - n) / n) \n}\n\n\n\n\nPartial Scoring [-1/q, +1/p]\nOne drawback of the Partial Scoring \\([-1/n, +1/n]\\) approach is that treats the choice to select, and choice to not select options as equally indicative of the respondent’s understanding. That is to say, incorrectly selecting one particular option is no more or less informative than incorrectly not-selecting a different item. This represents an important difference between MAMC (i.e. “select all correct options”) vs MTF (i.e. “Mark each option as true or false”) questions.\nIn our study, the selection of any particular option (remember options represent data points on the stimulus graph) is indicative of a particular interpretation of the stimulus. Incorrectly selecting an option indicates an interpretation of the graph with respect to that particular option. Alternatively, failing to select a correct option might mean the individual has a different interpretation, or that they failed to find all the data points consistent with the interpretation.\nFor this reason, we consider another alternative Partial Scoring scheme that takes into consideration only the selected statements, without penalizing statements incorrectly not selected. (See Schmidt et al. (2021) method #26; also referred to as the Morgan-Method) This partial scoring scheme takes into consideration that the most effort-free (or ‘default’) response for any given item is the null, or blank response. Blank responses indicate no understanding, perhaps confusion, or refusal to answer. These lack of responses are awarded zero credit. Whereas taking the action to select an incorrect option is effortful, and is indicative of incorrect understanding.\nPartial Scoring \\([-1/q, +1/p]\\):\n\nawards +1/p for each correctly selected option (\\(p_s\\)), and subtracts \\(1/(n-p) = 1/q\\) for each incorrectly selected option (\\(q_s\\))\nonly considers selected options; does not penalize nor reward unselected options\n\nProperties of Item\n\\[\\begin{align}\np &= \\text{number of true-select options (i.e. should be selected)} \\\\\nq &= \\text{number of true-unselect options (i.e. should not be selected)} \\\\\nn &= \\text{number of options} \\: ( n = p + q)\n\\end{align}\\]\nProperties of Response\n\\[\\begin{align}\np_s &= \\text{number of true-select options selected (i.e. number of correctly checked options)}\\\\\nq_s &= \\text{number of true-unselect options selected (i.e. number of incorrectly checked options }\n\\end{align}\\]\nThe algorithm for partial scoring \\([-1/q, +1/p]\\) is given by:\n\\[\\begin{align}\nf &= (p_s / p) - ({q_s}/{q}) \\\\\n\\end{align}\\]\n\n\nCODE\nf_partialP <- function(t,p,f,q) {\n\n  #t = number of correct-selected options\n  #p = number of true options\n  #f = number of incorrect-selected options\n  #q = number of false options\n  #n = number of options + p + q\n  \n  ifelse( (p == 0), return(NA), \"\") #handle empty response set gracefully by returning nothing rather than 0\n  ifelse( (p != 0), return( (t / p) - (f/q)), \"\")\n}\n\n\n\n\n\nComparison of Schemes\nWhich scoring scheme is most appropriate for the goals of the graph comprehension task?\nConsider the following example:\nFor a question with \\(n = 5\\) response options (data points A, B, C, D and E) with a correct response of A, the schemes under consideration yield the following scores:\n\n\nCODE\ntitle <- \"Comparison of Scoring Schemes for n = 5 options [ A,B,C,D,E ]\"\n\ncorrect <- c( \"A____\",  \n              \"A____\",      \n              \"A____\",        \n              \"A____\",        \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\" ) \n\nresponse <- c(\"A____\",  \n              \"AB___\",      \n              \"A___E\",      \n              \"AB__E\",        \n              \"____E\",\n              \"___DE\",\n              \"_BCDE\",      \n              \"ABCDE\",      \n              \"_____\" )\n\ni <- c(        5,       \n               4,              \n               4,              \n               3,               \n               \n               3,\n               2,\n               0,\n               1,\n               4)\n\nabs <- c(f_dichom(5,5), \n         f_dichom(4,5), \n         f_dichom(4,5), \n         f_dichom(3,5), \n         \n         f_dichom(3,5), \n         f_dichom(2,5),\n         f_dichom(0,5),\n         f_dichom(1,5),\n         f_dichom(4,5))\n\npartial1 <- c(f_partialN(5,5), \n              f_partialN(4,5), \n              f_partialN(4,5), \n              f_partialN(3,5), \n              \n              f_partialN(3,5), \n              f_partialN(2,5),\n              f_partialN(0,5),\n              f_partialN(1,5),\n              f_partialN(4,5))\n\npartial2 <- c(f_partialP(1,1,0,4), \n              f_partialP(1,1,1,4), \n              f_partialP(1,1,1,4), \n              f_partialP(1,1,2,4), \n              \n              f_partialP(0,1,1,4),\n              f_partialP(0,1,2,4),\n              f_partialP(0,1,4,4),\n              f_partialP(1,1,4,4), \n              f_partialP(0,1,0,4))\n\nnames = c(    \"Correct Answer\",\n              \"Response\",\n              \"i \",\n              \"Dichotomous\",\n              \"Partial [-1/n, +1/n]\",\n              \"Partial[-1/q, +1/p]\")\n\ndt <- data.frame(correct, response, i, abs, partial1 , partial2)\n\nkbl(dt, col.names = names, caption = title, digits=3) %>%\n  kable_classic() %>%\n    add_header_above(c(\"Response Scenario \" = 3, \"Scores\" = 3)) %>% \n    pack_rows(\"Perfect Response\", 1, 1) %>%\n    pack_rows(\"Correct + Extra Incorrect Selections\", 2, 4) %>%\n    pack_rows(\"Only Incorrect Selections\", 5, 6) %>%\n    pack_rows(\"Completely Inverse Response \", 7, 7) %>%\n    pack_rows(\"Selected ALL or NONE\", 8, 9) %>%\n    footnote(general = paste(\"i = number of options in correct state; _ indicates option not selected\"),\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nComparison of Scoring Schemes for n = 5 options [ A,B,C,D,E ]\n \n\nResponse Scenario \nScores\n\n  \n    Correct Answer \n    Response \n    i  \n    Dichotomous \n    Partial [-1/n, +1/n] \n    Partial[-1/q, +1/p] \n  \n \n\n  Perfect Response\n\n    A____ \n    A____ \n    5 \n    1 \n    1.0 \n    1.00 \n  \n  Correct + Extra Incorrect Selections\n\n    A____ \n    AB___ \n    4 \n    0 \n    0.6 \n    0.75 \n  \n  \n    A____ \n    A___E \n    4 \n    0 \n    0.6 \n    0.75 \n  \n  \n    A____ \n    AB__E \n    3 \n    0 \n    0.2 \n    0.50 \n  \n  Only Incorrect Selections\n\n    A____ \n    ____E \n    3 \n    0 \n    0.2 \n    -0.25 \n  \n  \n    A____ \n    ___DE \n    2 \n    0 \n    -0.2 \n    -0.50 \n  \n  Completely Inverse Response \n\n    A____ \n    _BCDE \n    0 \n    0 \n    -1.0 \n    -1.00 \n  \n  Selected ALL or NONE\n\n    A____ \n    ABCDE \n    1 \n    0 \n    -0.6 \n    0.00 \n  \n  \n    A____ \n    _____ \n    4 \n    0 \n    0.6 \n    0.00 \n  \n\n\nNote:   i = number of options in correct state; _ indicates option not selected\n\n\n\n\nCODE\n#cleanup\nrm(dt, abs, correct,i,names,partial1,partial2,response,title)\n\n\n\nWe see that in the Dichotomous scheme, only the precisely correct response (row 1) yields a score other than zero. This scheme does now allow us to differentiate between different response patters.\nThe Partial \\([-1/n, +1/n]\\) scheme yields a range from \\([-1,1]\\), differentiating between responses. However, a blank response (bottom row) receives the same score (0.6) as the selection of the correct option + 1 incorrect option (row 2), which is problematic with for the goals of this study, where we need to differentiate between states of confusion or uncertainty yielding blank responses and the intentional selection of incorrect items.\nThe Partial \\([-1/q, +1/p]\\) scheme also yields a range of scores from \\([-1,1]\\). A blank response (bottom row) yields the same score (\\(0\\)) as the selection of all answer options (row 7); both are patterns of behavior we would expect to see if a respondent is confused or uncertain that there is a correct answer to the question.\n\nNext we consider an example from our study, with \\(n = 15\\) options and \\(p = 1\\) correct option to be selected.\n\n\nCODE\ntitle <- \"Comparison of Scoring Schemes for SGC3 with n=15 and p=1 options [A,B...N,O]  \"\n\ncorrect <- c( \"A____\",  \n              \"A____\",      \n              \"A____\",      \n              \"A____\",        \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\",      \n              \"A____\" ) \n\nresponse <- c(\"A__...__\",  \n              \"AB_...__\",      \n              \"A__..._O\",      \n              \"AB_..._O\",        \n              \"___..._O\",      \n              \"___...NO\",      \n              \"_BC...NO\",\n              \"ABC...NO\",      \n              \"___...__\" )\n\ni <- c(        15,       \n               14,              \n               14,              \n               13,\n               13,               \n               12,          \n               0,\n               1,\n               14)\n\nabs <- c(f_dichom(15,15), \n         f_dichom(14,15), \n         f_dichom(14,15), \n         f_dichom(13,15), \n         f_dichom(13,15),\n         f_dichom(12,15),\n         f_dichom(0,15),\n         f_dichom(1,15),\n         f_dichom(14,15))\n\npartial1 <- c(f_partialN(15,15), \n              f_partialN(14,15), \n              f_partialN(14,15), \n              f_partialN(13,15), \n              f_partialN(13,15),\n              f_partialN(12,15),\n              f_partialN(0,15),\n              f_partialN(1,15),\n              f_partialN(14,15))\n\npartial2 <- c(f_partialP(1,1,0,14), \n              f_partialP(1,1,1,14), \n              f_partialP(1,1,1,14), \n              f_partialP(1,1,2,14), \n              f_partialP(0,1,1,14),\n              f_partialP(0,1,2,14),\n              f_partialP(0,1,14,14),\n              f_partialP(1,1,14,14), \n              f_partialP(0,1,0,14))\n\nnames = c(    \"Correct Answer\",\n              \"Response\",\n              \"$i$ \",\n              \"Dichotomous\",\n              \"Partial [-1/n, +1/n]\",\n              \"Partial [-1/q, +1/p]\")\n\ndt <- data.frame(correct, response, i, abs, partial1 , partial2)\n\nkbl(dt, col.names = names, caption = title, digits=3) %>%\n  kable_classic() %>%\n    add_header_above(c(\"Response Scenario \" = 3, \"Scores\" = 3)) %>% \n    pack_rows(\"Perfect Response\", 1, 1) %>%\n    pack_rows(\"Correct + Extra Incorrect Selections\", 2, 4) %>%\n    pack_rows(\"Only Incorrect Selections\", 5, 6) %>%\n    pack_rows(\"Completely Inverse Response \", 7, 7) %>%\n    pack_rows(\"Selected ALL or NONE\", 8, 9) %>%\n    footnote(general = paste(\"i = number of options in correct state; _ indicates option not selected\"),\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nComparison of Scoring Schemes for SGC3 with n=15 and p=1 options [A,B...N,O]  \n \n\nResponse Scenario \nScores\n\n  \n    Correct Answer \n    Response \n    $i$  \n    Dichotomous \n    Partial [-1/n, +1/n] \n    Partial [-1/q, +1/p] \n  \n \n\n  Perfect Response\n\n    A____ \n    A__...__ \n    15 \n    1 \n    1.000 \n    1.000 \n  \n  Correct + Extra Incorrect Selections\n\n    A____ \n    AB_...__ \n    14 \n    0 \n    0.867 \n    0.929 \n  \n  \n    A____ \n    A__..._O \n    14 \n    0 \n    0.867 \n    0.929 \n  \n  \n    A____ \n    AB_..._O \n    13 \n    0 \n    0.733 \n    0.857 \n  \n  Only Incorrect Selections\n\n    A____ \n    ___..._O \n    13 \n    0 \n    0.733 \n    -0.071 \n  \n  \n    A____ \n    ___...NO \n    12 \n    0 \n    0.600 \n    -0.143 \n  \n  Completely Inverse Response \n\n    A____ \n    _BC...NO \n    0 \n    0 \n    -1.000 \n    -1.000 \n  \n  Selected ALL or NONE\n\n    A____ \n    ABC...NO \n    1 \n    0 \n    -0.867 \n    0.000 \n  \n  \n    A____ \n    ___...__ \n    14 \n    0 \n    0.867 \n    0.000 \n  \n\n\nNote:   i = number of options in correct state; _ indicates option not selected\n\n\n\n\nCODE\n#cleanup\nrm(dt, abs, correct,i,names,partial1,partial2,response,title)\n\n\nHere again we see that the Partial \\([-1/q, +1/p]\\) scheme allows us differentiate between patterns of responses, in a way that is more sensible for the goals of the SGC3 graph comprehension task.\n\n\n\n\n\n\nDecision\n\n\n\nThe Partial \\([-1/q, +1/p]\\) scheme is more appropriate for scoring the graph comprehension task than the Partial \\([-1/n, +1/n]\\) scheme because it allows us to differentially penalize incorrectly selected and incorrectly not selected answer options."
  },
  {
    "objectID": "analysis/utils/scoring.html#sec-scoringOverview",
    "href": "analysis/utils/scoring.html#sec-scoringOverview",
    "title": "Scoring Strategy",
    "section": "SCORING SGC DATA",
    "text": "SCORING SGC DATA\nIn SGC_3A we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key.\n5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\nFor each study in the SGC project, MR data will be scored by following these steps:\n(1) Preparing answer keys: For each dataset+question set combination, an answer key is that defines the ‘correct’ answer set under each interpretation of the graph (i.e. a triangular answer, an orthogonal answer, etc).\n(2) Calculate strategy scores: Using the strategy specific answer keys, an interpretation subscore is calculated for each response for each interpretation.\n(3) Interpretation classification: The interpretation subscores are compared in order to classify each response as a particular interpretation. If no classification can be made, the response is classified as ‘?’.\n(4) Calculate Absolute and Scaled Scores: Two final scores are calculated for each response; an Absolute score that indicates if the response was precisely correct according to the triangular interpretation, and a Scaled score that assigns a numeric value to the interpretation given by the response (ranging from -1 to +1)\n\n1. Prepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#LOAD INDIVIDUAL KEY FILES \nkey_111_raw <- read_csv('analysis/utils/keys/SGCX_scaffold_111_key.csv') %>% mutate(condition = \"DEFAULT\", phase = \"scaffold\")\nkey_121_raw <- read_csv('analysis/utils/keys/SGCX_scaffold_121_key.csv')%>% mutate(condition = 121, phase = \"scaffold\")\ncs = rep('c', 23) %>% str_c(collapse=\"\") #create column spec \nkey_test_raw <- read_csv('analysis/utils/keys/SGCX_test_key.csv', col_types = cs)%>% mutate(condition = \"DEFAULT\", phase = \"test\") \n\n#JOIN THEM\nkeys_raw <- rbind(key_111_raw, key_121_raw, key_test_raw )\n\n#CLEANUP\nrm(key_111_raw, key_121_raw, key_test_raw)\n\n\nIn order to calculate scores using the \\([-1/q, +1/p]\\) algorithm, we need to define the subset of all response options (set N) that should be selected (set P) and should not be selected (set Q). In order to calculate subscores for each graph interpretation (i.e. triangular, orthogonal, tversky) we must define these sets independently for each interpretation. For each question, the keys_raw dataframe gives us set N (all response options), and a set P (options that should be selected) for each interpretation. From these we must derive set Q for each interpretation.\n\nSET \\(N\\), all response options (superset) . This set is the same across all interpretations (a property of the question) and is given in the answer key.\nSET \\(P\\), \\(P \\subset N\\) , the subset of options that should be selected (rewarded as +1/p) . This set differs by interpretation, and is given in the answer key.\nSET \\(A, A \\subset N, A \\sqcup P\\) , the subset of options that should not be selected, but if they are, aren’t penalized (i.e. these options are ignored. Not rewarded, nor penalized). These include any options referenced in the question (i.e. select shifts that start at the same time as X; don’t penalize if they also select ‘X’), as well as options within 0.5hr offset from the data point to accommodate reasonable visual errors. This set differs by interpretation, and is given in the answer key (columns REF_POINT and _also).\nSET \\(Q\\), the subset of options that should not be selected and are penalized (as -1/q). This set differs by interpretation and is not given in the answer key. We can derive set Q for each interpretation by \\(Q = N - (P \\cup A)\\)\n\nThe next step in scoring is preparing interpretation-specific answer keys that specify sets N, P, A and Q for each question.\n\nTriangular Key\nFirst we construct a key set based on the ‘Triangular’ interpretation (i.e. the actually correct answers).\n\n\nCODE\nverify_tri = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TRIANGULAR KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tri <- keys_raw %>% \n  select(Q, condition, OPTIONS, TRIANGULAR, TRI_allow, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    TRI_allow = str_replace_na(TRI_allow,\"\"),\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TRIANGULAR,\n    set_p = str_replace_na(set_p,\"\"),#replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TRI_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"),#replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DEFINE SETS N, P, A\nfor (x in 1:nrow(keys_tri)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tri[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tri[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tri[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tri[x,'set_q'] = Q\n  keys_tri[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tri[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tri <- keys_tri %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tri <- keys_tri %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup \nrm(N,A,P,Q,n_q,s,x,tempunion)\n\n\nThis leaves us a dataframe keys_tri that define the sets of response options consistent with the triangular graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each option in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nOrthogonal Key\nNext we construct a key set based on the ‘Orthogonal’ interpretation.\n\n\nCODE\nverify_orth = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT ORTHOGONAL KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_orth <- keys_raw %>% \n  select(Q, condition, OPTIONS, ORTHOGONAL, ORTH_allow, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    ORTH_allow = str_replace_na(ORTH_allow,\"\"),\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = ORTHOGONAL,\n    set_p = str_replace_na(set_p,\"\"),#replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(ORTH_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answer options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_orth)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_orth[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_orth[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_orth[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_orth[x,'set_q'] = Q\n  keys_orth[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  # print(tempunion)\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n\n  verify_orth[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_orth <- keys_orth %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_orth <- keys_orth %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x, cs)\n\n\nThis leaves us a dataframe keys_orth that define the sets of response options consistent with the orthogonal graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nTversky Keys\nNext we construct the key set based on a partial-understanding strategy we refer to as ‘Tversky’. We use the label Tversky as shorthand for a partial interpretation of the coordinate system where subjects select a set of responses that lay along a connecting line from the referenced data point or referenced time for that item. The term is named for Barbara Tversky based on her work on graphical primitives (e.g. “lines connect, arrows direct, boxes contain”).\n\n\nCODE\nverify_max = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-MAX\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_max <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_max, TV_max_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_max = str_replace_na(TV_max,\"\"),\n    TV_max_allow = str_replace_na(TV_max_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_max,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_max_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_max)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_max[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_max[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_max[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_max[x,'set_q'] = Q\n  keys_tversky_max[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_max[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_max <- keys_tversky_max %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_max <- keys_tversky_max %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\nverify_tversky_start = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-START\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_start <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_start, TV_start_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_start = str_replace_na(TV_start,\"\"),\n    TV_start_allow = str_replace_na(TV_start_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_start,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_start_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_start)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_start[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_start[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_start[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_start[x,'set_q'] = Q\n  keys_tversky_start[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_start[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_start <- keys_tversky_start %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_start <- keys_tversky_start %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\nverify_tversky_end = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-END\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_end <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_end, TV_end_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_end = str_replace_na(TV_end,\"\"),\n    TV_end_allow = str_replace_na(TV_end_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_end,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_end_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_end)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_end[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_end[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_end[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_end[x,'set_q'] = Q\n  keys_tversky_end[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_end[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_end <- keys_tversky_end %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_end <- keys_tversky_end %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\nverify_tversky_duration = c()\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT TVERSKY KEY SET for TVERSKY-DURATION\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_tversky_duration <- keys_raw %>% \n  select(Q, condition, OPTIONS, REF_POINT, TV_dur, TV_dur_allow) %>% \n  mutate(\n  \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    TV_dur = str_replace_na(TV_dur,\"\"),\n    TV_dur_allow = str_replace_na(TV_dur_allow,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = TV_dur,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(TV_dur_allow,REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_tversky_duration)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_tversky_duration[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_tversky_duration[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_tversky_duration[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist()\n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  \n  #save set to dataframe\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_tversky_duration[x,'set_q'] = Q\n  keys_tversky_duration[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_tversky_duration[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_tversky_duration <- keys_tversky_duration %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q) %>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_tversky_duration <- keys_tversky_duration %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\nThis leaves us four dataframes, each corresponding to a different variant of a ‘lines connecting to reference point’ strategy.\n- keys_tversky_max : the superset of lines connecting options - keys_tversky_start : lines connecting to the rightward diagonal (start time) of the reference point - keys_tversky_end: lines connecting to the leftward diagonal (end time) of the reference point - keys_tversky_duration: lines connecting to the horizontal y-intercept (duration) of the reference point\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nSatisficing Key\nNext we construct two keys based on the ‘Satisficing’ strategy. Satisficing involves selecting any data points within 0.5hr visual offset of the orthogonal interpretation of the graph (because no orthogonal response option is available). One key represents selecting a point slightly to the left of the orthogonal, and the other key represents selecting a point slightly to the right of the orthogonal. The “Satisficing” strategy involves the reader selecting data points nearest to the orthogonal projection from the reference point in the question. We observe this strategy in some readers when there is no orthogonal response available (i.e. in the impasse condition), so they select the points nearest to the projection (i.e. “close enough”).\n\n\nCODE\nverify_satisfice_right = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT SATISFICE RIGHT KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_satisfice_right <- keys_raw %>% \n  select(Q, condition, OPTIONS, SATISFICE_right, REF_POINT) %>% \n  mutate(\n    #replace NAs\n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n\n    #P options that SHOULD be selected (rewarded)\n    set_p = SATISFICE_right,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n\n    #A options that are ignored if selected\n    set_a = str_c(REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n\n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_satisfice_right)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_satisfice_right[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_satisfice_right[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_satisfice_right[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to data frame\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_satisfice_right[x,'set_q'] = Q\n  keys_satisfice_right[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_satisfice_right[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_satisfice_right <- keys_satisfice_right %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q)%>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_satisfice_right <- keys_satisfice_right %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\n\nverify_satisfice_left = c() #sanity check\n##——————————————————————————————————————————————————————————————————————\n##CONSTRUCT SATISFICE left KEY SET\n##——————————————————————————————————————————————————————————————————————\n#1. DEFINE SETS N, P, A\nkeys_satisfice_left <- keys_raw %>% \n  select(Q, condition, OPTIONS, SATISFICE_left, REF_POINT) %>% \n  mutate(\n    \n    #replace NAs \n    REF_POINT = str_replace_na(REF_POINT,\"\"),\n    \n    #P options that SHOULD be selected (rewarded)\n    set_p = SATISFICE_left,\n    set_p = str_replace_na(set_p,\"\"), #replace na if empty\n    n_p = nchar(set_p), #number of true-select options\n    \n    #A options that are ignored if selected \n    set_a = str_c(REF_POINT, sep=\"\"),\n    set_a = str_replace_na(set_a,\"\"), #replace na if empty\n    n_a = nchar(set_a),\n    \n    #N store all answr options (superset)\n    set_n = OPTIONS,  \n    n_n = nchar(set_n)\n  \n) %>% select(Q, condition, set_n, set_p, set_a, n_n, n_p, n_a)\n\n#2. DO THE STUFF THAT'S EASIER IN A LOOP\nfor (x in 1:nrow(keys_satisfice_left)) {\n  \n  #UNWIND STRINGS FOR SETDIFF\n  #n all answer options\n  N = keys_satisfice_left[x,'set_n'] %>% pull(set_n) %>% strsplit(\"\") %>% unlist()\n  #p correct-select answer options\n  P = keys_satisfice_left[x,'set_p'] %>% pull(set_p) %>% strsplit(\"\") %>% unlist()\n  #a ignore-select answer options (should not be selected, but if they are, don't penalize)\n  A = keys_satisfice_left[x,'set_a'] %>% pull(set_a) %>% strsplit(\"\") %>% unlist() \n  \n  #Q = N - (P+A)\n  #answers that are penalized (at -1/q) if selected \n  s = union(P,A) #rewarded plus ignored \n  s = str_replace_na(s,\"\")\n  # print(s)\n  # s = union(s,X) # + trapdoor \n  Q = setdiff(N,s) # = penalized at -1/q when selected \n  #save set to data frame\n  Q = str_c(Q, collapse=\"\")\n  n_q = nchar(Q)\n  keys_satisfice_left[x,'set_q'] = Q\n  keys_satisfice_left[x,'n_q'] = n_q\n  \n  #verify each element in N is included in one and only one of P,A,Q\n  tempunion = union(s,Q) %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  N = N %>% str_c(collapse=\"\") %>% strsplit(\"\") %>% unlist()\n  verify_satisfice_left[x] = setequal(tempunion,N)\n}\n\n#3. reorder cols for ease of use\nkeys_satisfice_left <- keys_satisfice_left %>% select(Q, condition, set_n, set_p, set_a, set_q, n_n, n_p, n_a, n_q)%>% mutate(verify = n_p + n_a + n_q)\n\n#4. replace condition 111 with \"general\" to accomodate other conditions [only 121 is special]\nkeys_satisfice_left <- keys_satisfice_left %>% mutate(\n  condition = replace(condition, condition != \"121\", \"DEFAULT\")\n)\n\n#cleanup\nrm(A, N, n_q, P, Q, s, tempunion, x)\n\n\nThis leaves us a dataframe keys_satisfice that define the sets of response options consistent with the orthogonal graph interpretation.\nTo verify we have generated the correct sets, we verify that for each question, each response in N is included in either set P, A or Q (once and only once).\nTRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE\n\n\nCODE\n#cleanup\nrm(verify_tri, verify_orth, verify_max, verify_tversky_duration, verify_tversky_end, verify_tversky_start, verify_satisfice_right, verify_satisfice_left)\n\n\nFinally, we need to clean up and generalize our answer keys to accommodate the experimental conditions for Study SGC4-SGC5. In both of these studies the answer set (and underlying graphed data set) are identical, the conditions differ only based on the structure of the gridlines or marks used to represent the data, or interactive mode of the answer format.\n\n\nCODE\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nwrite.csv(keys_raw,\"analysis/utils/keys/parsed_keys/keys_raw\", row.names = FALSE)\nwrite.csv(keys_orth,\"analysis/utils/keys/parsed_keys/keys_orth\", row.names = FALSE)\nwrite.csv(keys_tri,\"analysis/utils/keys/parsed_keys/keys_tri\", row.names = FALSE)\nwrite.csv(keys_satisfice_left,\"analysis/utils/keys/parsed_keys/keys_satisfice_left\", row.names = FALSE)\nwrite.csv(keys_satisfice_right,\"analysis/utils/keys/parsed_keys/keys_satisfice_right\", row.names = FALSE)\nwrite.csv(keys_tversky_duration,\"analysis/utils/keys/parsed_keys/keys_tversky_duration\", row.names = FALSE)\nwrite.csv(keys_tversky_end,\"analysis/utils/keys/parsed_keys/keys_tversky_end\", row.names = FALSE)\nwrite.csv(keys_tversky_max,\"analysis/utils/keys/parsed_keys/keys_tversky_max\", row.names = FALSE)\nwrite.csv(keys_tversky_start,\"analysis/utils/keys/parsed_keys/keys_tversky_start\", row.names = FALSE)\n\n\n\n\n\n2. Calculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangular interpretation?\nscore_ORTH How consistent is the response with the orthogonal interpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation?\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\nTo facilitate scoring, we import the following helper functions in each scoring script.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\nprint(calc_subscore)\n\n\nfunction (question, cond, response, keyframe) \n{\n    if (cond == 121 & question < 6) {\n        p = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% \n            unlist()\n        q = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% \n            unlist()\n        pn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(n_p)\n        qn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"121\") %>% select(n_q)\n    }\n    else {\n        p = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(set_p) %>% pull(set_p) %>% \n            str_split(\"\") %>% unlist()\n        q = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(set_q) %>% pull(set_q) %>% \n            str_split(\"\") %>% unlist()\n        pn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(n_p)\n        qn = keyframe %>% filter(Q == question) %>% filter(condition == \n            \"DEFAULT\") %>% select(n_q)\n    }\n    if (response != \"\") {\n        response = response %>% str_split(\"\") %>% unlist()\n    }\n    ps = length(intersect(response, p))\n    qs = length(intersect(response, q))\n    x = f_partialP(ps, pn, qs, qn) %>% unlist() %>% as.numeric()\n    rm(p, q, pn, qn, ps, qs)\n    return(x)\n}\n\n\n\n\n3. Derive Interpretation\nNext, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\nprint(derive_interpretation)\n\n\nfunction (df) \n{\n    threshold_range = 0.5\n    threshold_frenzy = 4\n    for (x in 1:nrow(df)) {\n        t = df[x, ] %>% dplyr::select(score_TV_max, score_TV_start, \n            score_TV_end, score_TV_duration)\n        t.long = gather(t, score, value, 1:4)\n        t.long[t.long == \"\"] = NA\n        if (length(unique(t.long$value)) == 1) {\n            if (is.na(unique(t.long$value))) {\n                df[x, \"score_TVERSKY\"] = NA\n                df[x, \"tv_type\"] = NA\n            }\n        }\n        else {\n            df[x, \"score_TVERSKY\"] = as.numeric(max(t.long$value, \n                na.rm = TRUE))\n            df[x, \"tv_type\"] = t.long[which.max(t.long$value), \n                \"score\"]\n        }\n        t = df[x, ] %>% dplyr::select(score_SAT_left, score_SAT_right)\n        t.long = gather(t, score, value, 1:2)\n        t.long[t.long == \"\"] = NA\n        if (length(unique(t.long$value)) == 1) {\n            if (is.na(unique(t.long$value))) {\n                df[x, \"score_SATISFICE\"] = NA\n                df[x, \"sat_type\"] = NA\n            }\n        }\n        else {\n            df[x, \"score_SATISFICE\"] = as.numeric(max(t.long$value, \n                na.rm = TRUE))\n            df[x, \"sat_type\"] = t.long[which.max(t.long$value), \n                \"score\"]\n        }\n        t = df[x, ] %>% dplyr::select(score_TRI, score_TVERSKY, \n            score_SATISFICE, score_ORTH)\n        t.long = gather(t, score, value, 1:4)\n        t.long[t.long == \"\"] = NA\n        df[x, \"top_score\"] = as.numeric(max(t.long$value, na.rm = TRUE))\n        df[x, \"top_type\"] = t.long[which.max(t.long$value), \"score\"]\n        r = as.numeric(range(t.long$value, na.rm = TRUE))\n        r = diff(r)\n        df[x, \"range\"] = r\n        if (r < threshold_range) {\n            df[x, \"best\"] = \"?\"\n        }\n        else {\n            p = df[x, \"top_type\"]\n            if (p == \"score_TRI\") {\n                df[x, \"best\"] = \"Triangular\"\n            }\n            else if (p == \"score_ORTH\") {\n                df[x, \"best\"] = \"Orthogonal\"\n            }\n            else if (p == \"score_TVERSKY\") {\n                df[x, \"best\"] = \"Tversky\"\n            }\n            else if (p == \"score_SATISFICE\") {\n                df[x, \"best\"] = \"Satisfice\"\n            }\n        }\n        if (!is.na(df[x, \"score_BOTH\"])) {\n            if (df[x, \"score_BOTH\"] == 1) {\n                df[x, \"best\"] = \"both tri + orth\"\n            }\n        }\n        if (df[x, \"num_o\"] == 0) {\n            df[x, \"best\"] = \"blank\"\n        }\n        if (df[x, \"num_o\"] > threshold_frenzy) {\n            df[x, \"best\"] = \"frenzy\"\n        }\n        if (!is.na(df[x, \"score_REF\"])) {\n            if (df[x, \"score_REF\"] == 1) {\n                df[x, \"best\"] = \"reference\"\n            }\n        }\n    }\n    rm(t, t.long, x, r, p)\n    rm(threshold_frenzy, threshold_range)\n    df$int2 <- factor(df$best, levels = c(\"Triangular\", \"Tversky\", \n        \"Satisfice\", \"Orthogonal\", \"reference\", \"both tri + orth\", \n        \"blank\", \"frenzy\", \"?\"))\n    df$interpretation <- factor(df$best, levels = c(\"Orthogonal\", \n        \"Satisfice\", \"frenzy\", \"?\", \"reference\", \"blank\", \"both tri + orth\", \n        \"Tversky\", \"Triangular\"))\n    df$high_interpretation <- fct_collapse(df$interpretation, \n        orthogonal = c(\"Satisfice\", \"Orthogonal\"), neg.trans = c(\"frenzy\", \n            \"?\"), neutral = c(\"reference\", \"blank\"), pos.trans = c(\"Tversky\", \n            \"both tri + orth\"), triangular = \"Triangular\")\n    df$tv_type = as.factor(df$tv_type)\n    df$top_type = as.factor(df$top_type)\n    df$high_interpretation = factor(df$high_interpretation, levels = c(\"orthogonal\", \n        \"neg.trans\", \"neutral\", \"pos.trans\", \"triangular\"))\n    df <- df %>% dplyr::select(-best)\n    return(df)\n}\n\n\n\n\n4. Derive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\nprint(calc_scaled)\n\n\nfunction (v) \n{\n    v <- recode(v, Orthogonal = -1, Satisfice = -1, frenzy = -0.5, \n        `?` = -0.5, reference = 0, blank = 0, `both tri + orth` = 0.5, \n        Tversky = 0.5, Triangular = 1)\n    return(v)\n}\n\n\n\n\n5. Summarize by Subject\nThe final step in the scoring procedure is to summarise the item-level scores by subject, and save certain summaries to the subject-level record. We also construct two long-format dataframes containing cummulative progress scores (the point-in-time [absolute, scaled] scores for each subject on each question).\n\n\nCODE\nprint(summarise_bySubject)\n\n\nfunction (subjects, items) \n{\n    subjects_summary <- items %>% filter(q %nin% c(6, 9)) %>% \n        group_by(subject) %>% dplyr::summarise(subject = as.character(subject), \n        s_TRI = sum(score_TRI, na.rm = TRUE), s_ORTH = sum(score_ORTH, \n            na.rm = TRUE), s_TVERSKY = sum(score_TVERSKY, na.rm = TRUE), \n        s_SATISFICE = sum(score_SATISFICE, na.rm = TRUE), s_REF = sum(score_REF, \n            na.rm = TRUE), s_ABS = sum(score_ABS, na.rm = TRUE), \n        s_NABS = sum(score_niceABS, na.rm = TRUE), s_SCALED = sum(score_SCALED, \n            na.rm = TRUE), DV_percent_NABS = s_NABS/13, rt_m = sum(rt_s)/60, \n        item_avg_rt = mean(rt_s), item_min_rt = min(rt_s), item_max_rt = max(rt_s), \n        item_n_TRI = sum(interpretation == \"Triangular\"), item_n_ORTH = sum(interpretation == \n            \"Orthogonal\"), item_n_TV = sum(interpretation == \n            \"Tversky\"), item_n_SAT = sum(interpretation == \"Satisfice\"), \n        item_n_OTHER = sum(interpretation %nin% c(\"Triangular\", \n            \"Orthogonal\", \"Tversky\", \"Satisfice\")), item_n_POS = sum(high_interpretation == \n            \"pos.trans\"), item_n_NEG = sum(high_interpretation == \n            \"neg.trans\"), item_n_NEUTRAL = sum(high_interpretation == \n            \"neutral\")) %>% arrange(subject) %>% slice(1L)\n    subjects_q1 <- items %>% filter(q == 1) %>% mutate(item_q1_NABS = score_niceABS, \n        item_q1_SCALED = score_SCALED, item_q1_interpretation = interpretation, \n        item_q1_rt = rt_s, ) %>% dplyr::select(subject, item_q1_NABS, \n        item_q1_SCALED, item_q1_interpretation, item_q1_rt) %>% \n        arrange(subject)\n    subjects_q5 <- items %>% filter(q == 5) %>% mutate(item_q5_NABS = score_niceABS, \n        item_q5_SCALED = score_SCALED, item_q5_interpretation = interpretation, \n        item_q5_rt = rt_s, ) %>% dplyr::select(subject, item_q5_NABS, \n        item_q5_SCALED, item_q5_interpretation, item_q5_rt) %>% \n        arrange(subject)\n    subjects_q7 <- items %>% filter(q == 7) %>% mutate(item_q7_NABS = score_niceABS, \n        item_q7_interpretation = interpretation, item_q7_rt = rt_s, \n        ) %>% dplyr::select(subject, item_q7_NABS, item_q7_interpretation, \n        item_q7_rt) %>% arrange(subject)\n    subjects_q15 <- items %>% filter(q == 15) %>% mutate(item_q15_NABS = score_niceABS, \n        item_q15_interpretation = interpretation, item_q15_rt = rt_s, \n        ) %>% dplyr::select(subject, item_q15_NABS, item_q15_interpretation, \n        item_q15_rt) %>% arrange(subject)\n    subjects_scaffold <- items %>% filter(q < 6) %>% group_by(subject) %>% \n        dplyr::summarise(item_scaffold_NABS = sum(score_niceABS), \n            item_scaffold_SCALED = sum(score_SCALED), item_scaffold_rt = sum(rt_s)/60) %>% \n        dplyr::select(subject, item_scaffold_NABS, item_scaffold_SCALED, \n            item_scaffold_rt) %>% arrange(subject)\n    subjects_test <- items %>% filter(q %nin% c(1, 2, 3, 4, 5, \n        6, 9)) %>% group_by(subject) %>% dplyr::summarise(item_test_NABS = sum(score_niceABS), \n        item_test_SCALED = sum(score_SCALED), item_test_rt = sum(rt_s)/60) %>% \n        dplyr::select(subject, item_test_NABS, item_test_SCALED, \n            item_test_rt) %>% arrange(subject)\n    print(unique(subjects_summary$subject == subjects$subject))\n    print(unique(subjects_summary$subject == subjects_q1$subject))\n    print(unique(subjects_summary$subject == subjects_q5$subject))\n    print(unique(subjects_summary$subject == subjects_q7$subject))\n    print(unique(subjects_summary$subject == subjects_q15$subject))\n    print(unique(subjects_summary$subject == subjects_scaffold$subject))\n    print(unique(subjects_summary$subject == subjects_test$subject))\n    x = merge(subjects, subjects_summary, by.x = \"subject\", by.y = \"subject\")\n    x = merge(x, subjects_q1)\n    x = merge(x, subjects_q5)\n    x = merge(x, subjects_q7)\n    x = merge(x, subjects_q15)\n    x = merge(x, subjects_scaffold)\n    x = merge(x, subjects_test)\n    subjects <- x\n    rm(subjects_q1, subjects_q5, subjects_q7, subjects_q15, subjects_scaffold, \n        subjects_test, subjects_summary, x)\n    return(subjects)\n}\n\n\nCODE\nprint(progress_Absolute)\n\n\nfunction (items) \n{\n    x <- items %>% filter(q %nin% c(6, 9)) %>% dplyr::select(subject, \n        mode, pretty_condition, q, score_niceABS)\n    wide <- x %>% pivot_wider(names_from = q, names_glue = \"q_{q}\", \n        values_from = score_niceABS)\n    wide$c1 = wide$q_1\n    wide$c2 = wide$c1 + wide$q_2\n    wide$c3 = wide$c2 + wide$q_3\n    wide$c4 = wide$c3 + wide$q_4\n    wide$c5 = wide$c4 + wide$q_5\n    wide$c6 = wide$c5 + wide$q_7\n    wide$c7 = wide$c6 + wide$q_8\n    wide$c8 = wide$c7 + wide$q_10\n    wide$c9 = wide$c8 + wide$q_11\n    wide$c10 = wide$c9 + wide$q_12\n    wide$c11 = wide$c10 + wide$q_13\n    wide$c12 = wide$c11 + wide$q_14\n    wide$c13 = wide$c12 + wide$q_15\n    wide <- wide %>% dplyr::select(subject, mode, pretty_condition, \n        c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13)\n    df_absolute_progress <- wide %>% pivot_longer(cols = c1:c13, \n        names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n    df_absolute_progress$question <- as.integer(df_absolute_progress$question)\n    rm(x, wide)\n    return(df_absolute_progress)\n}\n\n\nCODE\nprint(progress_Scaled)\n\n\nfunction (items) \n{\n    x <- items %>% filter(q %nin% c(6, 9)) %>% select(subject, \n        mode, pretty_condition, q, score_SCALED)\n    wide <- x %>% pivot_wider(names_from = q, names_glue = \"q_{q}\", \n        values_from = score_SCALED)\n    wide$c1 = wide$q_1\n    wide$c2 = wide$c1 + wide$q_2\n    wide$c3 = wide$c2 + wide$q_3\n    wide$c4 = wide$c3 + wide$q_4\n    wide$c5 = wide$c4 + wide$q_5\n    wide$c6 = wide$c5 + wide$q_7\n    wide$c7 = wide$c6 + wide$q_8\n    wide$c8 = wide$c7 + wide$q_10\n    wide$c9 = wide$c8 + wide$q_11\n    wide$c10 = wide$c9 + wide$q_12\n    wide$c11 = wide$c10 + wide$q_13\n    wide$c12 = wide$c11 + wide$q_14\n    wide$c13 = wide$c12 + wide$q_15\n    wide <- wide %>% select(subject, mode, pretty_condition, \n        c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13)\n    df_scaled_progress <- wide %>% pivot_longer(cols = c1:c13, \n        names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n    df_scaled_progress$question <- as.integer(df_scaled_progress$question)\n    rm(x, wide)\n    return(df_scaled_progress)\n}\n\n\n\n\n\n\n\n\nSchmidt, Dennis, Tobias Raupach, Annette Wiegand, Manfred Herrmann, and Philipp Kanzow. 2021. “Relation Between Examinees’ True Knowledge and Examination Scores: Systematic Review and Exemplary Calculations on Multiple-True-False Items.” Educational Research Review 34 (November): 100409. https://doi.org/10.1016/j.edurev.2021.100409."
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_introduction.html",
    "href": "analysis/SGC3A/1_sgc3A_introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In Study 3A we explore the extent to which confronting a learner with an implicit obstacle (a mental impasse) influences their interpretation of the underlying coordinate system. This is a hypothesis that emerged from analysis of Study 2, leading us to suspect that presenting a learner with a situation that induces a state of impasse will increase the probability that learners experience a moment of insight, and in turn restructure their interpretation of the coordinate system.\nIn the context of Study 2, an impasse state was (unintentionally) induced when the combination of question + data set yielded no available answer in the incorrect (cartesian) interpretation of the graph. In Study 3A, we test this hypothesis by comparing performance between a (treatment) group receiving impasse-inducing questions followed by normal questions, and a non-impasse control."
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_introduction.html#methods",
    "href": "analysis/SGC3A/1_sgc3A_introduction.html#methods",
    "title": "1  Introduction",
    "section": "METHODS",
    "text": "METHODS\nDesign\nWe employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Scaffold: control,impasse)\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 6.1. The list of questions can be found here.\n\n\nFigure 1.2: Sample Question (Q=1) for Graph Comprehension Task\n\n\nNote that across both control and impasse conditions, both the question, response options and graph structure were identical. The experimental manipulation (posing a mental impasse) was accomplished by changing the position of datapoints in the impasse-condition graph, such that for any given question, there was no available response option if the reader were to interpret the graph as cartesian (making an orthogonal rather than diagonal projection from the x-axis.)\nThe green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. We hypothesize that this presents the reader with an obstacle, at which point they are forced to confront their interpretation of the coordinate system and (ideally) develop a new strategy.\n\n\nFigure 1.3: Sample Question (Q=1) graphs for each condition\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items.\n(3A) The first five items in the task are defined as the SCAFFOLDING block. In the IMPASSE condition, the first five questions included an IMPASSE problem state. For participants in the CONTROL condition, the dataset was structure such that there was always an available ‘orthogonal answer’ for the first 5 questions.\n(3B) The remaining 10 items are defined as the TESTING block. In both conditions, these questions were not structured as impasse (i.e. contained an available orthogonal answer)\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\nSample\nData was collected by convenience sample of a university subject pool. Initial data (Fall 2017, Spring 2018) were collected in-person, with large groups of students simultaneously completing the study (independently) in a computer lab. In Fall 2021 and Winter 2022 we collected additional data to replicate results in a remote format (students completing the study asynchronously on their own computers)."
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_introduction.html#analysis",
    "href": "analysis/SGC3A/1_sgc3A_introduction.html#analysis",
    "title": "1  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\nBefore analysis, data files from individual data collection periods are harmonized into a common data format.\n\n\n\n\n\n\nPre-Requisite\nFollowed By\n\n\nspring17_clean_data.Rmd  spring18_clean_data.Rmd  fall21_clean_data.Rmd  winter2022_clean_sgc3a.Rmd\n2_sgc3A_scoring.qmd\n\n\nData for study SGC_3A were collected across four time periods, interrupted by the Covid-19 pandemic.\n\n\nPeriod\nModality\n\n\n\nFall 2017\nin person, SONA groups in computer lab\n\n\nSpring 2018\nin person, SONA groups in computer lab\n\n\nFall 2021\nasynchronous, online, SONA\n\n\nWinter 2022\nasynchronous, online, SONA\n\n\n\nData collected in Fall 2017, Spring 2018 constitute the original SGC_3A study, conducted in person. Data collected in Fall 2021, Winter 2022 constitute the web-based replication, conducted online (asynchronously). In all cases, the experiment was administered via a web application.\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\nParticipants\nFirst we import participant-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\nCODE#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3A/data/0-session-level/fall17_sgc3a_participants.csv\"\nspring18 <- \"analysis/SGC3A/data/0-session-level/spring18_sgc3a_participants.csv\"\nfall21 <- \"analysis/SGC3A/data/0-session-level/fall21_sgc3a_participants.csv\"\nwinter22 <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_participants.rds\"\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_subjects_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% \n  dplyr::select(\n  subject,condition,term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m, \n  # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n  #exploratory factors\n  violations, browser, width, height\n)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\",\n    violations = \"NULL\",\n    browser = \"NULL\",\n    width = \"NULL\",\n    height = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, \n                # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                #exploratory factors\n                violations, browser, width, height)\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_winter22_q16 <- df_subjects_winter22 %>% \n  dplyr::select(subject, condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% dplyr::select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects_winter22 <- df_subjects_winter22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors \n                 violations, browser, width, height)\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_winter22, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\"),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#CLEANUP\nrm(df_subjects_fall17,df_subjects_fall21, df_subjects_spring18, df_subjects_winter22,df_subjects_before)\nrm(fall17,fall21,spring18,winter22)\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\nCODE# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3A/data/0-session-level/fall17_sgc3a_blocks.csv\"\nspring18 <- \"analysis/SGC3A/data/0-session-level/spring18_sgc3a_blocks.csv\"\nfall21 <- \"analysis/SGC3A/data/0-session-level/fall21_sgc3a_blocks.csv\"\nwinter22 <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_items.rds\"\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\ndf_items_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- df_items_winter22 %>% group_by(q) %>% select(q,relation) %>% unique()\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- df_items_winter22 %>% filter(condition=='X') %>% select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n  \n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18, df_items_fall21) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n  \n#reduce data collected using new webapp\ndf_items_winter22 <- df_items_winter22 %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_winter22,df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% select(-question,-correct,-rt_s,-num_o)\n#add data from wi22 [stored on subject data]\ndf_freeresponse <- rbind(df_freeresponse, df_winter22_q16)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n    question = \"Please describe how to determine what event(s) start at 12pm?\",\n    response = as.character(response) #doesn't need to be factor\n  ) \n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#add back pretty condition \ndf_items <- df_items %>% mutate(\n  pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n  pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_items_fall17,df_items_fall21, df_items_spring18, df_items_winter22, df_items_before, df_winter22_q16)\nrm(fall17,fall21,spring18,winter22, map_relations)\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\nCODE#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n[1] TRUE\n\nCODE#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n[1] TRUE\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\nCODE# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3A/data/1-study-level/sgc3a_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3A/data/1-study-level/sgc3a_items.csv\", row.names = FALSE)\nwrite.csv(df_freeresponse,\"analysis/SGC3A/data/1-study-level/sgc3a_freeresponse.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3A/data/1-study-level/sgc3a_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3A/data/1-study-level/sgc3a_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3A/1_sgc3A_introduction.html#resources",
    "href": "analysis/SGC3A/1_sgc3A_introduction.html#resources",
    "title": "1  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\nCODEsessionInfo()\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.8     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.6    \n [9] ggplot2_3.3.5    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.0    evaluate_0.15    \n[13] highr_0.9         httr_1.4.2        pillar_1.7.0      rlang_1.0.2      \n[17] curl_4.3.2        readxl_1.3.1      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.13    webshot_0.5.2     foreign_0.8-82    htmlwidgets_1.5.4\n[25] bit_4.0.4         munsell_0.5.0     broom_0.7.12      compiler_4.1.1   \n[29] modelr_0.1.8      xfun_0.30         pkgconfig_2.0.3   systemfonts_1.0.4\n[33] htmltools_0.5.2   tidyselect_1.1.2  rio_0.5.29        fansi_1.0.2      \n[37] viridisLite_0.4.0 crayon_1.5.0      tzdb_0.2.0        dbplyr_2.1.1     \n[41] withr_2.5.0       grid_4.1.1        jsonlite_1.8.0    gtable_0.3.0     \n[45] lifecycle_1.0.1   DBI_1.1.2         magrittr_2.0.2    scales_1.1.1     \n[49] zip_2.2.0         vroom_1.5.7       cli_3.2.0         stringi_1.7.6    \n[53] fs_1.5.2          xml2_1.3.3        ellipsis_0.3.2    generics_0.1.2   \n[57] vctrs_0.3.8       openxlsx_4.2.5    tools_4.1.1       bit64_4.0.5      \n[61] glue_1.6.2        hms_1.1.1         parallel_4.1.1    fastmap_1.1.0    \n[65] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.38       \n[69] haven_2.4.3"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html",
    "title": "2  Response Scoring",
    "section": "",
    "text": "TODO\nThe purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC3A study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#score-sgc-data",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#score-sgc-data",
    "title": "2  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#backup <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#summarize-by-subject",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#summarize-by-subject",
    "title": "2  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#explore-distributions",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#explore-distributions",
    "title": "2  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"Impasse Condition (blue) yields more correct responses across the entire task\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(pretty_condition~q) + \n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields more correct responses on each item\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>% \ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\", \n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher total absolute scores\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE TEST PHASE\ngf_histogram(~item_test_NABS, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Absolute Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores across the entire task\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(q~pretty_condition) + \n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores on each item\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) + \n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>% \n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\", \n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher cumulative scaled scores\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE TEST PHASE\ngf_histogram(~item_test_SCALED, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Scaled Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Scaled Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\nTODO: INVESTIGATE if some of the scores assigned to 0 should be assigned to -0.5 to balance\nTODO: INVESTIGATE DISTRIBUTIONS of each subscore type\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"Impasse condition (blue) yields fewer Orthogonal and more Triangular responses\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more Triangular responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more positive trending responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"Impasse shifts density toward higher Triagular scores\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"Impasse shifts density toward lower Orthogonal scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#explore-responses",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#explore-responses",
    "title": "2  Response Scoring",
    "section": "EXPLORE RESPONSES",
    "text": "EXPLORE RESPONSES\nIn this section we explore responses given by participants to each particular item in the graph comprehension task, indicate how each response was scored, and what interpretation of the graph is indicated by different responses.\n\nScaffold Phase\nThe first five questions constitute the ‘scaffold’ (or learning) phase, where participants see a different version of the stimulus (specifically a different dataset is visualized) invoking a different experimental condition.\n\nQuestion #1\n\nQ1. Control Condition\nWe start by exploring the range of response options checked by participants on Question 1, for those assigned to the control (non-impasse) condition (condition = 111).\n\n\n\nFigure 2.1: Question 1 — Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==1)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q1 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q1 Control Condition :  Which shift(s) start at 11 am?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n    Z \n  \n  \n    Orthgonal \n    A \n    OI \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    CF \n    Z \n  \n  \n    Tversky [start diagonal] \n    F \n    Z \n  \n  \n    Tversky [end diagonal] \n    C \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nHere we summarize the distinct response options given by participants on this item. Each letter in response indicates a checkbox selected by the participant (See Figure 2.1 ). n indicates the number of participants who gave this response, while interpretation indicates the graph interpretation most consistent with that response. At the right of this table are the Absolute, followed by Partial Credit subscores for each response. NA indicates that there is no score calculated (occurs when there is no subset of response options that accord with that interpretation for this question).\nNotice that for this Question, the Triangular answer is the same as the Tversky [start diagonal] answer. In fact, for most questions, one of the Tversky sub-types will match the correct response.\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #1 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 1 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 1) %>% \n  pack_rows(\"Lines-Connect\", 2, 2) %>% \n  pack_rows(\"Orthogonal\", 3, 3) %>% \n  pack_rows(\"Other\", 4, 4)  %>% \n  pack_rows(\"Unknown\", 5, 7)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #1 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    22 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.083 \n    1.0 \n  \n  Lines-Connect\n\n    CF \n    3 \n    Tversky \n    0 \n    0.923 \n    1.000 \n    NA \n    -0.167 \n    0.5 \n  \n  Orthogonal\n\n    A \n    129 \n    Orthogonal \n    0 \n    -0.077 \n    -0.071 \n    NA \n    1.000 \n    -1.0 \n  \n  Other\n\n    AF \n    1 \n    ? \n    0 \n    0.923 \n    0.923 \n    NA \n    0.917 \n    -0.5 \n  \n  Unknown\n\n    DIJ \n    1 \n    ? \n    0 \n    -0.231 \n    -0.214 \n    NA \n    -0.167 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.077 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    0.000 \n    0.000 \n    NA \n    -0.083 \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nWe see that nearly all of the subjects selected a response consistent with one of the identified interpretations. Responses that do not accord with any interpretation are indicated as ? .\n\n\n\n\n\n\n\nWhich shifts start at 11am?\n\n\n\n\n\n\nResponse: A\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, projecting an invisible orthogonal line upward, and locating data point A.\n\n\n\n\nResponse: F\n\nindicates the triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following the right-diagonal gridline, identifying data point F.\n\n\n\n\nResponse: C, F\n\nindicates a maximal-Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following both the right-diagonal and left-diagonal gridlines, identifying both datapoints F and C.\n\n\n\n\nResponse: A , F\n\nThe reader selects both triangular and orthogonal-consistent data points\nPossibly indicates uncertainty or confusion\n\n\n\n\nThree responses were given that were not consistent with any of the identified interpretations. Note that options highlighted in light grey are considered within the range of ‘visual error’, defined by 0.5hr offset from the interpretation-specific projection.\n\n\n\n\n\n\n\n\nD I J\nX\nZ\nTODO find this person, did they subsequently give tri answers?\n\n\n\n\n\n\n\n\n\n\n\n\nQ1. Impasse Condition\nNext we explore the range of response options checked by participants on Question 1, for those assigned to the control (non-impasse) condition (condition = 111).\n\n\n\nFigure 2.2: Question 1 — Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==1)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q1 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q1 Impasse Condition :  Which shift(s) start at 11 am?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    O \n     \n  \n  \n    Satisficing [right] \n    AI \n     \n  \n  \n    Tversky [maximal] \n    CF \n     \n  \n  \n    Tversky [start diagonal] \n    F \n     \n  \n  \n    Tversky [end diagonal] \n    C \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nNotice that there is no orthogonal answer for this question. This is the purpose of the impasse condition, to remove the possibility of selecting the orthogonal answer, we expect learners will be more likely to restructure their understanding of the coordinate system, and arrive at a correct (triangular) interpretation.\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #1 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 1 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 1) %>% \n  pack_rows(\"Lines-Connect\", 2, 4) %>% \n  pack_rows(\"Satisfice\", 5, 9) %>% \n  pack_rows(\"Other\", 10, 10) %>% \n  pack_rows(\"Unknown\", 11, 12) %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #1 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    49 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.071 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    CF \n    14 \n    Tversky \n    0 \n    0.929 \n    1.000 \n    -0.143 \n    NA \n    0.5 \n  \n  \n    C \n    3 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    -0.071 \n    NA \n    0.5 \n  \n  \n    CO \n    1 \n    Tversky \n    0 \n    -0.143 \n    0.929 \n    0.929 \n    NA \n    0.5 \n  \n  Satisfice\n\n    O \n    28 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AI \n    9 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    A \n    4 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    AO \n    2 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    0.929 \n    NA \n    -1.0 \n  \n  \n    I \n    2 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  Other\n\n     \n    57 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  Unknown\n\n    E \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.071 \n    NA \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.071 \n    NA \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nWe see that nearly all of the subjects selected a response consistent with one of the identified interpretations. Responses that do not accord with any interpretation are indicated as ? .\nTODO ADJUST ‘both’ to select for both tri/satisfice or both tri/orth\n\n\n\n\n\n\n\nWhich shifts start at 11am?\n\n\n\n\n\n\nResponse: F\n\nindicates the triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following the right-diagonal gridline, identifying data point F.\n\n\n\n\nResponse: [C, F]\n\nindicates a maximal-Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (11am) on the x-axis, and following both the right-diagonal and left-diagonal gridlines, identifying both datapoints F and C gridline.\n\n\n\n\nResponses: [AOI]\n\nindicates a satisficing strategy\nConsistent with the reader identifying the datapoints nearest to the orthogonal projection from the reference point point\n\n\n\n\nTwo responses were given that were not consistent with any of the identified interpretations.\n\n\n\n\n\n\n[E],[X]\n\n\n\n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==1)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q1 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==1)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q1 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #2\n\nQ2. Control Condition\n\n\n\nFigure 2.3: Q2—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==2)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q2 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q2 Control Condition :  Which shift(s) start at the same time as D?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    K \n    Z \n  \n  \n    Orthgonal \n    E \n    G \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    AKJX \n    Z \n  \n  \n    Tversky [start diagonal] \n    AK \n    Z \n  \n  \n    Tversky [end diagonal] \n    X \n     \n  \n  \n    Tversky [duration line] \n    J \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #2 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 2 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 4) %>% \n  pack_rows(\"Orthogonal\", 5, 7) %>%\n  pack_rows(\"Other\", 8, 8)  %>% \n  pack_rows(\"Unknown\", 9, 10)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #2 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    K \n    24 \n    Triangular \n    1 \n    1.000 \n    0.500 \n    NA \n    -0.083 \n    1.0 \n  \n  \n    DK \n    1 \n    Triangular \n    1 \n    1.000 \n    0.500 \n    NA \n    -0.083 \n    1.0 \n  \n  Lines-Connect\n\n    J \n    4 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    NA \n    -0.083 \n    0.5 \n  \n  \n    AK \n    1 \n    Tversky \n    0 \n    0.917 \n    1.000 \n    NA \n    -0.167 \n    0.5 \n  \n  Orthogonal\n\n    E \n    121 \n    Orthogonal \n    0 \n    -0.083 \n    -0.077 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    DE \n    3 \n    Orthogonal \n    0 \n    -0.083 \n    -0.077 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EG \n    1 \n    Orthogonal \n    0 \n    -0.167 \n    -0.154 \n    NA \n    1.000 \n    -1.0 \n  \n  Other\n\n    D \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    B \n    1 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    NA \n    -0.083 \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\nAgain, we see that most subjects selected a response consistent with one of the identified interpretations. (note, when the question stem includes a data point rather than time as reference, we do not penalize respondents for selecting the reference data point in addition to an interpretation consistent response. For example, in this question, we do not penalize respondents for selecting option D, the reference point in the question. )\n\n\n\n\n\n\n\nWhich shift(s) start at the same time as D?\n\n\n\n\nReponse: E (also EG, DE)\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (D) on the graph, projecting an invisible orthogonal line through it, and locating data point E.\n\n\n\n\nResponse: K (also KD)\n\nindicates an triangular (correct) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (D) on the graph, and following its descending-leftward diagonal gridline, and locating data point K.\n\n\n\n\nResponse: AK\n\nindicates an Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (D) on the graph, and following its descending-leftward diagonal gridline, and locating data point K then continuing along the connecting ascending leftward diagonal locating data point A.\n\n\n\n\nResponse: J\n\nindicates an Tversky strategy following connecting lines\nConsistent with the reader identifying the reference point (D) on the graph, and following its horizontal gridline to the y-axis, locating data point J.\n\n\n\n\nResponse: D\n\nthe reader selected only the reference point\nConsistent with the reader identifying the reference point (D) on the graph\nPossibly indicates uncertainty or confusion\n\n\n\n\n\n\n\n\n\n\n\nB\nC\n\n\n\n\n\n\n\n\n\n\n\nQ2. Impasse Condition\n\n\n\nFigure 2.4: Q2—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==2)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q2 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q2 Impasse Condition :  Which shift(s) start at the same time as D?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    K \n    Z \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n    G \n     \n  \n  \n    Tversky [maximal] \n    JKE \n    Z \n  \n  \n    Tversky [start diagonal] \n    K \n    Z \n  \n  \n    Tversky [end diagonal] \n    E \n     \n  \n  \n    Tversky [duration line] \n    J \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #2 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 2 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 10) %>% \n  pack_rows(\"Satisfice\", 11, 12) %>%\n  pack_rows(\"Other\", 13, 16)  %>% \n  pack_rows(\"Unknown\", 17, 18)  %>% \n  footnote(general = \"n = number of responses in sample\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nFrequency of Selected Response Options for Question #2 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    K \n    69 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    DK \n    1 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    J \n    12 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    EK \n    3 \n    Tversky \n    0 \n    0.917 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    EX \n    2 \n    Tversky \n    0 \n    -0.167 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BEG \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.846 \n    0.846 \n    NA \n    0.5 \n  \n  \n    E \n    1 \n    Tversky \n    0 \n    -0.083 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    EKX \n    1 \n    Tversky \n    0 \n    0.833 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    HJZ \n    1 \n    Tversky \n    0 \n    -0.167 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    JK \n    1 \n    Tversky \n    0 \n    0.917 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  Satisfice\n\n    G \n    19 \n    Satisfice \n    0 \n    -0.083 \n    -0.077 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    BG \n    2 \n    Satisfice \n    0 \n    -0.167 \n    -0.154 \n    0.923 \n    NA \n    -1.0 \n  \n  Other\n\n    D \n    7 \n    reference \n    0 \n    0.000 \n    NA \n    0.000 \n    NA \n    0.0 \n  \n  \n     \n    43 \n    blank \n    0 \n    0.000 \n    NA \n    0.000 \n    NA \n    0.0 \n  \n  \n    ACDFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.250 \n    0.250 \n    -0.846 \n    NA \n    -0.5 \n  \n  \n    BEGKUZ \n    1 \n    frenzy \n    0 \n    0.667 \n    0.667 \n    0.615 \n    NA \n    -0.5 \n  \n  Unknown\n\n    C \n    6 \n    ? \n    0 \n    -0.083 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    FO \n    1 \n    ? \n    0 \n    -0.167 \n    -0.154 \n    -0.154 \n    NA \n    -0.5 \n  \n\n\nNote:   n = number of responses in sample\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==2)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q2 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==2)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q2 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #3\n\nQ3. Control Condition\n\n\n\nFigure 2.5: Q3—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==3)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q3 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q3 Control Condition :  Which shift(s) begin when C ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n    Z \n  \n  \n    Orthgonal \n    Z \n    FIO \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    AUBFOJ \n     \n  \n  \n    Tversky [start diagonal] \n    OJ \n     \n  \n  \n    Tversky [end diagonal] \n    F \n    Z \n  \n  \n    Tversky [duration line] \n    AUB \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #3 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 3 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 7) %>% \n  pack_rows(\"Orthogonal\", 8, 8) %>% \n  pack_rows(\"Other\", 9, 10) %>% \n  pack_rows(\"Unknown\", 11, 17)  \n\n\n\nFrequency of Selected Response Options for Question #3 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    24 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    0.0 \n    1.0 \n  \n  \n    EFK \n    1 \n    Triangular \n    0 \n    0.833 \n    0.833 \n    NA \n    -0.2 \n    1.0 \n  \n  Lines-Connect\n\n    ABU \n    4 \n    Tversky \n    0 \n    -0.250 \n    1.000 \n    NA \n    -0.3 \n    0.5 \n  \n  \n    O \n    3 \n    Tversky \n    0 \n    -0.083 \n    0.500 \n    NA \n    0.0 \n    0.5 \n  \n  \n    JO \n    2 \n    Tversky \n    0 \n    -0.167 \n    1.000 \n    NA \n    -0.1 \n    0.5 \n  \n  \n    DJO \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.917 \n    NA \n    -0.2 \n    0.5 \n  \n  \n    KO \n    1 \n    Tversky \n    0 \n    -0.167 \n    0.417 \n    NA \n    -0.1 \n    0.5 \n  \n  Orthogonal\n\n    Z \n    94 \n    Orthogonal \n    0 \n    0.000 \n    0.000 \n    NA \n    1.0 \n    -1.0 \n  \n  Other\n\n    C \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.0 \n    0.0 \n  \n  \n    ABDEFGHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.000 \n    NA \n    NA \n    0.0 \n    -0.5 \n  \n  Unknown\n\n    A \n    18 \n    ? \n    0 \n    -0.083 \n    0.333 \n    NA \n    -0.1 \n    -0.5 \n  \n  \n    K \n    3 \n    ? \n    0 \n    -0.083 \n    -0.083 \n    NA \n    -0.1 \n    -0.5 \n  \n  \n    AH \n    1 \n    ? \n    0 \n    -0.167 \n    0.242 \n    NA \n    -0.2 \n    -0.5 \n  \n  \n    DE \n    1 \n    ? \n    0 \n    -0.167 \n    -0.167 \n    NA \n    -0.2 \n    -0.5 \n  \n  \n    E \n    1 \n    ? \n    0 \n    -0.083 \n    -0.083 \n    NA \n    -0.1 \n    -0.5 \n  \n  \n    EU \n    1 \n    ? \n    0 \n    -0.167 \n    0.242 \n    NA \n    -0.2 \n    -0.5 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.083 \n    0.333 \n    NA \n    -0.1 \n    -0.5 \n  \n\n\n\n\n\nTODO\n\naddress RESPONSE FKE which is classified as Triangular but doesn’t seem to fit this interpretation?\nShould O,K be considered Tvresky ?\nconsider adding trapdoor on n_q, such that score is penalized (OR interpretation is not predicted?) if the Ss selects more than 1 extra options, or is missing more than 2 options?\nLEFT OFF HERE\n\n\n\n\n\n\n\n\nWhat shift(s) begin when C ends?\n\n\n\n\n\n\nResponse: Z\n\nindicates an orthogonal (incorrect) interpretation of the coordinate system\nConsistent with the reader identifying the reference point (C) then using the duration encoded on the y-axis (2) , project along the horizontal gridline by two hours, and then project an invisible orthogonal line through that time (12PM) locating data point Z.\n\n\n\n\nResponse: F\n\nindicates a (correct) triangular interpretation of the coordinate system\nConsistent with the reader identifying the reference point (C) on the graph, and following the descending gridline to the x-axis to identify the end-time (11AM) and then following the ascending gridline to identify datapoints starting at 11AM and locating data point F.\n\n\n\n\nResponse: AUB (also A)\n\nindicates a Tversky strategy following connecting lines (duration)\nConsistent with the reader identifying the reference point (C) on the graph, and following the horizontal y-axis gridline and locating data points A U B.\n\n\n\n\nResponse: OJ\n\nindicates a Tversky strategy following connecting lines (start-time)\nConsistent with the reader identifying the reference point (C) on the graph, and following the ascending diagonal gridline and locating data points O J.\n\n\n\n\nResponse: C\n\nthe participant selected the point referenced in the question\npossibly indicates confusion or uncertainty\n\n\n\n\nResponse: AIOZFHJXKUDEGB\n\nthe participant selects all (or nearly all) the data points\npossibly indicates confusion or uncertainty\n\n\n\n\nSix responses (from 9 participants) appear inconsistent with any interpretation.\n\n\n\n\n\n\n\n\nK (n=3)\nAH (n=1)\nDE (n=1)\n\n\n\n\n\n\n\n\n\nUE (n=1)\nU (n=1)\nE (n=1)\n\n\n\n\n\n\n\n\n\n\nQ3. Impasse Condition\n\n\n\nFigure 2.6: Q3—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==3)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q3 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q3 Impasse Condition :  Which shift(s) begin when C ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    F \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    AI \n     \n  \n  \n    Satisficing [right] \n    F \n     \n  \n  \n    Tversky [maximal] \n    BJ \n     \n  \n  \n    Tversky [start diagonal] \n    J \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n    B \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nTODO investigate these responses 17 at O?\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #3 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 3 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 5) %>% \n  pack_rows(\"Lines-Connect\", 3, 5) %>% \n  pack_rows(\"Satisfice\", 6, 15) %>% \n  pack_rows(\"Other\", 16, 21) %>% \n  pack_rows(\"Unknown\", 22, 29) \n\n\n\nFrequency of Selected Response Options for Question #3 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    F \n    61 \n    Triangular \n    1 \n    1.000 \n    -0.077 \n    1.000 \n    NA \n    1.0 \n  \n  \n    AF \n    5 \n    Triangular \n    0 \n    0.923 \n    -0.154 \n    0.923 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    AFG \n    1 \n    Triangular \n    0 \n    0.846 \n    -0.231 \n    0.846 \n    NA \n    1.0 \n  \n  \n    B \n    8 \n    Tversky \n    0 \n    -0.077 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    J \n    3 \n    Tversky \n    0 \n    -0.077 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  Satisfice\n\n    BE \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.923 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BJ \n    1 \n    Tversky \n    0 \n    -0.154 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    HJZ \n    1 \n    Tversky \n    0 \n    -0.231 \n    0.846 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    A \n    7 \n    Satisfice \n    0 \n    -0.077 \n    -0.077 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    AH \n    5 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    0.417 \n    NA \n    -1.0 \n  \n  \n    AI \n    3 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AOU \n    3 \n    Satisfice \n    0 \n    -0.231 \n    -0.231 \n    0.333 \n    NA \n    -1.0 \n  \n  \n    AFI \n    2 \n    Satisfice \n    0 \n    0.846 \n    -0.231 \n    0.917 \n    NA \n    -1.0 \n  \n  \n    AIO \n    2 \n    Satisfice \n    0 \n    -0.231 \n    -0.231 \n    0.917 \n    NA \n    -1.0 \n  \n  \n    AO \n    2 \n    Satisfice \n    0 \n    -0.154 \n    -0.154 \n    0.417 \n    NA \n    -1.0 \n  \n  Other\n\n    C \n    2 \n    reference \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  \n     \n    36 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    0.0 \n  \n  \n    ABDEFGHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.000 \n    0.000 \n    NA \n    NA \n    -0.5 \n  \n  \n    ABDEFGHJKUZ \n    1 \n    frenzy \n    0 \n    0.231 \n    0.250 \n    0.231 \n    NA \n    -0.5 \n  \n  \n    BDEFGHJKU \n    1 \n    frenzy \n    0 \n    0.385 \n    0.417 \n    0.385 \n    NA \n    -0.5 \n  \n  \n    BDEFGHJKUXZ \n    1 \n    frenzy \n    0 \n    0.231 \n    0.250 \n    0.231 \n    NA \n    -0.5 \n  \n  Unknown\n\n    O \n    17 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    DK \n    2 \n    ? \n    0 \n    -0.154 \n    -0.154 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    FJZ \n    1 \n    ? \n    0 \n    0.846 \n    0.846 \n    0.846 \n    NA \n    -0.5 \n  \n  \n    K \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    KO \n    1 \n    ? \n    0 \n    -0.154 \n    -0.154 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    -0.077 \n    -0.077 \n    -0.077 \n    NA \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==3)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q3 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==3)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q3 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #4\n[PLACEHOLDER — NOT YET CONSIDERED THIS QUESTION]\n\nQ4. Control Condition\n\n\n\nFigure 2.7: Q4—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==4)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q4 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q4 Control Condition :  Which shift(s) end at 4 pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    H \n     \n  \n  \n    Orthgonal \n    U \n    OF \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    BH \n     \n  \n  \n    Tversky [start diagonal] \n    B \n     \n  \n  \n    Tversky [end diagonal] \n    H \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #4 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 4 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 3) %>% \n  pack_rows(\"Orthogonal\", 4, 8) %>% \n  pack_rows(\"Other\", 9, 10) %>% \n  pack_rows(\"Unknown\", 11, 16) \n\n\n\nFrequency of Selected Response Options for Question #4 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    H \n    29 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.083 \n    1.0 \n  \n  \n    AH \n    1 \n    Triangular \n    0 \n    0.929 \n    0.929 \n    NA \n    -0.167 \n    1.0 \n  \n  Lines-Connect\n\n    B \n    3 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    NA \n    -0.083 \n    0.5 \n  \n  Orthogonal\n\n    U \n    87 \n    Orthogonal \n    0 \n    -0.071 \n    -0.071 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    FU \n    2 \n    Orthogonal \n    0 \n    -0.143 \n    -0.143 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    DEOU \n    1 \n    Orthogonal \n    0 \n    -0.286 \n    -0.286 \n    NA \n    0.833 \n    -1.0 \n  \n  \n    DEU \n    1 \n    Orthogonal \n    0 \n    -0.214 \n    -0.214 \n    NA \n    0.833 \n    -1.0 \n  \n  \n    KU \n    1 \n    Orthogonal \n    0 \n    -0.143 \n    -0.143 \n    NA \n    0.917 \n    -1.0 \n  \n  Other\n\n     \n    6 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n    ACFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.286 \n    0.286 \n    NA \n    0.333 \n    -0.5 \n  \n  Unknown\n\n    DE \n    14 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    NA \n    -0.167 \n    -0.5 \n  \n  \n    E \n    6 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    O \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    0.000 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n  \n    G \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    NA \n    -0.083 \n    -0.5 \n  \n\n\n\n\n\n\nTBL4 test\n\n\n\n\n\n\nOrthogonal\nOrthogonal-LinesConnecting\n\n\n\n\n |\n\n\n\nIf the subject calculates end time for each data point (using duration on the y axis), they find that an (incorrect) projection of point U ‘end time’ intersects with the (incorrect) orthogonal projection of 4:00PM.\nAlternatively, some subjects selected points E and D which intersect with an orthogonal projection from 4:00pm. We call this an ’orthogonal-lines connect” strategy, because it (incorrectly) adapts the orthogonal procedure for finding events that start at 4:00pm in order to find those that end at 4:00pm, thus selecting any data point with an orthogonal intersection with 4:00pm.\n\n\n\n\n\nQ4. Impasse Condition\n\n\n\nFigure 2.8: Q4—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==4)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q4 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q4 Impasse Condition :  Which shift(s) end at 4 pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    H \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    FO \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    BH \n     \n  \n  \n    Tversky [start diagonal] \n    B \n     \n  \n  \n    Tversky [end diagonal] \n    H \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\nTODO investigate D? add to tversky or orth?\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #4 (Impasse Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 4 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>% \n  pack_rows(\"Lines-Connect\", 3, 6) %>% \n  pack_rows(\"Satisfice\", 7, 10) %>% \n  pack_rows(\"Other\", 11, 12) %>% \n  pack_rows(\"Unknown\", 13, 19) \n\n\n\nFrequency of Selected Response Options for Question #4 (Impasse Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    H \n    64 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    DH \n    1 \n    Triangular \n    0 \n    0.929 \n    0.929 \n    -0.154 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    B \n    6 \n    Tversky \n    0 \n    -0.071 \n    1.000 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    BD \n    2 \n    Tversky \n    0 \n    -0.143 \n    0.929 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BH \n    2 \n    Tversky \n    0 \n    0.929 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    BDEG \n    1 \n    Tversky \n    0 \n    -0.286 \n    0.786 \n    -0.308 \n    NA \n    0.5 \n  \n  Satisfice\n\n    O \n    11 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    F \n    8 \n    Satisfice \n    0 \n    -0.071 \n    -0.071 \n    0.500 \n    NA \n    -1.0 \n  \n  \n    FO \n    7 \n    Satisfice \n    0 \n    -0.143 \n    -0.143 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    AFG \n    1 \n    Satisfice \n    0 \n    -0.214 \n    -0.214 \n    0.346 \n    NA \n    -1.0 \n  \n  Other\n\n     \n    20 \n    blank \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n    ACFHIJKOUZ \n    1 \n    frenzy \n    0 \n    0.357 \n    0.357 \n    0.385 \n    NA \n    -0.5 \n  \n  Unknown\n\n    D \n    35 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    A \n    5 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    K \n    3 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    G \n    2 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    AI \n    1 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    DK \n    1 \n    ? \n    0 \n    -0.143 \n    -0.143 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    J \n    1 \n    ? \n    0 \n    -0.071 \n    -0.071 \n    -0.077 \n    NA \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==4)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q4 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==4)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q4 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\nQuestion #5\n\nQ5. Control Condition\n\n\n\nFigure 2.9: Q5—Control Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == \"DEFAULT\") %>% filter(Q==5)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q5 Control Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q5 Control Condition :  Coffee breaks happen halfway through a shift. Which shift(s) share a break with I?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    O \n    AZ \n  \n  \n    Orthgonal \n    U \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    UGX \n    AZKD \n  \n  \n    Tversky [start diagonal] \n    X \n     \n  \n  \n    Tversky [end diagonal] \n    UG \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #5 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 5 & condition == 111) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>% \n  pack_rows(\"Lines-Connect\", 5, 7) %>% \n  pack_rows(\"Orthogonal\", 8, 9) %>% \n  pack_rows(\"Other\", 10, 11) %>% \n  pack_rows(\"Unknown\", 12, 22) \n\n\n\nFrequency of Selected Response Options for Question #5 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    O \n    50 \n    Triangular \n    1 \n    1.000 \n    -0.077 \n    NA \n    -0.077 \n    1.0 \n  \n  \n    FO \n    3 \n    Triangular \n    0 \n    0.909 \n    -0.154 \n    NA \n    -0.154 \n    1.0 \n  \n  \n    HO \n    1 \n    Triangular \n    0 \n    0.909 \n    -0.154 \n    NA \n    -0.154 \n    1.0 \n  \n  \n    KO \n    1 \n    Triangular \n    0 \n    0.909 \n    -0.143 \n    NA \n    -0.154 \n    1.0 \n  \n  Lines-Connect\n\n    FG \n    2 \n    Tversky \n    0 \n    -0.182 \n    0.417 \n    NA \n    -0.154 \n    0.5 \n  \n  \n    G \n    1 \n    Tversky \n    0 \n    -0.091 \n    0.500 \n    NA \n    -0.077 \n    0.5 \n  \n  \n    X \n    1 \n    Tversky \n    0 \n    -0.091 \n    1.000 \n    NA \n    -0.077 \n    0.5 \n  \n  Orthogonal\n\n    U \n    64 \n    Orthogonal \n    0 \n    -0.091 \n    0.500 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    HU \n    1 \n    Orthogonal \n    0 \n    -0.182 \n    0.417 \n    NA \n    0.923 \n    -1.0 \n  \n  Other\n\n    I \n    1 \n    reference \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n     \n    6 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    F \n    10 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    H \n    3 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    B \n    2 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    DJ \n    2 \n    ? \n    0 \n    -0.182 \n    -0.143 \n    NA \n    -0.154 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.091 \n    0.000 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.091 \n    -0.077 \n    NA \n    -0.077 \n    -0.5 \n  \n  \n    DEHJ \n    1 \n    ? \n    0 \n    -0.364 \n    -0.308 \n    NA \n    -0.308 \n    -0.5 \n  \n  \n    FK \n    1 \n    ? \n    0 \n    -0.182 \n    -0.143 \n    NA \n    -0.154 \n    -0.5 \n  \n  \n    HJ \n    1 \n    ? \n    0 \n    -0.182 \n    -0.154 \n    NA \n    -0.154 \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    0.000 \n    0.000 \n    NA \n    -0.077 \n    -0.5 \n  \n\n\n\n\n\nTODO note the compelling cases of internal inconsistency (HJDE)\n\n\nQ5. Impasse Condition\n\n\n\nFigure 2.10: Q5—Impasse Condition\n\n\n\n\nCODE\nq <- keys_raw %>% filter(condition == 121) %>% filter(Q==5)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist() \noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\", \n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q5 Impasse Condition : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>% \n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nAnswer Key | Q5 Impasse Condition :  Coffee breaks happen halfway through a shift. Which shift(s) share a break with I?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    A \n     \n  \n  \n    Orthgonal \n     \n     \n  \n  \n    Satisficing [left] \n    K \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    OX \n     \n  \n  \n    Tversky [start diagonal] \n    OX \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  AIKGXJDBCHUZOFE\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #5 (Control Condition)\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 5 & condition == 121) %>% group_by(response) %>% \n  dplyr::summarise( count = n(), \n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI), \n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>% \n  arrange(interpretation, desc(count)) %>% \n  select(response, count, interpretation, nice, \n         triangular, tversky, satisficing, orthogonal, scaled) %>% \n  kbl(caption = title, col.names = names) %>%  kable_classic() %>% \n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 7) %>% \n  pack_rows(\"Lines-Connect\", 8, 13) %>% \n  pack_rows(\"Orthogonal\", 14, 16) %>% \n  pack_rows(\"Other\", 17, 21) %>% \n  pack_rows(\"Unknown\", 22, 31) \n\n\n\nFrequency of Selected Response Options for Question #5 (Control Condition)\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    A \n    83 \n    Triangular \n    1 \n    1.000 \n    -0.083 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    AFG \n    5 \n    Triangular \n    0 \n    0.846 \n    -0.250 \n    -0.231 \n    NA \n    1.0 \n  \n  \n    AF \n    4 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AO \n    2 \n    Triangular \n    0 \n    0.923 \n    0.417 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AI \n    1 \n    Triangular \n    1 \n    1.000 \n    -0.083 \n    -0.077 \n    NA \n    1.0 \n  \n  \n    AU \n    1 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  \n    AZ \n    1 \n    Triangular \n    0 \n    0.923 \n    -0.167 \n    -0.154 \n    NA \n    1.0 \n  \n  Lines-Connect\n\n    O \n    6 \n    Tversky \n    0 \n    -0.077 \n    0.500 \n    -0.077 \n    NA \n    0.5 \n  \n  \n    CO \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.417 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    JO \n    1 \n    Tversky \n    0 \n    -0.154 \n    0.417 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    OX \n    1 \n    Tversky \n    0 \n    -0.154 \n    1.000 \n    -0.154 \n    NA \n    0.5 \n  \n  \n    UXZ \n    1 \n    Tversky \n    0 \n    -0.231 \n    0.333 \n    -0.231 \n    NA \n    0.5 \n  \n  \n    X \n    1 \n    Tversky \n    0 \n    -0.077 \n    0.500 \n    -0.077 \n    NA \n    0.5 \n  \n  Orthogonal\n\n    K \n    5 \n    Satisfice \n    0 \n    -0.077 \n    -0.083 \n    1.000 \n    NA \n    -1.0 \n  \n  \n    HK \n    3 \n    Satisfice \n    0 \n    -0.154 \n    -0.167 \n    0.923 \n    NA \n    -1.0 \n  \n  \n    HKUZ \n    1 \n    Satisfice \n    0 \n    -0.308 \n    -0.333 \n    0.769 \n    NA \n    -1.0 \n  \n  Other\n\n    I \n    2 \n    reference \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n     \n    24 \n    blank \n    0 \n    0.000 \n    0.000 \n    0.000 \n    NA \n    0.0 \n  \n  \n    ABCFGUZ \n    1 \n    frenzy \n    0 \n    0.538 \n    -0.583 \n    -0.538 \n    NA \n    -0.5 \n  \n  \n    ACDEFHIJKOUXZ \n    1 \n    frenzy \n    0 \n    0.154 \n    0.167 \n    0.154 \n    NA \n    -0.5 \n  \n  \n    FHJKX \n    1 \n    frenzy \n    0 \n    -0.385 \n    0.167 \n    0.692 \n    NA \n    -0.5 \n  \n  Unknown\n\n    H \n    11 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    C \n    2 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    DJ \n    2 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    F \n    2 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    FU \n    2 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    DG \n    1 \n    ? \n    0 \n    -0.154 \n    -0.167 \n    -0.154 \n    NA \n    -0.5 \n  \n  \n    FHZ \n    1 \n    ? \n    0 \n    -0.231 \n    -0.250 \n    -0.231 \n    NA \n    -0.5 \n  \n  \n    U \n    1 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n  \n    Z \n    1 \n    ? \n    0 \n    -0.077 \n    -0.083 \n    -0.077 \n    NA \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==5)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q5 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==5)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q5 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\n\n\nTesting Phase\nThe following 10 questions were the same for both conditions.\n\nQuestion #6 NONDISCRIM\n\n\n\nFigure 2.11: Q6-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==6)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) are six hours long?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    EG \n     \n  \n  \n    Orthgonal \n    EG \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\nTODO discuss non discriminant\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #6\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 6) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) \n\n\n\nFrequency of Selected Response Options for Question #6\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  \n    EG \n    330 \n    both tri + orth \n    1 \n    1 \n    NA \n    NA \n    1 \n    0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q ==6)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q6 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q ==6)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q6 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #7\n\n\n\nFigure 2.12: Q7-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==7)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which 2 shifts less than 5 hours long start at the same time?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    OX \n     \n  \n  \n    Orthgonal \n    FB \n    M \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    IJZNCHOX \n     \n  \n  \n    Tversky [start diagonal] \n    OX \n     \n  \n  \n    Tversky [end diagonal] \n    IJZN \n     \n  \n  \n    Tversky [duration line] \n    CH \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #7\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 7) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 5) %>%\n  pack_rows(\"Lines-Connect\", 6, 9) %>%\n  pack_rows(\"Orthogonal\", 10, 13) %>%\n  pack_rows(\"Other\", 14, 14) %>%\n  pack_rows(\"Unknown\", 15, 17)\n\n\n\nFrequency of Selected Response Options for Question #7\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    OX \n    93 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MO \n    2 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    AX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MOX \n    1 \n    Triangular \n    0 \n    0.938 \n    0.938 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    MX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.438 \n    NA \n    -0.067 \n    1.0 \n  \n  Lines-Connect\n\n    IJ \n    3 \n    Tversky \n    0 \n    -0.125 \n    0.500 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    CH \n    1 \n    Tversky \n    0 \n    -0.125 \n    1.000 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    DJNX \n    1 \n    Tversky \n    0 \n    0.312 \n    0.357 \n    NA \n    -0.267 \n    0.5 \n  \n  \n    HK \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.438 \n    NA \n    -0.133 \n    0.5 \n  \n  Orthogonal\n\n    BF \n    203 \n    Orthogonal \n    0 \n    -0.125 \n    -0.125 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    FZ \n    16 \n    Orthogonal \n    0 \n    -0.125 \n    0.179 \n    NA \n    0.433 \n    -1.0 \n  \n  \n    B \n    1 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    1 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    0.500 \n    -1.0 \n  \n  Other\n\n     \n    2 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    GK \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.125 \n    0.179 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    KM \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 7)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q7 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 7)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q7 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #8\n\n\n\nFigure 2.13: Q8-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==8)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q: \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q:  Which shift(s) under 7 hours long starts before B starts, and ends after X ends?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    G \n     \n  \n  \n    Orthgonal \n    E \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #8\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 8) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 10) %>%\n  pack_rows(\"Orthogonal\", 11, 16) %>%\n  pack_rows(\"Other\", 17, 21) %>%\n  pack_rows(\"Unknown\", 22, 45)\n\n\n\nFrequency of Selected Response Options for Question #8\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    G \n    64 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    -0.067 \n    1.0 \n  \n  \n    AGK \n    4 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1.0 \n  \n  \n    CG \n    3 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    FG \n    3 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    AG \n    2 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    CFGO \n    2 \n    Triangular \n    0 \n    0.800 \n    NA \n    NA \n    -0.267 \n    1.0 \n  \n  \n    ACGP \n    1 \n    Triangular \n    0 \n    0.800 \n    NA \n    NA \n    -0.267 \n    1.0 \n  \n  \n    CFG \n    1 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1.0 \n  \n  \n    CGM \n    1 \n    Triangular \n    0 \n    0.867 \n    NA \n    NA \n    -0.200 \n    1.0 \n  \n  \n    GM \n    1 \n    Triangular \n    0 \n    0.933 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  Orthogonal\n\n    E \n    157 \n    Orthogonal \n    0 \n    -0.067 \n    NA \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EIJ \n    5 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.867 \n    -1.0 \n  \n  \n    EFIJ \n    3 \n    Orthogonal \n    0 \n    -0.267 \n    NA \n    NA \n    0.800 \n    -1.0 \n  \n  \n    EF \n    2 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.933 \n    -1.0 \n  \n  \n    EI \n    2 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.933 \n    -1.0 \n  \n  \n    EFI \n    1 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.867 \n    -1.0 \n  \n  Other\n\n     \n    12 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    DEHIJNOZ \n    2 \n    frenzy \n    0 \n    -0.533 \n    NA \n    NA \n    0.533 \n    -0.5 \n  \n  \n    EFGIJ \n    2 \n    frenzy \n    0 \n    0.733 \n    NA \n    NA \n    0.733 \n    -0.5 \n  \n  \n    CDGHLNOXZ \n    1 \n    frenzy \n    0 \n    0.533 \n    NA \n    NA \n    -0.533 \n    -0.5 \n  \n  \n    DEIJN \n    1 \n    frenzy \n    0 \n    -0.333 \n    NA \n    NA \n    0.733 \n    -0.5 \n  \n  Unknown\n\n    IJ \n    17 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    I \n    7 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    EFG \n    3 \n    ? \n    0 \n    0.867 \n    NA \n    NA \n    0.867 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    O \n    3 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    A \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    AK \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    C \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DN \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    F \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    IJM \n    2 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    L \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    M \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CM \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    CX \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DHNZ \n    1 \n    ? \n    0 \n    -0.267 \n    NA \n    NA \n    -0.267 \n    -0.5 \n  \n  \n    DIJN \n    1 \n    ? \n    0 \n    -0.267 \n    NA \n    NA \n    -0.267 \n    -0.5 \n  \n  \n    EFGI \n    1 \n    ? \n    0 \n    0.800 \n    NA \n    NA \n    0.800 \n    -0.5 \n  \n  \n    HLO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    IO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    KL \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 8)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q8 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 8)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q8 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #9 NONDISCRIM\n\n\n\nFigure 2.14: Q9-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==9)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) begins before J begins and ends during B?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    I \n     \n  \n  \n    Orthgonal \n    I \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #9\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q ==9) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Other\", 1, 2) %>%\n  pack_rows(\"Unknown\", 3, 19)\n\n\n\nFrequency of Selected Response Options for Question #9\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Other\n\n    I \n    247 \n    both tri + orth \n    1 \n    1.000 \n    NA \n    NA \n    1.000 \n    0.5 \n  \n  \n    IJ \n    1 \n    both tri + orth \n    1 \n    1.000 \n    NA \n    NA \n    1.000 \n    0.5 \n  \n  Unknown\n\n     \n    23 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    E \n    29 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    F \n    6 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    M \n    4 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    EI \n    3 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    FI \n    3 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    AGN \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    B \n    1 \n    ? \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CHO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DK \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    IM \n    1 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    IO \n    1 \n    ? \n    0 \n    0.933 \n    NA \n    NA \n    0.933 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 9)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q9 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 9)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q9 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #10\n\n\n\nFigure 2.15: Q10-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==10)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) end at the same time as F?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    E \n     \n  \n  \n    Orthgonal \n    X \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    EGZ \n     \n  \n  \n    Tversky [start diagonal] \n    G \n     \n  \n  \n    Tversky [end diagonal] \n    E \n     \n  \n  \n    Tversky [duration line] \n    Z \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #10\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 10) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 2) %>%\n  pack_rows(\"Lines-Connect\", 3, 7) %>%\n  pack_rows(\"Orthogonal\", 8, 11) %>%\n  pack_rows(\"Other\", 12, 14) %>%\n  pack_rows(\"Unknown\", 15, 27)\n\n\n\nFrequency of Selected Response Options for Question #10\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    E \n    103 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    EF \n    1 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  Lines-Connect\n\n    Z \n    23 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    XZ \n    2 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    0.938 \n    0.5 \n  \n  \n    CG \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    -0.125 \n    0.5 \n  \n  \n    G \n    1 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    HLPZ \n    1 \n    Tversky \n    0 \n    -0.250 \n    0.812 \n    NA \n    -0.250 \n    0.5 \n  \n  Orthogonal\n\n    X \n    139 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BX \n    2 \n    Orthogonal \n    0 \n    -0.125 \n    -0.125 \n    NA \n    0.938 \n    -1.0 \n  \n  \n    FX \n    2 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    AMX \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    -0.188 \n    NA \n    0.875 \n    -1.0 \n  \n  Other\n\n    F \n    1 \n    reference \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n     \n    6 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    CEGIO \n    1 \n    frenzy \n    0 \n    0.750 \n    0.750 \n    NA \n    -0.312 \n    -0.5 \n  \n  Unknown\n\n    B \n    27 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    J \n    6 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    IJ \n    2 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    P \n    2 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    BO \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    HLP \n    1 \n    ? \n    0 \n    -0.188 \n    -0.188 \n    NA \n    -0.188 \n    -0.5 \n  \n  \n    I \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    JM \n    1 \n    ? \n    0 \n    -0.125 \n    -0.125 \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    K \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    L \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    O \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 10)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q10 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 10)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q10 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #11\n\n\n\nFigure 2.16: Q11-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==11)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) start at 12pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    ML \n     \n  \n  \n    Orthgonal \n    FB \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #11\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 11) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>%\n  pack_rows(\"Orthogonal\", 5, 9) %>%\n  pack_rows(\"Other\", 10, 12) %>%\n  pack_rows(\"Unknown\", 13, 17)\n\n\n\nFrequency of Selected Response Options for Question #11\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    LM \n    99 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    -0.125 \n    1.0 \n  \n  \n    M \n    7 \n    Triangular \n    0 \n    0.500 \n    NA \n    NA \n    -0.062 \n    1.0 \n  \n  \n    BLM \n    2 \n    Triangular \n    0 \n    0.938 \n    NA \n    NA \n    0.375 \n    1.0 \n  \n  \n    EKM \n    1 \n    Triangular \n    0 \n    0.375 \n    NA \n    NA \n    -0.188 \n    1.0 \n  \n  Orthogonal\n\n    BF \n    201 \n    Orthogonal \n    0 \n    -0.125 \n    NA \n    NA \n    1.000 \n    -1.0 \n  \n  \n    B \n    4 \n    Orthogonal \n    0 \n    -0.062 \n    NA \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    2 \n    Orthogonal \n    0 \n    -0.062 \n    NA \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BFXZ \n    1 \n    Orthogonal \n    0 \n    -0.250 \n    NA \n    NA \n    0.875 \n    -1.0 \n  \n  \n    BH \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    NA \n    NA \n    0.438 \n    -1.0 \n  \n  Other\n\n     \n    4 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  \n    ACDGHKLMNOPXZ \n    1 \n    frenzy \n    0 \n    0.312 \n    NA \n    NA \n    -0.812 \n    -0.5 \n  \n  \n    DHLMNOXZ \n    1 \n    frenzy \n    0 \n    0.625 \n    NA \n    NA \n    -0.500 \n    -0.5 \n  \n  Unknown\n\n    J \n    2 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    CX \n    1 \n    ? \n    0 \n    -0.125 \n    NA \n    NA \n    -0.125 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.062 \n    NA \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    XZ \n    1 \n    ? \n    0 \n    -0.125 \n    NA \n    NA \n    -0.125 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 11)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q11 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 11)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q11 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #12\n\n\n\nFigure 2.17: Q12-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==12)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) start at the same time as F?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    G \n     \n  \n  \n    Orthgonal \n    B \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    GZ \n     \n  \n  \n    Tversky [start diagonal] \n    G \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n    Z \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #12\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 12) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 3) %>%\n  pack_rows(\"Lines-Connect\", 4, 6) %>%\n  pack_rows(\"Orthogonal\", 7, 8) %>%\n  pack_rows(\"Other\", 9, 10) %>%\n  pack_rows(\"Unknown\", 11, 14)\n\n\n\nFrequency of Selected Response Options for Question #12\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    G \n    98 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    FG \n    3 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.062 \n    1.0 \n  \n  \n    GP \n    1 \n    Triangular \n    0 \n    0.938 \n    0.938 \n    NA \n    -0.125 \n    1.0 \n  \n  Lines-Connect\n\n    Z \n    4 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.062 \n    0.5 \n  \n  \n    BZ \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.938 \n    NA \n    0.938 \n    0.5 \n  \n  \n    B \n    206 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  Orthogonal\n\n    BF \n    5 \n    Orthogonal \n    0 \n    -0.062 \n    -0.062 \n    NA \n    1.000 \n    -1.0 \n  \n  \n     \n    3 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Other\n\n    CEGIO \n    1 \n    frenzy \n    0 \n    0.750 \n    0.750 \n    NA \n    -0.312 \n    -0.5 \n  \n  \n    J \n    3 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  Unknown\n\n    E \n    2 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    FM \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n  \n    X \n    1 \n    ? \n    0 \n    -0.062 \n    -0.062 \n    NA \n    -0.062 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 12)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q12 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 12)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q12 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #13\n\n\n\nFigure 2.18: Q13-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==13)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which 2 shifts end when Z begins?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    EF \n     \n  \n  \n    Orthgonal \n    FX \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n     \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #13\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 13) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 3) %>%\n  pack_rows(\"Orthogonal\", 4, 13) %>%\n  pack_rows(\"Other\", 14, 14) %>%\n  pack_rows(\"Unknown\", 15, 36)\n\n\n\nFrequency of Selected Response Options for Question #13\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    EF \n    91 \n    Triangular \n    1 \n    1.000 \n    NA \n    NA \n    0.433 \n    1.0 \n  \n  \n    CE \n    1 \n    Triangular \n    0 \n    0.433 \n    NA \n    NA \n    -0.133 \n    1.0 \n  \n  \n    E \n    1 \n    Triangular \n    0 \n    0.500 \n    NA \n    NA \n    -0.067 \n    1.0 \n  \n  Orthogonal\n\n    FX \n    141 \n    Orthogonal \n    0 \n    0.433 \n    NA \n    NA \n    1.000 \n    -1.0 \n  \n  \n    X \n    9 \n    Orthogonal \n    0 \n    -0.067 \n    NA \n    NA \n    0.500 \n    -1.0 \n  \n  \n    OX \n    4 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    KX \n    3 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    ACX \n    1 \n    Orthogonal \n    0 \n    -0.200 \n    NA \n    NA \n    0.367 \n    -1.0 \n  \n  \n    BX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    CX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    DJNX \n    1 \n    Orthogonal \n    0 \n    -0.267 \n    NA \n    NA \n    0.300 \n    -1.0 \n  \n  \n    GX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  \n    JX \n    1 \n    Orthogonal \n    0 \n    -0.133 \n    NA \n    NA \n    0.433 \n    -1.0 \n  \n  Other\n\n     \n    5 \n    blank \n    0 \n    0.000 \n    NA \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    HN \n    13 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    BF \n    11 \n    ? \n    0 \n    0.433 \n    NA \n    NA \n    0.433 \n    -0.5 \n  \n  \n    F \n    10 \n    ? \n    0 \n    0.500 \n    NA \n    NA \n    0.500 \n    -0.5 \n  \n  \n    EX \n    6 \n    ? \n    0 \n    0.433 \n    NA \n    NA \n    0.433 \n    -0.5 \n  \n  \n    HL \n    5 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    HLP \n    5 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    BM \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    CO \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    DN \n    2 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    AG \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    BO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    C \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CG \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    CGO \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    CH \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    D \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DKM \n    1 \n    ? \n    0 \n    -0.200 \n    NA \n    NA \n    -0.200 \n    -0.5 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    HZ \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    LP \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    NO \n    1 \n    ? \n    0 \n    -0.133 \n    NA \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    NZ \n    1 \n    ? \n    0 \n    -0.067 \n    NA \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 13)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q13 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 13)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q13 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #14\n\n\n\nFigure 2.19: Q14-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==14)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Which shift(s) end at 3pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    X \n     \n  \n  \n    Orthgonal \n    B \n     \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    XJND \n     \n  \n  \n    Tversky [start diagonal] \n     \n     \n  \n  \n    Tversky [end diagonal] \n    X \n     \n  \n  \n    Tversky [duration line] \n    JND \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #14\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 14) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 4) %>%\n  pack_rows(\"Orthogonal\", 5, 7) %>%\n  pack_rows(\"Other\", 8, 9) %>%\n  pack_rows(\"Unknown\", 10, 22)\n\n\n\nFrequency of Selected Response Options for Question #14\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    X \n    107 \n    Triangular \n    1 \n    1.000 \n    1.000 \n    NA \n    -0.059 \n    1.0 \n  \n  \n    FX \n    2 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  \n    EX \n    1 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  \n    OX \n    1 \n    Triangular \n    0 \n    0.941 \n    0.941 \n    NA \n    -0.118 \n    1.0 \n  \n  Orthogonal\n\n    B \n    150 \n    Orthogonal \n    0 \n    -0.059 \n    -0.059 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BF \n    12 \n    Orthogonal \n    0 \n    -0.118 \n    -0.118 \n    NA \n    0.941 \n    -1.0 \n  \n  \n    BIO \n    2 \n    Orthogonal \n    0 \n    -0.176 \n    -0.176 \n    NA \n    0.882 \n    -1.0 \n  \n  Other\n\n     \n    29 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  \n    O \n    5 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  Unknown\n\n    F \n    3 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    G \n    3 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    A \n    2 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    BX \n    2 \n    ? \n    0 \n    0.941 \n    0.941 \n    NA \n    0.941 \n    -0.5 \n  \n  \n    HLP \n    2 \n    ? \n    0 \n    -0.176 \n    -0.176 \n    NA \n    -0.176 \n    -0.5 \n  \n  \n    K \n    2 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    AH \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    DHO \n    1 \n    ? \n    0 \n    -0.176 \n    0.200 \n    NA \n    -0.176 \n    -0.5 \n  \n  \n    FG \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    HL \n    1 \n    ? \n    0 \n    -0.118 \n    -0.118 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    IJ \n    1 \n    ? \n    0 \n    -0.118 \n    0.267 \n    NA \n    -0.118 \n    -0.5 \n  \n  \n    M \n    1 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n  \n    P \n    1 \n    ? \n    0 \n    -0.059 \n    -0.059 \n    NA \n    -0.059 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 14)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q14 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 14)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q14 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nQuestion #15\n\n\n\nFigure 2.20: Q15-Question\n\n\n\n\nCODE\nq <- keys_raw %>% filter(Q==15)\nignore <- q %>% select(\"REF_POINT\")\nanswers <- q %>% select(\"TRIANGULAR\", \"ORTHOGONAL\", \"SATISFICE_left\", \"SATISFICE_right\",\"TV_max\",\"TV_start\", \"TV_end\", \"TV_dur\") %>% unlist()\nves <- q %>% mutate(\n  SATISFICE_left_allow = \"\",\n  SATISFICE_right_allow = \"\"\n) %>% select(\"TRI_allow\", \"ORTH_allow\", \"SATISFICE_left_allow\",\"SATISFICE_right_allow\", \"TV_max_allow\",\"TV_start_allow\",\"TV_end_allow\", \"TV_dur_allow\")%>% unlist()\noptions <- q %>% select(\"OPTIONS\")\nquestion = q %>%  select(\"TEXT\")\nscores <- c(\"Triangular\", \"Orthgonal\", \"Satisficing [left]\", \"Satisficing [right]\", \"Tversky [maximal]\", \"Tversky [start diagonal]\",\n            \"Tversky [end diagonal]\", \"Tversky [duration line]\")\nd = tibble(interpretation = scores, answer = answers, allowed=ves)\nd$answer <- replace_na(d$answer, \"\")\nd$allowed <- replace_na(d$allowed, \"\")\n\ntitle = paste(\"Answer Key | Q : \", question)\ncols = c(\"interpretation\", \"answer\",\"not penalized\")\n\nd %>% kbl(caption = title, col.names = cols) %>% kable_classic() %>%\n  footnote(general = paste(\"15 response options: \", options), general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nAnswer Key | Q :  Coffee breaks happen halfway through a shift. Which shifts share a break at 2pm?\n \n  \n    interpretation \n    answer \n    not penalized \n  \n \n\n  \n    Triangular \n    XK \n     \n  \n  \n    Orthgonal \n    EF \n    B \n  \n  \n    Satisficing [left] \n     \n     \n  \n  \n    Satisficing [right] \n     \n     \n  \n  \n    Tversky [maximal] \n    XKZ \n     \n  \n  \n    Tversky [start diagonal] \n    Z \n     \n  \n  \n    Tversky [end diagonal] \n     \n     \n  \n  \n    Tversky [duration line] \n     \n     \n  \n\n\nNote:   15 response options:  ABCDEFGHIJKLMNOPZX\n\n\n\n\n\n\nCODE\ntitle <- \"Frequency of Selected Response Options for Question #15\"\nnames = c(\"response\",\"n\",\"interpretation\",\"absolute\",\"tri\",\"tversky\",\"satisfice\",\"orthogonal\", \"scaled score\")\n\ndf_items %>% filter(q == 15) %>% group_by(response) %>%\n  dplyr::summarise( count = n(),\n                    nice = unique(score_niceABS),\n                    triangular = unique(score_TRI),\n                    orthogonal =  unique(score_ORTH),\n                    satisficing =  unique(score_SATISFICE),\n                    tversky = unique(score_TVERSKY),\n                    interpretation = unique(int2),\n                    scaled = unique(score_SCALED)) %>%\n  arrange(interpretation, desc(count)) %>%\n  select(response, count, interpretation, nice,\n         triangular, tversky, satisficing, orthogonal, scaled) %>%\n  kbl(caption = title, col.names = names) %>%  kable_classic() %>%\n  add_header_above(c(\" \" = 3, \"Strict Score\" = 1, \"Interpretation Scores\"=4, \"Discriminant\"=1)) %>%\n  pack_rows(\"Triangular\", 1, 10) %>%\n  pack_rows(\"Lines-Connect\", 11, 13) %>%\n  pack_rows(\"Orthogonal\", 14, 22) %>%\n  pack_rows(\"Other\", 23, 23) %>%\n  pack_rows(\"Unknown\", 24, 44)\n\n\n\nFrequency of Selected Response Options for Question #15\n \n\n\nStrict Score\nInterpretation Scores\nDiscriminant\n\n  \n    response \n    n \n    interpretation \n    absolute \n    tri \n    tversky \n    satisfice \n    orthogonal \n    scaled score \n  \n \n\n  Triangular\n\n    KX \n    100 \n    Triangular \n    1 \n    1.000 \n    0.667 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    X \n    6 \n    Triangular \n    0 \n    0.500 \n    0.333 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    CX \n    2 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    DJNX \n    2 \n    Triangular \n    0 \n    0.312 \n    0.133 \n    NA \n    -0.267 \n    1.0 \n  \n  \n    AKPX \n    1 \n    Triangular \n    0 \n    0.875 \n    0.533 \n    NA \n    -0.267 \n    1.0 \n  \n  \n    CK \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    GK \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    JX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  \n    K \n    1 \n    Triangular \n    0 \n    0.500 \n    0.333 \n    NA \n    -0.067 \n    1.0 \n  \n  \n    LX \n    1 \n    Triangular \n    0 \n    0.438 \n    0.267 \n    NA \n    -0.133 \n    1.0 \n  \n  Lines-Connect\n\n    FZ \n    3 \n    Tversky \n    0 \n    -0.125 \n    0.941 \n    NA \n    0.433 \n    0.5 \n  \n  \n    OZ \n    1 \n    Tversky \n    0 \n    -0.125 \n    0.941 \n    NA \n    -0.133 \n    0.5 \n  \n  \n    Z \n    1 \n    Tversky \n    0 \n    -0.062 \n    1.000 \n    NA \n    -0.067 \n    0.5 \n  \n  Orthogonal\n\n    EF \n    118 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    BF \n    17 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    F \n    13 \n    Orthogonal \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    E \n    8 \n    Orthogonal \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BE \n    4 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.500 \n    -1.0 \n  \n  \n    BEF \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    -0.176 \n    NA \n    1.000 \n    -1.0 \n  \n  \n    EFZ \n    1 \n    Orthogonal \n    0 \n    -0.188 \n    0.882 \n    NA \n    0.933 \n    -1.0 \n  \n  \n    EI \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.433 \n    -1.0 \n  \n  \n    FI \n    1 \n    Orthogonal \n    0 \n    -0.125 \n    -0.118 \n    NA \n    0.433 \n    -1.0 \n  \n  Other\n\n     \n    11 \n    blank \n    0 \n    0.000 \n    0.000 \n    NA \n    0.000 \n    0.0 \n  \n  Unknown\n\n    G \n    4 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    B \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    0.000 \n    -0.5 \n  \n  \n    C \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    O \n    3 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    AG \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    BM \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    CG \n    2 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    M \n    2 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    A \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    BG \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    DN \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    FK \n    1 \n    ? \n    0 \n    0.438 \n    0.267 \n    NA \n    0.433 \n    -0.5 \n  \n  \n    FX \n    1 \n    ? \n    0 \n    0.438 \n    0.267 \n    NA \n    0.433 \n    -0.5 \n  \n  \n    H \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    HN \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    HO \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    I \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    IJ \n    1 \n    ? \n    0 \n    -0.125 \n    -0.118 \n    NA \n    -0.133 \n    -0.5 \n  \n  \n    J \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    L \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n  \n    N \n    1 \n    ? \n    0 \n    -0.062 \n    -0.059 \n    NA \n    -0.067 \n    -0.5 \n  \n\n\n\n\n\n\n\nCODE\ngf_dhistogram(~ score_niceABS, fill = ~condition, data = df_items %>% filter(q == 15)) %>% \n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Scaled Item Score\", title = \"Distribution of Scaled Scores | Q15 \") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_props(~interpretation, fill = ~condition, data = df_items %>% filter(q == 15)) %>%\n  gf_facet_grid( condition ~ ., labeller = label_both) + \n  labs( x = \"Interpretation\", title = \"Distribution of Interpretations | Q15 \") + \n  theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#export",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#export",
    "title": "2  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC3A/data/2-scored-data/sgc3a_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC3A/data/2-scored-data/sgc3a_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures\n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#resources",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#resources",
    "title": "2  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\nset operations\nhttps://stat.ethz.ch/R-manual/R-devel/library/base/html/sets.html\nkableExtra tables\nhttps://haozhu233.github.io/kableExtra/awesome_table_in_html.html#grouped_columns__rows\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     \n [5] readr_2.1.2      tidyr_1.2.0      tibble_3.1.7     tidyverse_1.3.1 \n [9] Hmisc_4.7-0      Formula_1.2-4    survival_3.3-1   lattice_0.20-45 \n[13] pbapply_1.5-0    ggformula_0.10.1 ggridges_0.5.3   scales_1.2.0    \n[17] ggstance_0.3.5   ggplot2_3.3.6    kableExtra_1.3.4\n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            bit64_4.0.5         lubridate_1.8.0    \n [4] webshot_0.5.3       RColorBrewer_1.1-3  httr_1.4.3         \n [7] tools_4.2.1         backports_1.4.1     utf8_1.2.2         \n[10] R6_2.5.1            rpart_4.1.16        DBI_1.1.3          \n[13] colorspace_2.0-3    nnet_7.3-17         withr_2.5.0        \n[16] tidyselect_1.1.2    gridExtra_2.3       curl_4.3.2         \n[19] bit_4.0.4           compiler_4.2.1      cli_3.3.0          \n[22] rvest_1.0.2         htmlTable_2.4.0     xml2_1.3.3         \n[25] labeling_0.4.2      mosaicCore_0.9.0    checkmate_2.1.0    \n[28] systemfonts_1.0.4   digest_0.6.29       foreign_0.8-82     \n[31] rmarkdown_2.14      svglite_2.1.0       rio_0.5.29         \n[34] base64enc_0.1-3     jpeg_0.1-9          pkgconfig_2.0.3    \n[37] htmltools_0.5.2     labelled_2.9.1      highr_0.9          \n[40] dbplyr_2.2.1        fastmap_1.1.0       readxl_1.4.0       \n[43] htmlwidgets_1.5.4   rlang_1.0.3         rstudioapi_0.13    \n[46] farver_2.1.0        generics_0.1.2      jsonlite_1.8.0     \n[49] vroom_1.5.7         zip_2.2.0           magrittr_2.0.3     \n[52] Matrix_1.4-1        Rcpp_1.0.8.3        munsell_0.5.0      \n[55] fansi_1.0.3         lifecycle_1.0.1     stringi_1.7.6      \n[58] yaml_2.3.5          MASS_7.3-57         plyr_1.8.7         \n[61] grid_4.2.1          parallel_4.2.1      crayon_1.5.1       \n[64] haven_2.5.0         splines_4.2.1       hms_1.1.1          \n[67] knitr_1.39          pillar_1.7.0        codetools_0.2-18   \n[70] reprex_2.0.1        glue_1.6.2          evaluate_0.15      \n[73] latticeExtra_0.6-29 data.table_1.14.2   modelr_0.1.8       \n[76] tzdb_0.3.0          png_0.1-7           vctrs_0.4.1        \n[79] tweenr_1.0.2        cellranger_1.1.0    gtable_0.3.0       \n[82] polyclip_1.10-0     assertthat_0.2.1    openxlsx_4.2.5     \n[85] xfun_0.31           ggforce_0.3.3       broom_0.8.0        \n[88] viridisLite_0.4.0   cluster_2.1.3       ellipsis_0.3.2"
  },
  {
    "objectID": "analysis/SGC3A/2_sgc3A_scoring.html#archive",
    "href": "analysis/SGC3A/2_sgc3A_scoring.html#archive",
    "title": "2  Response Scoring",
    "section": "ARCHIVE",
    "text": "ARCHIVE\nPrior versions of functions for for-loop version of scoring, not optimized to use mapply\n\n\nCODE\n# #CALCULATE THE TRIANGULAR, ORTHOGONAL OR TVERSKIAN SUBSCORES FROM KEYFRAME\n# calc_sub_score <- function(question, cond, response,keyframe){\n# \n#   #STEP 1 GET KEY\n#   if (question < 6) #for q1 - q5 find key for question by condition\n#   {\n#     # print(keyframe)\n#     #GET KEY FOR THIS SCORE TYPE, QUESTION AND CONDITION\n#     p =  keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#     q =  keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% unlist()\n#     pn = keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(n_p)\n#     qn = keyframe %>% filter(Q == question) %>% filter(condition == cond) %>% select(n_q)\n# \n#   } else {\n#     #GET KEY FOR THIS SCORE TYPE, QUESTION\n#     p =  keyframe %>% filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#     q =  keyframe %>% filter(Q == question) %>% select(set_q) %>% pull(set_q) %>% str_split(\"\") %>% unlist()\n#     pn = keyframe %>% filter(Q == question) %>% select(n_p)\n#     qn = keyframe %>% filter(Q == question) %>% select(n_q)\n#   }\n# \n#   #STEP 2 CALC INTERSECTIONS BETWEEN RESPONSE AND KEY\n#   \n#   #if response is not empty, split apart response for set comparison\n#     if(response != \"\")\n#     { response = response %>% str_split(\"\") %>% unlist()}\n#     \n#   ps = length(intersect(response,p))\n#   qs = length(intersect(response,q))\n#   # df_items[x,'tri_ps'] = tri_ps\n#   # df_items[x,'tri_qs'] = tri_qs\n# \n#   #STEP 3 CALC f_partialP schema SCORE FOR THIS INTERSECTION\n#   x = f_partialP(ps,pn,qs,qn) %>% unlist() %>% as.numeric()\n#   \n#   #cleanup\n#   rm(p,q,pn,qn,ps,qs)\n#   return(x)\n# \n# }\n# \n# #CALCULATE THE REFERENCE SCORES\n# calc_ref_score <- function(question, cond, response){\n#   \n#     #1. GET reference point from REF_POINT column in raw keys\n#     ref_p = keys_raw %>% filter(Q == question) %>% filter(condition == cond) %>% select(REF_POINT) %>% pull(REF_POINT) %>% str_split(\"\") %>% unlist()\n#      \n#     #2. if response has more than one character, it can't be correct\n#     #there is only ever 1 reference character\n#     n = nchar(response)\n#     if (n == 0) {x = 0}\n#     else if(n>1) {x = 0}\n#     else {\n#       #3 is the response PRECISELY the REFERENCE POINT?\n#       x = ref_p == response\n#       x = as.numeric(x)  \n#     }\n#     \n#     #cleanup\n#     rm(ref_p, response, question, cond)   \n#     return(x) #1 = match, 0 = not match\n# }\n# \n# \n# #CALCULATE SCORE BASED ON UNION OF ORTH & TRI (SUBJECT SELECTS BOTH ANSWERS )\n# calc_both_score <- function(question, cond, response){\n#   \n#TRAPDOOR \n#   #since no orth responses exist for impasse condition q1 - q5, set to 0\n#   if (question < 6 & cond == 121) {x = NA}\n#   \n#   #ELSE \n#   #calculate union of ORTH and TRI\n#   else {\n#     if (question < 6 & cond == 111) #for q1 - q5 find key for question by condition\n#   {\n#      #grab the tri and orth keys for this question as well as N option set\n#      tri_p =  keys_tri %>%  filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      orth_p = keys_orth %>% filter(Q == question) %>% filter(condition == cond) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      set_n =  keys_tri %>%  filter(Q == question) %>% filter(condition == cond) %>% select(set_n) %>% pull(set_n) %>% str_split(\"\") %>% unlist() \n#      #1. calc answer that is both tri and orth and only these --> union of tri_p and orth_p\n#      both_p = union(tri_p, orth_p) #the selection of tri and p\n#      #2. calc answers that should't be selected as diffrence between N [same for all keys] and both_p\n#      both_q = setdiff(set_n,both_p)\n#      both_pn = length(both_p)\n#      both_qn = length(both_q)\n#   } else{\n#     \n#      #grab the tri and orth keys for this question as well as N option set\n#      tri_p =  keys_tri %>%  filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      orth_p = keys_orth %>% filter(Q == question) %>% select(set_p) %>% pull(set_p) %>% str_split(\"\") %>% unlist()\n#      set_n =  keys_tri %>%  filter(Q == question) %>% select(set_n) %>% pull(set_n) %>% str_split(\"\") %>% unlist() \n#      #1. calc answer that is both tri and orth and only these --> union of tri_p and orth_p\n#      both_p = union(tri_p, orth_p) #the selection of tri and p\n#      #2. calc answers that shouldn't be selected as difference between N [same for all keys] and both_p\n#      both_q = setdiff(set_n,both_p)\n#      both_pn = length(both_p)\n#      both_qn = length(both_q)\n#   }\n#     \n#   #STEP 2 CALC INTERSECTIONS BETWEEN RESPONSE AND KEY\n#   \n#   #if response is not empty, split apart response for set comparison\n#     if(response != \"\")\n#     { response = response %>% str_split(\"\") %>% unlist()}\n#   \n#     both_ps = length(intersect(response,both_p))\n#     both_qs = length(intersect(response,both_q))\n#   \n#  \n#   #STEP 3 CALC f_partialP schema SCORE FOR THIS INTERSECTION \n#   x = f_partialP(both_ps,both_pn,both_qs,both_qn)%>% unlist() %>% as.numeric()\n#   \n#   #cleanup\n#   rm(both_p,both_q,both_pn,both_qn,both_ps,both_qs, question, cond, response )   \n#   }\n#   \n#   return(x) #true correct, trues, false correct, falses\n# }\n\n\nLooping to do the scoring (not using MAPPLY)\n\n\nCODE\n#RUN THIS OR THE CALCULATE-SCORES-MAPPLY\n# df_items = trad \n# \n# pb <- timerProgressBar() \n# on.exit(close(pb)) \n#  \n# #CALCULATE SUBSCORES (in loop)\n# \n# for (x in 1:nrow(df_items)) {\n#   \n#   #show progress bar \n#   setTimerProgressBar(pb, x) \n#   \n#   #PREPARE ITEMS FOR SCORING\n#   #sort response vectors alphabetically\n#   #doesn't impact scoring, but does impact response display tables\n#    df_items[x,'response'] <-  df_items[x,'response'] %>% str_split(\"\") %>% unlist() %>% sort() %>% str_c(collapse=\"\")\n# \n#   #get properties of the RESPONSE ITEM\n#   qu = df_items[x,'q'] %>% as.numeric()\n#   cond = as.character(df_items[x,'condition']) %>% as.numeric()\n#   r = df_items[x,'response'] \n# \n#   #calculate the main subscores\n#   df_items[x,'score_TRI'] = calc_sub_score(qu, cond, r,keys_tri)\n#   df_items[x,'score_ORTH'] = calc_sub_score(qu, cond, r,keys_orth)\n#   df_items[x,'score_SATISFICE'] = calc_sub_score(qu, cond, r,keys_satisfice)\n#   df_items[x,'score_TV_max'] = calc_sub_score(qu, cond, r,keys_tversky_max)\n#   df_items[x,'score_TV_start'] = calc_sub_score(qu, cond, r,keys_tversky_start)\n#   df_items[x,'score_TV_end'] = calc_sub_score(qu, cond, r,keys_tversky_end)\n#   df_items[x,'score_TV_duration'] = calc_sub_score(qu, cond, r, keys_tversky_duration)\n#   \n#   #calculate special subscores\n#   df_items[x,'score_REF'] = calc_ref_score(qu, cond, r)\n#   df_items[x,'score_BOTH'] = calc_both_score(qu, cond, r)\n# }\n# \n# #CALCULATE ABSOLUTE SCORES\n# #calculate absolute scores dichotomous\n# df_items$score_ABS = as.integer(df_items$correct)\n# #niceABS indicates if the response is correct without penalizing the allowable triangular options(ie. the ref point)\n# df_items$score_niceABS  <- as.integer((df_items$score_TRI == 1))\n#  \n# #cleanup\n# rm(qu,cond,r, x)\n\n# trad_scored = df_items\n\n\nsanity check equivalence of for-loop and mapply scoring\n\n\nCODE\n#CHECK EQUIVALENCE OF LOOP AND MAPPLY SCORING \n# tests = data.frame (\n#   alt_tri = alt_scored$score_TRI,\n#   trad_tri = trad_scored$score_TRI,\n#   alt_orth = alt_scored$score_ORTH,\n#   trad_orth = trad_scored$score_ORTH,\n#   alt_ref = alt_scored$score_REF,\n#   trad_ref = trad_scored$score_REF,\n#   alt_tv_max = alt_scored$score_TV_max,\n#   trad_tv_max = trad_scored$score_TV_max,\n#   alt_tv_dur = alt_scored$score_TV_duration,\n#   trad_tv_dur = trad_scored$score_TV_duration,\n#   alt_tv_start = alt_scored$score_TV_start,\n#   trad_tv_start = trad_scored$score_TV_start,\n#   alt_tv_end = alt_scored$score_TV_end,\n#   trad_tv_end = trad_scored$score_TV_end,\n#   alt_both = alt_scored$score_BOTH,\n#   trad_both = trad_scored$score_BOTH,\n#   trad_response = trad_scored$response,\n#   alt_response = alt_scored$response,\n#   q_match = trad_scored$q == alt_scored$q,\n#   q = trad_scored$q,\n#   c_match = trad_scored$condition == alt_scored$condition,\n#   condition = trad_scored$condition\n# )\n# \n# tests$tri = tests$alt_tri == tests$trad_tri\n# tests$orth = tests$alt_orth == tests$trad_orth\n# tests$ref = tests$alt_ref == tests$trad_ref\n# tests$tvdur = tests$alt_tv_dur == tests$trad_tv_dur\n# tests$tvstart = tests$alt_tv_start == tests$trad_tv_start\n# tests$tvend = tests$alt_tv_end == tests$trad_tv_end\n# tests$both = tests$alt_both == tests$trad_both\n# \n# #CHECKS \n# unique(tests$tri)\n# unique(tests$orth)\n# unique(tests$ref)\n# unique(tests$tvdur)\n# unique(tests$tvstart)\n# unique(tests$tvend)\n# unique(tests$both)\n# \n# unique(alt_scored$score_ABS == trad_scored$score_ABS)\n# unique(alt_scored$score_niceABS == trad_scored$score_niceABS)\n\n\nPrior inline version of derive interpretation, before externalizing to a function in the scoring script. ::: {.cell hash=‘2_sgc3A_scoring_cache/html/unnamed-chunk-76_bf696e67f090e1aa4600bc72a7fe120f’}\n\nCODE\n# threshold_range = 0.5 #set required variance in subscores to be discriminant\n# threshold_frenzy = 4\n# \n# for (x in 1:nrow(df_items)) {\n#   \n#   #CALCULATE MAX TVERSKY SUBSCORE\n#   t = df_items[x,] %>% select(score_TV_max, score_TV_start, score_TV_end, score_TV_duration) #reshape\n#   t.long = gather(t,score, value, 1:4)\n#   t.long[t.long == \"\"] = NA #replace empty scores with NA so we can ignore them\n#   if(length(unique(t.long$value)) == 1 ){\n#     if(is.na(unique(t.long$value))){\n#       df_items[x,'score_TVERSKY'] = NA\n#       df_items[x,'tv_type'] = NA   \n#     }\n#   } else {\n#     df_items[x,'score_TVERSKY'] = as.numeric(max(t.long$value,na.rm = TRUE))\n#     df_items[x,'tv_type'] = t.long[which.max(t.long$value),'score']\n#   }\n#   \n#   #CALCULATE MAX SATISFICING SUBSCORE\n#   t = df_items[x,] %>% select(score_SAT_left, score_SAT_right)\n#   t.long = gather(t,score, value, 1:2)\n#   t.long[t.long == \"\"] = NA #replace empty scores\n#   if(length(unique(t.long$value)) == 1 ){\n#     if(is.na(unique(t.long$value))){\n#       df_items[x,'score_SATISFICE'] = NA\n#       df_items[x,'sat_type'] = NA   \n#     }\n#   } else {\n#     df_items[x,'score_SATISFICE'] = as.numeric(max(t.long$value,na.rm = TRUE))\n#     df_items[x,'sat_type'] = t.long[which.max(t.long$value),'score']  \n#   }\n#   \n#   #NOW CALCULATE RANGE AMONG SUBSCORES\n#   #order of this selection matters in breaking ties! \n#   t = df_items[x,] %>% select(score_TRI, score_TVERSKY, score_SATISFICE, score_ORTH)\n#   t.long = gather(t,score, value, 1:4)\n#   t.long[t.long == \"\"] = NA\n#   \n#   df_items[x,'top_score'] = as.numeric(max(t.long$value,na.rm = TRUE))\n#   df_items[x,'top_type'] = t.long[which.max(t.long$value),'score']\n#   \n#   #calculate the range between highest and lowest scores \n#   r = as.numeric(range(t.long$value,na.rm = TRUE))\n#   r = diff(r)\n#   df_items[x,'range'] = r\n#   \n#   #DISCRIMINANT BETWEEN SUBSCORES TO PREDICT BEST FIT INTERPRETATION\n#   \n#   if (r < threshold_range) {\n#       #then we can't predict the interpretation, leave it as \"?\"\n#     df_items[x,'best'] = \"?\"\n#   } else {\n#       p =  df_items[x,'top_type']\n#       if (p == \"score_TRI\") {df_items[x,'best'] = \"Triangular\"\n#       } else if(p == \"score_ORTH\") {df_items[x,'best'] = \"Orthogonal\"\n#       } else if(p == \"score_TVERSKY\") {df_items[x,'best'] = \"Tversky\"\n#       } else if(p == \"score_SATISFICE\") {df_items[x,'best'] = \"Satisfice\"}\n#   }\n#   \n#   #CHECK SPECIAL SITUATIONS\n# \n#   #BOTH TRI AND ORTH?  \n#   if (!is.na(df_items[x,'score_BOTH'])) { #only check if both is not null\n#       if( df_items[x,'score_BOTH'] == 1) {\n#         df_items[x,'best'] = \"both tri + orth\"}\n#   }\n#   \n#   #IS BLANK?\n#   if( df_items[x,'num_o'] == 0) {  \n#     df_items[x,'best'] = \"blank\"\n#   }\n#   \n#   #IS FRENZY?\n#   if( df_items[x,'num_o'] > threshold_frenzy) { \n#       df_items[x,'best'] = \"frenzy\"\n#   }\n# \n#   #IS REF POINT?\n#   if (!is.na(df_items[x,'score_REF'])) { #only check if the score is NOT null\n#       if( df_items[x,'score_REF'] == 1) {\n#           df_items[x,'best'] = \"reference\"\n#       }\n#   }\n# \n# }#end loop\n# \n# #cleanup \n# rm(t, t.long, x, r,p)\n# rm(threshold_frenzy, threshold_range)\n# \n# #set order of levels for response exploration table\n# df_items$int2 <- factor(df_items$best,\n#                                   levels = c(\"Triangular\", \"Tversky\",\n#                                              \"Satisfice\", \"Orthogonal\", \"reference\", \"both tri + orth\", \"blank\",\"frenzy\",\"?\"))\n# \n# #set order of levels\n# df_items$interpretation <- factor(df_items$best,\n#                                   levels = c(\"Orthogonal\",\"Satisfice\", \"frenzy\",\"?\",\"reference\",\"blank\",\n#                                                \"both tri + orth\", \"Tversky\",\"Triangular\"))\n# \n# #collapsed representation of scale of interpretations\n# df_items$high_interpretation <- fct_collapse(df_items$interpretation,\n#   orthogonal = c(\"Satisfice\", \"Orthogonal\"),\n#   neg.trans = c(\"frenzy\",\"?\"),\n#   neutral = c(\"reference\",\"blank\"),\n#   pos.trans = c(\"Tversky\",\"both tri + orth\"),\n#   triangular = \"Triangular\"\n# ) \n# \n# #reorder levels\n# df_items$high_interpretation = factor(df_items$high_interpretation, levels= c(\"orthogonal\", \"neg.trans\",\"neutral\",\"pos.trans\",\"triangular\"))\n# \n# #cleanup \n# df_items <- df_items %>% dplyr::select(-best)\n# \n# #recode as numeric inase they are char \n# # df_items$score_TV_duration <- df_items$score_TV_duration %>% as.numeric()\n# # df_items$score_SATISFICE <- df_items$score_SATISFICE %>% as.numeric()\n\n:::\nOld inline calculation of score_SCALED before externalizing as function ::: {.cell hash=‘2_sgc3A_scoring_cache/html/unnamed-chunk-77_c6b29f964635e61613bc77d3d74e2d7a’}\n\nCODE\n# df_items$score_SCALED <- recode(df_items$interpretation,\n#                           \"Orthogonal\" = -1,\n#                           \"Satisfice\" = -1,\n#                           \"frenzy\" = -0.5,\n#                           \"?\" = -0.5,\n#                           \"reference\" = 0,\n#                           \"blank\" = 0, \n#                           \"both tri + orth\" = 0.5,\n#                           \"Tversky\" = 0.5,\n#                           \"Triangular\" = 1)\n\n:::\nOriginal summary by subject before externalizing as function ::: {.cell hash=‘2_sgc3A_scoring_cache/html/unnamed-chunk-78_305d043282cdbe5181a1e8e8665bbe5e’}\n\nCODE\n# #prep items\n# df_items <- df_items %>% mutate(\n#   tv_type = as.factor(tv_type),\n#   top_type = as.factor(top_type)\n# )\n# \n# #summarize SCORES and TIME by subject\n# subjects_summary <- df_items %>% filter(q %nin% c(6,9)) %>% group_by(subject) %>% dplyr::summarise (\n#   subject = as.character(subject),\n#   pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n#   s_TRI = sum(score_TRI,na.rm=TRUE),\n#   s_ORTH = sum(score_ORTH,na.rm=TRUE),\n#   s_TVERSKY = sum(score_TVERSKY,na.rm=TRUE),\n#   s_SATISFICE = sum(score_SATISFICE, na.rm=TRUE),\n#   s_REF = sum(score_REF,na.rm=TRUE),\n#   s_ABS = sum(score_ABS,na.rm=TRUE),\n#   s_NABS = sum(score_niceABS,na.rm=TRUE),\n#   s_SCALED = sum(score_SCALED,na.rm=TRUE),\n#   DV_percent_NABS = s_NABS/13,\n#   rt_m = sum(rt_s)/60,\n#   item_avg_rt = mean(rt_s),\n#   item_min_rt = min(rt_s),\n#   item_max_rt = max(rt_s),\n#   item_n_TRI = sum(interpretation == \"Triangular\"),\n#   item_n_ORTH = sum(interpretation == \"Orthogonal\"),\n#   item_n_TV = sum(interpretation == \"Tversky\"),\n#   item_n_SAT = sum(interpretation == \"Satisfice\"),\n#   item_n_OTHER = sum(interpretation %nin% c(\"Triangular\",\"Orthogonal\",\"Tversky\",\"Satisfice\")),\n#   item_n_POS = sum(high_interpretation == \"pos.trans\"),\n#   item_n_NEG = sum(high_interpretation == \"neg.trans\"),\n#   item_n_NEUTRAL = sum(high_interpretation == \"neutral\")\n# ) %>% arrange(subject) %>% slice(1L)\n# \n# #summarize first scaffold item of interest by subject\n# subjects_q1 <- df_items %>% filter(q == 1) %>% mutate(\n#   item_q1_NABS = score_niceABS,\n#   item_q1_SCALED = score_SCALED,\n#   item_q1_interpretation = interpretation,\n#   item_q1_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q1_NABS, item_q1_SCALED, item_q1_interpretation,item_q1_rt) %>% arrange(subject)\n# \n# #summarize last test item of interest by subject\n# subjects_q5 <- df_items %>% filter(q == 5) %>% mutate(\n#   item_q5_NABS = score_niceABS,\n#   item_q5_SCALED = score_SCALED,\n#   item_q5_interpretation = interpretation,\n#   item_q5_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q5_NABS, item_q5_SCALED, item_q5_interpretation,item_q5_rt) %>% arrange(subject)\n# \n# #summarize first test item of interest by subject\n# subjects_q7 <- df_items %>% filter(q == 7) %>% mutate(\n#   item_q7_NABS = score_niceABS,\n#   item_q7_interpretation = interpretation,\n#   item_q7_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q7_NABS, item_q7_interpretation,item_q7_rt) %>% arrange(subject)\n# \n# #summarize last test item of interest by subject\n# subjects_q15 <- df_items %>% filter(q == 15) %>% mutate(\n#   item_q15_NABS = score_niceABS,\n#   item_q15_interpretation = interpretation,\n#   item_q15_rt = rt_s,\n# ) %>% dplyr::select(subject, item_q15_NABS, item_q15_interpretation,item_q15_rt) %>% arrange(subject)\n# \n# #summarize scaffold phase performance\n# subjects_scaffold <- df_items %>% filter(q<6)  %>% group_by(subject) %>% dplyr::summarise (\n#   item_scaffold_NABS = sum(score_niceABS),\n#   item_scaffold_SCALED = sum(score_SCALED),\n#   item_scaffold_rt = sum(rt_s)\n# )%>% dplyr::select(subject, item_scaffold_NABS, item_scaffold_SCALED, item_scaffold_rt) %>% arrange(subject)\n# \n# #summarize test phase performance\n# subjects_test <- df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% group_by(subject) %>% dplyr::summarise (\n#   item_test_NABS = sum(score_niceABS),\n#   item_test_SCALED = sum(score_SCALED),\n#   item_test_rt = sum(rt_s)\n# )%>% dplyr::select(subject, item_test_NABS, item_test_SCALED, item_test_rt) %>% arrange(subject)\n# \n# #import subjects\n# df_subjects <- read_rds('analysis/SGC3A/data/1-study-level/sgc3a_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n# \n# #SANITY CHECK SUBJECT ORDER BEFORE MERGE; BOTH SHOULD BE TRUE\n# unique(subjects_summary$subject == df_subjects$subject)\n# unique(subjects_summary$subject == subjects_q1$subject)\n# unique(subjects_summary$subject == subjects_q5$subject)\n# unique(subjects_summary$subject == subjects_q7$subject)\n# unique(subjects_summary$subject == subjects_q15$subject)\n# unique(subjects_summary$subject == subjects_scaffold$subject)\n# unique(subjects_summary$subject == subjects_test$subject)\n# \n# #CAREFULLY CHECK THIS — RELIES ON \n# x = merge(df_subjects, subjects_summary)\n# x = merge(x, subjects_q1)\n# x = merge(x, subjects_q5)\n# x = merge(x, subjects_q7)\n# x = merge(x, subjects_q15)\n# x = merge(x, subjects_scaffold)\n# x = merge(x, subjects_test)\n# df_subjects <- x %>% dplyr::select(-absolute_score) #drop absolute score from webapp that includes Q6 and Q9\n# \n# #cleanup\n# rm(subjects_q1, subjects_q5, subjects_q7, subjects_q15, subjects_scaffold, subjects_test, subjects_summary, x)\n\n:::\nSummarize Cummulative Progress versions before functionize\n\n\nCODE\n# #SUMMARIZE-CUMULATIVE ABSOLUTE PROGRESS\n# \n# \n# #filter for valid items\n# x <- df_items %>% filter(q %nin% c(6,9)) %>% dplyr::select(subject,mode, pretty_condition, q,score_niceABS) \n# \n# #pivot wider\n# wide <- x %>% pivot_wider(names_from=q, names_glue = \"q_{q}\", values_from = score_niceABS)\n# \n# #calc stepwise cumulative score\n# wide$c1 = wide$q_1\n# wide$c2 = wide$c1 + wide$q_2\n# wide$c3 = wide$c2 + wide$q_3\n# wide$c4 = wide$c3 + wide$q_4\n# wide$c5 = wide$c4 + wide$q_5\n# wide$c6 = wide$c5 + wide$q_7\n# wide$c7 = wide$c6 + wide$q_8\n# wide$c8 = wide$c7 + wide$q_10\n# wide$c9 = wide$c8 + wide$q_11\n# wide$c10 = wide$c9 + wide$q_12\n# wide$c11 = wide$c10 + wide$q_13\n# wide$c12 = wide$c11 + wide$q_14\n# wide$c13 = wide$c12 + wide$q_15\n# wide <- wide %>% dplyr::select(subject,mode, pretty_condition,c1,c2,c3,c4,c5,c6, c7,c8,c9, c10,c11,c12,c13)\n# \n# #lengthen \n# df_absolute_progress <- wide %>% pivot_longer(cols= c1:c13, names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n# df_absolute_progress$question <- as.integer(df_absolute_progress$question)\n# \n# \n# #cleanup \n# rm(x,wide)\n#   \n# # SUMMARIZE-CUMULATIVE SCALED PROGRESS\n# \n# #filter for valid items\n# x <- df_items %>% filter(q %nin% c(6,9)) %>% select(subject,mode, pretty_condition, q,score_SCALED)\n# \n# #pivot wider\n# wide <- x %>% pivot_wider(names_from=q, names_glue = \"q_{q}\", values_from = score_SCALED)\n# \n# #calc stepwise cumulative score\n# wide$c1 = wide$q_1\n# wide$c2 = wide$c1 + wide$q_2\n# wide$c3 = wide$c2 + wide$q_3\n# wide$c4 = wide$c3 + wide$q_4\n# wide$c5 = wide$c4 + wide$q_5\n# wide$c6 = wide$c5 + wide$q_7\n# wide$c7 = wide$c6 + wide$q_8\n# wide$c8 = wide$c7 + wide$q_10\n# wide$c9 = wide$c8 + wide$q_11\n# wide$c10 = wide$c9 + wide$q_12\n# wide$c11 = wide$c10 + wide$q_13\n# wide$c12 = wide$c11 + wide$q_14\n# wide$c13 = wide$c12 + wide$q_15\n# wide <- wide %>% select(subject,mode, pretty_condition,c1,c2,c3,c4,c5,c6, c7,c8,c9, c10,c11,c12,c13)\n# \n# #lengthen \n# df_scaled_progress <- wide %>% pivot_longer(cols= c1:c13, names_to = \"question\", names_pattern = \"c(.*)\", values_to = \"score\")\n# df_scaled_progress$question <- as.integer(df_scaled_progress$question)\n# \n# #cleanup \n# rm(x,wide)"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html",
    "href": "analysis/SGC3A/3_sgc3A_description.html",
    "title": "3  Description",
    "section": "",
    "text": "The purpose of this notebook is describe the distributions of dependent variables for Study SGC3A."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#sample",
    "href": "analysis/SGC3A/3_sgc3A_description.html#sample",
    "title": "3  Description",
    "section": "SAMPLE",
    "text": "SAMPLE\n\nData Collection\nData was initially collected (in person, SONA groups in computer lab) in Fall 2017. In Spring 2018, additional data were collected after small modifications were made to the experimental platform to increase the size of multiple-choice input buttons, and to add an additional free-response question following the main task block. In Fall 2021, the study was replicated using asynchronous, online SONA pool, with additional participants collected in Winter 2022.\n\n\nCODE\ntitle = \"Participants by Condition and Data Collection Period\"\ncols = c(\"Control Condition\",\"Impasse Condition\",\"Total for Period\")\ncont <- table(df_subjects$pretty_mode, df_subjects$condition)\ncont %>% addmargins() %>% kbl(caption = title, col.names = cols) %>%  kable_classic()\n\n\n\nParticipants by Condition and Data Collection Period\n \n  \n      \n    Control Condition \n    Impasse Condition \n    Total for Period \n  \n \n\n  \n    laboratory \n    62 \n    64 \n    126 \n  \n  \n    online-replication \n    96 \n    108 \n    204 \n  \n  \n    Sum \n    158 \n    172 \n    330 \n  \n\n\n\n\n\n\n\nParticipants\n\n\nCODE\n#Describe participants\nsubject.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(age) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% filter(mode == \"asynch\") %>% dplyr::select(age) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(age) %>% unlist() %>% favstats()\n) \nsubject.stats$percent.female <- c(\n  (df_lab %>%  filter(gender==\"Female\") %>% count())$n/count(df_lab) %>% unlist(),\n  (df_online %>% filter(gender==\"Female\") %>% count())$n/count(df_online) %>% unlist(),\n  (df_subjects %>% filter(gender==\"Female\") %>% count())$n/count(df_subjects) %>% unlist()\n)\n\ntitle = \"Descriptive Statistics of Participant Age and Gender\"\nsubject.stats %>% kbl (caption = title) %>% kable_classic()%>% \n  footnote(general = \"Age in Years\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of Participant Age and Gender\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n    percent.female \n  \n \n\n  \n    lab \n    18 \n    19 \n    20 \n    21 \n    33 \n    20.4 \n    2.12 \n    126 \n    0 \n    0.619 \n  \n  \n    online \n    18 \n    20 \n    20 \n    21 \n    31 \n    20.6 \n    2.00 \n    204 \n    0 \n    0.672 \n  \n  \n    combined \n    18 \n    19 \n    20 \n    21 \n    33 \n    20.5 \n    2.05 \n    330 \n    0 \n    0.652 \n  \n\n\nNote:   Age in Years\n\n\n\n\nFor in-person collection, 126 participants (62 % female ) undergraduate STEM majors at a public American University participated in person in exchange for course credit (age: 18 - 33 years). Participants were randomly assigned to one of two experimental groups.\nFor online replication 204 participants (67 % female ) undergraduate STEM majors at a public American University participated online, asynchronously in exchange for course credit (age: 18 - 31 years). Participants were randomly assigned to one of two experimental groups.\nCombined overall 330 participants (65 % female ) undergraduate STEM majors at a public American University participated in exchange for course credit (age: 18 - 33 years)."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#response-accuracy",
    "href": "analysis/SGC3A/3_sgc3A_description.html#response-accuracy",
    "title": "3  Description",
    "section": "RESPONSE ACCURACY",
    "text": "RESPONSE ACCURACY\n\nSubject Level Scores\nSubject level scores summarize the the response accuracy by a particular participant across all discriminant items in the graph comprehension task.\n\nTest Phase Absolute Score\nRecall from Section 3.1.2.1 that the absolute score (following the dichotomous scoring approach) s_NABS indicates if the subject’s response for a particular item was perfectly correct: whether they selected all correct answer options and no others (excluding certain allowed exceptions, such as also selecting the data point referenced in the question). The absolute score for an individual item is either 0 or 1. When summarized across the entire set of discriminant items, the total absolute score for an individual subject ranges from [0,13]. When summarized across just the test phase (final items following scaffolding phase) scores for an individual subject range from [0,8]. First we examine performance on the test phase (final 8 questions, appears after scaffolding phase). This tells us how the participants perform after exposure to the 5 scaffolding questions (in the impasse condition).\n\n\nCODE\ntitle = \"Descriptive Statistics of TEST PHASE Response Accuracy (Total Absolute Score)\"\nabs.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_test_NABS) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(item_test_NABS) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(item_test_NABS) %>% unlist() %>% favstats()\n) \nabs.stats %>% kbl (caption = title) %>% kable_classic() %>% \n  footnote(general = \"# questions correct [0,8]\", \n           general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of TEST PHASE Response Accuracy (Total Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    0 \n    0 \n    0 \n    6 \n    8 \n    2.53 \n    3.32 \n    126 \n    0 \n  \n  \n    online \n    0 \n    0 \n    0 \n    6 \n    8 \n    2.16 \n    3.19 \n    204 \n    0 \n  \n  \n    combined \n    0 \n    0 \n    0 \n    6 \n    8 \n    2.30 \n    3.24 \n    330 \n    0 \n  \n\n\nNote:   # questions correct [0,8]\n\n\n\n\nFor in person collection, total absolute scores in the TEST phase (n = 126) range from 0 to 8 with a mean score of (M = 2.53, SD = 3.32).\nFor online replication, (online) total absolute scores in the TEST phase (n = 204) range from 0 to 8 with a slightly lower mean score of (M = 2.16, SD = 3.19).\nWhen combined overall, total absolute accuracy scores in the TEST phase (n = 330) range from 0 to 8 with a slightly lower mean score of (M = 2.3, SD = 3.24).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL ABSOLUTE\n  gf_props(~item_test_NABS, data = df_subjects) + \n  labs(x = \"number of correct responses (test phase)\",\n       y = \"% of subjects\",\n       title = \"Distribution of TEST Absolute Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_test_NABS\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of TEST Absolute Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"Total Absolute Score (Test Phase)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_test_NABS,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_NABS),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_NABS, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n# set seed\nset.seed(1234)\n  \n# create a random data frame \nsample_data <- data.frame(x = round(rnorm(700, mean=800, \n                                          sd=450)))\n  \n\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(item_test_NABS)) + \n  stat_ecdf(geom = \"step\") +\n  facet_grid(pretty_condition~pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — TEST Absolute Score \",\n        x = \"Total Absolute Score (Test Phase) [0,8]\", \n        y = \"Cumulative Probability\")\n\n\n\n\n\nCODE\n#NOTE this is clobbered by the shift function imports; so I load those later\n\n\nVisual inspection of this distribution suggests it is not normal, and likely bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018). TODO REFERENCE\n\n\nCODE\nmultimode::modetest(df_subjects$item_test_NABS)\n\n\nWarning in multimode::modetest(df_subjects$item_test_NABS): A modification of\nthe data was made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$item_test_NABS\nExcess mass = 0.1, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$item_test_NABS, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$item_test_NABS,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$item_test_NABS, mod0 = n_modes, : If\nthe density function has an unbounded support, artificial modes may have been\ncreated in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is infact multimodal (m = 0.1, p < 0.001), with two identifiable modes at 0.013 and 7.894, and an antimode at 2.867.\n\n\n\n\n\n\nNote\n\n\n\nCondition appears (through visual inspection) to yield a positive influence on Total Absolute Score in the TEST Phase, across data collection modalities.\n\n\n\n\nTest Phase Scaled Scores\nThe total scaled score s_SCALED summarizes the scaled score on the 13 strategy-discriminant questions, for each subject. This score ranges from from -13 (all orthogonal) to 13 (all triangular). Recall that the s_SCALED score for an item is a numeric representation of the strategy-consistent response, scaled from -1 to +1 (see Section 2.1.4)\nMost importantly, the Scaled score gives us a way of quantitatively examining how correctly a participant interpreted the coordinate system across the entire set of items. It offers a more nuanced look into performance than absolute score.\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total Scaled Score)\"\nscaled.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_test_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(item_test_SCALED) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(item_test_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -8 \n    -8.0 \n    -6.00 \n    6 \n    8 \n    -2.11 \n    6.69 \n    126 \n    0 \n  \n  \n    online \n    -8 \n    -7.5 \n    -5.75 \n    5 \n    8 \n    -2.32 \n    6.29 \n    204 \n    0 \n  \n  \n    combined \n    -8 \n    -8.0 \n    -6.00 \n    6 \n    8 \n    -2.24 \n    6.43 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, TEST phase scaled scores (n = 126) range from -8 to 8 with a mean score of (M = -2.11, SD = 6.69).\nFor online replication, TEST phase scaled scores (n = 204) range from -8 to 8 with a slightly lower mean score of (M = -2.32, SD = 6.29).\nWhen combined overall, TEST phase scaled scores (n = 330) range from -8 to 8 with a slightly lower mean score of (M = -2.24, SD = 6.44).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL SCALED\ngf_props(~item_test_SCALED, data = df_subjects) +\n  labs(x = \"total scaled score (test phase)\",\n       y = \"% of subjects\",\n       title = \"Distribution of TEST Scaled Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_test_SCALED\",binwidth=1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) + \n  labs( title = \"Distribution of TEST Scaled Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"total scaled score (test phase)\", y = \"number of participants\") + \n theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_test_SCALED,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_SCALED),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_SCALED, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(item_test_SCALED)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Test Phase Scaled Score\",\n        x = \"Test Phase Scaled Score [-8,8]\", \n        y = \"Cumulative Probability\") \n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and perhaps perhaps bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018).\n\n\nCODE\nmultimode::modetest(df_subjects$item_test_SCALED)\n\n\nWarning in multimode::modetest(df_subjects$item_test_SCALED): A modification of\nthe data was made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$item_test_SCALED\nExcess mass = 0.2, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$item_test_SCALED, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$item_test_SCALED,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$item_test_SCALED, mod0 = n_modes, :\nIf the density function has an unbounded support, artificial modes may have been\ncreated in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is in fact multimodal (m = 0.1, p < 0.001), with two identifiable modes at -7.721 and 7.822, and an antimode at 1.93.\n\n\n\nFirst Item Scores\nNext we consider the response accuracy on just the first question of the graph comprehension task: a subject’s first exposure to the TM graph.\n\nFirst Item Absolute Score\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Lab)\"\nitem.contingency <- df_lab %>% dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Lab)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    0 \n    0.413 \n    0.357 \n    0.77 \n  \n  \n    1 \n    0.079 \n    0.151 \n    0.23 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.00 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Online)\"\nitem.contingency <- df_online  %>% dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Online)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    0 \n    0.412 \n    0.382 \n    0.794 \n  \n  \n    1 \n    0.059 \n    0.147 \n    0.206 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Response on First Item (Combined)\"\nitem.contingency <- df_subjects %>%  dplyr::select(item_q1_NABS, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Response on First Item (Combined)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    0 \n    0.412 \n    0.373 \n    0.785 \n  \n  \n    1 \n    0.067 \n    0.148 \n    0.215 \n  \n  \n    Sum \n    0.479 \n    0.521 \n    1.000 \n  \n\n\n\n\n\nAcross data collection sessions, first-item accuracy is consistent across experimental conditions. Incorrect answers are far more frequent (78%) than correct answers (22%). Accuracy is somewhat improved in the IMPASSE condition, with roughly 15% of all IMPASSE-condition questions answered correctly, compared to only 7% in the CONTROL condition.\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_props(~item_q1_NABS, data = df_subjects) +\n  labs(x = \"response accuracy\",\n       y = \"% subjects\",\n       title = \"Proportion of Correct Responses on First Item\",\n       subtitle=\"\")+\n  theme(legend.position = \"none\")+theme_ggdist()\n\n\n\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_props(~item_q1_NABS, data = df_subjects, fill = ~pretty_condition) %>% \n  gf_facet_grid(pretty_condition~pretty_mode) +\n  labs(x = \"response accuracy\",\n       title = \"Proportion of Correct Responses on First Item (by Modality and Condition)\",\n       subtitle=\"\")+\n  theme(legend.position = \"none\") \n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Proportion of Correct Responses on First Item\",\n            data = df_subjects, pretty_condition ~ item_q1_NABS, rot_labels=c(0,90,0,0), \n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\"))) \n\n\n\n\n\n\n\nFirst Item Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1. (note: we evaluate scaled_score on the first item rather than interpretation, because no orthogonal interpretation is available in the impasse condition)\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (First Item Scaled Score)\"\nfirstscaled.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(item_q1_SCALED) %>% unlist() %>% favstats()\n) \nfirstscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (First Item Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -1 \n    -1 \n    -1 \n    0.5 \n    1 \n    -0.298 \n    0.849 \n    126 \n    0 \n  \n  \n    online \n    -1 \n    -1 \n    -1 \n    0.5 \n    1 \n    -0.287 \n    0.812 \n    204 \n    0 \n  \n  \n    combined \n    -1 \n    -1 \n    -1 \n    0.5 \n    1 \n    -0.291 \n    0.825 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, first item scaled scores (n = 126) range from -1 to 1 with a mean score of (M = -0.3, SD = 0.85).\nFor online replication, (online) first item scaled scores (n = 204) range from -1 to 1 with a slightly lower mean score of (M = -0.29, SD = 0.81).\nWhen combined overall, first item scaled scores (n = 330) range from -1 to 1 with a slightly lower mean score of (M = -0.29, SD = 0.83).\n\n\nCODE\n#GGFORMULA | PROPORTIONAL HISTOGRAM SUBJECT FIRST SCALED\ngf_props(~item_q1_SCALED, data = df_subjects) +\n  labs(x = \"scaled score (first item)\",\n       y = \"% of subjects\",\n       title = \"Distribution of First Item Scaled Score\",\n       subtitle = \"\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_q1_SCALED\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) + \n  labs( title = \"Distribution of First Item Scaled Score (by Mode and Condition)\",\n        subtitle =\"Impasse condition yields more intermediate scores (indicating uncertainty)\",\n        x = \"scaled score (firt item) \", y = \"number of participants\") + \n  theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n##GGFORMULA | HIST+DENSITY SCORE BY CONDITION/MODE\n# stats = df_subjects %>% group_by(pretty_condition, mode) %>% dplyr::summarise(mean = mean(item_q1_SCALED))\n# gf_density(~item_q1_SCALED, data = df_subjects) %>%\n#   gf_facet_grid(pretty_condition~mode, labeller = label_both) %>%\n#   gf_lims(x = c(-1, 1)) %>%\n#   gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n# labs( title = \"Distribution of First Item Scaled Score (by Mode and Condition)\",\n#         subtitle =\"Pattern of response is the same across data collection modes but differs by condition\",\n#         x = \"scaled score (firt item) \", y = \"number of participants\") + \n#   theme_minimal()\n\n\n\n\n\nInterpretation Scores\nNext we consider the the interpretations assigned to each response. For each response given by a participant to a question, we assign an interpretation label based on the interpretation the response most closely matches (see Section 3.2.3).\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Lab)\"\nitem.contingency <- df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(interpretation, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Lab)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.297 \n    0.116 \n    0.414 \n  \n  \n    Satisfice \n    0.000 \n    0.028 \n    0.028 \n  \n  \n    frenzy \n    0.002 \n    0.005 \n    0.007 \n  \n  \n    ? \n    0.026 \n    0.053 \n    0.079 \n  \n  \n    reference \n    0.001 \n    0.004 \n    0.005 \n  \n  \n    blank \n    0.008 \n    0.034 \n    0.042 \n  \n  \n    both tri + orth \n    0.060 \n    0.056 \n    0.116 \n  \n  \n    Tversky \n    0.004 \n    0.017 \n    0.021 \n  \n  \n    Triangular \n    0.094 \n    0.195 \n    0.288 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Online)\"\nitem.contingency <- df_items %>% filter(mode == \"asynch\") %>% dplyr::select(interpretation, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Online)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.260 \n    0.122 \n    0.382 \n  \n  \n    Satisfice \n    0.000 \n    0.024 \n    0.024 \n  \n  \n    frenzy \n    0.002 \n    0.001 \n    0.003 \n  \n  \n    ? \n    0.050 \n    0.066 \n    0.116 \n  \n  \n    reference \n    0.000 \n    0.002 \n    0.002 \n  \n  \n    blank \n    0.013 \n    0.055 \n    0.068 \n  \n  \n    both tri + orth \n    0.056 \n    0.061 \n    0.117 \n  \n  \n    Tversky \n    0.011 \n    0.023 \n    0.035 \n  \n  \n    Triangular \n    0.078 \n    0.175 \n    0.253 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Interpretations Across Items Items By Condition (Combined)\"\nitem.contingency <- df_items %>%  dplyr::select(interpretation, pretty_condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Interpretations Across Items Items By Condition (Combined)\n \n  \n      \n    control \n    impasse \n    Sum \n  \n \n\n  \n    Orthogonal \n    0.274 \n    0.120 \n    0.394 \n  \n  \n    Satisfice \n    0.000 \n    0.025 \n    0.025 \n  \n  \n    frenzy \n    0.002 \n    0.003 \n    0.004 \n  \n  \n    ? \n    0.041 \n    0.061 \n    0.102 \n  \n  \n    reference \n    0.001 \n    0.002 \n    0.003 \n  \n  \n    blank \n    0.011 \n    0.047 \n    0.058 \n  \n  \n    both tri + orth \n    0.058 \n    0.059 \n    0.117 \n  \n  \n    Tversky \n    0.009 \n    0.021 \n    0.029 \n  \n  \n    Triangular \n    0.084 \n    0.183 \n    0.267 \n  \n  \n    Sum \n    0.479 \n    0.521 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\n#PROPORTIONAL BAR CHART\ngf_propsh(~interpretation, data = df_items, fill = ~pretty_condition) %>% \n  gf_facet_grid(pretty_condition~pretty_mode) +\n  labs(x = \"% of items\",\n       title = \"Proportion of Interpretations Across Items\",\n       subtitle=\"Impasse Condition yields shift from Orthogonal to alternative interpretations\")+\n  theme(legend.position = \"none\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\n# vcd::mosaic(main=\"Proportion of Interpretations across Conditions\",\n#             data = df_items, pretty_condition ~ interpretation, rot_labels=c(0,90,0,0), \n#             offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n#             spacing = spacing_dimequal(unit(1:2, \"lines\"))) \n\n\n\n\nCumulative Task Performance\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#response-latency",
    "href": "analysis/SGC3A/3_sgc3A_description.html#response-latency",
    "title": "3  Description",
    "section": "RESPONSE LATENCY",
    "text": "RESPONSE LATENCY\n\nTime on First Item\nHere we consider the time spent on just the first individual item (first exposure to graph).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_lab%>% dplyr::select(item_q1_rt) %>% unlist()  %>%  favstats(),\n  \"online\"= df_online %>% dplyr::select(item_q1_rt) %>% unlist() %>% favstats(),\n  \"combined\"= df_subjects %>% dplyr::select(item_q1_rt) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of First Response Time (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of First Response Time (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    7.22 \n    26.6 \n    39.3 \n    52.2 \n    161 \n    44.5 \n    26.2 \n    126 \n    0 \n  \n  \n    online \n    4.84 \n    19.9 \n    31.0 \n    48.9 \n    306 \n    43.3 \n    41.3 \n    204 \n    0 \n  \n  \n    combined \n    4.84 \n    22.3 \n    34.0 \n    50.7 \n    306 \n    43.8 \n    36.2 \n    330 \n    0 \n  \n\n\n\n\n\nResponse time on the first item for in person subjects (n = 126) ranged from 7.22 to 161.36 minutes with a mean duration of (M = 44.53, SD = 26.22).\nResponse time on the first item for online replication subjects (n = 204) ranged from 4.84 to 305.94 minutes with a mean duration of (M = 43.32, SD = 41.27).\nResponse time on the first item for combined subjects (n = 330) ranged from 4.84 to 305.94 minutes with a mean duration of (M = 43.78, SD = 36.23).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_q1_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of First Item Response Time (seconds)\", subtitle = \"fit by gamma distribution\", x = \"First Item Response Time (seconds)\", y = \"% items\")\n\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_q1_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"First Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#recode as boolean correct\ndf_subjects <- df_subjects %>% mutate(\n  item_q1_NABS = as.logical(item_q1_NABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_q1_rt, color = item_q1_NABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .1\n  )) + \n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"First Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\n\n\nCODE\n# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n\n\nTime on Item\nHere we consider the time spent on an individual item (across all items).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(rt_s) %>% unlist()  %>%  favstats(),\n  \"online\"= df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(rt_s) %>% unlist() %>% favstats(),\n  \"combined\"= df_items %>%   dplyr::select(rt_s) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of Item Response Latency (seconds)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Item Response Latency (seconds)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    1.264 \n    13.8 \n    24.9 \n    46.1 \n    336 \n    35.5 \n    33.1 \n    1890 \n    0 \n  \n  \n    online \n    1.264 \n    13.8 \n    24.9 \n    46.1 \n    336 \n    35.5 \n    33.1 \n    1890 \n    0 \n  \n  \n    combined \n    0.003 \n    12.5 \n    23.7 \n    43.9 \n    532 \n    35.2 \n    37.2 \n    4950 \n    0 \n  \n\n\n\n\n\nTime on an individual item for in person subjects (n = 1890) ranged from 1.26 to 336.03 minutes with a mean duration of (M = 35.47, SD = 33.12).\nTime on an individual item for online replication subjects (n = 1890) ranged from 1.26 to 336.03 minutes with a mean duration of (M = 35.47, SD = 33.12).\nTime on an individual item for combined subjects (n = 4950) ranged from 0 to 336.03 minutes with a mean duration of (M = 35.24, SD = 37.21).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~rt_s, data = df_items) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of Item Response Time (seconds)\", \n       subtitle = \"fit by gamma distribution\", x = \"Item Response Time (seconds)\", y = \"% items\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_items, x = \"rt_s\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#recode as boolean correct\ndf_items <- df_items %>% mutate(\n  score_niceABS = as.logical(score_niceABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_items, aes(x = pretty_condition, y = rt_s, color = score_niceABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    # position = position_dodgejust(),\n    justification = 1.5, \n    # adjust = .5, \n    width = .5, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA,\n    position = position_dodge2()\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitterdodge(\n      # seed = 1,\n      dodge.width = 0.5,\n      jitter.width = 0.075\n  )) +\n  labs( title = \"Distribution of Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\n\n\nCODE\n# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n\n\nTime on SCAFFOLD Phase\nHere we consider just the time spent on the first five items of the task (the scaffold phase).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_scaffold_rt) %>% unlist()  %>%  favstats(),\n  \"online\"= df_online %>% dplyr::select(item_scaffold_rt) %>% unlist() %>% favstats(),\n  \"combined\"= df_subjects %>% dplyr::select(item_scaffold_rt) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of SCAFFOLD Phase Response Latency (minutes)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of SCAFFOLD Phase Response Latency (minutes)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    1.235 \n    2.66 \n    3.71 \n    4.92 \n    11.1 \n    4.03 \n    1.88 \n    126 \n    0 \n  \n  \n    online \n    0.614 \n    2.10 \n    2.92 \n    4.31 \n    15.4 \n    3.52 \n    2.26 \n    204 \n    0 \n  \n  \n    combined \n    0.614 \n    2.29 \n    3.25 \n    4.58 \n    15.4 \n    3.72 \n    2.13 \n    330 \n    0 \n  \n\n\n\n\n\nTotal time on SCAFFOLD phase for in person subjects (n = 126) ranged from 1.24 to 11.1 minutes with a mean duration of (M = 4.03, SD = 1.88).\nTotal time on SCAFFOLD phase for online replication subjects (n = 204) ranged from 0.61 to 15.39 minutes with a mean duration of (M = 3.52, SD = 2.26).\nTotal time on SCAFFOLD phase for combined subjects (n = 330) ranged from 0.61 to 15.39 minutes with a mean duration of (M = 3.72, SD = 2.13).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_scaffold_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of SCAFFOLD Phase Response Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Scaffold Phase Time (minutes)\", y = \"% subjects\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_scaffold_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of SCAFFOLD Phase Response Time (minutes)\",\n        subtitle =\"\",\n        x = \"Scaffold Phase Time (minutes)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_scaffold_rt, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_scaffold_rt),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_scaffold_rt, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  ))+ labs( title = \"Distribution of SCAFFOLD Phase Response Time (minutes)\",\n        subtitle =\"\",\n        y = \"Total Study Time (minutes)\", x = \"Condition\") +\n  theme_ggdist() + theme(legend.position = \"blank\") #+\n\n\n\n\n\nCODE\n  # coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\n\nTime on TEST Phase\nHere we consider just the time spent on the remaining eight discriminant items of the task (the test phase).\n\n\nCODE\n#DESCRIBE distribution of response time\ntime.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(item_test_rt) %>% unlist()  %>%  favstats(),\n  \"online\"= df_online %>% dplyr::select(item_test_rt) %>% unlist() %>% favstats(),\n  \"combined\"= df_subjects %>% dplyr::select(item_test_rt) %>% unlist() %>% favstats()\n)\n\ntitle = \"Descriptive Statistics of TEST Phase Response Latency (minutes)\"\ntime.stats %>% kbl(caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of TEST Phase Response Latency (minutes)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    1.022 \n    2.97 \n    3.75 \n    4.76 \n    10.8 \n    4.00 \n    1.37 \n    126 \n    0 \n  \n  \n    online \n    0.703 \n    3.10 \n    3.89 \n    5.17 \n    13.5 \n    4.41 \n    2.24 \n    204 \n    0 \n  \n  \n    combined \n    0.703 \n    3.03 \n    3.80 \n    4.99 \n    13.5 \n    4.26 \n    1.96 \n    330 \n    0 \n  \n\n\n\n\n\nTotal time on TEST phase for in person subjects (n = 126) ranged from 1.02 to 10.85 minutes with a mean duration of (M = 4, SD = 1.37).\nTotal time on TEST phase for online replication subjects (n = 204) ranged from 0.7 to 13.49 minutes with a mean duration of (M = 4.41, SD = 2.24).\nTotal time on TEST phase for combined subjects (n = 330) ranged from 0.7 to 13.49 minutes with a mean duration of (M = 4.26, SD = 1.96).\n\n\nCODE\n#HISTOGRAM\ngf_dhistogram(~item_test_rt, data = df_subjects) %>%\n  gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of TEST Phase Response Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Test Phase Time (minutes)\", y = \"% subjects\") \n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"item_test_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of TEST Phase Response Time (minutes)\",\n        subtitle =\"\",\n        x = \"Test Phase Time (minutes)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = item_test_rt, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_rt),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_rt, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  ))+ \n  labs( title = \"Distribution of TEST Phase Response Time (minutes)\",\n        subtitle =\"\",\n        y = \"Total Study Time (minutes)\", x = \"Condition\") +\n  theme_ggdist() + theme(legend.position = \"blank\") #+\n\n\n\n\n\nCODE\n  # coord_cartesian(xlim = c(0.5, NA), clip = \"off\")"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#replication-check",
    "href": "analysis/SGC3A/3_sgc3A_description.html#replication-check",
    "title": "3  Description",
    "section": "REPLICATION CHECK",
    "text": "REPLICATION CHECK\n\nData Collection Mode on Absolute Score\nDoes Mode Change Effect of Condition on Score?\nTo verify that the data collected in the lab and remotely online are comparable, we perform a t-test on group means of ABSOLUTE SCORE for each condition, and examine whether data collection modality is a significant predictor of variance in absolute score\n\n\nCODE\npaste(\"Two Sample T-Test for S_ABS LAB vs. ONLINE control condition\")\n\n\n[1] \"Two Sample T-Test for S_ABS LAB vs. ONLINE control condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 111), s_ABS ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_ABS by mode\nt = 0.5, df = 120, p-value = 0.6\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -1.09  1.84\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                   2.68                    2.30 \n\n\nCODE\npaste(\"Two Sample T-Test for S_ABS LAB vs. ONLINE impasse condition\")\n\n\n[1] \"Two Sample T-Test for S_ABS LAB vs. ONLINE impasse condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 121), s_ABS ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_ABS by mode\nt = 1, df = 135, p-value = 0.3\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -0.727  2.435\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                   5.44                    4.58 \n\n\nCODE\npaste(\"OLS Linear Regression Predicting Absolute Score by Data Collection Mode\")\n\n\n[1] \"OLS Linear Regression Predicting Absolute Score by Data Collection Mode\"\n\n\nCODE\nsummary(lm(data = df_subjects, formula = s_ABS ~ mode ))\n\n\n\nCall:\nlm(formula = s_ABS ~ mode, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.08  -3.51  -2.51   4.49   9.49 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.08       0.44    9.27   <2e-16 ***\nmodeasynch     -0.57       0.56   -1.02     0.31    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.94 on 328 degrees of freedom\nMultiple R-squared:  0.00314,   Adjusted R-squared:  0.000105 \nF-statistic: 1.03 on 1 and 328 DF,  p-value: 0.31\n\n\nBoth t-tests are non-significant with 95% confidence intervals including 0. Further, an OLS linear regression model predicting cumulative absolute score indicates that data collection mode is not a significant predictor, explaining only 0.01% of variance in absolute score, F(1,328) = 1.03, p > 0.05.\n\n\n\n\n\n\nDecision\n\n\n\nIt is reasonable to infer that data from both in-person and remote studies arise from the same data generating process.\n\n\n\n\nData Collection Mode on Cumulative Score\nAre the by-condition group means significantly different by data collection modality?\nTo verify that the data collected in the lab and remotely online are comparable, we perform a t-test on group means of SCALED SCORE for each condition.\n\n\nCODE\npaste(\"Two Sample T-Test for s_SCALED LAB vs. ONLINE control condition\")\n\n\n[1] \"Two Sample T-Test for s_SCALED LAB vs. ONLINE control condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 111), s_SCALED ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by mode\nt = -0.1, df = 117, p-value = 0.9\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -3.15  2.83\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                  -6.52                   -6.36 \n\n\nCODE\npaste(\"Two Sample T-Test for s_SCALED LAB vs. ONLINE impasse condition\")\n\n\n[1] \"Two Sample T-Test for s_SCALED LAB vs. ONLINE impasse condition\"\n\n\nCODE\nt.test(data = df_subjects %>% filter(condition == 121), s_SCALED ~ mode )\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by mode\nt = 0.5, df = 130, p-value = 0.6\nalternative hypothesis: true difference in means between group lab-synch and group asynch is not equal to 0\n95 percent confidence interval:\n -2.08  3.49\nsample estimates:\nmean in group lab-synch    mean in group asynch \n                  1.008                   0.306 \n\n\nCODE\npaste(\"OLS Linear Regression Predicting Scaled Score by Data Collection Mode\")\n\n\n[1] \"OLS Linear Regression Predicting Scaled Score by Data Collection Mode\"\n\n\nCODE\nsummary(lm(data = df_subjects, formula = s_SCALED ~ mode ))\n\n\n\nCall:\nlm(formula = s_SCALED ~ mode, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10.30  -7.80  -4.42  10.33  15.83 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   -2.698      0.853   -3.16   0.0017 **\nmodeasynch    -0.135      1.085   -0.12   0.9011   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.58 on 328 degrees of freedom\nMultiple R-squared:  4.71e-05,  Adjusted R-squared:  -0.003 \nF-statistic: 0.0155 on 1 and 328 DF,  p-value: 0.901\n\n\nBoth t-tests are non-significant with 95% confidence intervals including 0. Further, an OLS linear regression model predicting cumulative scaled score indicates that data collection mode is not a significant predictor, explaining less than 0.001% of variance in absolute score, F(1,328) = 0.0078, p > 0.05.\n\n\n\n\n\n\nDecision\n\n\n\nIt is reasonable to infer that data from both in-person and remote studies arise from the same data generating process."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#resources",
    "href": "analysis/SGC3A/3_sgc3A_description.html#resources",
    "title": "3  Description",
    "section": "RESOURCES",
    "text": "RESOURCES\n\nhttps://rpkgs.datanovia.com/ggpubr/reference/index.html\nAppropriate models for response time data. (see: https://lindeloev.github.io/shiny-rt/)]{style=“color: red;”}.\nEspecially see https://lindeloev.github.io/shiny-rt/ for ideas on modelling reaction time data\n\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] forcats_0.5.1      stringr_1.4.0      purrr_0.3.4        readr_2.1.2       \n [5] tidyr_1.2.0        tibble_3.1.7       tidyverse_1.3.1    performance_0.9.1 \n [9] fitdistrplus_1.1-8 MASS_7.3-57        multimode_1.5      ggdist_3.1.1      \n[13] ggpubr_0.4.0       vcd_1.4-10         kableExtra_1.3.4   mosaic_1.8.3      \n[17] ggridges_0.5.3     mosaicData_0.20.2  ggformula_0.10.1   ggstance_0.3.5    \n[21] dplyr_1.0.9        Matrix_1.4-1       Hmisc_4.7-0        ggplot2_3.3.6     \n[25] Formula_1.2-4      survival_3.3-1     lattice_0.20-45   \n\nloaded via a namespace (and not attached):\n  [1] colorspace_2.0-3     ggsignif_0.6.3       ellipsis_0.3.2      \n  [4] mclust_5.4.10        leaflet_2.1.1        htmlTable_2.4.0     \n  [7] fs_1.5.2             base64enc_0.1-3      ggdendro_0.1.23     \n [10] rstudioapi_0.13      farver_2.1.0         bit64_4.0.5         \n [13] ggrepel_0.9.1        lubridate_1.8.0      mvtnorm_1.1-3       \n [16] fansi_1.0.3          xml2_1.3.3           splines_4.2.1       \n [19] rootSolve_1.8.2.3    knitr_1.39           polyclip_1.10-0     \n [22] jsonlite_1.8.0       broom_0.8.0          dbplyr_2.2.1        \n [25] cluster_2.1.3        png_0.1-7            ggforce_0.3.3       \n [28] compiler_4.2.1       httr_1.4.3           backports_1.4.1     \n [31] assertthat_0.2.1     fastmap_1.1.0        cli_3.3.0           \n [34] tweenr_1.0.2         htmltools_0.5.2      tools_4.2.1         \n [37] gtable_0.3.0         glue_1.6.2           Rcpp_1.0.8.3        \n [40] carData_3.0-5        cellranger_1.1.0     vctrs_0.4.1         \n [43] svglite_2.1.0        crosstalk_1.2.0      insight_0.17.1      \n [46] lmtest_0.9-40        xfun_0.31            rvest_1.0.2         \n [49] lifecycle_1.0.1      mosaicCore_0.9.0     rstatix_0.7.0       \n [52] zoo_1.8-10           scales_1.2.0         vroom_1.5.7         \n [55] hms_1.1.1            parallel_4.2.1       RColorBrewer_1.1-3  \n [58] yaml_2.3.5           gridExtra_2.3        labelled_2.9.1      \n [61] rpart_4.1.16         latticeExtra_0.6-29  stringi_1.7.6       \n [64] highr_0.9            checkmate_2.1.0      rlang_1.0.3         \n [67] pkgconfig_2.0.3      systemfonts_1.0.4    distributional_0.3.0\n [70] pracma_2.3.8         evaluate_0.15        labeling_0.4.2      \n [73] ks_1.13.5            htmlwidgets_1.5.4    bit_4.0.4           \n [76] tidyselect_1.1.2     plyr_1.8.7           magrittr_2.0.3      \n [79] R6_2.5.1             generics_0.1.2       DBI_1.1.3           \n [82] pillar_1.7.0         haven_2.5.0          foreign_0.8-82      \n [85] withr_2.5.0          abind_1.4-5          nnet_7.3-17         \n [88] modelr_0.1.8         crayon_1.5.1         car_3.1-0           \n [91] KernSmooth_2.23-20   utf8_1.2.2           tzdb_0.3.0          \n [94] rmarkdown_2.14       jpeg_0.1-9           readxl_1.4.0        \n [97] data.table_1.14.2    reprex_2.0.1         diptest_0.76-0      \n[100] digest_0.6.29        webshot_0.5.3        munsell_0.5.0       \n[103] viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#archive",
    "href": "analysis/SGC3A/3_sgc3A_description.html#archive",
    "title": "3  Description",
    "section": "ARCHIVE",
    "text": "ARCHIVE\nSample ridgeplot code\n\n\nCODE\n#RIDGEPLOT\n# ggplot(data = df_subjects, aes(x = s_NABS, y = mode)) +\n#   geom_density_ridges() + xlim(0,13)+\n#   facet_wrap(~condition, labeller = label_both) +\n# labs(x = \"total number correct \",\n# y = \"proportion of subjects\",\n#        title = \"Subject Cumulative Score (Absolute)\",\n#        subtitle = \"Score distributions are comparable across modalities and different across conditions\") +\n#   theme_minimal()\n\n\n\nWhat Kind of Distribution is Total Score?\nWhat kind of distribution is the Total Absolute Score (TEST Phase) data? We use the fitdistrplus package to compare the distribution of this variable to a variety of probability distribution families. First, we transform the # correct items to % correct items by dividing it by the total number of items (n = 8).\n\n\nCODE\n#describe the distribution\ndescdist(data = df_subjects$item_test_NABS/8, discrete = FALSE, boot = 1000)\n\n\n\n\n\nsummary statistics\n------\nmin:  0   max:  1 \nmedian:  0 \nmean:  0.287 \nestimated sd:  0.405 \nestimated skewness:  0.876 \nestimated kurtosis:  1.93 \n\n\nCODE\nprint(\"FIT A NORMAL DISTRIBUTION\")\n\n\n[1] \"FIT A NORMAL DISTRIBUTION\"\n\n\nCODE\nnormal_ = fitdist(df_subjects$item_test_NABS/8,\"norm\")\nplot(normal_)\n\n\n\n\n\nCODE\nprint(\"FIT A BETA DISTRIBUTION\")\n\n\n[1] \"FIT A BETA DISTRIBUTION\"\n\n\nCODE\nbeta_ = fitdist(df_subjects$item_test_NABS/8,\"beta\", method=\"mme\" )\nplot(beta_)\n\n\n\n\n\nCODE\nsummary(beta_)\n\n\nFitting of the distribution ' beta ' by matching moments \nParameters : \n       estimate\nshape1   0.0721\nshape2   0.1786\nLoglikelihood:  Inf   AIC:  -Inf   BIC:  -Inf \n\n\nInterpreting the Cullen and Frey graph, it appears that number percentage of correct responses per subject may follow a beta distribution (u-shape tpe). If we fit this variable using both a normal and beta distribution (using method of moments), it appears that the beta distribution provides a much better fit. The parameter estimates for the beta distribution are: shape1 = 0.072, shape2 = 0.179. The beta distribution is a flexible distribution insofar as it can model a wide range of shapes with its two parameters. TODO: HOW might this be applied to the total score data?\nAnalysis Notes - This distribution is very bimodal, so OLS linear regression estimating means may not be informative, as the mean actually falls near the location of the anitmode (least common value) - Should investigate log transform to see if residuals of LM will be normal (no) - Should investigate beta regression\n\n\nWhole Task Scores\n\nAbsolute Score\nTotal Scores that include both Scaffolding Phase as well as Test Phase performance.\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total Absolute Score)\"\nabs.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(s_NABS) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(s_NABS) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(s_NABS) %>% unlist() %>% favstats()\n) \nabs.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total Absolute Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    0 \n    0 \n    1 \n    9.00 \n    13 \n    4.11 \n    5.09 \n    126 \n    0 \n  \n  \n    online \n    0 \n    0 \n    1 \n    8.00 \n    13 \n    3.52 \n    4.89 \n    204 \n    0 \n  \n  \n    combined \n    0 \n    0 \n    1 \n    8.75 \n    13 \n    3.75 \n    4.97 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, total absolute scores (n = 126) range from 0 to 13 with a mean score of (M = 4.11, SD = 5.09).\nFor online replication, (online) total absolute accuracy scores (n = 204) range from 0 to 13 with a slightly lower mean score of (M = 3.52, SD = 4.89).\nWhen combined overall, total absolute accuracy scores (n = 330) range from 0 to 13 with a slightly lower mean score of (M = 3.75, SD = 4.97).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL ABSOLUTE\n  gf_props(~s_NABS, data = df_subjects) + \n  labs(x = \"number of correct responses\",\n       y = \"% of subjects\",\n       title = \"Distribution of Task Absolute Score\",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") + \n  theme_minimal()\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"s_NABS\", binwidth = 1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n  labs( title = \"Distribution of Task Absolute Score (by Mode and Condition)\",\n        subtitle =\"Pattern of response is the same across data collection modes but differs by condition\",\n        x = \"Total Absolute Score\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = s_NABS, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    adjust = .5, \n    width = .6, \n    .width = 0, \n    justification = -.3, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter(\n      seed = 1, width = .1\n    )\n  ) + labs(\n    title = \"Distribution of Task Absolute Score\",\n    x = \"Condition\", y = \"Total Absolute Score\"\n  ) + theme_ggdist() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(s_NABS)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Task Absolute Score\",\n        x = \"Task Absolute Score [0,13]\", \n        y = \"Cumulative Probability\") + theme_minimal()\n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and likely bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018). TODO REFERENCE\n\n\nCODE\nmultimode::modetest(df_subjects$s_NABS)\n\n\nWarning in multimode::modetest(df_subjects$s_NABS): A modification of the data\nwas made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$s_NABS\nExcess mass = 0.1, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$s_NABS, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$s_NABS,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$s_NABS, mod0 = n_modes, display =\nTRUE): If the density function has an unbounded support, artificial modes may\nhave been created in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is infact multimodal (m = 0.1, p < 0.001), with two identifiable modes at 0.26 and 12.261, and an antimode at 6.985.\n\n\n\n\n\n\nNote\n\n\n\nCondition appears (through visual inspection) to yield a positive influence on Total Absolute Score (across the entire task), across data collection modalities.\n\n\n\n\nScaled Score\n\n\nCODE\ntitle = \"Descriptive Statistics of Response Accuracy (Total Scaled Score)\"\nscaled.stats <- rbind(\n  \"lab\"= df_lab %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_online %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats(),\n  \"combined\" = df_subjects %>% dplyr::select(s_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Response Accuracy (Total Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -13 \n    -12.0 \n    -7.50 \n    8.75 \n    13 \n    -2.70 \n    10.08 \n    126 \n    0 \n  \n  \n    online \n    -13 \n    -10.0 \n    -7.00 \n    6.62 \n    13 \n    -2.83 \n    9.26 \n    204 \n    0 \n  \n  \n    combined \n    -13 \n    -10.5 \n    -7.25 \n    7.50 \n    13 \n    -2.78 \n    9.56 \n    330 \n    0 \n  \n\n\n\n\n\nFor in person collection, total scaled scores (n = 126) range from -13 to 13 with a mean score of (M = -2.7, SD = 10.08).\nFor online replication, total scaled scores (n = 204) range from -13 to 13 with a slightly lower mean score of (M = -2.83, SD = 9.26).\nWhen combined overall, total scaled scores (n = 330) range from -13 to 13 with a slightly lower mean score of (M = -2.78, SD = 9.56).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL SCALED\ngf_props(~s_SCALED, data = df_subjects) +\n  labs(x = \"total scaled score\",\n       y = \"% of subjects\",\n       title = \"Distribution of Total Scaled Score\",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") + \n  theme_minimal()\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"s_SCALED\",binwidth=1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) + \n  labs( title = \"Distribution of Total Scaled Score (by Condition and Mode)\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"total scaled score\", y = \"number of participants\") + \n  theme_minimal() + theme(legend.position = \"blank\") \n\n\n\n\n\nCODE\n##RAINCLOUD USING GGDISTR\nggplot(df_subjects, aes(x = pretty_condition, y = s_SCALED, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    adjust = .5, \n    width = .6, \n    .width = 0, \n    justification = -.3, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter(\n      seed = 1, width = .1\n    )\n  ) + labs(\n    title = \"Distribution of Task Scaled Score \",\n    x = \"Condition\", y = \"Total Scaled Score\"\n  ) + theme_ggdist() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(s_SCALED)) + \n  stat_ecdf(geom = \"step\") + \n  facet_grid(pretty_condition ~ pretty_mode) + \n  labs( title = \"Empirical Cumulative Density Function — Task Scaled Score\",\n        x = \"Task Scaled Score [-13, 13]\", \n        y = \"Cumulative Probability\") + theme_minimal()\n\n\n\n\n\nVisual inspection of this distribution suggests it is not normal, and perhaps perhaps bimodal. We verify this via an excess mass test (Ameijeiras-Alsonso et. al 2018).\n\n\nCODE\nmultimode::modetest(df_subjects$s_SCALED)\n\n\nWarning in multimode::modetest(df_subjects$s_SCALED): A modification of the data\nwas made in order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df_subjects$s_SCALED\nExcess mass = 0.1, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\n\nCODE\nn_modes = multimode::nmodes(df_subjects$s_SCALED, bw=2) #bw = 2questions/15 = 0.15%\nl_modes = multimode::locmodes(df_subjects$s_SCALED,mod0 =  n_modes, display = TRUE)\n\n\nWarning in multimode::locmodes(df_subjects$s_SCALED, mod0 = n_modes, display =\nTRUE): If the density function has an unbounded support, artificial modes may\nhave been created in the tails\n\n\n\n\n\nThe excess mass test for multimodality suggests the distribution is in fact multimodal (m = 0.1, p < 0.001), with two identifiable modes at -11.195 and 12.103, and an antimode at 2.942.\nAnalysis Notes - As with absolute score, the distribution of scaled score is very bimodal - Same need to investigate transformations and alternative distributions for regression\n\n\n\n\n\n\nNote\n\n\n\nCondition appears (through visual inspection) to yield a positive influence on Total Scaled Score across data collection modalities.\n\n\n\n\n\nItem Level Scores\n\nItem Absolute Score\nWhote Task Accuracy summarized over items rather than subjects\n\n\nCODE\nx <- df_items %>% mutate(score = as.logical(score_ABS))\n\ntitle = \"Proportion of Correct Items By Condition (Lab)\"\n\nitem.contingency <- df_items %>% filter(mode == \"lab-synch\") %>% dplyr::select(score_ABS, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Items By Condition (Lab)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    0 \n    0.344 \n    0.268 \n    0.613 \n  \n  \n    1 \n    0.148 \n    0.240 \n    0.387 \n  \n  \n    Sum \n    0.492 \n    0.508 \n    1.000 \n  \n\n\n\n\n\nCODE\ntitle = \"Proportion of Correct Items By Condition (Online)\"\nitem.contingency <- df_items %>% filter(mode == \"asynch\") %>% dplyr::select(score_ABS, condition) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Items By Condition (Online)\n \n  \n      \n    111 \n    121 \n    Sum \n  \n \n\n  \n    0 \n    0.342 \n    0.307 \n    0.649 \n  \n  \n    1 \n    0.128 \n    0.223 \n    0.351 \n  \n  \n    Sum \n    0.471 \n    0.529 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM\nstats = df_items %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(score_niceABS))\ngf_props(~score_niceABS, data = df_items) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) +\n  labs(x = \"Item Absolute Score\",\n       title = \"Item Absolute Score\",\n       subtitle=\"Across modalities, the impasse condition yielded more correct responses\")+\n  theme_minimal()\n\n\n\n\n\n\n\nItem Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1.\n\n\nCODE\ntitle = \"Descriptive Statistics of Item Response Accuracy (Scaled Score)\"\nscaled.stats.items <- rbind(\n  \"lab\"= df_items %>% filter(mode == 'lab-synch') %>% dplyr::select(score_SCALED) %>% unlist() %>% favstats(),\n  \"online\" = df_items %>% filter(mode == \"asynch\") %>% dplyr::select(score_SCALED) %>% unlist() %>% favstats()\n) \nscaled.stats.items %>% kbl (caption = title) %>% kable_classic()\n\n\n\nDescriptive Statistics of Item Response Accuracy (Scaled Score)\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    lab \n    -1 \n    -1 \n    -0.5 \n    1 \n    1 \n    -0.128 \n    0.877 \n    1890 \n    0 \n  \n  \n    online \n    -1 \n    -1 \n    -0.5 \n    1 \n    1 \n    -0.136 \n    0.842 \n    3060 \n    0 \n  \n\n\n\n\n\n\n\nCODE\n#VISUALIZE distribution of response accuracy across ITEMS\n\n#HISTOGRAM\nstats = df_items %>% group_by(condition, mode) %>% dplyr::summarise(mean = mean(score_SCALED))\ngf_props(~score_SCALED, data = df_items) %>% \n  gf_facet_grid(condition~mode, labeller = label_both) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Scaled Score for Item\",\n       y = \"Proportion of Items\",\n       title = \"Distribution of Accuracy per Item (Scale Score)\",\n       subtitle=\"The impasse condition shifts density toward the positive score\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nGRAPH CODE\n\nHorizontal Raincloud\n\n\nCODE\nggplot(df_subjects, aes(x = pretty_condition, y = item_test_NABS,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"top\",\n    justification = -0.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  scale_x_discrete(limits=c(\"impasse\",\"control\"))+\n  geom_boxplot(\n    position = \"dodge\",\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_NABS),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_test_NABS, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitterdodge( seed = 8,\n      jitter.width = 0.5, jitter.height = 0.1\n      # seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of TEST Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + coord_flip()\n\n\n\n\n\nCODE\n  # coord_cartesian(xlim = c(0.5, NA), clip = \"off\")"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "TODO\nThe purpose of this notebook is test the hypotheses that determined the design of the SGC3A study.\nResearch Questions\nIn SGC3A we set out to answer the following question: Does posing a mental impasse improve performance on the graph comprehension task?\nExperimental HypothesisLearners posed with scenario designed to evoke a mental impasse will be more likely to correct interpret the graph.\nNull HypothesisNo significant differences in performance will exist between learners in the IMPASSE and CONTROL conditions."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1a-test-phase-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1a-test-phase-accuracy",
    "title": "4  Hypothesis Testing",
    "section": "H1A | TEST PHASE ACCURACY",
    "text": "H1A | TEST PHASE ACCURACY\nOn the TEST Phase of the graph comprehension task (the final 8 questions, encountered after the 5 scaffolded questions) does the impasse condition affect performance on the graph comprehension task?\n\n\n\n\n\n\nResearch Question\nDoes posing a mental impasse improve performance?\n\n\n\nHypothesis\n(H1A) Participants in the IMPASSE condition will have significantly higher TEST PHASE performance than those in the CONTROL condition.\n\n\nAnalysis Strategy\nOLS Linear Regression DV_percent_test_NABS ~ condition (absolute scoring)\nOLS Linear Regression item_test_SCALED ~ condition (scaled scoring)\n\n\nAlternatives\n\nExploring alternatives.Simple linear regression models do a poor job of fitting the (bimodal) outcome distributions (both absolute and scaled scores)\n\nHurdle model (mixture model w/ binomial + count)\nNegative Binomial / Zero Inflated Negative Binom for overdispersed count?\nBeta regression?\nOther way to account for the severe bimodality?\n“shift function” way to characterize difference in bimodal distributions\n\n\n\n\nInference\n\nTODO when done\n\n\n\n\nTest Phase Absolute Score\nShift in Modal Mass\n\nCODE#HISTOGRAM\nstats = df_subjects %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(DV_percent_test_NABS))\ngf_props(~DV_percent_test_NABS, \n         fill = ~pretty_condition, data = df_subjects) %>% gf_facet_grid(~pretty_condition) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"% Correct\",\n       y = \"proportion of subjects\",\n       title = \"Test Phase Absolute Score (% Correct)\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n\n\n\n\nThe Effect of Condition on Total Absolute Test Score can be described as a ‘shift’ in mass between the two modes of each distribution.\nFIRST, we use the Kolmogorov-Smirnov test as a Robust alternative to the t-test to test if the two distributions likely come from different populations.\n\nCODE#(requires shift function files loaded)\n\n#PREP DATA \ndf <- df_subjects %>% dplyr::select(DV_percent_test_NABS, pretty_condition) %>% \n  mutate(\n    data = as.numeric(DV_percent_test_NABS),\n    #flip order levels to correctly orient graph\n    # gr = recode_factor(pretty_condition, \"impasse\" = \"impasse\", \"control\"=\"control\")\n    gr = as.character(pretty_condition)\n  ) %>% dplyr::select(data,gr)\n\ng1 <- df %>% filter(gr == \"control\") %>% dplyr::pull(data)\ng2 <- df %>% filter(gr == \"impasse\") %>% dplyr::pull(data)\n\n\n#COMPARE DISTRIBUTIONS WITH ROBUST TESTS\n\n#What do common tests say about the difference?\n\n# Kolmogorov-Smirnov test\n#If y is numeric, a two-sample (Smirnov) test of the null hypothesis that x and y \n#were drawn from the same continuous distribution is performed. Alternatively, y ...\n\n#null is X is drawn from CDF EQUAL TO Y\nks.test(g1,g2) \n\nWarning in ks.test(g1, g2): p-value will be approximate in the presence of ties\n\n\n\n    Two-sample Kolmogorov-Smirnov test\n\ndata:  g1 and g2\nD = 0.2, p-value = 0.0001\nalternative hypothesis: two-sided\n\nCODEprint(\"SUGGESTS that impasse and control come from different population distributions\")\n\n[1] \"SUGGESTS that impasse and control come from different population distributions\"\n\nCODE# #null is X is NOT LESS THAN Y\nks.test(g1,g2, alternative = \"greater\") \n\nWarning in ks.test(g1, g2, alternative = \"greater\"): p-value will be approximate\nin the presence of ties\n\n\n\n    Two-sample Kolmogorov-Smirnov test\n\ndata:  g1 and g2\nD^+ = 0.2, p-value = 5e-05\nalternative hypothesis: the CDF of x lies above that of y\n\nCODEprint(\"SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]\")\n\n[1] \"SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]\"\n\nCODE#REGULAR T-TEST\nt.test(g1,g2) # regular Welsh t-test\n\n\n    Welch Two Sample t-test\n\ndata:  g1 and g2\nt = -4, df = 325, p-value = 2e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -27.2 -10.2\nsample estimates:\nmean of x mean of y \n     19.0      37.7 \n\n\n\nCODE#IF THIS ERRORS, consider loadling plyr (older than dplyr)\n# kernel density estimate + rug plot + superimposed deciles\nkde <- plot.kde_rug_dec2(df)\n\nLoading required package: plyr\n\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\n\nThe following object is masked from 'package:vcdExtra':\n\n    summarise\n\n\nThe following object is masked from 'package:ggpubr':\n\n    mutate\n\n\nThe following objects are masked from 'package:Hmisc':\n\n    is.discrete, summarize\n\nCODE# kde\n\n# compute shift function\nout <- shifthd( g1, g2, nboot=200)\n\n# plot shift function\nsf <- plot.sf(data=out) # function from rgar_visualisation.txt\n# sf\n\n# combine KDE + SF\ncowplot::plot_grid(kde, sf, labels=c(\"A\", \"B\"), ncol = 1, nrow = 2, rel_heights = c(1.5, 1),label_size = 18,hjust = -1,scale=.95)\n\n\n\n\nTODO | Ordinal Regression on ITEM-Interpretation\nWIP | Mixed Logistic Regression ITEM-ABS\n\nCODElibrary(lme4)\n\nWarning: package 'lme4' was built under R version 4.1.2\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    bdiag\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nCODEdf <- df_items %>% filter(q %nin% c(1,2,3,4,5,6,9))\ndf$q = as.factor(df$q)\n\n# SUBJECT INTERCEPT + FIXED CONDITION\nmm1 <- glmer(score_niceABS ~ condition + (1|subject), data = df,family = \"binomial\")\nsummary(mm1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: score_niceABS ~ condition + (1 | subject)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    1385     1402     -689     1379     2637 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5307 -0.0193 -0.0097  0.1135  2.7426 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 117      10.8    \nNumber of obs: 2640, groups:  subject, 330\n\nFixed effects:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -9.182      0.677  -13.56   <2e-16 ***\ncondition121    1.632      0.753    2.17     0.03 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nconditin121 -0.394\n\nCODEreport(mm1) \n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict score_niceABS with condition (formula: score_niceABS ~ condition). The model included subject as random effect (formula: ~1 | subject). The model's total explanatory power is substantial (conditional R2 = 0.97) and the part related to the fixed effects alone (marginal R2) is of 5.50e-03. The model's intercept, corresponding to condition = 111, is at -9.18 (95% CI [-10.51, -7.85], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 1.63, 95% CI [0.16, 3.11], p = 0.030; Std. beta = 1.63, 95% CI [0.16, 3.11])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE# CONDITION SLOPE per SUBJECT INTERCEPT + FIXED CONDITION\nmm2 <- glmer(score_niceABS ~ condition + (1|subject) + (1 | q) , data = df, family = \"binomial\")\nsummary(mm2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: score_niceABS ~ condition + (1 | subject) + (1 | q)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    1324     1348     -658     1316     2636 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.386 -0.014 -0.008  0.067  7.780 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 166.294  12.896  \n q       (Intercept)   0.543   0.737  \nNumber of obs: 2640, groups:  subject, 330; q, 8\n\nFixed effects:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -10.562      0.733  -14.41   <2e-16 ***\ncondition121    1.501      0.775    1.94    0.053 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nconditin121 -0.485\n\nCODEreport(mm2)\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict score_niceABS with condition (formula: score_niceABS ~ condition). The model included subject and q as random effects (formula: list(~1 | subject, ~1 | q)). The model's total explanatory power is substantial (conditional R2 = 0.98) and the part related to the fixed effects alone (marginal R2) is of 3.30e-03. The model's intercept, corresponding to condition = 111, is at -10.56 (95% CI [-12.00, -9.13], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically non-significant and positive (beta = 1.50, 95% CI [-0.02, 3.02], p = 0.053; Std. beta = 1.50, 95% CI [-0.02, 3.02])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE# check_model(mm2)\n\n# SUBJECT INTERCEPT + Q  INTERCEPT +  FIXED CONDITION\n# DOESN'T CONVERGE\n# mm3 <- glmer(score_niceABS ~ condition + q + (1 | subject), data = df, family = \"binomial\")\n# summary(mm3)\n# report(mm3)\n\n#RANDOM ONLY\nmm.r0 <- glmer(score_niceABS ~  (1 | subject), data = df, family = \"binomial\")\nsummary(mm.r0)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: score_niceABS ~ (1 | subject)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    1388     1400     -692     1384     2638 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5320 -0.0121 -0.0121  0.1117  2.7246 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 132      11.5    \nNumber of obs: 2640, groups:  subject, 330\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -8.66       0.55   -15.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEreport(mm.r0)\n\nWe fitted a constant (intercept-only) logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict score_niceABS (formula: score_niceABS ~ 1). The model included subject as random effect (formula: ~1 | subject). . The model's intercept is at -8.66 (95% CI [-9.74, -7.59], p < .001). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#RANDOM ONLY\nmm.r1 <- glmer(score_niceABS ~  (1 | subject) + (1 | q), data = df, family = \"binomial\")\nsummary(mm.r1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: score_niceABS ~ (1 | subject) + (1 | q)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    1326     1344     -660     1320     2637 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.396 -0.011 -0.008  0.066  7.858 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 180.606  13.44   \n q       (Intercept)   0.563   0.75   \nNumber of obs: 2640, groups:  subject, 330; q, 8\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -9.995      0.596   -16.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEreport(mm.r1)\n\nWe fitted a constant (intercept-only) logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict score_niceABS (formula: score_niceABS ~ 1). The model included subject and q as random effects (formula: list(~1 | subject, ~1 | q)). . The model's intercept is at -10.00 (95% CI [-11.16, -8.83], p < .001). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#COMPARE PERFORMANCE\ncompare_performance(mm.r0, mm.r1, mm1,mm2)\n\n# Comparison of Model Performance Indices\n\nName  |    Model |      AIC | AIC weights |      BIC | BIC weights | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n-------------------------------------------------------------------------------------------------------------------------------------------------------------\nmm.r0 | glmerMod | 1387.730 |     < 0.001 | 1399.487 |     < 0.001 |      0.976 |      0.000 | 0.976 | 0.203 | 1.000 |    0.129 |      -Inf |           0.015\nmm.r1 | glmerMod | 1326.298 |       0.277 | 1343.934 |       0.878 |      0.982 |      0.000 | 0.982 | 0.186 | 1.000 |    0.111 |      -Inf |           0.015\nmm1   | glmerMod | 1384.749 |     < 0.001 | 1402.385 |     < 0.001 |      0.973 |      0.005 | 0.973 | 0.203 | 1.000 |    0.130 |      -Inf |           0.015\nmm2   | glmerMod | 1324.375 |       0.723 | 1347.889 |       0.122 |      0.981 |      0.003 | 0.981 | 0.186 | 1.000 |    0.112 |      -Inf |           0.015\n\nCODE#SJ PLOTS\n#table of effects\nsjPlot:: tab_model(mm2)\n\n\n\n\n \nscore nice ABS\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.00\n<0.001\n\n\nID indicates randomlyassignedcondition(111->control,121->impasse):condition 121\n4.49\n0.98 – 20.52\n0.053\n\n\nRandom Effects\n\n\nσ2\n\n3.29\n\n\nτ00subject\n\n166.29\n\n\n\nτ00q\n\n0.54\n\n\n\nICC\n0.98\n\n\n\nN subject\n\n330\n\n\n\nN q\n\n8\n\n\nObservations\n2640\n\n\nMarginal R2 / Conditional R2\n\n0.003 / 0.981\n\n\n\nCODEsjPlot::plot_model(mm2)\n\n\n\nCODEsjPlot::plot_model(mm2, transform = \"plogis\")\n\n\n\nCODEsjPlot::plot_model(mm2, type = \"pred\")\n\n$condition\n\n\n\n\nCODElibrary(lattice)\nrandoms <- ranef(mm2)\ndotplot(randoms)\n\n$subject\n\n\n\n\n\n\n$q\n\n\n\n\nCODElibrary(lmerTest)\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nCODE# ranova(mm1,mm2)\n\n\nLinear Regression\n(In Person)\nModel\n\nCODE#SCORE predicted by CONDITION\nlab.testabs.lm1 <- lm(DV_percent_test_NABS ~ pretty_condition, data = df_lab)\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(lab.testabs.lm1)\n\n\nCall:\nlm(formula = DV_percent_test_NABS ~ pretty_condition, data = df_lab)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -41.6  -21.4  -21.4   45.9   78.6 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                21.37       5.13    4.16 0.000058 ***\npretty_conditionimpasse    20.23       7.20    2.81   0.0058 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.4 on 124 degrees of freedom\nMultiple R-squared:  0.0598,    Adjusted R-squared:  0.0522 \nF-statistic: 7.89 on 1 and 124 DF,  p-value: 0.00579\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(lab.testabs.lm1)\n\nAnalysis of Variance Table\n\nResponse: DV_percent_test_NABS\n                  Df Sum Sq Mean Sq F value Pr(>F)   \npretty_condition   1  12889   12889    7.89 0.0058 **\nResiduals        124 202638    1634                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(lab.testabs.lm1)\n\n                        2.5 % 97.5 %\n(Intercept)             11.21   31.5\npretty_conditionimpasse  5.97   34.5\n\nCODEreport(lab.testabs.lm1) #sanity check\n\nWe fitted a linear model (estimated using OLS) to predict DV_percent_test_NABS with pretty_condition (formula: DV_percent_test_NABS ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.06, F(1, 124) = 7.89, p = 0.006, adj. R2 = 0.05). The model's intercept, corresponding to pretty_condition = control, is at 21.37 (95% CI [11.21, 31.53], t(124) = 4.16, p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 20.23, 95% CI [5.97, 34.49], t(124) = 2.81, p = 0.006; Std. beta = 0.49, 95% CI [0.14, 0.83])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#print model equation\neq <- extract_eq(lab.testabs.lm1, use_coefs = TRUE)\n\n\n\nCODE#MODEL ESTIMATES WITH UNCERTAINTY\n\n#setup references \nm <- lab.testabs.lm1\ndf <- df_lab \ncall <- m$call %>% as.character()\n\n# uncertainty model visualization\ndf  %>%\n  data_grid(pretty_condition) %>%\n  augment(m, newdata = ., se_fit = TRUE) %>% \n  ggplot(aes(y = pretty_condition, color = pretty_condition)) +\n  stat_halfeye( scale = .5,\n      aes(\n        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n        fill = stat(cut_cdf_qi(cdf, \n                .width = c(.90, .95),\n                labels = scales::percent_format())))) +\n  scale_fill_brewer(direction = -1) + \n  labs (title = \"(LAB) Test Phase Accuracy ~ Condition\", \n        x = \"model predicted mean (% correct)\", y = \"Condition\", fill = \"Interval\",\n        subtitle = paste(\"lm(\",call[2],\")\")\n  ) + theme(legend.position = \"blank\")\n\n\n\n\nDiagnostics\n\nCODE#model diagnostics\ncheck_model(lab.testabs.lm1, panel = TRUE)\n\n\n\n\n(1) RESIDUAL DISTRIBUTION:\n(2) HOMOGENEITY:\n(3) HETERSCEDASTICITY: (4) AUTOCORRELATION:\nInference\nOLS Linear Regression on % correct in the TEST PHASE shows that condition explains a small but statistically significant amount of variance (impasse > control). However, the model is a poor fit to the data: (1) the model predictions for each group are closer to the anitimode of each of distribution than the group modes, and (2) the distribution of residuals is not normal.\n(Online Replication)\nVisualization\n\nCODE#HISTOGRAM\nstats = df_online %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(DV_percent_test_NABS)*100)\ngmean = df_online %>% dplyr::summarise(mean = mean(DV_percent_test_NABS)*100)\ngf_props(~DV_percent_test_NABS*100, fill = ~pretty_condition, data = df_online) %>% \n  gf_facet_grid(~pretty_condition) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"% Correct\",\n       y = \"proportion of subjects\",\n       title = \"(ONLINE) TEST Phase Absolute Score (% Correct)\",\n       subtitle = \"\") + theme_minimal()\n\n\n\n\nModel\n\nCODE#SCORE predicted by CONDITION\nrep.testabs.lm1 <- lm(DV_percent_test_NABS ~ pretty_condition, data = df_online)\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(rep.testabs.lm1)\n\n\nCall:\nlm(formula = DV_percent_test_NABS ~ pretty_condition, data = df_online)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -35.4  -35.4  -17.4   39.6   82.5 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                17.45       3.98    4.39 0.000019 ***\npretty_conditionimpasse    17.97       5.47    3.29   0.0012 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39 on 202 degrees of freedom\nMultiple R-squared:  0.0508,    Adjusted R-squared:  0.0461 \nF-statistic: 10.8 on 1 and 202 DF,  p-value: 0.0012\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(rep.testabs.lm1)\n\nAnalysis of Variance Table\n\nResponse: DV_percent_test_NABS\n                  Df Sum Sq Mean Sq F value Pr(>F)   \npretty_condition   1  16410   16410    10.8 0.0012 **\nResiduals        202 306868    1519                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(rep.testabs.lm1)\n\n                        2.5 % 97.5 %\n(Intercept)              9.60   25.3\npretty_conditionimpasse  7.19   28.7\n\nCODE# report(m1) #sanity check\n#print model equation\neq <- extract_eq(rep.testabs.lm1)\n\n\nModel equation $$ = + {1}({}) + \n$$\nFor online replication an OLS linear regression predicting test-phase (% correct) by experimental condition explains a statistically significant though small 5% variance in accuracy (F(1,202) = 10.8, p < 0.01). The estimated beta coefficient (\\(\\beta\\) = 0.18, 95% CI [0.07, 0.29]) predicts that participants in the impasse condition will on average score 18% higher than those in the control condition.\n\nCODE#MODEL ESTIMATES WITH UNCERTAINTY\n\n#setup references \nm <- rep.testabs.lm1\ndf <- df_online \ncall <- m$call %>% as.character()\n\n# uncertainty model visualization\ndf  %>%\n  data_grid(pretty_condition) %>%\n  augment(m, newdata = ., se_fit = TRUE) %>% \n  ggplot(aes(y = pretty_condition, color = pretty_condition)) +\n  stat_halfeye( scale = .5,\n      aes(\n        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n        fill = stat(cut_cdf_qi(cdf, \n                .width = c(.90, .95),\n                labels = scales::percent_format())))) +\n  scale_fill_brewer(direction = -1) + \n  labs (title = \"(ONLINE) Test Phase Accuracy ~ Condition\", \n        x = \"model predicted mean (% correct)\", y = \"Condition\", fill = \"Interval\",\n        subtitle = paste(\"lm(\",call[2],\")\")\n  ) + theme(legend.position = \"blank\")\n\n\n\n\nDiagnostics\n\nCODE#model diagnostics\ncheck_model(rep.testabs.lm1, panel = TRUE)\n\n\n\n\n(1) RESIDUAL DISTRIBUTION: (2) HOMOGENEITY: (3) HETERSCEDASTICITY: (4) AUTOCORRELATION: (5) OUTLIERS: FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE\nInference\nFor in person collection OLS Linear Regression on % correct in the TEST PHASE shows that condition explains a small but statistically significant amount of variance (impasse > control). However, the model is a poor fit to the data: (1) the model predictions for each group are closer to the anitimode of each of distribution than the group modes, and (2) the distribution of residuals is not normal, and the LM assumptions of homogeneity of variance (between groups) and homogeneity of error variance appears to be violated.\nTest Phase Scaled Score\nWhile Absolute Score (as # or % correct) gives an indication of accuracy, it does not differentiate between different kinds of incorrect answers. The Scaled score includes this extra information see ?sec-scoring-scaledScore\nShift in Modal Mass\n\nCODE#HISTOGRAM\nstats = df_subjects %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_SCALED))\ngf_props(~item_test_SCALED, \n         fill = ~pretty_condition, data = df_subjects) %>% gf_facet_grid(~pretty_condition) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"% Correct\",\n       y = \"proportion of subjects\",\n       title = \"Test Phase Scaled Score\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n\n\n\n\nThe Effect of Condition on Test Phase Scaled Score can be described as a ‘shift’ in mass between the two modes of each distribution.\nFIRST, we use the Kolmogorov-Smirnov test as a Robust alternative to the t-test to test if the two distributions likely come from different populations.\n\nCODE#(requires shift function files loaded)\n\n\n#PREP DATA \ndf <- df_subjects %>% dplyr::select(item_test_SCALED, pretty_condition) %>% \n  mutate(\n    data = as.numeric(item_test_SCALED),\n    #flip order levels to correctly orient graph\n    # gr = recode_factor(pretty_condition, \"impasse\" = \"impasse\", \"control\"=\"control\")\n    gr = as.character(pretty_condition)\n  ) %>% dplyr::select(data,gr)\n\ng1 <- df %>% filter(gr == \"control\") %>% dplyr::pull(data)\ng2 <- df %>% filter(gr == \"impasse\") %>% dplyr::pull(data)\n\n\n#COMPARE DISTRIBUTIONS WITH ROBUST TESTS\n\n#What do common tests say about the difference?\n\n# Kolmogorov-Smirnov test\n#If y is numeric, a two-sample (Smirnov) test of the null hypothesis that x and y \n#were drawn from the same continuous distribution is performed. Alternatively, y ...\n\n#null is X is drawn from CDF EQUAL TO Y\nks.test(g1,g2) \n\nWarning in ks.test(g1, g2): p-value will be approximate in the presence of ties\n\n\n\n    Two-sample Kolmogorov-Smirnov test\n\ndata:  g1 and g2\nD = 0.3, p-value = 1e-06\nalternative hypothesis: two-sided\n\nCODEprint(\"SUGGESTS that impasse and control come from different population distributions\")\n\n[1] \"SUGGESTS that impasse and control come from different population distributions\"\n\nCODE# #null is X is NOT LESS THAN Y\nks.test(g1,g2, alternative = \"greater\") \n\nWarning in ks.test(g1, g2, alternative = \"greater\"): p-value will be approximate\nin the presence of ties\n\n\n\n    Two-sample Kolmogorov-Smirnov test\n\ndata:  g1 and g2\nD^+ = 0.3, p-value = 6e-07\nalternative hypothesis: the CDF of x lies above that of y\n\nCODEprint(\"SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]\")\n\n[1] \"SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]\"\n\nCODE#REGULAR T-TEST\nt.test(g1,g2) # regular Welsh t-test\n\n\n    Welch Two Sample t-test\n\ndata:  g1 and g2\nt = -5, df = 327, p-value = 8e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.77 -2.09\nsample estimates:\nmean of x mean of y \n   -4.032    -0.599 \n\n\n\nCODE#IF THIS ERRORS, consider loadling plyr (older than dplyr)\n# kernel density estimate + rug plot + superimposed deciles\nkde <- plot.kde_rug_dec2(df)\n# kde\n\n# compute shift function\nout <- shifthd( g1, g2, nboot=200)\n\n# plot shift function\nsf <- plot.sf(data=out) # function from rgar_visualisation.txt\n# sf\n\n# combine KDE + SF\nplot_grid(kde, sf, labels=c(\"A\", \"B\"), ncol = 1, nrow = 2, rel_heights = c(1.5, 1),label_size = 18,hjust = -1,scale=.95)\n\n\n\n\nLinear Regression\n(In Person)\nVisualization\n\nCODE#HISTOGRAM\nstats = df_lab %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_SCALED))\ngf_props(~item_test_SCALED, fill = ~pretty_condition, data = df_lab) %>% gf_facet_grid(~pretty_condition) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Test Phase Scaled Score [-8, +8]\",\n       y = \"proportion of subjects\",\n       title = \"(LAB) TEST Phase Scaled Score \",\n       subtitle = \"\") + \n  theme_minimal()\n\n\n\n\nModel\n\nCODE#SCORE predicted by CONDITION\nlab.test_scaled.lm1 <- lm(item_test_SCALED ~ pretty_condition, data = df_lab)\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(lab.test_scaled.lm1)\n\n\nCall:\nlm(formula = item_test_SCALED ~ pretty_condition, data = df_lab)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -7.74  -3.98  -3.23   6.76  12.02 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               -4.024      0.818   -4.92  2.7e-06 ***\npretty_conditionimpasse    3.766      1.148    3.28   0.0013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.44 on 124 degrees of freedom\nMultiple R-squared:  0.0798,    Adjusted R-squared:  0.0724 \nF-statistic: 10.8 on 1 and 124 DF,  p-value: 0.00135\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(lab.test_scaled.lm1)\n\nAnalysis of Variance Table\n\nResponse: item_test_SCALED\n                  Df Sum Sq Mean Sq F value Pr(>F)   \npretty_condition   1    447     447    10.8 0.0013 **\nResiduals        124   5150      42                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(lab.test_scaled.lm1)\n\n                        2.5 % 97.5 %\n(Intercept)             -5.64  -2.40\npretty_conditionimpasse  1.49   6.04\n\nCODE# report(m1) #sanity check\n#print model equation\neq <- extract_eq(lab.test_scaled.lm1, use_coefs = TRUE)\n\n\nModel equation \\[\n\\operatorname{\\widehat{item\\_test\\_SCALED}} = -4.02 + 3.77(\\operatorname{pretty\\_condition}_{\\operatorname{impasse}})\n\\]\nFor (In Person) an OLS linear regression predicting test-phase (% correct) by experimental condition explains a statistically significant though small 8% variance in accuracy (F(1,124) = 10.8, p < 0.005). The estimated beta coefficient (\\(\\beta\\) = 3.77, 95% CI [1.49, 6.04]) predicts that participants in the impasse condition will on average 4 points higher than those in the control condition.\n\nCODE#MODEL ESTIMATES WITH UNCERTAINTY\n\n#setup references \nm <- lab.test_scaled.lm1\ndf <- df_lab \ncall <- m$call %>% as.character()\n\n# uncertainty model visualization\ndf  %>%\n  data_grid(pretty_condition) %>%\n  augment(m, newdata = ., se_fit = TRUE) %>% \n  ggplot(aes(y = pretty_condition, color = pretty_condition)) +\n  stat_halfeye( scale = .5,\n      aes(\n        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n        fill = stat(cut_cdf_qi(cdf, \n                .width = c(.90, .95),\n                labels = scales::percent_format())))) +\n  scale_fill_brewer(direction = -1) + \n  labs (title = \"(LAB) Test Phase Scaled Score ~ Condition\", \n        x = \"model predicted mean\", y = \"Condition\", fill = \"Interval\",\n        subtitle = paste(\"lm(\",call[2],\")\")\n  ) + theme(legend.position = \"blank\")\n\n\n\n\nDiagnostics\n\nCODE#model diagnostics\ncheck_model(lab.test_scaled.lm1, panel = TRUE)\n\n\n\n\n(1) RESIDUAL DISTRIBUTION:\n(2) HOMOGENEITY:\n(3) HETERSCEDASTICITY:\n(4) AUTOCORRELATION:\nInference\nOLS Linear Regression on SCALED SCORE in the TEST PHASE shows that condition explains a small but statistically significant amount of variance (impasse > control). However, the model is a poor fit to the data: (1) the model predictions for each group are closer to the anitimode of each of distribution than the group modes, and (2) the distribution of residuals is not normal. (Assumptions of homogenity of variance across groups, and homogeneity of variance in residuals are met)\n(Online Replication)\nVisualization\n\nCODE#HISTOGRAM\nstats = df_online %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_SCALED))\ngf_props(~item_test_SCALED, fill = ~pretty_condition, data = df_lab) %>% gf_facet_grid(~pretty_condition) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"Test Phase Scaled Score [-8, +8]\",\n       y = \"proportion of subjects\",\n       title = \"(ONLINE) TEST Phase Scaled Score \",\n       subtitle = \"\") + \n  theme_minimal()\n\n\n\n\nModel\n\nCODE#SCORE predicted by CONDITION\nrep.test_scaled.lm1 <- lm(item_test_SCALED ~ pretty_condition, data = df_online)\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(rep.test_scaled.lm1)\n\n\nCall:\nlm(formula = item_test_SCALED ~ pretty_condition, data = df_online)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -7.20  -3.96  -2.46   6.80  12.04 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               -4.036      0.622   -6.49  6.4e-10 ***\npretty_conditionimpasse    3.236      0.854    3.79   0.0002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.09 on 202 degrees of freedom\nMultiple R-squared:  0.0663,    Adjusted R-squared:  0.0617 \nF-statistic: 14.3 on 1 and 202 DF,  p-value: 0.000201\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(rep.test_scaled.lm1)\n\nAnalysis of Variance Table\n\nResponse: item_test_SCALED\n                  Df Sum Sq Mean Sq F value Pr(>F)    \npretty_condition   1    532     532    14.3 0.0002 ***\nResiduals        202   7493      37                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(rep.test_scaled.lm1)\n\n                        2.5 % 97.5 %\n(Intercept)             -5.26  -2.81\npretty_conditionimpasse  1.55   4.92\n\nCODE# report(m1) #sanity check\n#print model equation\neq <- extract_eq(rep.test_scaled.lm1, use_coefs = TRUE)\n\n\nModel equation $$ = -4.04 + 3.24(_{})\n$$\nFor online replication an OLS linear regression predicting test-phase (% correct) by experimental condition explains a statistically significant though small 7% variance in accuracy (F(1,202) = 14.3, p < 0.001). The estimated beta coefficient (\\(\\beta\\) = 3.24, 95% CI [1.55, 4.62]) predicts that participants in the impasse condition will on average 3 points higher than those in the control condition.\n\nCODE#MODEL ESTIMATES WITH UNCERTAINTY\n\n#setup references \nm <- rep.test_scaled.lm1\ndf <- df_online \ncall <- m$call %>% as.character()\n\n# uncertainty model visualization\ndf  %>%\n  data_grid(pretty_condition) %>%\n  augment(m, newdata = ., se_fit = TRUE) %>% \n  ggplot(aes(y = pretty_condition, color = pretty_condition)) +\n  stat_halfeye( scale = .5,\n      aes(\n        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n        fill = stat(cut_cdf_qi(cdf, \n                .width = c(.90, .95),\n                labels = scales::percent_format())))) +\n  scale_fill_brewer(direction = -1) + \n  labs (title = \"(LAB) Test Phase Scaled Score ~ Condition\", \n        x = \"model predicted mean (scaled score)\", y = \"Condition\", fill = \"Interval\",\n        subtitle = paste(\"lm(\",call[2],\")\")\n  ) + theme(legend.position = \"blank\")\n\n\n\n\nDiagnostics\n\nCODE#model diagnostics\ncheck_model(rep.test_scaled.lm1, panel = TRUE)\n\n\n\n\n(1) RESIDUAL DISTRIBUTION:\n(2) HOMOGENEITY:\n(3) HETERSCEDASTICITY:\n(4) AUTOCORRELATION:\nInference\nFor online replication an OLS Linear Regression on SCALED SCORE in the TEST PHASE shows that condition explains a small but statistically significant amount of variance (impasse > control). However, the model is a poor fit to the data: (1) the model predictions for each group are closer to the anitimode of each of distribution than the group modes, and (2) the distribution of residuals is not normal. (Assumptions of homogenity of variance across groups, and homogeneity of variance in residuals are met)"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#todo-robust-alternative",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#todo-robust-alternative",
    "title": "4  Hypothesis Testing",
    "section": "TODO ROBUST ALTERNATIVE",
    "text": "TODO ROBUST ALTERNATIVE\n\nWIP EXPLORING\n\nBeta Regression (% Correct)\nBeta regression on % correct (with standard transformation for including [0,1])\n\n\nCODE\nlibrary(betareg)\n\nsub <- df_subjects %>% dplyr::select(condition, DV_percent_NABS)\nn = nrow(sub) %>% unlist()\nsub$dv_transformed = (sub$DV_percent_NABS * (n-1) + 0.5)/n\n\nhistogram(sub$dv_transformed)\n\n\n\n\n\nCODE\nhistogram(df_subjects$DV_percent_NABS)\n\n\n\n\n\nCODE\nmb <- betareg(dv_transformed ~ condition, data = sub)\n\nsummary(mb)\n\n\n\nCall:\nbetareg(formula = dv_transformed ~ condition, data = sub)\n\nStandardized weighted residuals 2:\n   Min     1Q Median     3Q    Max \n-1.057 -0.453 -0.216  0.541  1.690 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.969      0.108   -8.97   <2e-16 ***\ncondition121    0.556      0.143    3.89   0.0001 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(>|z|)    \n(phi)   0.6604     0.0425    15.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  506 on 3 Df\nPseudo R-squared: 0.0725\nNumber of iterations: 12 (BFGS) + 1 (Fisher scoring) \n\n\nCODE\nplot(mb)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression (Transformed # Correct) ??"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1b-q1-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1b-q1-accuracy",
    "title": "4  Hypothesis Testing",
    "section": "H1B | Q1 ACCURACY",
    "text": "H1B | Q1 ACCURACY\nThe graph comprehension tasks includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their first exposure to the unconventional triangular coordinate system.\nTODO: - does impasse yield different exploration behavior? (characterize mouse) - does impasse yield more time on task? (characterize response time ? number of answers then de-selected?)\nTODO: Think about characterizing how variable the interpretations are across a participant. Do they form an interpretation and hold it constant? Or do they change question to question.\nResponse Accuracy of First Question by Condition\nChi Square | Accuracy ~ Condition\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\nAnalysis Strategy\nChi-Square test of independence on outcome score_niceABS by condition for df_items where q == 1\n\n\n\nJustification\n\n(0) simplest method to examine independence of two categorical factors; logistic regression is recommended for binomial ~ continuous\n(1) independence assumption : as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\n(2) frequency size assumption : expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\n\nSteps\n\n(1) Express raw data as contingency table & visualize\n(2) Calculate Chi-Squared Statistic and p-value\n(3) Interpret Odds-Ratio as effect size\n\n\n\nInference\n\nLab For the (In Person) (n=126) the Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition approaching statistical significance, \\(\\chi^2\\) (1) = 10.3, p = 0.07. In this particular data sample, the odds ratio (2.18, p = 0.055, 95% CI [0.982, +Inf]) indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition.\nOnline For online data collection (n=204), a Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. The odds ratio (2.68, p = 0.005, 95% CI [1.37, +Inf]) indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition.\n\n\n\n\n\nCODE#FITER THE DATASET\ndf = df_items %>% filter(q==1) \n\n#PROPORTIONAL BAR CHART\ngf_props(~score_niceABS, data = df, fill = ~mode) %>% \n  gf_facet_grid(mode~condition, labeller = label_both) +\n  labs(x = \"Correct Response on Q 1\",\n       title = \"Accuracy on First Question by Condition (Both Modalities)\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses than control \")+\n  theme_minimal()+ theme(legend.position = \"none\")\n\n\n\n\nA proportional bar chart visualizing the proportion of incorrect (x =0) vs correct (x = 1) responses in each condition (right/left facet) for each data collection modality (top/bottom) reveal that the pattern of responses appear the same regardless of the data collection modality. In both data collection sessions, the proportion of incorrect responses is much greater than the proportion of correct responses, regardless of condition. In the impasse condition, the difference in proportions is smaller than the control condition (i.e. There are more correct responses in the impasse condition than the control condition).\n\nCODE#MOSAIC PLOT\nvcd::mosaic(main=\"Accuracy on First Question by Condition (Both Modalities)\",\n            data = df, score_niceABS ~ condition, rot_labels=c(0,90,0,0),\n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\")))\n\n\n\nCODE#PRINT CONTINGENCY TABLE\ntitle = \"Proportion of Correct Responses On First Item (Both Modalities)\"\nitem.contingency <-  df %>% dplyr::select(condition, score_niceABS) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Responses On First Item (Both Modalities)\n \n   \n    0 \n    1 \n    Sum \n  \n\n\n 111 \n    0.412 \n    0.067 \n    0.479 \n  \n\n 121 \n    0.373 \n    0.148 \n    0.521 \n  \n\n Sum \n    0.785 \n    0.215 \n    1.000 \n  \n\n\n\n\nA mosaic plot condition by response accuracy on the first question (across both data collection modalities) reveals the same pattern (the mosaic plot is an alternative visualization technique to the proportional bar chart). The relative size of condition boxes (111 vs 121) reflects that the sample is roughly evenly split across experimental conditions. The difference in size between 0 (incorrect) and 1 (correct) reflects that the proportion of correct responses (1) is greater in the impasse condition (121).\nNext, we compute a contingency table and Pearson’s Chi-Squared test for each data collection modality.\n\nCODEdf = df_items %>% filter(q==1) %>% filter(mode == \"lab-synch\")\nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  126 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |        52 |        10 |        62 | \n             |    47.730 |    14.270 |           | \n             |     0.382 |     1.278 |           | \n             |     0.839 |     0.161 |     0.492 | \n             |     0.536 |     0.345 |           | \n             |     0.413 |     0.079 |           | \n-------------|-----------|-----------|-----------|\n         121 |        45 |        19 |        64 | \n             |    49.270 |    14.730 |           | \n             |     0.370 |     1.238 |           | \n             |     0.703 |     0.297 |     0.508 | \n             |     0.464 |     0.655 |           | \n             |     0.357 |     0.151 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |        97 |        29 |       126 | \n             |     0.770 |     0.230 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  3.27     d.f. =  1     p =  0.0707 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  2.55     d.f. =  1     p =  0.111 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.18 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.0909 \n95% confidence interval:  0.86 5.84 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  0.979 \n95% confidence interval:  0 5.03 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.0547 \n95% confidence interval:  0.982 Inf \n\n\n \n\n\nFor the (In Person) (n=126) the Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition approaching statistical significance, \\(\\chi^2\\) (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. In this particular data sample, the odds ratio (Odds Ratio = 2.18, p = 0.055, 95% CI [0.982, +Inf]) indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition .\n\nCODEdf = df_items %>% filter(q==1) %>% filter(mode == \"asynch\")\nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  204 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |        84 |        12 |        96 | \n             |    76.235 |    19.765 |           | \n             |     0.791 |     3.050 |           | \n             |     0.875 |     0.125 |     0.471 | \n             |     0.519 |     0.286 |           | \n             |     0.412 |     0.059 |           | \n-------------|-----------|-----------|-----------|\n         121 |        78 |        30 |       108 | \n             |    85.765 |    22.235 |           | \n             |     0.703 |     2.711 |           | \n             |     0.722 |     0.278 |     0.529 | \n             |     0.481 |     0.714 |           | \n             |     0.382 |     0.147 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |       162 |        42 |       204 | \n             |     0.794 |     0.206 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  7.26     d.f. =  1     p =  0.00707 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  6.35     d.f. =  1     p =  0.0117 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.68 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.00894 \n95% confidence interval:  1.23 6.17 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  0.998 \n95% confidence interval:  0 5.42 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.00539 \n95% confidence interval:  1.37 Inf \n\n\n \n\n\nFor online data collection (n=204), a Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. The odds ratio (Odds Ratio = 2.68, p = 0.005, 95% CI [1.37, +Inf]) indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition .\n\nCODEdf = df_items %>% filter(q==1) \nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  330 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |       136 |        22 |       158 | \n             |   124.006 |    33.994 |           | \n             |     1.160 |     4.232 |           | \n             |     0.861 |     0.139 |     0.479 | \n             |     0.525 |     0.310 |           | \n             |     0.412 |     0.067 |           | \n-------------|-----------|-----------|-----------|\n         121 |       123 |        49 |       172 | \n             |   134.994 |    37.006 |           | \n             |     1.066 |     3.887 |           | \n             |     0.715 |     0.285 |     0.521 | \n             |     0.475 |     0.690 |           | \n             |     0.373 |     0.148 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |       259 |        71 |       330 | \n             |     0.785 |     0.215 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  10.3     d.f. =  1     p =  0.0013 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  9.5     d.f. =  1     p =  0.00205 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.46 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.00131 \n95% confidence interval:  1.37 4.53 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  1 \n95% confidence interval:  0 4.12 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.000928 \n95% confidence interval:  1.49 Inf \n\n\n \n\n\nCombining data across both sessions (n=330), a Pearson’s Chi-squared test suggests a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi_2\\) (1) = 10.3, p = 0.001. The sample odds ratio (2.46, p = 0.001, 95% CI [1.37, 4.53]) indicates that the odds of providing a correct response to the first question are 2.46 higher for subjects in the impasse condition than those in the control condition.\nTODO | (multiple) Logistic Regression Model | What predicts Q1 Accuracy?"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1c-q1-latency",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1c-q1-latency",
    "title": "4  Hypothesis Testing",
    "section": "H1C | Q1 LATENCY",
    "text": "H1C | Q1 LATENCY"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#resources",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#resources",
    "title": "5  Hypothesis Testing",
    "section": "RESOURCES",
    "text": "RESOURCES\nreset plot margins par(mar=c(1,1,1,1))\nLogistic Regression Notes\nThe logistic regression intercept gives the log odds of the outcome for the reference level of the predictor variable\nThe logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#archive",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#archive",
    "title": "4  Hypothesis Testing",
    "section": "ARCHIVE",
    "text": "ARCHIVE\n\nTest Phase Absolute Score (# questions)\n\nLinear Regression\nLM on Test Phase absolute score as number of questions, rather than % correct.\n\n\nCODE\n#SCORE predicted by CONDITION\nlm.1 <- lm(item_test_NABS ~ condition, data = df_subjects)\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(lm.1)\n\n\n\nCall:\nlm(formula = item_test_NABS ~ condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3.02  -2.77  -1.52   2.98   6.48 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.519      0.251    6.04  4.1e-09 ***\ncondition121    1.498      0.348    4.30  2.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.16 on 328 degrees of freedom\nMultiple R-squared:  0.0535,    Adjusted R-squared:  0.0506 \nF-statistic: 18.5 on 1 and 328 DF,  p-value: 0.0000222\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(lm.1)\n\n\nAnalysis of Variance Table\n\nResponse: item_test_NABS\n           Df Sum Sq Mean Sq F value   Pr(>F)    \ncondition   1    185     185    18.5 0.000022 ***\nResiduals 328   3274      10                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(lm.1)\n\n\n             2.5 % 97.5 %\n(Intercept)  1.025   2.01\ncondition121 0.814   2.18\n\n\nCODE\nreport(lm.1) #sanity check\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict item_test_NABS with condition (formula: item_test_NABS ~ condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.05, F(1, 328) = 18.52, p < .001, adj. R2 = 0.05). The model's intercept, corresponding to condition = 111, is at 1.52 (95% CI [1.02, 2.01], t(328) = 6.04, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 1.50, 95% CI [0.81, 2.18], t(328) = 4.30, p < .001; Std. beta = 0.46, 95% CI [0.25, 0.67])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\ncheck_model(lm.1)\n\n\n\n\n\n\n\nPoisson Regression TODO\nhttps://stats.oarc.ucla.edu/r/dae/poisson-regression/\nThe outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can consider it a type of count, (ie. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function).\n\n\nCODE\n#POISSON\n\n#SCORE predicted by CONDITION --> POISSON DISTRIBUTION\np.1 <- glm(item_test_NABS ~ condition, data = df_subjects, family = \"poisson\")\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(p.1)\n\n\n\nCall:\nglm(formula = item_test_NABS ~ condition, family = \"poisson\", \n    data = df_subjects)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -2.46   -2.28   -1.74    1.51    3.69  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    0.4180     0.0645    6.48  9.4e-11 ***\ncondition121   0.6864     0.0781    8.79  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1579.3  on 329  degrees of freedom\nResidual deviance: 1496.7  on 328  degrees of freedom\nAIC: 1956\n\nNumber of Fisher Scoring iterations: 6\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(p.1)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: item_test_NABS\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev\nNULL                        329       1579\ncondition  1     82.7       328       1497\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(p.1)\n\n\nWaiting for profiling to be done...\n\n\n             2.5 % 97.5 %\n(Intercept)  0.289  0.542\ncondition121 0.535  0.841\n\n\nCODE\nreport(p.1) #sanity check\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a poisson model (estimated using ML) to predict item_test_NABS with condition (formula: item_test_NABS ~ condition). The model's explanatory power is moderate (Nagelkerke's R2 = 0.22). The model's intercept, corresponding to condition = 111, is at 0.42 (95% CI [0.29, 0.54], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.69, 95% CI [0.53, 0.84], p < .001; Std. beta = 0.69, 95% CI [0.53, 0.84])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nCODE\ncheck_model(p.1)\n\n\n\n\n\n\n\nZero Inflated Poisson\nhttps://stats.oarc.ucla.edu/r/dae/zip/\nPoisson count process with excess zeros\n\n\nCODE\n#ZERO INFLATED POISSON\n\nzinfp.1 <- zeroinfl(item_test_NABS ~  item_q1_rt| condition , data = df_subjects)\nsummary(zinfp.1)\n\n\n\nCall:\nzeroinfl(formula = item_test_NABS ~ item_q1_rt | condition, data = df_subjects)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.934 -0.821 -0.548  0.965  2.421 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 1.654243   0.059975   27.58   <2e-16 ***\nitem_q1_rt  0.001690   0.000849    1.99    0.047 *  \n\nZero-inflation model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     0.978      0.179    5.46  4.7e-08 ***\ncondition121   -1.055      0.236   -4.48  7.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -531 on 4 Df\n\n\nCODE\nreport(zinfp.1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a zero-inflated poisson model to predict item_test_NABS with item_q1_rt and condition (formula: item_test_NABS ~ item_q1_rt). The model's explanatory power is substantial (R2 = 0.35, adj. R2 = 0.35). The model's intercept, corresponding to item_q1_rt = 0, is at 1.65 (95% CI [1.54, 1.77], p < .001). Within this model:\n\n  - The effect of item q1 rt is statistically significant and positive (beta = 1.69e-03, 95% CI [2.52e-05, 3.35e-03], p = 0.047; Std. beta = 0.06, 95% CI [7.11e-04, 0.12])\n  - The effect of condition [121] is statistically significant and negative (beta = -1.06, 95% CI [-1.52, -0.59], p < .001; Std. beta = -1.06, 95% CI [-1.52, -0.59])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\n\nCODE\nperformance(zinfp.1)\n\n\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------\n1070.173 | 1085.370 | 0.354 |     0.350 | 3.131 | 3.150 |    -1.609 |           0.044\n\n\nCODE\n# check_model(zinfp.1)\n\n\n\n\nNegative Binomial Regression\nhttps://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/ - overdispersed count data (variance much greater than mean)\n\n\nCODE\n#NEGATIVE BIONOMIAL REGRESSION\n# - https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/\n# - Overdispersed Count variables\n\nlibrary(MASS)\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nCODE\nnb.1 <- glm.nb(item_test_NABS ~ condition, data = df_subjects)\nsummary(nb.1)\n\n\n\nCall:\nglm.nb(formula = item_test_NABS ~ condition, data = df_subjects, \n    init.theta = 0.253501538, link = log)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.139  -1.102  -0.993   0.378   1.091  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)   \n(Intercept)     0.418      0.171    2.45   0.0143 * \ncondition121    0.686      0.232    2.95   0.0031 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.254) family taken to be 1)\n\n    Null deviance: 279.52  on 329  degrees of freedom\nResidual deviance: 270.97  on 328  degrees of freedom\nAIC: 1194\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2535 \n          Std. Err.:  0.0315 \n\n 2 x log-likelihood:  -1188.1290 \n\n\nCODE\nreport(nb.1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a negative-binomial model (estimated using ML) to predict item_test_NABS with condition (formula: item_test_NABS ~ condition). The model's explanatory power is weak (Nagelkerke's R2 = 0.04). The model's intercept, corresponding to condition = 111, is at 0.42 (95% CI [0.10, 0.77], p = 0.014). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.69, 95% CI [0.23, 1.14], p = 0.003; Std. beta = 0.69, 95% CI [0.23, 1.14])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nCODE\ncheck_model(nb.1)\n\n\n\n\n\nCODE\n#check model assumption\n#assumes conditional means are not equal to conditional variances\n#conduct likelihood ration test to compare and test [need poisson]\nm3 <- glm(item_test_NABS ~ condition, family = \"poisson\", data = df_subjects)\npchisq(2 * (logLik(nb.1) - logLik(m3)), df = 1, lower.tail = FALSE)\n\n\n'log Lik.' 4.3e-168 (df=3)\n\n\nCODE\n#A large (+) log likelihood suggests that the negative binomial is more appropriate than the Poisson model\n\n\n#EXPONENTIATE PARAMETER ESTIMATES\nest <- cbind(Estimate = coef(nb.1), confint(nb.1))\n\n\nWaiting for profiling to be done...\n\n\nCODE\n#exponentiate parameter estimates\nprint(\"Exponentiated Estimates\")\n\n\n[1] \"Exponentiated Estimates\"\n\n\nCODE\nexp(est)\n\n\n             Estimate 2.5 % 97.5 %\n(Intercept)      1.52  1.10   2.15\ncondition121     1.99  1.26   3.13\n\n\nThe variable condition has a coefficient of 0.67, (p < 0.005). This means that for the impasse condition, the expected log count # of questions increases by 0.67. By exponentiating the estimate we see that # question correct rate for the impasse condition is nearly 2x that of the control condition.\nDiagnostics ??\n\n\nZero Inflated Negative Binomial Regression\nhttps://stats.oarc.ucla.edu/r/dae/zinb/ count data that are overdispersed and have excess zeros\nZero-inflated negative binomial regression is for modelling count variables with excessive zeros, and especially when the count data are overdispersed (mean is much larger than variance). It can help account for situations where theory suggests that excess zeros are generated by 2 separate processes, one that includes the other count values, and the other that is just the zeros, and thus that the excess zeros can be modelled independently.\nTotal Absolute Score (# items correct) may fit this situation, as the data are overdispersed (variance much greater than the mean) and there are are very large number of zeros. It is theoretically plausible that these excess zeros (no answers correct) are the result of a different ‘process’ … (i.e) little understanding and/or resistance to restructuring understanding of the coordinate system. However, I am not certain if it is plausible to suggest that the zeros themselves are the result of two different processes: (ie. perhaps trying to understand, and not trying to understand?) <- this could maybe be disentangled by first question latency?\nThe model includes: - A logistic model to model which of the two processes the zero outcome is associated with - A negative binomial model to model the count process\n\n\nCODE\nlibrary(pscl) #  for zeroinfl negbinomial\n\n#ZERO INFLATED NEGATIVE BINOMIAL\nzinb.1 <- zeroinfl(item_test_NABS ~ condition | condition , data = df_subjects, dist = \"negbin\")\n#before the | is the count part, after the | is the logit model\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(zinb.1)\n\n\n\nCall:\nzeroinfl(formula = item_test_NABS ~ condition | condition, data = df_subjects, \n    dist = \"negbin\")\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.866 -0.794 -0.538  0.856  2.294 \n\nCount model coefficients (negbin with log link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    1.7126     0.0728   23.54  < 2e-16 ***\ncondition121   0.0451     0.0880    0.51  0.60810    \nLog(theta)     3.1851     0.8732    3.65  0.00026 ***\n\nZero-inflation model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     0.974      0.179    5.43  5.5e-08 ***\ncondition121   -1.056      0.236   -4.47  7.7e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 24.169 \nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -532 on 5 Df\n\n\nCODE\nreport(zinb.1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a zero-inflated negative-binomial model to predict item_test_NABS with condition (formula: item_test_NABS ~ condition). The model's explanatory power is substantial (R2 = 0.36, adj. R2 = 0.36). The model's intercept, corresponding to condition = 111, is at 1.71 (95% CI [1.57, 1.86], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically non-significant and positive (beta = 0.05, 95% CI [-0.13, 0.22], p = 0.608; Std. beta = 0.05, 95% CI [-0.13, 0.22])\n  - The effect of condition [121] is statistically significant and negative (beta = -1.06, 95% CI [-1.52, -0.59], p < .001; Std. beta = -1.06, 95% CI [-1.52, -0.59])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\n\nCODE\nperformance(zinb.1)\n\n\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------\n1073.880 | 1092.876 | 0.363 |     0.359 | 3.150 | 3.174 |    -1.649 |           0.043\n\n\nCODE\n# #EXPONENTIATE PARAMETER ESTIMATES\n# est <- cbind(Estimate = coef(zinb.1), confint(zinb.1))\n# #exponentiate parameter estimates\n# print(\"Exponentiated Estimates\")\n# exp(est)\n\n\nIn the count model, the coefficient for the condition is very small, and not significant (suggesting it does not contribute to the count yielding process?).\nIn the zero-inflation model, the coefficient for the condition variable is -1.056 and statistically significant. This suggests that the log odds of being an excessive zero decrease by 1.06 if you are in the impasse condition (exponentiate it?)\nTODO come back to this and discuss further\n\n\n\nHURDLE MODEL\nhttps://data.library.virginia.edu/getting-started-with-hurdle-models/ https://en.wikipedia.org/wiki/Hurdle_model#:~:text=A%20hurdle%20model%20is%20a,of%20the%20non%2Dzero%20values.\nclass of models for count data with both overdispersion and excess zeros;\ndifferent from zero-inflated models where the excess zeros are theorized to arise from two different processes; in the hurdle model, there is a model for P(x=0) and a separate model for P(x!=0)\nThe model includes: - A binary logit model to model whether the observation takes a positive count or not. - a truncated Poisson or Negative binomial model that only fits positive counts\nThis allows us to model: (1) Does the student get any questions right? (2) How many questions does the student get right?\n\n\nCODE\nlibrary(pscl) #zero-inf and hurdle models \nlibrary(countreg) #rootogram\n\n\nRegistered S3 methods overwritten by 'countreg':\n  method                 from\n  print.zeroinfl         pscl\n  print.summary.zeroinfl pscl\n  summary.zeroinfl       pscl\n  coef.zeroinfl          pscl\n  vcov.zeroinfl          pscl\n  logLik.zeroinfl        pscl\n  predict.zeroinfl       pscl\n  residuals.zeroinfl     pscl\n  fitted.zeroinfl        pscl\n  terms.zeroinfl         pscl\n  model.matrix.zeroinfl  pscl\n  extractAIC.zeroinfl    pscl\n  print.hurdle           pscl\n  print.summary.hurdle   pscl\n  summary.hurdle         pscl\n  coef.hurdle            pscl\n  vcov.hurdle            pscl\n  logLik.hurdle          pscl\n  predict.hurdle         pscl\n  residuals.hurdle       pscl\n  fitted.hurdle          pscl\n  terms.hurdle           pscl\n  model.matrix.hurdle    pscl\n  extractAIC.hurdle      pscl\n\n\n\nAttaching package: 'countreg'\n\n\nThe following objects are masked from 'package:pscl':\n\n    hurdle, hurdle.control, hurdletest, zeroinfl, zeroinfl.control\n\n\nThe following object is masked from 'package:vcd':\n\n    rootogram\n\n\nCODE\nh.1 <- hurdle(s_NABS ~ condition | condition , data = df_subjects,\n              zero.dist = \"binomial\", dist = \"poisson\")\n\nsummary(h.1)\n\n\n\nCall:\nhurdle(formula = s_NABS ~ condition | condition, data = df_subjects, \n    dist = \"poisson\", zero.dist = \"binomial\")\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-1.255 -0.936 -0.693  1.038  2.959 \n\nCount model coefficients (truncated poisson with log link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    1.8872     0.0509   37.10   <2e-16 ***\ncondition121   0.0661     0.0614    1.08     0.28    \nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.518      0.164   -3.15   0.0016 ** \ncondition121    1.354      0.234    5.79  6.9e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -873 on 4 Df\n\n\nCODE\n# h.2 <- hurdle(item_test_NABS ~ condition, data = df_subjects,\n#               zero.dist = \"binomial\", dist = \"negbin\")\n# \n# summary(h.2)\n\n\n\nrootogram(h.1)\n\n\n\n\n\nCODE\nperformance(h.1)\n\n\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------\n1753.435 | 1768.631 | 0.480 |     0.476 | 4.808 | 4.838 |    -3.434 |           0.038\n\n\n\n\nModel Comparison\n\n\nCODE\ncompare_performance(lm.1, p.1, nb.1, zinb.1)\n\n\n# Comparison of Model Performance Indices\n\nName   |    Model |      AIC | AIC weights |      BIC | BIC weights |  RMSE | Sigma | Score_log | Score_spherical |    R2 | R2 (adj.) | Nagelkerke's R2\n-------------------------------------------------------------------------------------------------------------------------------------------------------\nlm.1   |       lm | 1699.782 |     < 0.001 | 1711.179 |     < 0.001 | 3.150 | 3.160 |           |                 | 0.053 |     0.051 |                \np.1    |      glm | 1955.788 |     < 0.001 | 1963.386 |     < 0.001 | 3.150 | 2.136 |    -2.957 |           0.042 |       |           |           0.223\nnb.1   |   negbin | 1194.129 |     < 0.001 | 1205.526 |     < 0.001 | 3.150 | 0.909 |    -2.137 |           0.046 |       |           |           0.045\nzinb.1 | zeroinfl | 1073.880 |        1.00 | 1092.876 |        1.00 | 3.150 | 3.174 |    -1.649 |           0.043 | 0.363 |     0.359 |                \n\n\nFor modelling test phase absolute score (# items correct) it seems that the zero inflated negative binomial model is the best fit according to R2 and AIC, however, I am not clear on the implications of the interpretation (non significant in count process, significant on logit process), and also not clear if # items correct is truly a count process."
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html",
    "title": "5  Exploratory Analyses",
    "section": "",
    "text": "TODO\n- response consistency - clarify core questions being asked\n- review models already created in ARCHIVE?\n- explore response consistency - fix references - what predicts consistency?\nThe purpose of this notebook is exploratory analyses of data collected for study SGC3A.\nExploratory Questions\nConsistency | How consistent are learners in their interpretation of the graph? Do they adopt an interpretation on the first question and hold constant? Or do they change interpretations from question to question? Are there any interpretations that serve as ‘absorbing states’ (i.e. once encountered, the learner does not exist this state).\nTime Course of Exploration | What is the relationship between response accuracy (and interpretation) and time spent on each item?\nCan exploration strategies be derived from mouse cursor activity?\nTODO: - does impasse yield different exploration behavior? (characterize mouse) - does impasse yield more time on task? (characterize response time ? number of answers then de-selected?)\nTODO: Think about characterizing how variable the interpretations are across a participant. Do they form an interpretation and hold it constant? Or do they change question to question."
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#response-consistency",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#response-consistency",
    "title": "5  Exploratory Analyses",
    "section": "RESPONSE CONSISTENCY",
    "text": "RESPONSE CONSISTENCY\nTODO"
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#xyz",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#xyz",
    "title": "5  Exploratory Analyses",
    "section": "XYZ",
    "text": "XYZ\n\nXYZ\n\nXYZ\n\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\n\nAnalysis Strategy\nChi-Square test of independence on outcome score_niceABS by condition for df_items where q == 1\n\n\nJustification\n(0) simplest method to examine independence of two categorical factors; logistic regression is recommended for binomial ~ continuous\n(1) independence assumption : as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\n(2) frequency size assumption : expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\nSteps\n(1) Express raw data as contingency table & visualize\n(2) Calculate Chi-Squared Statistic and p-value\n(3) Interpret Odds-Ratio as effect size\n\n\nInference\nLab A Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition approaching statistical significance, \\(\\chi^2\\) (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. In this particular data sample, the sample odds ratio 2.18 indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than if the control condition (Odds Ratio = 2.18, p = 0.055, 95% CI [0.982, +Inf]).\nOnline A Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. The sample odds ratio = 2.68 indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition (Odds Ratio = 2.68, p = 0.005, 95% CI [1.37, +Inf]).\n\n\n\n\n\nCODE\n#FITER THE DATASET\ndf = df_items %>% filter(q==1) \n\n#PROPORTIONAL BAR CHART\ngf_props(~score_niceABS, data = df, fill = ~mode) %>% \n  gf_facet_grid(mode~condition, labeller = label_both) +\n  labs(x = \"Correct Response on Q 1\",\n       title = \"Accuracy on First Question by Condition (Both Modalities)\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses than control \")+\n  theme_minimal()+ theme(legend.position = \"none\")\n\n\n\n\n\nCODE\n#MOSAIC PLOT\nvcd::mosaic(main=\"Accuracy on First Question by Condition (Both Modalities)\",\n            data = df, score_niceABS ~ condition, rot_labels=c(0,90,0,0),\n            offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n            spacing = spacing_dimequal(unit(1:2, \"lines\")))\n\n\n\n\n\nCODE\n#PRINT CONTINGENCY TABLE\ntitle = \"Proportion of Correct Responses On First Item (Both Modalities)\"\nitem.contingency <-  df %>% dplyr::select(condition, score_niceABS) %>% table() %>% prop.table() %>% addmargins()\nitem.contingency %>% kbl (caption = title) %>% kable_classic()\n\n\n\nProportion of Correct Responses On First Item (Both Modalities)\n \n  \n      \n    0 \n    1 \n    Sum \n  \n \n\n  \n    111 \n    0.412 \n    0.067 \n    0.479 \n  \n  \n    121 \n    0.373 \n    0.148 \n    0.521 \n  \n  \n    Sum \n    0.785 \n    0.215 \n    1.000 \n  \n\n\n\n\n\n\n\nCODE\ndf = df_items %>% filter(q==1) %>% filter(mode == \"lab-synch\")\nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  126 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |        52 |        10 |        62 | \n             |    47.730 |    14.270 |           | \n             |     0.382 |     1.278 |           | \n             |     0.839 |     0.161 |     0.492 | \n             |     0.536 |     0.345 |           | \n             |     0.413 |     0.079 |           | \n-------------|-----------|-----------|-----------|\n         121 |        45 |        19 |        64 | \n             |    49.270 |    14.730 |           | \n             |     0.370 |     1.238 |           | \n             |     0.703 |     0.297 |     0.508 | \n             |     0.464 |     0.655 |           | \n             |     0.357 |     0.151 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |        97 |        29 |       126 | \n             |     0.770 |     0.230 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  3.27     d.f. =  1     p =  0.0707 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  2.55     d.f. =  1     p =  0.111 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.18 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.0909 \n95% confidence interval:  0.86 5.84 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  0.979 \n95% confidence interval:  0 5.03 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.0547 \n95% confidence interval:  0.982 Inf \n\n\n \n\n\nInspecting the output of the Chi-Squared test, we first see that we meet the assumption of expected frequency in each cell (indicated by the second row in each box, ‘Expected N’. The model predicts more than 5 observations in each cell.) The Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition approaching statistical significance, \\(\\chi^2\\) (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is not equal to 1. In this particular data sample, the odds ratio 2.18 indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition (Odds Ratio = 2.18, p = 0.055, 95% CI [0.982, +Inf]).\n\n\nCODE\ndf = df_items %>% filter(q==1) %>% filter(mode == \"asynch\")\nCrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  204 \n\n \n             | df$score_niceABS \ndf$condition |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n         111 |        84 |        12 |        96 | \n             |    76.235 |    19.765 |           | \n             |     0.791 |     3.050 |           | \n             |     0.875 |     0.125 |     0.471 | \n             |     0.519 |     0.286 |           | \n             |     0.412 |     0.059 |           | \n-------------|-----------|-----------|-----------|\n         121 |        78 |        30 |       108 | \n             |    85.765 |    22.235 |           | \n             |     0.703 |     2.711 |           | \n             |     0.722 |     0.278 |     0.529 | \n             |     0.481 |     0.714 |           | \n             |     0.382 |     0.147 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |       162 |        42 |       204 | \n             |     0.794 |     0.206 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  7.26     d.f. =  1     p =  0.00707 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  6.35     d.f. =  1     p =  0.0117 \n\n \nFisher's Exact Test for Count Data\n------------------------------------------------------------\nSample estimate odds ratio:  2.68 \n\nAlternative hypothesis: true odds ratio is not equal to 1\np =  0.00894 \n95% confidence interval:  1.23 6.17 \n\nAlternative hypothesis: true odds ratio is less than 1\np =  0.998 \n95% confidence interval:  0 5.42 \n\nAlternative hypothesis: true odds ratio is greater than 1\np =  0.00539 \n95% confidence interval:  1.37 Inf \n\n\n \n\n\nInspecting the output of the Chi-Squared test, we first see that we meet the assumption of expected frequency in each cell (indicated by the second row in each box, ‘Expected N’. The model predicts more than 5 observations in each cell.) The Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is not equal to 1. The odds ratio = 2.68 indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition (Odds Ratio = 2.68, p = 0.005, 95% CI [1.37, +Inf])."
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#copied-from-3",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#copied-from-3",
    "title": "5  Exploratory Analyses",
    "section": "COPIED FROM 3",
    "text": "COPIED FROM 3\nDoes the IMPASSE condition more accurate interpretation?\nTo address this question we assess how much variance in cumulative (absolute) score is explained by experimental condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_SCALED ~ condition, data = df_subjects %>% filter(mode==\"lab-synch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"lab-synch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12.51  -6.48  -4.48   9.38  19.52 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -6.52       1.19   -5.48  2.3e-07 ***\ncondition121     7.53       1.67    4.51  1.5e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.38 on 124 degrees of freedom\nMultiple R-squared:  0.141, Adjusted R-squared:  0.134 \nF-statistic: 20.3 on 1 and 124 DF,  p-value: 0.0000151\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value   Pr(>F)    \ncondition   1   1787    1787    20.3 0.000015 ***\nResiduals 124  10910      88                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  -8.88  -4.17\ncondition121  4.22  10.84\n\n\nCODE\n# report(m1) #sanity check\n\n\nFor in-lab data collection an OLS linear regression predicting scaled score by experimental condition explains a statistically significant and moderate 15% variance in score (F(1,124) = 22.7, p < 0.001). The estimated beta coefficient (\\(/beta\\) = 7.88, 95% CI [4.61, 11.2]) predicts that participants in the impasse condition will on average score around 8 points (31%) higher than those in the control condition.\n\n\nCODE\n#SCORE predicted by CONDITION\nm1 <- lm(s_SCALED ~ condition, data = df_subjects %>% filter(mode==\"asynch\"))\npaste(\"Model\")\n\n\n[1] \"Model\"\n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12.31  -6.63  -3.63   7.74  19.36 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -6.365      0.883   -7.21  1.1e-11 ***\ncondition121    6.670      1.214    5.49  1.2e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.65 on 202 degrees of freedom\nMultiple R-squared:  0.13,  Adjusted R-squared:  0.126 \nF-statistic: 30.2 on 1 and 202 DF,  p-value: 1.17e-07\n\n\nCODE\npaste(\"Partition Variance\")\n\n\n[1] \"Partition Variance\"\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   2261    2261    30.2 1.2e-07 ***\nResiduals 202  15128      75                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\npaste(\"Confidence Interval on Parameter Estimates\")\n\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\n\nCODE\nconfint(m1)\n\n\n             2.5 % 97.5 %\n(Intercept)  -8.11  -4.62\ncondition121  4.28   9.06\n\n\nCODE\n#report(m1) #sanity check\n\n\nFor the online replication, an OLS linear regression model predicting scaled score by condition explains a statistically significant and moderate 14% of variance in absolute score (F(1,202) = 32.4, p < 0.001). The beta coefficient for condition indicates that on average, participants in the IMPASSE group scored 6.8 points higher on the task than those in the control condition (CI[4.43, 9.12]).\n\n\n\n\n\n\nNote\n\n\n\nFrom these models we can reasonably conclude that the impasse condition yields a reliable, moderate sized effect of improved interpretation on the graph reading tasks."
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#item-level-performance",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#item-level-performance",
    "title": "5  Exploratory Analyses",
    "section": "Item-Level Performance",
    "text": "Item-Level Performance\nIndividual differences with a mixed model."
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#model-peeking",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#model-peeking",
    "title": "5  Exploratory Analyses",
    "section": "Model Peeking",
    "text": "Model Peeking\nTODO\n\nmultiple regression with condition and response time\n\n\n\nCODE\nlibrary(supernova)\n\n\n\nAttaching package: 'supernova'\n\n\nThe following object is masked from 'package:scales':\n\n    number\n\n\nCODE\nlibrary(report)\nlibrary(lmerTest)\n\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nCODE\nlibrary(lme4)\nm1 <- lm( s_SCALED ~ condition, data = df_subjects %>% filter(mode=='asynch'))\nm1\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nCoefficients:\n (Intercept)  condition121  \n       -6.36          6.67  \n\n\nCODE\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ condition, data = df_subjects %>% filter(mode == \n    \"asynch\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12.31  -6.63  -3.63   7.74  19.36 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -6.365      0.883   -7.21  1.1e-11 ***\ncondition121    6.670      1.214    5.49  1.2e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.65 on 202 degrees of freedom\nMultiple R-squared:  0.13,  Adjusted R-squared:  0.126 \nF-statistic: 30.2 on 1 and 202 DF,  p-value: 1.17e-07\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1   2261    2261    30.2 1.2e-07 ***\nResiduals 202  15128      75                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nsuperanova(m1)\n\n\n Analysis of Variance Table (Type III SS)\n Model: s_SCALED ~ condition\n\n                                SS  df       MS      F    PRE     p\n ----- --------------- | --------- --- -------- ------ ------ -----\n Model (error reduced) |  2261.177   1 2261.177 30.193 0.1300 .0000\n Error (from model)    | 15128.156 202   74.892                    \n ----- --------------- | --------- --- -------- ------ ------ -----\n Total (empty model)   | 17389.333 203   85.662                    \n\n\nCODE\nplot(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCODE\ngf_histogram(~s_SCALED, data = df_subjects)\n\n\n\n\n\nCODE\ngf_histogram(~m1$residuals)\n\n\n\n\n\nCODE\n#Assess assumption of independence of errors\n#DW statistic should be close to 2\nlibrary(car)\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'carData'\n\n\nThe following object is masked from 'package:vcdExtra':\n\n    Burt\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCODE\ndurbinWatsonTest(m1)\n\n\n lag Autocorrelation D-W Statistic p-value\n   1        -0.00262          1.99   0.962\n Alternative hypothesis: rho != 0\n\n\nCODE\n#Test for equality of variance\n#H0 is equality; p > 0.05 infer you can't reject null\nleveneTest(m1)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)  \ngroup   1     6.3  0.013 *\n      202                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA simple linear regression model predicting cumulative scaled score (at subject level) by condition explains 13% of the total variance, F(1,329) = 47.8, p < 0.001. The model predicts that participants in the impasse condition will score on average 6.38 points higher than those in the control condition, 95% CI [4.56, 8.19].\n\n\nCODE\nt.test(s_SCALED ~ condition, data = df_subjects)\n\n\n\n    Welch Two Sample t-test\n\ndata:  s_SCALED by condition\nt = -7, df = 325, p-value = 7e-12\nalternative hypothesis: true difference in means between group 111 and group 121 is not equal to 0\n95 percent confidence interval:\n -8.93 -5.06\nsample estimates:\nmean in group 111 mean in group 121 \n           -6.427             0.567 \n\n\nCODE\n#%>% report()\n\n\n\n\nCODE\n# report_participants(df_subjects)\nm1 %>% report()\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with condition (formula: s_SCALED ~ condition). The model explains a statistically significant and moderate proportion of variance (R2 = 0.13, F(1, 202) = 30.19, p < .001, adj. R2 = 0.13). The model's intercept, corresponding to condition = 111, is at -6.36 (95% CI [-8.11, -4.62], t(202) = -7.21, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 6.67, 95% CI [4.28, 9.06], t(202) = 5.49, p < .001; Std. beta = 0.72, 95% CI [0.46, 0.98])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nCODE\nanova(m1) %>% report()\n\n\nFor one-way between subjects designs, partial eta squared is equivalent to eta squared.\nReturning eta squared.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nThe ANOVA suggests that:\n\n  - The main effect of condition is statistically significant and medium (F(1, 202) = 30.19, p < .001; Eta2 = 0.13, 95% CI [0.07, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations.\n\n\nCODE\n#significant intercept means that group is significantly different than zero\n\n\n\n\nCODE\n#logistic regression on on scaled df_subjects because residuals not normal in lm?\nmlog <- glm(s_SCALED ~ condition , data = df_subjects, family = gaussian)\nsummary(mlog)\n\n\n\nCall:\nglm(formula = s_SCALED ~ condition, family = gaussian, data = df_subjects)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-12.57   -6.57   -3.82    8.43   19.43  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -6.427      0.709   -9.06  < 2e-16 ***\ncondition121    6.994      0.982    7.12  6.8e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 79.4)\n\n    Null deviance: 30088  on 329  degrees of freedom\nResidual deviance: 26059  on 328  degrees of freedom\nAIC: 2384\n\nNumber of Fisher Scoring iterations: 2\n\n\nCODE\nreport(mlog)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using ML) to predict s_SCALED with condition (formula: s_SCALED ~ condition). The model's explanatory power is moderate (R2 = 0.13). The model's intercept, corresponding to condition = 111, is at -6.43 (95% CI [-7.82, -5.04], t(328) = -9.06, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 6.99, 95% CI [5.07, 8.92], t(328) = 7.12, p < .001; Std. beta = 0.73, 95% CI [0.53, 0.93])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\n#logistic regression on niceABS by condition\n#pretends that questions are independent and not from same subjects INVALID\nmlog <- glm(score_niceABS ~ condition , data = df_items %>% filter(q<6), family = binomial())\nsummary(mlog)\n\n\n\nCall:\nglm(formula = score_niceABS ~ condition, family = binomial(), \n    data = df_items %>% filter(q < 6))\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.980  -0.980  -0.649   1.389   1.823  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.4508     0.0907  -15.99   <2e-16 ***\ncondition121   0.9672     0.1147    8.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1986.2  on 1649  degrees of freedom\nResidual deviance: 1911.3  on 1648  degrees of freedom\nAIC: 1915\n\nNumber of Fisher Scoring iterations: 4\n\n\nCODE\nreport(mlog)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic model (estimated using ML) to predict score_niceABS with condition (formula: score_niceABS ~ condition). The model's explanatory power is weak (Tjur's R2 = 0.04). The model's intercept, corresponding to condition = 111, is at -1.45 (95% CI [-1.63, -1.28], p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 0.97, 95% CI [0.74, 1.19], p < .001; Std. beta = 0.97, 95% CI [0.74, 1.19])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm2 <- lm( s_NABS ~ condition, data = df_subjects)\nm2\n\n\n\nCall:\nlm(formula = s_NABS ~ condition, data = df_subjects)\n\nCoefficients:\n (Intercept)  condition121  \n        2.47          2.46  \n\n\nCODE\nsummary(m2)\n\n\n\nCall:\nlm(formula = s_NABS ~ condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4.92  -3.67  -2.47   4.08  10.53 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.468      0.384    6.43  4.4e-10 ***\ncondition121    2.456      0.531    4.62  5.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.82 on 328 degrees of freedom\nMultiple R-squared:  0.0611,    Adjusted R-squared:  0.0583 \nF-statistic: 21.4 on 1 and 328 DF,  p-value: 5.49e-06\n\n\nCODE\nanova(m2)\n\n\nAnalysis of Variance Table\n\nResponse: s_NABS\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1    497     497    21.4 5.5e-06 ***\nResiduals 328   7629      23                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nsupernova(m2)\n\n\n Analysis of Variance Table (Type III SS)\n Model: s_NABS ~ condition\n\n                               SS  df      MS      F    PRE     p\n ----- --------------- | -------- --- ------- ------ ------ -----\n Model (error reduced) |  496.765   1 496.765 21.357 0.0611 .0000\n Error (from model)    | 7629.359 328  23.260                    \n ----- --------------- | -------- --- ------- ------ ------ -----\n Total (empty model)   | 8126.124 329  24.699                    \n\n\nA simple linear regression model predicting cumulative absolute score by condition explains 5% of variance, F(1,328) = 16.36, p < 0.001. The model predicts that subjects in the impasse condition will score on average 2 points higher than those in the control condition (Beta = 2.02, 95% CI [1.04, 3.00])\n\n\nCODE\nreport(m2)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_NABS with condition (formula: s_NABS ~ condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.06, F(1, 328) = 21.36, p < .001, adj. R2 = 0.06). The model's intercept, corresponding to condition = 111, is at 2.47 (95% CI [1.71, 3.22], t(328) = 6.43, p < .001). Within this model:\n\n  - The effect of condition [121] is statistically significant and positive (beta = 2.46, 95% CI [1.41, 3.50], t(328) = 4.62, p < .001; Std. beta = 0.49, 95% CI [0.28, 0.70])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\n\n\nCODE\nm.m1 <- lmer( score_SCALED ~ (1 + condition|subject), data = df_items)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00344864 (tol = 0.002, component 1)\n\n\nCODE\nm.m1\n\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: score_SCALED ~ (1 + condition | subject)\n   Data: df_items\nREML criterion at convergence: 9940\nRandom effects:\n Groups   Name         Std.Dev. Corr \n subject  (Intercept)  0.620         \n          condition121 0.865    -0.72\n Residual              0.601         \nNumber of obs: 4950, groups:  subject, 330\nFixed Effects:\n(Intercept)  \n     -0.126  \noptimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings \n\n\nCODE\nsummary(m.m1)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | subject)\n   Data: df_items\n\nREML criterion at convergence: 9940\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-2.902 -0.659 -0.299  0.533  2.718 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n subject  (Intercept)  0.384    0.620         \n          condition121 0.748    0.865    -0.72\n Residual              0.362    0.601         \nNumber of obs: 4950, groups:  subject, 330\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)  -0.1259     0.0346 328.5075   -3.64  0.00032 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00344864 (tol = 0.002, component 1)\n\n\nCODE\nreport(m.m1)\n\n\nPackage 'merDeriv' needs to be installed to compute confidence intervals\n  for random effect parameters.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nPackage 'merDeriv' needs to be installed to compute confidence intervals\n  for random effect parameters.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition and subject as random effects (formula: ~1 + condition | subject). . The model's intercept is at -0.13 (95% CI [-0.19, -0.06], t(4945) = -3.64, p < .001). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm.m2 <- lmer( score_SCALED ~ (1+ condition|q), data = df_items)\nm.m2\n\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: score_SCALED ~ (1 + condition | q)\n   Data: df_items\nREML criterion at convergence: 11669\nRandom effects:\n Groups   Name         Std.Dev. Corr \n q        (Intercept)  0.577         \n          condition121 0.517    -0.93\n Residual              0.778         \nNumber of obs: 4950, groups:  q, 15\nFixed Effects:\n(Intercept)  \n     0.0983  \n\n\nCODE\nsummary(m.m2)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | q)\n   Data: df_items\n\nREML criterion at convergence: 11669\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-1.830 -0.732 -0.424  0.910  2.147 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n q        (Intercept)  0.333    0.577         \n          condition121 0.267    0.517    -0.93\n Residual              0.605    0.778         \nNumber of obs: 4950, groups:  q, 15\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)\n(Intercept)   0.0983     0.0570 14.0000    1.73     0.11\n\n\nCODE\nreport(m.m2)\n\n\nPackage 'merDeriv' needs to be installed to compute confidence intervals\n  for random effect parameters.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nPackage 'merDeriv' needs to be installed to compute confidence intervals\n  for random effect parameters.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition and q as random effects (formula: ~1 + condition | q). . The model's intercept is at 0.10 (95% CI [-0.01, 0.21], t(4945) = 1.73, p = 0.084). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nm.m3 <- lmer( score_SCALED ~ (1+ condition|q) + (1+condition|subject), data = df_items)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0117282 (tol = 0.002, component 1)\n\n\nCODE\nm.m3 %>% summary() \n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: score_SCALED ~ (1 + condition | q) + (1 + condition | subject)\n   Data: df_items\n\nREML criterion at convergence: 8836\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.796 -0.585 -0.083  0.662  3.431 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n subject  (Intercept)  0.3334   0.577         \n          condition121 0.0951   0.308    -0.30\n q        (Intercept)  0.2445   0.494         \n          condition121 0.1848   0.430    -0.90\n Residual              0.2804   0.530         \nNumber of obs: 4950, groups:  subject, 330; q, 15\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)\n(Intercept)   0.0434     0.0706 24.8205    0.61     0.54\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0117282 (tol = 0.002, component 1)\n\n\nCODE\nm.m3 %>% report()\n\n\nPackage 'merDeriv' needs to be installed to compute confidence intervals\n  for random effect parameters.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nPackage 'merDeriv' needs to be installed to compute confidence intervals\n  for random effect parameters.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict score_SCALED (formula: score_SCALED ~ 1). The model included condition, q and subject as random effects (formula: list(~1 + condition | q, ~1 + condition | subject)). . The model's intercept is at 0.04 (95% CI [-0.10, 0.18], t(4942) = 0.61, p = 0.539). Within this model:\n\n  -  ()\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\n\n\nCODE\nanova(m.m1, m.m2, m.m3)\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: df_items\nModels:\nm.m1: score_SCALED ~ (1 + condition | subject)\nm.m2: score_SCALED ~ (1 + condition | q)\nm.m3: score_SCALED ~ (1 + condition | q) + (1 + condition | subject)\n     npar   AIC   BIC logLik deviance Chisq Df Pr(>Chisq)    \nm.m1    5  9945  9978  -4968     9935                        \nm.m2    5 11675 11708  -5832    11665     0  0               \nm.m3    8  8849  8901  -4416     8833  2832  3     <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#resources",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#resources",
    "title": "5  Exploratory Analyses",
    "section": "RESOURCES",
    "text": "RESOURCES"
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html",
    "title": "6  Introduction",
    "section": "",
    "text": "TODO UPDATE ALL\nIn Study 3B we compare the efficacy of the explicit [interaction] scaffold and the implicit [impasse] scaffold."
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html#methods",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html#methods",
    "title": "6  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Scaffold: control,impasse)\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 12.1. The list of questions can be found here.\n\n\n\nFigure 6.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nNote that across both control and impasse conditions, both the question, response options and graph structure were identical. The experimental manipulation (posing a mental impasse) was accomplished by changing the position of datapoints in the impasse-condition graph, such that for any given question, there was no available response option if the reader were to interpret the graph as cartesian (making an orthogonal rather than diagonal projection from the x-axis.)\nThe green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. We hypothesize that this presents the reader with an obstacle, at which point they are forced to confront their interpretation of the coordinate system and (ideally) develop a new strategy.\n\n\n\nFigure 6.2: Sample Question (Q=1) graphs for each condition\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items.\n(3B) The first five items in the task are defined as the SCAFFOLDING block. In the IMPASSE condition, the first five questions included an IMPASSE problem state. For participants in the CONTROL condition, the dataset was structure such that there was always an available ‘orthogonal answer’ for the first 5 questions.\n(3B) The remaining 10 items are defined as the TESTING block. In both conditions, these questions were not structured as impasse (i.e. contained an available orthogonal answer)\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData was collected by convenience sample of a university subject pool. Initial data (Fall 2017, Spring 2018) were collected in-person, with large groups of students simultaneously completing the study (independently) in a computer lab. In Fall 2021 and Winter 2022 we collected additional data to replicate results in a remote format (students completing the study asynchronously on their own computers)."
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html#analysis",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html#analysis",
    "title": "6  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\nBefore analysis, data files from individual data collection periods are harmonized into a common data format.\n\n\n\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nspring17_clean_data.Rmd  spring18_clean_data.Rmd  fall21_clean_data.Rmd  winter2022_clean_sgc3b.Rmd\n2_sgc3B_scoring.qmd\n\n\n\nData for study SGC_3B were collected across four time periods, interrupted by the Covid-19 pandemic.\n\n\n\nPeriod\nModality\n\n\n\n\nFall 2017\nin person, SONA groups in computer lab\n\n\nSpring 2018\nin person, SONA groups in computer lab\n\n\nFall 2021\nasynchronous, online, SONA\n\n\n\nData collected in Fall 2017, Spring 2018 constitute the original SGC_3B study, conducted in person. Data collected in Fall 2021 constitute the web-based replication, conducted online (asynchronously). In all cases, the experiment was administered via a web application.\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3B/data/0-session-level/fall17_sgc3b_participants.csv\"\nspring18 <- \"analysis/SGC3B/data/0-session-level/spring18_sgc3b_participants.csv\"\nfall21 <- \"analysis/SGC3B/data/0-session-level/fall21_sgc3b_participants.csv\"\nmeta <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_participants.csv\" #FOR SCHEMA ONLY\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\nmeta <- read_csv(meta)\n\n#SAVE METADATA FROM SGC3A, but no rows \ndf_subjects <- meta %>% filter(condition=='X') %>% \n  dplyr::select(\n  subject,condition,term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m, \n  # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n  #exploratory factors\n  violations, browser, width, height\n)\n\n#COMPARE COLS\n# janitor::compare_df_cols(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21,meta)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18, df_subjects_fall21) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\",\n    violations = \"NULL\",\n    browser = \"NULL\",\n    width = \"NULL\",\n    height = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, \n                # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                #exploratory factors\n                violations, browser, width, height)\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    pretty_condition = recode_factor(condition, \"111\" = \"control\", \"121\" =  \"impasse\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\"),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#CLEANUP\nrm(df_subjects_fall17,df_subjects_fall21, df_subjects_spring18, df_subjects_before)\nrm(fall17,fall21,spring18,meta)\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis, and renaming columns to be consistent across each file. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC3B/data/0-session-level/fall17_sgc3b_blocks.csv\"\nspring18 <- \"analysis/SGC3B/data/0-session-level/spring18_sgc3b_blocks.csv\"\nfall21 <- \"analysis/SGC3B/data/0-session-level/fall21_sgc3b_blocks.csv\"\nmeta <- \"analysis/SGC3A/data/0-session-level/winter22_sgc3a_items.rds\" #FOR SCHEMA ONLY\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_fall21 <- read_csv(fall21) %>% mutate(mode = \"asynch\", term = \"fall21\")\nmeta <- read_rds(meta) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- meta %>% group_by(q) %>% dplyr::select(q,relation) %>% unique()\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- meta %>% filter(condition=='X') %>% dplyr::select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n  \n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18, df_items_fall21) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  dplyr::select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n\n#COMPARE COLS\njanitor::compare_df_cols(df_items_before, df_items)\n\n\n  column_name df_items_before  df_items\n1      answer       character character\n2   condition         numeric    factor\n3     correct         logical    factor\n4        mode       character    factor\n5           q         numeric    factor\n6    question       character character\n7        rt_s         numeric   numeric\n8     subject       character    factor\n9        term       character    factor\n\n\nCODE\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% dplyr::select(-question,-correct,-rt_s,-num_o)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n  question = \"Please describe how to determine what event(s) start at 12pm?\",\n  response = as.character(response) #doesn't need to be factor\n)\n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#add back pretty condition \ndf_items <- df_items %>% mutate(\n  pretty_condition = recode_factor(condition, \n                                   \"111\" = \"none-none\", \"121\" =  \"none-impasse\", \n                                   \"211\" = \"img-none\", \"221\" =  \"img-impasse\", \n                                   \"311\" = \"ixv-none\", \"321\" =  \"ixv-impasse\", \n                                   ),\n  pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_items_fall17,df_items_fall21, df_items_spring18, df_items_before)\nrm(fall17,fall21,spring18,meta, map_relations)\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3B/data/1-study-level/sgc3b_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3B/data/1-study-level/sgc3b_items.csv\", row.names = FALSE)\nwrite.csv(df_freeresponse,\"analysis/SGC3B/data/1-study-level/sgc3b_freeresponse.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3B/data/1-study-level/sgc3b_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3B/data/1-study-level/sgc3b_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3B/1_sgc3B_introduction.html#resources",
    "href": "analysis/SGC3B/1_sgc3B_introduction.html#resources",
    "title": "6  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.3        bit64_4.0.5       vroom_1.5.7       jsonlite_1.8.0   \n [5] viridisLite_0.4.0 modelr_0.1.8      assertthat_0.2.1  highr_0.9        \n [9] cellranger_1.1.0  yaml_2.3.5        pillar_1.7.0      backports_1.4.1  \n[13] glue_1.6.2        digest_0.6.29     rvest_1.0.2       snakecase_0.11.0 \n[17] colorspace_2.0-3  htmltools_0.5.2   pkgconfig_2.0.3   broom_0.8.0      \n[21] labelled_2.9.1    haven_2.5.0       scales_1.2.0      webshot_0.5.3    \n[25] svglite_2.1.0     openxlsx_4.2.5    rio_0.5.29        tzdb_0.3.0       \n[29] generics_0.1.2    ellipsis_0.3.2    withr_2.5.0       janitor_2.1.0    \n[33] cli_3.3.0         magrittr_2.0.3    crayon_1.5.1      readxl_1.4.0     \n[37] evaluate_0.15     fs_1.5.2          fansi_1.0.3       xml2_1.3.3       \n[41] foreign_0.8-82    tools_4.2.1       data.table_1.14.2 hms_1.1.1        \n[45] lifecycle_1.0.1   munsell_0.5.0     reprex_2.0.1      zip_2.2.0        \n[49] compiler_4.2.1    systemfonts_1.0.4 rlang_1.0.3       grid_4.2.1       \n[53] rstudioapi_0.13   htmlwidgets_1.5.4 rmarkdown_2.14    gtable_0.3.0     \n[57] DBI_1.1.3         curl_4.3.2        R6_2.5.1          lubridate_1.8.0  \n[61] knitr_1.39        fastmap_1.1.0     bit_4.0.4         utf8_1.2.2       \n[65] stringi_1.7.6     parallel_4.2.1    Rcpp_1.0.8.3      vctrs_0.4.1      \n[69] dbplyr_2.2.1      tidyselect_1.1.2  xfun_0.31"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html",
    "title": "7  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC3B study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#score-sgc-data",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#score-sgc-data",
    "title": "7  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n# backup <- read_rds('analysis/SGC3B/data/1-study-level/sgc3b_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC3B/data/1-study-level/sgc3b_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#summarize-by-subject",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#summarize-by-subject",
    "title": "7  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC3B/data/1-study-level/sgc3b_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#explore-distributions",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#explore-distributions",
    "title": "7  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"Impasse Condition (blue) yields more correct responses across the entire task\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(pretty_condition~q) + \n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields more correct responses on each item\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>% \ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\", \n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher total absolute scores\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE TEST PHASE\ngf_histogram(~item_test_NABS, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Absolute Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores across the entire task\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(q~pretty_condition) + \n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"Impasse Condition (blue) yields higher scaled scores on each item\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) + \n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>% \n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\", \n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"Impasse Condition (blue) yields higher cumulative scaled scores\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE TEST PHASE\ngf_histogram(~item_test_SCALED, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Scaled Score in TEST Phase\", \n        title = \"Distribution of TEST PHASE Scaled Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\nTODO: INVESTIGATE if some of the scores assigned to 0 should be assigned to -0.5 to balance\nTODO: INVESTIGATE DISTRIBUTIONS of each subscore type\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"Impasse condition (blue) yields fewer Orthogonal and more Triangular responses\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more Triangular responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"Impasse condition (blue) yields more positive trending responses on each question\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"Impasse shifts density toward higher Triagular scores\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"Impasse shifts density toward lower Orthogonal scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#export",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#export",
    "title": "7  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC3B/data/2-scored-data/sgc3b_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC3B/data/2-scored-data/sgc3b_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC3B/data/2-scored-data/sgc3b_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC3B/data/2-scored-data/sgc3b_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures\n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC3B/data/2-scored-data/sgc3b_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC3B/data/2-scored-data/sgc3b_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC3B/2_sgc3B_scoring.html#resources",
    "href": "analysis/SGC3B/2_sgc3B_scoring.html#resources",
    "title": "7  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     \n [5] readr_2.1.2      tidyr_1.2.0      tibble_3.1.7     tidyverse_1.3.1 \n [9] Hmisc_4.7-0      Formula_1.2-4    survival_3.3-1   lattice_0.20-45 \n[13] pbapply_1.5-0    ggformula_0.10.1 ggridges_0.5.3   scales_1.2.0    \n[17] ggstance_0.3.5   ggplot2_3.3.6    kableExtra_1.3.4\n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            bit64_4.0.5         lubridate_1.8.0    \n [4] webshot_0.5.3       RColorBrewer_1.1-3  httr_1.4.3         \n [7] tools_4.2.1         backports_1.4.1     utf8_1.2.2         \n[10] R6_2.5.1            rpart_4.1.16        DBI_1.1.3          \n[13] colorspace_2.0-3    nnet_7.3-17         withr_2.5.0        \n[16] tidyselect_1.1.2    gridExtra_2.3       curl_4.3.2         \n[19] bit_4.0.4           compiler_4.2.1      cli_3.3.0          \n[22] rvest_1.0.2         htmlTable_2.4.0     xml2_1.3.3         \n[25] labeling_0.4.2      mosaicCore_0.9.0    checkmate_2.1.0    \n[28] systemfonts_1.0.4   digest_0.6.29       foreign_0.8-82     \n[31] rmarkdown_2.14      svglite_2.1.0       rio_0.5.29         \n[34] base64enc_0.1-3     jpeg_0.1-9          pkgconfig_2.0.3    \n[37] htmltools_0.5.2     labelled_2.9.1      dbplyr_2.2.1       \n[40] fastmap_1.1.0       readxl_1.4.0        htmlwidgets_1.5.4  \n[43] rlang_1.0.3         rstudioapi_0.13     farver_2.1.0       \n[46] generics_0.1.2      jsonlite_1.8.0      vroom_1.5.7        \n[49] zip_2.2.0           magrittr_2.0.3      Matrix_1.4-1       \n[52] Rcpp_1.0.8.3        munsell_0.5.0       fansi_1.0.3        \n[55] lifecycle_1.0.1     stringi_1.7.6       yaml_2.3.5         \n[58] MASS_7.3-57         plyr_1.8.7          grid_4.2.1         \n[61] parallel_4.2.1      crayon_1.5.1        haven_2.5.0        \n[64] splines_4.2.1       hms_1.1.1           knitr_1.39         \n[67] pillar_1.7.0        codetools_0.2-18    reprex_2.0.1       \n[70] glue_1.6.2          evaluate_0.15       latticeExtra_0.6-29\n[73] data.table_1.14.2   modelr_0.1.8        tzdb_0.3.0         \n[76] png_0.1-7           vctrs_0.4.1         tweenr_1.0.2       \n[79] cellranger_1.1.0    gtable_0.3.0        polyclip_1.10-0    \n[82] assertthat_0.2.1    openxlsx_4.2.5      xfun_0.31          \n[85] ggforce_0.3.3       broom_0.8.0         viridisLite_0.4.0  \n[88] cluster_2.1.3       ellipsis_0.3.2"
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html",
    "title": "8  Introduction",
    "section": "",
    "text": "In Study 4A we explore the extent to which the design of the axes and gridlines of the graph influence how a reader interprets its underlying coordinate system.\nExperimental Hypothesis:\nWe hypothesize that the design of the major axes (specifically orthogonal) axes establish for the learner the basis of the coordinate system. Differently oriented axes should lead the reader to be more open to alternative coordinate systems.\nExploratory Questions"
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html#methods",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html#methods",
    "title": "8  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 4 levels (Graphical Framework: ORTH-FULL, ORTH-SPARSE, ORTH-GRID, TRI-SPARSE) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Graphical Framework: ORTH-FULL, ORTH-SPARSE, ORTH-GRID, TRI-SPARSE)\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 12.1. The list of questions can be found here.\n\n\n\nFigure 8.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items : the Graph Comprehension Task\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData were collected by convenience sample of a university subject pool during the winter of 2022. Participants accessed the study via a web browser (asynchronously). The stimulus application required the participant stay in full-screen mode for the entirety of the study."
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html#analysis",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html#analysis",
    "title": "8  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nwinter2022_clean_sgc4a.Rmd\n2_sgc4A_scoring.qmd\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC4A/data/0-session-level/fall17_sgc4a_participants.csv\"\nspring18 <- \"analysis/SGC4A/data/0-session-level/spring18_sgc4a_participants.csv\"\nwinter22 <- \"analysis/SGC4A/data/0-session-level/winter22_sgc4a_participants.rds\"\n\n#read datafiles, set mode and term\ndf_subjects_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_subjects_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_subjects_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% \n  dplyr::select(\n  subject,condition,pretty_condition, term,mode,\n  gender,age,language, schoolyear, country,\n  effort,difficulty,confidence,enjoyment,other,\n  totaltime_m, \n  # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n  #exploratory factors\n  violations, browser, width, height\n)\n\n#reduce data collected using OLD webapp to useful columns\ndf_subjects_before <- rbind(df_subjects_fall17, df_subjects_spring18) %>% \n  #rename and summarize some columns\n  mutate(\n    totaltime_m = totalTime / 1000 / 60,  \n    absolute_score = triangular_score,\n    language = native_language,\n    gender = sex,\n    schoolyear = year) %>% \n  #create placeholders for cols not collected until NEW webapp [for later rbind]\n  mutate(\n    condition = as.factor(condition),\n    pretty_condition = \"NULL\",\n    effort = \"NULL\",\n    difficulty = \"NULL\",\n    confidence = \"NULL\",\n    enjoyment = \"NULL\",\n    other = \"NULL\",\n    disability = \"NULL\",\n    violations = \"NULL\",\n    browser = \"NULL\",\n    width = \"NULL\",\n    height = \"NULL\"\n  ) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select(subject, condition, pretty_condition, term, mode, \n                #demographics\n                gender, age, language, schoolyear, country,\n                #placeholder effort survey\n                effort, difficulty, confidence, enjoyment, \n                #placeholder misc \n                other, disability,\n                #response characteristics\n                totaltime_m, \n                # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                #exploratory factors\n                violations, browser, width, height)\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_winter22_q16 <- df_subjects_winter22 %>% \n  dplyr::select(subject, condition, pretty_condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% dplyr::select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects_winter22 <- df_subjects_winter22 %>% \n  mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 # absolute_score, #drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors \n                 violations, browser, width, height)\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#compare dataframe columns\n# janitor::compare_df_cols(df_subjects, df_subjects_winter22, df_subjects_before)\n\n#combine dataframes from old and new webapps\ndf_subjects <- rbind(df_subjects, df_subjects_winter22, df_subjects_before) %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\"),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_subjects$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_subjects$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_subjects$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_subjects$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_subjects$gender) <- \"What is your gender identity?\"\nvar_label(df_subjects$schoolyear) <- \"What is your year in school?\"\n\n#REFACTOR CONDITIONS\ndf_subjects <- df_subjects %>% mutate(\n    condition = recode_factor(condition, \"11111\" = \"111\", \"112\" = \"112\", \"111\" = \"111\", \"113\" = \"113\", \"114\" = \"114\", \"115\"=\"115\"),\n    pretty_condition = recode_factor(condition, \"111\" = \"Orth-Full\", \"114\" =  \"Orth-Sparse\", \"113\"=\"Tri-Sparse\", \n                                     \"115\"=\"Orth-Grid\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_subjects_fall17, df_subjects_spring18, df_subjects_winter22,df_subjects_before)\nrm(fall17,spring18,winter22)\n\n#FINALLY DROP CONDITION 112 (partial orthog with y axis lines extending only to right end of triangle)\n#this was an incomplete [pilot only] condition collected in FA17 SP18 for pilot purposes\ndf_subjects <- df_subjects %>% filter(condition != \"112\") %>% \n  mutate(\n    condition = droplevels(condition),\n    pretty_condition = droplevels(pretty_condition)\n  )\n\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(mbp)\n\n#import file\n# df_subjects <- read_rds(\"analysis/SGC4A/data/0-session-level/sgc4a_participants.rds\") #use RDS file as it contains metadata\n# \n# #save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\n# df_q16 <- df_subjects %>% \n#   select(subject, condition, term , mode, explanation) %>% \n#   mutate(\n#     q = 16,\n#     response = explanation\n#   ) %>% select(-explanation)\n# \n# #reduce data collected using NEW webapp to useful columns\n# df_subjects <- df_subjects %>% \n#   mutate(score = absolute_score) %>% \n#   #select only columns we'll be analyzing, discard others\n#   dplyr::select( subject, condition, pretty_condition, term, mode, \n#                  #demographics\n#                  gender, age, language, schoolyear, country,\n#                  #effort survey\n#                  effort, difficulty, confidence, enjoyment, \n#                  #explanations\n#                  other,disability,\n#                  #response characteristics\n#                  totaltime_m,\n#                  #exploratory factors\n#                  violations, browser, width, height\n#                  )\n# \n# \n# effort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n# \n# #set factors\n# df_subjects <- df_subjects %>% \n#   #refactor factors\n#   mutate (\n#     subject = factor(subject),\n#     condition = factor(condition),\n#     term = factor(term),\n#     mode = factor(mode),\n#     gender = factor(gender),\n#     schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n#   )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#set datafiles\nfall17 <- \"analysis/SGC4A/data/0-session-level/fall17_sgc4a_blocks.csv\"\nspring18 <- \"analysis/SGC4A/data/0-session-level/spring18_sgc4a_blocks.csv\"\nwinter22 <- \"analysis/SGC4A/data/0-session-level/winter22_sgc4a_items.rds\"\n\n#read datafiles, set mode and term\ndf_items_fall17 <- read_csv(fall17) %>% mutate(mode = \"lab-synch\", term = \"fall17\")\ndf_items_spring18 <- read_csv(spring18) %>% mutate(mode = \"lab-synch\", term = \"spring18\")\ndf_items_winter22 <- read_rds(winter22) #use RDS file as it contains metadata\n\n#get mapping being question # and interval relation the question tests, that is encoded only in the winter22 data files\nmap_relations <- df_items_winter22 %>% group_by(q) %>% select(q,relation) %>% unique()\n\n#SAVE METADATA FROM WINTER, but no rows \ndf_items <- df_items_winter22 %>% filter(condition=='X') %>% select(\n  subject,condition,term,mode,\n  question, q, answer, correct, rt_s\n) \n\n#reduce data collected using old webapp\ndf_items_before <- rbind(df_items_fall17, df_items_spring18) %>% \n  mutate(rt_s = rt / 1000, correct = as.logical(correct)) %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) \n  \n#reduce data collected using new webapp\ndf_items_winter22 <- df_items_winter22 %>% \n  select(subject, condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  )\n\n#combine dataframes from old and new webapps\ndf_items <- rbind(df_items, df_items_winter22,df_items_before) %>% \n  #refactorize columns\n  mutate(\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term, levels= c(\"fall17\",\"spring18\",\"fall21\",\"winter22\")),\n    mode = factor(mode, levels=c(\"lab-synch\",\"asynch\")),\n    q = as.integer(q)) %>% \n  #rename answer column to RESPONSE \n  rename(response = answer) %>% \n  #remove all commas and make as character string\n  mutate(\n    response = str_remove_all(as.character(response), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n#FIX METADATA\n#Add metadata for columns that lost it [factors, for some reason!]\nvar_label(df_items$subject) <- \"ID of subject (randomly assigned in stimulus app).\"\nvar_label(df_items$condition) <- \"ID indicates randomly assigned condition (111 -> control, 121 -> impasse).\"\nvar_label(df_items$term) <- \"indicates if session was run with experimenter present or asynchronously\"\nvar_label(df_items$mode) <- \"indicates mode in which the participant completed the study\"\nvar_label(df_items$q) <- \"Question Number (in order)\"\nvar_label(df_items$correct) <- \"Is the response (strictly) correct? [dichotomous scoring]\"\nvar_label(df_items$response) <- \"options (datapoints) selected by the subject\"\nvar_label(df_items$num_o) <- \"number of options selected by the subject\"\n\n#HANDLE FREE RESPONSE QUESTION #16 \n#save `free response` Q#16 in its own dataframe\ndf_freeresponse <- df_items %>% filter(q == 16) %>% select(-question,-correct,-rt_s,-num_o)\n#add data from wi22 [stored on subject data]\ndf_winter22_q16 <- df_winter22_q16 %>% dplyr::select(-pretty_condition)\ndf_freeresponse <- rbind(df_freeresponse, df_winter22_q16)\n#add question description\ndf_freeresponse <- df_freeresponse %>% mutate(\n    question = \"Please describe how to determine what event(s) start at 12pm?\",\n    response = as.character(response) #doesn't need to be factor\n  ) \n#remove 'free response' Q#16 from df_items\ndf_items <- df_items %>% filter (q != 16)\n\n#REFACTOR CONDITIONS\ndf_items <- df_items %>% mutate(\n    condition = recode_factor(condition, \"11111\" = \"111\", \"112\" = \"112\", \"111\" = \"111\", \"113\" = \"113\", \"114\" = \"114\", \"115\"=\"115\"),\n    pretty_condition = recode_factor(condition, \"111\" = \"Orth-Full\", \"114\" =  \"Orth-Sparse\", \"113\"=\"Tri-Sparse\", \n                                     \"115\"=\"Orth-Grid\"),\n    pretty_mode = recode_factor(mode, \"lab-synch\" = \"laboratory\", \"asynch\" =  \"online-replication\")\n) \n\n#CLEANUP\nrm(df_items_fall17, df_items_spring18, df_items_winter22, df_items_before, df_winter22_q16)\nrm(fall17,spring18,winter22, map_relations)\n\n#FINALLY DROP CONDITION 112 (partial orthog with y axis lines extending only to right end of triangle)\n#this was an incomplete [pilot only] condition collected in FA17 SP18 for pilot purposes\ndf_items <- df_items %>% filter(condition != \"112\") %>% \n  mutate(\n    condition = droplevels(condition),\n    pretty_condition = droplevels(pretty_condition)\n  )\n\n##OLD WITH ONLY WI22\n\n# # HACK WD FOR LOCAL RUNNING?\n# # imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# #setwd(mbp)\n# \n# #read datafiles\n# df_items <- read_rds(\"analysis/SGC4A/data/0-session-level/sgc4a_items.rds\") #use RDS file as it contains metadata\n# \n# #reduce data collected using new webapp\n# df_items <- df_items %>% \n#   select(subject, condition, pretty_condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n#   mutate(\n#     subject = as.character(subject),\n#     condition = as.character(condition),\n#     term = as.character(term),\n#     mode = as.character(mode),\n#     q = as.integer(q),\n#     correct = as.logical(correct)\n#   ) %>% \n#   mutate(\n#     response = str_remove_all(as.character(answer), \",\"),\n#     num_o = str_length(response)\n#   ) %>% \n#   # handle NA values (why are some empty responses blank and others NA?) \n#   mutate(\n#     response = replace_na(response, \"\"),\n#     num_o = replace_na(num_o, 0)\n#   )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4A/data/1-study-level/sgc4a_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4A/data/1-study-level/sgc4a_items.csv\", row.names = FALSE)\nwrite.csv(df_freeresponse,\"analysis/SGC4A/data/1-study-level/sgc4a_freeresponse.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4A/data/1-study-level/sgc4a_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4A/data/1-study-level/sgc4a_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4A/1_sgc4A_introduction.html#resources",
    "href": "analysis/SGC4A/1_sgc4A_introduction.html#resources",
    "title": "8  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     \n [5] readr_2.1.2      tidyr_1.2.0      tibble_3.1.7     ggplot2_3.3.6   \n [9] tidyverse_1.3.1  kableExtra_1.3.4 codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] httr_1.4.3        highr_0.9         pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] bit_4.0.4         munsell_0.5.0     broom_0.8.0       compiler_4.2.1   \n[29] modelr_0.1.8      xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4\n[33] htmltools_0.5.2   tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3      \n[37] viridisLite_0.4.0 crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1     \n[41] withr_2.5.0       grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0     \n[45] lifecycle_1.0.1   DBI_1.1.3         magrittr_2.0.3    scales_1.2.0     \n[49] zip_2.2.0         cli_3.3.0         stringi_1.7.6     vroom_1.5.7      \n[53] fs_1.5.2          xml2_1.3.3        ellipsis_0.3.2    generics_0.1.2   \n[57] vctrs_0.4.1       openxlsx_4.2.5    tools_4.2.1       bit64_4.0.5      \n[61] glue_1.6.2        hms_1.1.1         parallel_4.2.1    fastmap_1.1.0    \n[65] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[69] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html",
    "title": "9  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC4A study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#score-sgc-data",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#score-sgc-data",
    "title": "9  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#backup <- read_rds('analysis/SGC4A/data/1-study-level/sgc4a_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC4A/data/1-study-level/sgc4a_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#summarize-by-subject",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#summarize-by-subject",
    "title": "9  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC4A/data/1-study-level/sgc4a_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\nWarning in subjects_summary$subject == subjects$subject: longer object length is\nnot a multiple of shorter object length\n\n\n[1]  TRUE FALSE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#explore-distributions",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#explore-distributions",
    "title": "9  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_condition = as.factor(condition),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n                \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(pretty_condition~q) +\n  labs( x = \"Absolute Score\",\n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>%\ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\",\n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(q~pretty_condition) +\n  labs( x = \"Scaled Score\",\n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) +\n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>%\n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\",\n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.05), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.05), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"Impasse shifts density toward higher Triagular scores\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"Impasse shifts density toward lower Orthogonal scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#peeking",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#peeking",
    "title": "9  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\nm1 <- lm(s_SCALED ~ pretty_condition, data = df_subjects)\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ pretty_condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8.60  -5.60  -3.13   1.65  21.37 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   -7.153      0.867   -8.25  3.1e-15 ***\npretty_conditionOrth-Sparse    1.000      1.226    0.82    0.415    \npretty_conditionTri-Sparse     2.752      1.233    2.23    0.026 *  \npretty_conditionOrth-Grid     -1.219      1.195   -1.02    0.308    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.13 on 356 degrees of freedom\nMultiple R-squared:  0.0316,    Adjusted R-squared:  0.0234 \nF-statistic: 3.87 on 3 and 356 DF,  p-value: 0.00958\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n                  Df Sum Sq Mean Sq F value Pr(>F)   \npretty_condition   3    768   256.0    3.87 0.0096 **\nResiduals        356  23558    66.2                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with pretty_condition (formula: s_SCALED ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.03, F(3, 356) = 3.87, p = 0.010, adj. R2 = 0.02). The model's intercept, corresponding to pretty_condition = Orth-Full, is at -7.15 (95% CI [-8.86, -5.45], t(356) = -8.25, p < .001). Within this model:\n\n  - The effect of pretty condition [Orth-Sparse] is statistically non-significant and positive (beta = 1.00, 95% CI [-1.41, 3.41], t(356) = 0.82, p = 0.415; Std. beta = 0.12, 95% CI [-0.17, 0.41])\n  - The effect of pretty condition [Tri-Sparse] is statistically significant and positive (beta = 2.75, 95% CI [0.33, 5.18], t(356) = 2.23, p = 0.026; Std. beta = 0.33, 95% CI [0.04, 0.63])\n  - The effect of pretty condition [Orth-Grid] is statistically non-significant and negative (beta = -1.22, 95% CI [-3.57, 1.13], t(356) = -1.02, p = 0.308; Std. beta = -0.15, 95% CI [-0.43, 0.14])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#export",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#export",
    "title": "9  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4A/data/2-scored-data/sgc4a_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4A/data/2-scored-data/sgc4a_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC4A/data/2-scored-data/sgc4a_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC4A/data/2-scored-data/sgc4a_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4A/data/2-scored-data/sgc4a_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4A/data/2-scored-data/sgc4a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4A/2_sgc4A_scoring.html#resources",
    "href": "analysis/SGC4A/2_sgc4A_scoring.html#resources",
    "title": "9  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            bit64_4.0.5         lubridate_1.8.0    \n [4] insight_0.17.1      webshot_0.5.3       RColorBrewer_1.1-3 \n [7] httr_1.4.3          tools_4.2.1         backports_1.4.1    \n[10] utf8_1.2.2          R6_2.5.1            rpart_4.1.16       \n[13] DBI_1.1.3           colorspace_2.0-3    nnet_7.3-17        \n[16] withr_2.5.0         tidyselect_1.1.2    gridExtra_2.3      \n[19] curl_4.3.2          bit_4.0.4           compiler_4.2.1     \n[22] cli_3.3.0           rvest_1.0.2         htmlTable_2.4.0    \n[25] xml2_1.3.3          bayestestR_0.12.1   labeling_0.4.2     \n[28] mosaicCore_0.9.0    checkmate_2.1.0     systemfonts_1.0.4  \n[31] digest_0.6.29       foreign_0.8-82      rmarkdown_2.14     \n[34] svglite_2.1.0       rio_0.5.29          base64enc_0.1-3    \n[37] jpeg_0.1-9          pkgconfig_2.0.3     htmltools_0.5.2    \n[40] labelled_2.9.1      dbplyr_2.2.1        fastmap_1.1.0      \n[43] readxl_1.4.0        htmlwidgets_1.5.4   rlang_1.0.3        \n[46] rstudioapi_0.13     farver_2.1.0        generics_0.1.2     \n[49] jsonlite_1.8.0      vroom_1.5.7         zip_2.2.0          \n[52] magrittr_2.0.3      parameters_0.18.1   Matrix_1.4-1       \n[55] Rcpp_1.0.8.3        munsell_0.5.0       fansi_1.0.3        \n[58] lifecycle_1.0.1     stringi_1.7.6       yaml_2.3.5         \n[61] MASS_7.3-57         plyr_1.8.7          grid_4.2.1         \n[64] parallel_4.2.1      crayon_1.5.1        haven_2.5.0        \n[67] splines_4.2.1       hms_1.1.1           knitr_1.39         \n[70] pillar_1.7.0        effectsize_0.7.0    reprex_2.0.1       \n[73] glue_1.6.2          evaluate_0.15       latticeExtra_0.6-29\n[76] data.table_1.14.2   modelr_0.1.8        tzdb_0.3.0         \n[79] png_0.1-7           vctrs_0.4.1         tweenr_1.0.2       \n[82] cellranger_1.1.0    gtable_0.3.0        polyclip_1.10-0    \n[85] datawizard_0.4.1    assertthat_0.2.1    openxlsx_4.2.5     \n[88] xfun_0.31           ggforce_0.3.3       broom_0.8.0        \n[91] viridisLite_0.4.0   cluster_2.1.3       ellipsis_0.3.2"
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html",
    "title": "10  Introduction",
    "section": "",
    "text": "In Study 4B we explore the extent to which the design of the marks indicating data points influence how a reader interprets its underlying coordinate system."
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html#methods",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html#methods",
    "title": "10  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 3 levels (Mark: POINT, CROSS, ARROW) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Mark Design: Point, Arrow, Cross )\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 12.1. The list of questions can be found here.\n\n\n\nFigure 10.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nIn each experimental\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a Triangular Model (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items: The Graph Comprehension Task.\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData were collected by convenience sample of a university subject pool during the winter of 2022. Participants accessed the study via a web browser (asynchronously). The stimulus application required the participant stay in full-screen mode for the entirety of the study."
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html#analysis",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html#analysis",
    "title": "10  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nwinter2022_clean_sgc4b.Rmd\n2_sgc4B_scoring.qmd\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#import file\ndf_subjects <- read_rds(\"analysis/SGC4B/data/0-session-level/sgc4b_participants.rds\") #use RDS file as it contains metadata\n\n#NO EXPLANATION COLUMN IN SGC4B DATASET; TRIAL NOT COLLECTED \n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\n# df_q16 <- df_subjects %>% \n#   select(subject, condition, term , mode, explanation) %>% \n#   mutate(\n#     q = 16,\n#     response = explanation\n#   ) %>% select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\n#drop absolute score because we rescore in 2_scoring\ndf_subjects <- df_subjects %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, study, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m,\n                 #exploratory factors\n                 violations, browser, width, height\n                 )\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#set factors\ndf_subjects <- df_subjects %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#read datafiles\ndf_items <- read_rds(\"analysis/SGC4B/data/0-session-level/sgc4b_items.rds\") #use RDS file as it contains metadata\n\n#reduce data collected using new webapp\ndf_items <- df_items %>% \n  select(subject, condition, pretty_condition, study, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  ) %>% \n  mutate(\n    response = str_remove_all(as.character(answer), \",\"),\n    num_o = str_length(response)\n  )%>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4B/data/1-study-level/sgc4b_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4B/data/1-study-level/sgc4b_items.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4B/data/1-study-level/sgc4b_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4B/data/1-study-level/sgc4b_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4B/1_sgc4B_introduction.html#resources",
    "href": "analysis/SGC4B/1_sgc4B_introduction.html#resources",
    "title": "10  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] highr_0.9         httr_1.4.3        pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] munsell_0.5.0     broom_0.8.0       compiler_4.2.1    modelr_0.1.8     \n[29] xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2  \n[33] tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3       viridisLite_0.4.0\n[37] crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1      withr_2.5.0      \n[41] grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1  \n[45] DBI_1.1.3         magrittr_2.0.3    scales_1.2.0      zip_2.2.0        \n[49] cli_3.3.0         stringi_1.7.6     fs_1.5.2          xml2_1.3.3       \n[53] ellipsis_0.3.2    generics_0.1.2    vctrs_0.4.1       openxlsx_4.2.5   \n[57] tools_4.2.1       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n[61] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[65] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html",
    "title": "11  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC4B study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#score-sgc-data",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#score-sgc-data",
    "title": "11  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#backup <- read_rds('analysis/SGC4B/data/1-study-level/sgc4b_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC4B/data/1-study-level/sgc4b_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n#stoopid extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#summarize-by-subject",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#summarize-by-subject",
    "title": "11  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC4B/data/1-study-level/sgc4b_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#explore-distributions",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#explore-distributions",
    "title": "11  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  pretty_condition = pretty_condition,\n  score_niceABS = as.factor(score_niceABS),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(pretty_condition~q) +\n  labs( x = \"Absolute Score\",\n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, position = position_dodge(), data = df_subjects) %>%\ngf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Absolute Score\",\n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>%\n  gf_facet_grid(q~pretty_condition) +\n  labs( x = \"Scaled Score\",\n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) +\n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>%\n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\",\n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +\n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n facet_wrap(~pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_density(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_density(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>%\n  gf_facet_wrap( ~ pretty_condition) +\n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") +\n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#peeking",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#peeking",
    "title": "11  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\nm1 <- lm(s_SCALED ~ pretty_condition, data = df_subjects)\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ pretty_condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8.55  -5.55  -3.72   1.78  20.62 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             -7.621      0.891   -8.55  9.2e-16 ***\npretty_conditionarrow    3.172      1.237    2.56    0.011 *  \npretty_conditioncross    1.344      1.290    1.04    0.299    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.5 on 269 degrees of freedom\nMultiple R-squared:  0.0241,    Adjusted R-squared:  0.0168 \nF-statistic: 3.32 on 2 and 269 DF,  p-value: 0.0376\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n                  Df Sum Sq Mean Sq F value Pr(>F)  \npretty_condition   2    480   240.0    3.32  0.038 *\nResiduals        269  19434    72.2                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with pretty_condition (formula: s_SCALED ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.02, F(2, 269) = 3.32, p = 0.038, adj. R2 = 0.02). The model's intercept, corresponding to pretty_condition = point, is at -7.62 (95% CI [-9.38, -5.87], t(269) = -8.55, p < .001). Within this model:\n\n  - The effect of pretty condition [arrow] is statistically significant and positive (beta = 3.17, 95% CI [0.74, 5.61], t(269) = 2.56, p = 0.011; Std. beta = 0.37, 95% CI [0.09, 0.65])\n  - The effect of pretty condition [cross] is statistically non-significant and positive (beta = 1.34, 95% CI [-1.20, 3.88], t(269) = 1.04, p = 0.299; Std. beta = 0.16, 95% CI [-0.14, 0.45])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#export",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#export",
    "title": "11  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4B/data/2-scored-data/sgc4b_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4B/data/2-scored-data/sgc4b_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC4B/data/2-scored-data/sgc4b_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC4B/data/2-scored-data/sgc4b_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4B/data/2-scored-data/sgc4b_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4B/data/2-scored-data/sgc4b_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4B/2_sgc4B_scoring.html#resources",
    "href": "analysis/SGC4B/2_sgc4B_scoring.html#resources",
    "title": "11  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n [1] colorspace_2.0-3    ellipsis_0.3.2      rio_0.5.29         \n [4] htmlTable_2.4.0     parameters_0.18.1   base64enc_0.1-3    \n [7] fs_1.5.2            rstudioapi_0.13     farver_2.1.0       \n[10] bit64_4.0.5         fansi_1.0.3         lubridate_1.8.0    \n[13] xml2_1.3.3          codetools_0.2-18    splines_4.2.1      \n[16] knitr_1.39          polyclip_1.10-0     jsonlite_1.8.0     \n[19] broom_0.8.0         cluster_2.1.3       dbplyr_2.2.1       \n[22] png_0.1-7           ggforce_0.3.3       effectsize_0.7.0   \n[25] compiler_4.2.1      httr_1.4.3          backports_1.4.1    \n[28] assertthat_0.2.1    Matrix_1.4-1        fastmap_1.1.0      \n[31] cli_3.3.0           tweenr_1.0.2        htmltools_0.5.2    \n[34] tools_4.2.1         gtable_0.3.0        glue_1.6.2         \n[37] Rcpp_1.0.8.3        cellranger_1.1.0    vctrs_0.4.1        \n[40] svglite_2.1.0       insight_0.17.1      xfun_0.31          \n[43] openxlsx_4.2.5      rvest_1.0.2         lifecycle_1.0.1    \n[46] mosaicCore_0.9.0    MASS_7.3-57         vroom_1.5.7        \n[49] hms_1.1.1           parallel_4.2.1      RColorBrewer_1.1-3 \n[52] yaml_2.3.5          curl_4.3.2          gridExtra_2.3      \n[55] labelled_2.9.1      rpart_4.1.16        latticeExtra_0.6-29\n[58] stringi_1.7.6       bayestestR_0.12.1   checkmate_2.1.0    \n[61] zip_2.2.0           rlang_1.0.3         pkgconfig_2.0.3    \n[64] systemfonts_1.0.4   evaluate_0.15       htmlwidgets_1.5.4  \n[67] labeling_0.4.2      bit_4.0.4           tidyselect_1.1.2   \n[70] plyr_1.8.7          magrittr_2.0.3      R6_2.5.1           \n[73] generics_0.1.2      DBI_1.1.3           pillar_1.7.0       \n[76] haven_2.5.0         foreign_0.8-82      withr_2.5.0        \n[79] datawizard_0.4.1    nnet_7.3-17         modelr_0.1.8       \n[82] crayon_1.5.1        utf8_1.2.2          tzdb_0.3.0         \n[85] rmarkdown_2.14      jpeg_0.1-9          grid_4.2.1         \n[88] readxl_1.4.0        data.table_1.14.2   reprex_2.0.1       \n[91] digest_0.6.29       webshot_0.5.3       munsell_0.5.0      \n[94] viridisLite_0.4.0"
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html",
    "title": "12  Introduction",
    "section": "",
    "text": "In Study 5A we explore the extent to which requiring mouse-cursor interaction with the graph improves interpretation of the underlying coordinate system."
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html#methods",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html#methods",
    "title": "12  Introduction",
    "section": "METHODS",
    "text": "METHODS\n\nDesign\nWe employed a mixed design with 1 between-subjects factor with 2 levels (Scaffold: control, impasse) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\n\n\n\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 12.1. The list of questions can be found here.\n\n\n\nFigure 12.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nNote that across both control and impasse conditions, both the question, response options and graph structure were identical. The experimental manipulation (posing a mental impasse) was accomplished by changing the position of datapoints in the impasse-condition graph, such that for any given question, there was no available response option if the reader were to interpret the graph as cartesian (making an orthogonal rather than diagonal projection from the x-axis.)\nThe green line indicates the ideal-scanpath to the correct (triangular) answer to the first question, and the red line indicates the (incorrect) orthogonal interpretation. In the IMPASSE figure (at right), there are no data points that intersect the red line. We hypothesize that this presents the reader with an obstacle, at which point they are forced to confront their interpretation of the coordinate system and (ideally) develop a new strategy.\n\n\n\nFigure 12.2: Sample Question (Q=1) graphs for each condition\n\n\n\n\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a TriangularModel (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items : the Graph Comprehension Task\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\n\n\nSample\nData was collected by convenience sample …"
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html#analysis",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html#analysis",
    "title": "12  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\nPre-Requisite\nFollowed By\n\n\n\n\nwinter2022_clean_sgc5a.Rmd\n2_sgc5_scoring.qmd\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\n\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\n\nCODE\n#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#import file\ndf_subjects <- read_rds(\"analysis/SGC5A/data/0-session-level/sgc5_participants.rds\") #use RDS file as it contains metadata\n\n#SAVE METADATA FROM WINTER, but no rows \n# df_subjects <- df_subjects_winter22 %>% filter(condition=='X') %>% select(\n#   subject,condition,term,mode,\n#   gender,age,language, schoolyear, country,\n#   effort,difficulty,confidence,enjoyment,other,\n#   totaltime_m,absolute_score\n# )\n\n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\ndf_q16 <- df_subjects %>% \n  dplyr::select(subject, condition, term , mode, explanation) %>% \n  mutate(\n    q = 16,\n    response = explanation\n  ) %>% dplyr::select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\ndf_subjects <- df_subjects %>% \n  # mutate(score = absolute_score) %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m, \n                 #absolute_score,#drop absolute score as this is re-scored [though should be the same]\n                 #exploratory factors\n                 violations, browser, width, height\n                 )\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#set factors\ndf_subjects <- df_subjects %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#read datafiles\ndf_items <- read_rds(\"analysis/SGC5A/data/0-session-level/sgc5_items.rds\") #use RDS file as it contains metadata\n\n#reduce data collected using new webapp\ndf_items <- df_items %>% \n  dplyr::select(subject, condition, pretty_condition, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  ) %>% \n  mutate(\n    response = str_remove_all(as.character(answer), \",\"),\n    num_o = str_length(response)\n  ) %>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\n\nCODE\n#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n\n[1] TRUE\n\n\nCODE\n#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n\n[1] TRUE\n\n\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC5A/data/1-study-level/sgc5_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC5A/data/1-study-level/sgc5_items.csv\", row.names = FALSE)\nwrite.csv(df_q16,\"analysis/SGC5A/data/1-study-level/sgc5_freeresponse.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC5A/data/1-study-level/sgc5_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC5A/data/1-study-level/sgc5_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC5A/1_sgc5A_introduction.html#resources",
    "href": "analysis/SGC5A/1_sgc5A_introduction.html#resources",
    "title": "12  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.1    evaluate_0.15    \n[13] highr_0.9         httr_1.4.3        pillar_1.7.0      rlang_1.0.3      \n[17] curl_4.3.2        readxl_1.4.0      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.14    webshot_0.5.3     foreign_0.8-82    htmlwidgets_1.5.4\n[25] munsell_0.5.0     broom_0.8.0       compiler_4.2.1    modelr_0.1.8     \n[29] xfun_0.31         pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2  \n[33] tidyselect_1.1.2  rio_0.5.29        fansi_1.0.3       viridisLite_0.4.0\n[37] crayon_1.5.1      tzdb_0.3.0        dbplyr_2.2.1      withr_2.5.0      \n[41] grid_4.2.1        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1  \n[45] DBI_1.1.3         magrittr_2.0.3    scales_1.2.0      zip_2.2.0        \n[49] cli_3.3.0         stringi_1.7.6     fs_1.5.2          xml2_1.3.3       \n[53] ellipsis_0.3.2    generics_0.1.2    vctrs_0.4.1       openxlsx_4.2.5   \n[57] tools_4.2.1       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n[61] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.39       \n[65] haven_2.5.0"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html",
    "title": "13  Response Scoring",
    "section": "",
    "text": "The purpose of this notebook is to score (assign a measure of accuracy) to response data for the SGC5 study. This is required because the question type on the graph comprehension task used a ‘Multiple Response’ (MR) question design. Here, we evaluate different approaches to scoring multiple response questions, and transform raw item responses (e.g. boxes ABC are checked) to a measure of response accuracy. (Warning: this notebook takes several minutes to execute.) To review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3."
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#score-sgc-data",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#score-sgc-data",
    "title": "13  Response Scoring",
    "section": "SCORE SGC DATA",
    "text": "SCORE SGC DATA\nTo review the strategy behind Multiple Response scoring for the SGC project, refer to section Section 3.\nIn SGC we are fundamentally interested in understanding how a participant interprets the presented graph (stimulus). The graph comprehension task asks them to select the data points in the graph that meet the criteria posed in the question. To assess a participant’s performance, for each question (q=15) we will calculate the following scores:\nAn overall, strict score:\n1. Absolute Score : using dichotomous scoring referencing true (Triangular) answer. (see 1.2)\nSub-scores, for each alternative graph interpretation\n2. Triangular Score : using partial scoring [-1/q, +1/p] referencing true (Triangular) answer key.\n3. Orthogonal Score : using partial scoring [-1/q, +1/p] referencing (incorrect Orthogonal) answer key.\nBased on prior observational studies where we observed emergence of other alternative interpretations (i.e. transitional interpretations) we also calculate subscores for these alternatives.\n4. Tversky Score : using partial scoring [-1/q, +1/p] referencing (incorrect connecting-lines strategy) answer key. 5. Satisficing Score : using partial scoring [-1/q, +1/p] referencing (incorrect satisficing strategy) answer key.\n\nPrepare Answer Keys\nWe start by importing three answer keys: (1) Q1 - Q5 [control condition], (2) Q1-Q5 [impasse condition], (3) Q6-15. Separate answer keys by condition are required for Q1-Q5 because the stimuli for each condition visualize a different underlying dataset (i.e. the graphs show datapoints in different positions). Q6-Q15 are identical across conditions. Each answer key includes a row for each question, and a column defining the subset of response options that correspond to different graph interpretations.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE KEYS FOR FUTURE USE\nkeys_raw <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_raw\")\nkeys_orth <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_orth\")\nkeys_tri <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tri\")\nkeys_satisfice_left <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_left\")\nkeys_satisfice_right <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_satisfice_right\")\nkeys_tversky_duration <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_duration\")\nkeys_tversky_end <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_end\")\nkeys_tversky_max <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_max\")\nkeys_tversky_start <-  read_csv(\"analysis/utils/keys/parsed_keys/keys_tversky_start\")\n\n\n\n\nCalculate Subscores\nNext, we import the item-level response data. For each row in the item level dataset (indicating the response to a single question-item for a single subject), we compare the raw response df_items$response with the answer keys in each interpretation (e.g. keys_orth, keys_tri, etc…), then using those sets, determine the number of correctly selected items(p) and incorrectly selected items (q), which in turn are used to calculate partial[-1/q, +1/p] scores for each interpretation. The resulting scores are then stored on each item in df_items, and can be used to determine which graph interpretation the subject held.\nSpecifically, the following scores are calculated for each item:\nInterpretation Subscores\n\nscore_TRI How consistent is the response with the triangularinterpretation?\nscore_ORTH How consistent is the response with the orthogonalinterpretation?\nscore_SATISFICE is calculated by taking the maximum value of :\n\nscore_SAT_left How consistent is the response with the (left side) Satisficing interpretation?\nscore_SAT_right How consistent is the response with the (right side) Satisficing interpretation\n\nscore_TVERSKY is calculated by taking the maximum value of:\n\nscore_TV_max How consistent is the response with the (maximal) Tversky interpretation?\nscore_TV_start How consistent is the response with the (start-time) Tversky interpretation?\nscore_TV_end How consistent is the response with the (end-time) Tversky interpretation?\nscore_TV_duration How consistent is the response with the (duration) Tversky interpretation?\n\nscore_REF Did the response select only the reference point?\nscore_BOTH How consistent is the response with both the orthogonal and triangular interpretations?\n\nAbsolute Scores\n\nscore_ABS Is the response strictly correct? (triangular interpretation)\nscore_niceABS Is the response strictly correct? (triangular interpretation, not penalizing ref points). This is a more generous version of the Absolute score that does not penalize the participant if in addition to the correct answer in addition to they also select the data point referenced in the question.\n\n\n\nCODE\n#HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#backup <- read_rds('analysis/SGC5A/data/1-study-level/sgc5_items.rds') #for troubleshooting only\ndf_items <- read_rds('analysis/SGC5A/data/1-study-level/sgc5_items.rds')\n\n\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\nsource(\"analysis/utils/scoring.R\")\n\n\nnote: this cell takes approximately 30 minutes to run on the full df_items dataframe with 4950 records\n\n\nCODE\n#RUN THIS <OR> THE CALCULATE-SCORES-FORLOOP [not both]\n\n#ALPHEBETIZE RESPONSE\ndf_items$response = pbmapply(reorder_inplace, df_items$response)\n\n#STRATEGY PARTIAL-SUBSCORES\ndf_items$score_TRI = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tri))\ndf_items$score_ORTH = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_orth))\ndf_items$score_SAT_left = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_left))\ndf_items$score_SAT_right = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_satisfice_right))\ndf_items$score_TV_max = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_max))\ndf_items$score_TV_start = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_start))\ndf_items$score_TV_end = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_end))\ndf_items$score_TV_duration = pbmapply(calc_subscore, df_items$q, df_items$condition, df_items$response, MoreArgs = list(keyframe = keys_tversky_duration))\n\n#SPECIAL ABSOLUTE SUBSCORES\ndf_items$score_REF = pbmapply(calc_refscore, df_items$q, df_items$response)\ndf_items$score_BOTH = as.integer((df_items$score_TRI == 1) & (df_items$score_ORTH ==1))\n\n#ABSOLUTE SCORES\ndf_items$score_ABS = as.integer(df_items$correct) \ndf_items$score_niceABS  <- as.integer((df_items$score_TRI == 1)) #tri doesn't penalize ref or ve-area\n\n\n\n\nDerive Interpretation\nFinally, we use the interpretation subscores to classify the response as a particular interpretation. This classification algorithm : (1) First decides if the response matches one or more ‘special’ situations (blank response, reference point response, both ORTH and TRI) (2) If response doesn’t match a special situation, it compares the individual subscores, and subscores and decides if they are discriminant (i.e. are the scores different enough to make a prediction). A discriminant threshold of 0.5pts (on a scale from -1 to +1 is used) (2) If the variance in subscores surpasses the threshold, the interpretation is classified based on the highest subscore (TRIANGULAR, ORTHOGONAL, TVERSKY, SATISFICE) (3) If the variance does not surpass the threshold, the interpretation is labelled as “?”, indicating it cannot be classified, and is of an unknown interpretation.\nThe final output is called interpretation.\n\n\nCODE\n# extra copying for troubleshooting safety\ntemp <- df_items \ntemp <- derive_interpretation(temp)\ndf_items <- temp \n\n\n\n\nDerive Scaled Score\nThe interpretation response variable gives us the finest grain indication of the reader’s understanding of the graph for a particular question. However, it is a categorical variable, which poses a challenge for analyzing cumulative performance at the subject level. To address this challenge, we derive a scaled_score that converts each possible interpretation to a numeric value on a scale from -1 to +1. This scaling takes advantage of the observation that each interpretation can be positioned along a spectrum of understanding from completely incorrect (orthogonal) to completely correct (triangular). Alternative interpretations lay somewhere between.\nSpecifically, we assign the following values to each interpretation:\n\n(-1) : ORTHOGONAL, SATISFICE (satisfice represents an attempt at an orthogonal answer when none is available)\n(-0.5): ? (some alternative that cannot be identified; but meaningful that it is not orthogonal)\n(0): REFERENCE POINT, BLANK (indicates the individual thinks there is no answer, recognizes that ORTHOGONAL cannot be correct, but does not conceive of triangular)\n(+0.5) TVERSKY, BOTH TRI + ORTH (indicates that they “see” a triangular response, but lack certainty and also select the ORTHOGONAL response)\n(+1) TRIANGULAR +1\n\n\n\nCODE\ndf_items$score_SCALED <- calc_scaled(df_items$interpretation)"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#summarize-by-subject",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#summarize-by-subject",
    "title": "13  Response Scoring",
    "section": "SUMMARIZE BY SUBJECT",
    "text": "SUMMARIZE BY SUBJECT\nNext, we summarize the item level scores by subject in order to calculate cummulative subscores to be stored on the subject record.\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#import subjects\ndf_subjects <- read_rds('analysis/SGC5A/data/1-study-level/sgc5_participants.rds') %>% mutate(subject = as.character(subject)) %>% arrange(subject)\n\n#make temporary copies for testing safety\ns = df_subjects\ni = df_items \n\n#summarize\ntest_subs <- summarise_bySubject(s,i)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nCODE\ndf_subjects <- test_subs\n\n\nWe also summarize absolute and scaled score progress at each question in the task, to explore cumulative performance over the task.\n\n\nCODE\n#GET ABSOLUTE PROGRESS \ndf_absolute_progress <- progress_Absolute(df_items)\n\n#GET SCALED PROGRESS\ndf_scaled_progress <- progress_Scaled(df_items)"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#explore-distributions",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#explore-distributions",
    "title": "13  Response Scoring",
    "section": "EXPLORE DISTRIBUTIONS",
    "text": "EXPLORE DISTRIBUTIONS\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#create temp data frame for visualizations\ndf = df_items %>% filter (q %nin% c(6,9)) %>% mutate(\n  score_niceABS = as.factor(score_niceABS),\n  pretty_condition = recode_factor(condition, \"11115\" = \"point-click\"),\n  pretty_interpretation = factor(interpretation,\n    levels = c(\"Orthogonal\", \"Satisfice\", \n               \"frenzy\",\"?\",\n                \"reference\",\"blank\",\n                \"Tversky\", \"both tri + orth\",\n               \"Triangular\" ))\n  )\n\n\n\nAbsolute Score\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE FULL TASK\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (all Items)\",\n        subtitle = paste(\"\"),\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY ITEM\ngf_props(~score_niceABS, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(pretty_condition~q) + \n  labs( x = \"Absolute Score\", \n        title = \"Distribution of Absolute Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proprition of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION ABSOLUTE SCORE BY SUBJECT\ngf_props(~s_NABS, fill = ~pretty_condition, data = df_subjects) %>% \n   gf_facet_wrap(~pretty_condition) + \n  labs( x = \"Total Absolute Score\", \n        title = \"Distribution of Total Absolute Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nScaled Score\n\n\nCODE\noptions(repr.plot.width =9, repr.plot.height =12)\n\n#DISTRIBUTION SCALED SCORE FULL TASK\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df) +\n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (all Items)\",\n        subtitle = \"\",\n        y = \"Proportion of Items\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY ITEM\ngf_props(~score_SCALED, fill = ~pretty_condition, position = position_dodge(), data = df)  %>% \n  gf_facet_grid(q~pretty_condition) + \n  labs( x = \"Scaled Score\", \n        title = \"Distribution of Scaled Score (by Item)\",\n        subtitle = \"\",\n        y = \"Proportion of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  scale_y_continuous(breaks=c(0,0.5)) + \n  theme_minimal() + theme(legend.position=\"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION SCALED SCORE BY SUBJECT\ngf_props(~s_SCALED, fill = ~pretty_condition, data = df_subjects)  %>% \n  gf_facet_grid(pretty_condition ~. )+\n  labs( x = \"Total Scaled Score\", \n        title = \"Distribution of Total Scaled Score (by Subject)\",\n        subtitle = \"\",\n        y = \"Number of Subjects\") +\n  scale_fill_discrete(name = \"Condition\") +  \n  theme_minimal()\n\n\n\n\n\n\n\nInterpretations\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION\ngf_props(~pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition ~ ., labeller = label_both) + \n  labs( title = \"Distribution of Interpretations (across Task)\",\n        x = \"Graph Interpretation\",\n        y = \"Proportion of Responses\",\n        subtitle = \"\") + \n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION ACROSS ITEMS\ngf_propsh(~ pretty_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#DISTRIBUTION OF INTERPRETATION TYPE ACROSS ITEMS\ngf_propsh(~ high_interpretation, fill = ~pretty_condition, data = df) %>% \n  gf_facet_grid( pretty_condition~q) + \n  labs( title = \"Distribution of Interpretations (by Item)\",\n        subtitle = \"\",\n        y = \"Interpretation\", x = \"Proportion of Subjects\") + theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nProgress over Task\n\n\nCODE\n#VISUALIZE progress over time ABSOLUTE score \nggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01)) + \n geom_line(position=position_jitter(w=0.15, h=0.00), size=0.1) +\n facet_wrap( ~ pretty_condition) + \n labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#VISUALIZE progress over time SCALED score \nggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01)) + \n geom_line(position=position_jitter(w=0.15, h=0.00), size=0.1) +\n facet_wrap( ~ pretty_condition) + \n labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nInterpretation Subscores\n\n\nCODE\ngf_histogram(~ s_TRI, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Triangular Score\",\n        subtitle = \"\",\n        x = \"Item Triangular Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_ORTH, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Orthogonal Score\",\n        subtitle = \"\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_TVERSKY, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Tversky Score\",\n        subtitle = \"Impasse shifts density toward higher Tversky scores\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\ngf_histogram(~ s_SATISFICE, fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap( ~ pretty_condition) + \n  labs( title = \"Distribution of Total Satisfice Score\",\n        subtitle = \"Satisficing only occurs in impasse, when no orthogonal response is available\",\n        x = \"Item Orthogonal Score\", y = \"Proportion of Subjects\") + \n        theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#peeking",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#peeking",
    "title": "13  Response Scoring",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\nlibrary(performance)\nlibrary(report)\n\nsgc3a <- read_rds(\"analysis/SGC3A/data/2-scored-data/sgc3a_scored_participants.rds\") %>% filter(condition == \"111\") %>% dplyr::select(-pretty_mode)\n\n\ncomb <- rbind(sgc3a, df_subjects)  \n\ngf_histogram(~s_SCALED, data = comb) %>% \n  gf_facet_wrap(~pretty_condition)\n\n\n\n\n\nCODE\nm1 <- lm(s_SCALED ~ pretty_condition, data = comb)\nsummary(m1)\n\n\n\nCall:\nlm(formula = s_SCALED ~ pretty_condition, data = comb)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.57  -5.49  -3.57   1.51  20.51 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   -6.427      0.647   -9.93   <2e-16 ***\npretty_conditionpoint-click   -1.086      0.997   -1.09     0.28    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.14 on 271 degrees of freedom\nMultiple R-squared:  0.00436,   Adjusted R-squared:  0.000682 \nF-statistic: 1.19 on 1 and 271 DF,  p-value: 0.277\n\n\nCODE\nanova(m1)\n\n\nAnalysis of Variance Table\n\nResponse: s_SCALED\n                  Df Sum Sq Mean Sq F value Pr(>F)\npretty_condition   1     78    78.5    1.19   0.28\nResiduals        271  17935    66.2               \n\n\nCODE\nreport(m1)\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict s_SCALED with pretty_condition (formula: s_SCALED ~ pretty_condition). The model explains a statistically not significant and very weak proportion of variance (R2 = 4.36e-03, F(1, 271) = 1.19, p = 0.277, adj. R2 = 6.82e-04). The model's intercept, corresponding to pretty_condition = control, is at -6.43 (95% CI [-7.70, -5.15], t(271) = -9.93, p < .001). Within this model:\n\n  - The effect of pretty condition [point-click] is statistically non-significant and negative (beta = -1.09, 95% CI [-3.05, 0.88], t(271) = -1.09, p = 0.277; Std. beta = -0.13, 95% CI [-0.37, 0.11])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#export",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#export",
    "title": "13  Response Scoring",
    "section": "EXPORT",
    "text": "EXPORT\nFinally, we export the scores for each item (df_items) and summarized over subjects (df_subjects), as well as cumulative progress dataframes (df_absolute_progress, df_scaled_progress)\n\n\nCODE\n# #HACK WD FOR LOCAL RUNNING?\nimac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC5A/data/2-scored-data/sgc5a_scored_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC5A/data/2-scored-data/sgc5a_scored_items.csv\", row.names = FALSE)\nwrite.csv(df_absolute_progress,\"analysis/SGC5A/data/2-scored-data/sgc5a_absolute_progress.csv\", row.names = FALSE)\nwrite.csv(df_scaled_progress,\"analysis/SGC5A/data/2-scored-data/sgc5a_scaled_progress.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC5A/data/2-scored-data/sgc5a_scored_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC5A/data/2-scored-data/sgc5a_scored_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC5A/2_sgc5A_scoring.html#resources",
    "href": "analysis/SGC5A/2_sgc5A_scoring.html#resources",
    "title": "13  Response Scoring",
    "section": "RESOURCES",
    "text": "RESOURCES\n\n\nCODE\nsessionInfo()\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] report_0.5.1      performance_0.9.1 forcats_0.5.1     stringr_1.4.0    \n [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n [9] tibble_3.1.7      tidyverse_1.3.1   Hmisc_4.7-0       Formula_1.2-4    \n[13] survival_3.3-1    lattice_0.20-45   pbapply_1.5-0     ggformula_0.10.1 \n[17] ggridges_0.5.3    scales_1.2.0      ggstance_0.3.5    ggplot2_3.3.6    \n[21] kableExtra_1.3.4 \n\nloaded via a namespace (and not attached):\n [1] colorspace_2.0-3    ellipsis_0.3.2      rio_0.5.29         \n [4] htmlTable_2.4.0     parameters_0.18.1   base64enc_0.1-3    \n [7] fs_1.5.2            rstudioapi_0.13     farver_2.1.0       \n[10] bit64_4.0.5         fansi_1.0.3         lubridate_1.8.0    \n[13] xml2_1.3.3          codetools_0.2-18    splines_4.2.1      \n[16] knitr_1.39          polyclip_1.10-0     jsonlite_1.8.0     \n[19] broom_0.8.0         cluster_2.1.3       dbplyr_2.2.1       \n[22] png_0.1-7           ggforce_0.3.3       effectsize_0.7.0   \n[25] compiler_4.2.1      httr_1.4.3          backports_1.4.1    \n[28] assertthat_0.2.1    Matrix_1.4-1        fastmap_1.1.0      \n[31] cli_3.3.0           tweenr_1.0.2        htmltools_0.5.2    \n[34] tools_4.2.1         gtable_0.3.0        glue_1.6.2         \n[37] Rcpp_1.0.8.3        cellranger_1.1.0    vctrs_0.4.1        \n[40] svglite_2.1.0       insight_0.17.1      xfun_0.31          \n[43] openxlsx_4.2.5      rvest_1.0.2         lifecycle_1.0.1    \n[46] mosaicCore_0.9.0    MASS_7.3-57         vroom_1.5.7        \n[49] hms_1.1.1           parallel_4.2.1      RColorBrewer_1.1-3 \n[52] yaml_2.3.5          curl_4.3.2          gridExtra_2.3      \n[55] labelled_2.9.1      rpart_4.1.16        latticeExtra_0.6-29\n[58] stringi_1.7.6       bayestestR_0.12.1   checkmate_2.1.0    \n[61] zip_2.2.0           rlang_1.0.3         pkgconfig_2.0.3    \n[64] systemfonts_1.0.4   evaluate_0.15       htmlwidgets_1.5.4  \n[67] labeling_0.4.2      bit_4.0.4           tidyselect_1.1.2   \n[70] plyr_1.8.7          magrittr_2.0.3      R6_2.5.1           \n[73] generics_0.1.2      DBI_1.1.3           pillar_1.7.0       \n[76] haven_2.5.0         foreign_0.8-82      withr_2.5.0        \n[79] datawizard_0.4.1    nnet_7.3-17         modelr_0.1.8       \n[82] crayon_1.5.1        utf8_1.2.2          tzdb_0.3.0         \n[85] rmarkdown_2.14      jpeg_0.1-9          grid_4.2.1         \n[88] readxl_1.4.0        data.table_1.14.2   reprex_2.0.1       \n[91] digest_0.6.29       webshot_0.5.3       munsell_0.5.0      \n[94] viridisLite_0.4.0"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Schmidt, Dennis, Tobias Raupach, Annette Wiegand, Manfred Herrmann, and\nPhilipp Kanzow. 2021. “Relation Between Examinees’ True Knowledge\nand Examination Scores: Systematic Review and Exemplary Calculations on\nMultiple-True-False\nItems.” Educational Research Review 34 (November):\n100409. https://doi.org/10.1016/j.edurev.2021.100409."
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#wip-exploring",
    "href": "analysis/SGC3A/3_sgc3A_description.html#wip-exploring",
    "title": "3  Description",
    "section": "WIP EXPLORING",
    "text": "WIP EXPLORING"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#accuracy-vs-latency",
    "href": "analysis/SGC3A/3_sgc3A_description.html#accuracy-vs-latency",
    "title": "3  Description",
    "section": "ACCURACY (VS) LATENCY",
    "text": "ACCURACY (VS) LATENCY\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ rt_m, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by Total Item Response Time\",\n    subtitle = \"\", \n    x = \"Total Item Response Time (minutes)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_avg_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by Average Item Response Time\",\n    subtitle = \"\", \n    x = \"Average Item Response Time (seconds)\", y = \"Total Scaled Score\"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_scaffold_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_max_rt, data = df_subjects %>% filter(item_max_rt < 400), alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MAX Item Response Time\",\n    subtitle = \"\", \n    x = \"MAX Item Response Time (s)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\n\n\nCODE\nq.stats <- df_items %>% filter(q != 6) %>% dplyr::group_by(q, pretty_condition, score_niceABS) %>% summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  group = paste(pretty_condition,\"-\",score_niceABS)\n)\n\ngf_line( m ~ as.factor(q), group = ~group,  color = ~score_niceABS,data = q.stats) %>% \n  gf_point() %>% \n  gf_facet_wrap(~pretty_condition) + scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average ITEM response time by condition\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")\n\n\n\n\n\n\n\nCODE\nq.stats <- df_items %>% filter(q != 6) %>% dplyr::group_by(q, pretty_condition, interpretation) %>% summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  group = paste(pretty_condition,\"-\",score_SCALED)\n)\n\ngf_line( m ~ as.factor(q), group = ~group,  color = ~interpretation,data = q.stats) %>% \n  gf_point() %>% \n  gf_facet_wrap(~pretty_condition) + #+ scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average ITEM response time by condition\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Interpretation\")\n\n\n\n\n\n\n\nCODE\nx <- df_items %>% filter(q %nin% c(6,9)) %>% dplyr::group_by(subject) %>% mutate(\n                            total = sum(score_niceABS),\n                            perf = as.factor(total < 5),\n                            perf = recode(perf, \"FALSE\"=\"high-scoring\", \"TRUE\"=\"low-scoring\")) %>% ungroup()\n\ngf_line( rt_s ~ as.factor(q), group = ~subject, color = ~interpretation, data = x) %>% \n  gf_facet_grid(pretty_condition ~ perf)+ scale_x_discrete(labels = c(1 , 2,  3,  4,  5,  7,  8, 10, 11, 12, 13, 14, 15))+ labs(\n    title = \"Response Time (s) by Question [facet by score <5]\"\n  )\n\n\n\n\n\n\n\nCODE\ngf_boxplot( item_avg_rt ~ pretty_condition,  data = df_subjects) %>% \n  gf_facet_wrap(~pretty_mode) +\n  labs(title = \"Average item response time by mode and condition\")\n\n\n\n\n\nCODE\ngf_boxplot( item_scaffold_rt ~ pretty_condition,  data = df_subjects) %>% \n  gf_facet_wrap(~pretty_mode) +\n  labs(title = \"Average SCAFFOLD response time by mode and condition\")\n\n\n\n\n\nCODE\ngf_boxplot( item_test_rt ~ pretty_condition,  data = df_subjects) %>% \n  gf_facet_wrap(~pretty_mode) +\n  labs(title = \"Average TEST response time by mode and condition\")\n\n\n\n\n\nCODE\ngf_boxplot( totaltime_m ~ pretty_condition,  data = df_subjects) %>% \n  gf_facet_wrap(~pretty_mode) +\n  labs(title = \"Average TOTAL response time by mode and condition\")\n\n\n\n\n\nCODE\ngf_boxplot( item_q1_rt ~ pretty_condition,  data = df_subjects) %>% \n  gf_jitter(width=0.2, alpha = 0.5, size = 0.75, color = ~item_q1_NABS) %>% \n  gf_facet_wrap(~pretty_mode) +\n  labs(title = \"Average FIRST ITEM response time by mode and condition\")"
  },
  {
    "objectID": "analysis/SGC3A/3_sgc3A_description.html#exploring-relationships",
    "href": "analysis/SGC3A/3_sgc3A_description.html#exploring-relationships",
    "title": "3  Description",
    "section": "EXPLORING RELATIONSHIPS",
    "text": "EXPLORING RELATIONSHIPS\n\nACCURACY (VS) LATENCY\n\nTotal Task\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ rt_m, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by TOTAL Item Response Time\",\n    subtitle = \"\", \n    x = \"Total Item Response Time (minutes)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_avg_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MEAN Item Response Time\",\n    subtitle = \"\", \n    x = \"Average Item Response Time (seconds)\", y = \"Total Scaled Score\"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_max_rt, data = df_subjects %>% filter(item_max_rt < 400), alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MAX Item Response Time\",\n    subtitle = \"\", \n    x = \"MAX Item Response Time (s)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#NOTE: LOG transforms of the RT do not yield linear relationships\n\n\n\n\nPhase Specific\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_scaffold_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"TOTAL Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( item_scaffold_NABS ~ item_scaffold_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"SCAFFOLD (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"SCAFFOLD Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( item_test_NABS ~ item_scaffold_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"TEST (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"TEST Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( item_test_NABS ~ item_avg_rt, data = df_subjects, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"TEST (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"AVERAGE Item Response Time (minutes)\", y = \"TEST Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#NOTE: LOG transforms of the RT do not yield linear relationships\n\n\n\n\nAverage Item RT by Accuracy\n\n\nCODE\nq.stats <- df_items %>% filter(q != 6) %>% dplyr::group_by(q, pretty_condition, score_niceABS) %>% dplyr::summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score_niceABS)\n)\n\ngf_line( m ~ q, group = ~group,  color = ~as.factor(score_niceABS),data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~q) %>% \n  gf_facet_wrap(~pretty_condition) + scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Absolute Score\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\n# df_items %>%\n#   ggplot(aes(y = rt_s, x = q,  fill = pretty_condition)) +\n#   stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + facet_wrap(~pretty_condition)\n\n\n\n\nCODE\nq.stats <- df_items %>% filter(q %nin% c(6,9)) %>% dplyr::group_by(q, pretty_condition, interpretation) %>% dplyr::summarise(\n  m = mean(rt_s),\n  sd = sd(rt_s),\n  sd = tidyr::replace_na(sd,0),\n  lo = m-sd/2,\n  hi = m+sd/2,\n  group = paste(pretty_condition,\"-\",score_SCALED)\n)\n\ngf_line( m ~ as.factor(q), group = ~group,  color = ~interpretation,data = q.stats) %>% \n  gf_point() %>% \n  gf_ribbon(lo+hi~as.factor(q)) %>% \n  gf_facet_grid(interpretation~pretty_condition) + #+ scale_color_manual(values=c(\"red\",\"green\")) + \n  labs(title = \"Average Item Response Time by Interpretation\",\n       subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n       caption=\"NOTE: Points with no ribbon indicate singular response\",\n       x = \"Question\", y = \"Averate Item Response Time\", color=\"Interpretation\")\n\n\n\n\n\nCODE\n#GGDIST LINERIBBON\ndf_items %>% filter(q %nin% c(6,9)) %>% mutate( interpretation = recode(interpretation, \"reference\" = \"blank\", \"frenzy\" = \"?\")) %>% \n  ggplot(aes(y = rt_s, x = q,  fill = interpretation)) +\n  stat_lineribbon(alpha = 1/4, point_interval = \"mean_qi\") + \n  facet_grid(interpretation ~ pretty_condition) + \n  labs( title = \"Average Response Time by Question Interpretation\", x = \"Question\", y=\"Averate Item Response Time (s)\")"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#response-latency",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#response-latency",
    "title": "4  Hypothesis Testing",
    "section": "RESPONSE LATENCY",
    "text": "RESPONSE LATENCY\n\n\nTODO: Investigate super high and super low response times..\n\nTODO: Investigate appropriate models for response time data. (see: https://lindeloev.github.io/shiny-rt/).\nEspecially see https://lindeloev.github.io/shiny-rt/ for ideas on modelling reaction time data\n\n\n\n\n\n\n\nResearch Question\n\n\n\n\nHypothesis\n\n\n\nAnalysis Strategy\n\n\n\nAlternatives\n\n\n\nInference\n\n\n\n\nQ1 Response Latency\nLinear Regression (Log Transform)\n(In Person)\nVisualization\n\nCODE#HISTOGRAM\nstats = df_lab %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_q1_rt))\ngf_dhistogram(~log(item_q1_rt), fill = ~pretty_condition, data = df_lab) %>% gf_facet_grid(~pretty_condition) +\n  # gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(title = \"(LAB) First Question Response Time\",,\n       # x = \"Response Time (seconds)\",\n       # y = \"proportion of participants\",\n       subtitle = \"\") + \n  theme_minimal()\n\n\n\n\nModel\n\nCODE#SCORE predicted by CONDITION\nlab.q1t.lm1 <- lm(log(item_q1_rt) ~ pretty_condition, data = df_lab)\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(lab.q1t.lm1)\n\n\nCall:\nlm(formula = log(item_q1_rt) ~ pretty_condition, data = df_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8202 -0.3504  0.0503  0.3382  1.2862 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               3.4833     0.0687   50.69   <2e-16 ***\npretty_conditionimpasse   0.3142     0.0964    3.26   0.0014 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.541 on 124 degrees of freedom\nMultiple R-squared:  0.0789,    Adjusted R-squared:  0.0714 \nF-statistic: 10.6 on 1 and 124 DF,  p-value: 0.00145\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(lab.q1t.lm1)\n\nAnalysis of Variance Table\n\nResponse: log(item_q1_rt)\n                  Df Sum Sq Mean Sq F value Pr(>F)   \npretty_condition   1    3.1   3.108    10.6 0.0014 **\nResiduals        124   36.3   0.293                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(lab.q1t.lm1)\n\n                        2.5 % 97.5 %\n(Intercept)             3.347  3.619\npretty_conditionimpasse 0.123  0.505\n\nCODEreport(lab.q1t.lm1) #sanity check\n\nFormula contains log- or sqrt-terms. See help(\"standardize\") for how such terms are standardized.\nFormula contains log- or sqrt-terms. See help(\"standardize\") for how such terms are standardized.\n\n\nWe fitted a linear model (estimated using OLS) to predict item_q1_rt with pretty_condition (formula: log(item_q1_rt) ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.08, F(1, 124) = 10.61, p = 0.001, adj. R2 = 0.07). The model's intercept, corresponding to pretty_condition = control, is at 3.48 (95% CI [3.35, 3.62], t(124) = 50.69, p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.31, 95% CI [0.12, 0.51], t(124) = 3.26, p = 0.001; Std. beta = 0.21, 95% CI [0.09, 0.34])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#print model equation\neq <- extract_eq(lab.q1t.lm1, use_coefs = TRUE)\n\n\n\nCODE#MODEL ESTIMATES WITH UNCERTAINTY\n\n#setup references \n#lab.q1t.lm1 <- lm(log(item_q1_rt) ~ condition, data = df_lab)\nm <- lab.q1t.lm1\ndf <- df_lab \ncall <- m$call %>% as.character()\n\n# uncertainty model visualization\ndf <- df  %>%\n  data_grid(pretty_condition) %>%\n  augment(m, newdata = ., se_fit = TRUE) \n\n#transform log\ndf$.fitted <- exp(df$.fitted)\ndf$.se.fit <- exp(df$.se.fit)\n\ndf %>% \n  ggplot(aes(y = pretty_condition, color = pretty_condition)) +\n  stat_halfeye( scale = .5,\n      aes(\n        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n        fill = stat(cut_cdf_qi(cdf, \n                .width = c(.90, .95),\n                labels = scales::percent_format())))) +\n  scale_fill_brewer(direction = -1) + \n  labs (title = \"(LAB) Q1 Response Latency ~ Condition\", \n        x = \"model predicted mean (seconds)\", y = \"Condition\", fill = \"Interval\",\n        subtitle = paste(\"lm(\",call[2],\")\"),\n        caption = \"note: model log(predictions) have exponentiated to original scale\") + theme(legend.position = \"blank\")\n\n\n\n\nDiagnostics\n\nCODE#model diagnostics\ncheck_model(lab.q1t.lm1, panel = TRUE)\n\n\n\n\n\nRESIDUAL DISTRIBUTION:\nHOMOGENEITY:\nHETERSCEDASTICITY:\nAUTOCORRELATION:\nOUTLIERS: FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE\nInference\nOLS Linear Regression on Q1 response time shows that condition explains a small but statistically significant amount of variance (impasse > control). However, the model violates the assumption of normally distributed residuals.\n(Online Replication)\nVisualization\n\nCODE#HISTOGRAM\nstats = df_online %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_q1_rt))\ngf_dhistogram(~log(item_q1_rt), fill = ~pretty_condition, data = df_lab) %>% gf_facet_grid(~pretty_condition) +\n  # gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(title = \"(ONLINE) First Question Response Time\",\n       # x = \"Response Time (seconds)\",\n       # y = \"proportion of participants\",\n       subtitle = \"\") + \n  theme_minimal()\n\n\n\n\nModel\n\nCODE#SCORE predicted by CONDITION\nrep.q1t.lm1 <- lm(log(item_q1_rt) ~ pretty_condition, data = df_online)\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(rep.q1t.lm1)\n\n\nCall:\nlm(formula = log(item_q1_rt) ~ pretty_condition, data = df_online)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0092 -0.3640 -0.0372  0.3785  2.0718 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               3.2505     0.0733   44.36  < 2e-16 ***\npretty_conditionimpasse   0.4200     0.1007    4.17 0.000045 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.718 on 202 degrees of freedom\nMultiple R-squared:  0.0793,    Adjusted R-squared:  0.0747 \nF-statistic: 17.4 on 1 and 202 DF,  p-value: 0.0000451\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(rep.q1t.lm1)\n\nAnalysis of Variance Table\n\nResponse: log(item_q1_rt)\n                  Df Sum Sq Mean Sq F value   Pr(>F)    \npretty_condition   1      9    8.97    17.4 0.000045 ***\nResiduals        202    104    0.52                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(rep.q1t.lm1)\n\n                        2.5 % 97.5 %\n(Intercept)             3.106  3.395\npretty_conditionimpasse 0.221  0.619\n\nCODEreport(rep.q1t.lm1) #sanity check\n\nFormula contains log- or sqrt-terms. See help(\"standardize\") for how such terms are standardized.\nFormula contains log- or sqrt-terms. See help(\"standardize\") for how such terms are standardized.\n\n\nWe fitted a linear model (estimated using OLS) to predict item_q1_rt with pretty_condition (formula: log(item_q1_rt) ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.08, F(1, 202) = 17.39, p < .001, adj. R2 = 0.07). The model's intercept, corresponding to pretty_condition = control, is at 3.25 (95% CI [3.11, 3.39], t(202) = 44.36, p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.42, 95% CI [0.22, 0.62], t(202) = 4.17, p < .001; Std. beta = 0.21, 95% CI [0.11, 0.31])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#print model equation\neq <- extract_eq(rep.q1t.lm1, use_coefs = TRUE)\n\n\n\nCODE#MODEL ESTIMATES WITH UNCERTAINTY\n\n#setup references \n# rep.q1t.lm1 <- lm(log(item_q1_rt) ~ condition, data = df_online)\nm <- rep.q1t.lm1\ndf <- df_online \ncall <- m$call %>% as.character()\n\n# uncertainty model visualization\ndf <- df  %>%\n  data_grid(pretty_condition) %>%\n  augment(m, newdata = ., se_fit = TRUE) \n\n#transform log\ndf$.fitted <- exp(df$.fitted)\ndf$.se.fit <- exp(df$.se.fit)\n\ndf %>% \n  ggplot(aes(y = pretty_condition, color = pretty_condition)) +\n  stat_halfeye( scale = .5,\n      aes(\n        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n        fill = stat(cut_cdf_qi(cdf, \n                .width = c(.90, .95),\n                labels = scales::percent_format())))) +\n  scale_fill_brewer(direction = -1) + \n  labs (title = \"(ONLINE) Q1 Response Latency ~ Condition\", \n        x = \"model predicted mean (seconds)\", y = \"Condition\", fill = \"Interval\",\n        subtitle = paste(\"lm(\",call[2],\")\"),\n        caption = \"note: model log(predictions) have exponentiated to original scale\") + theme(legend.position = \"blank\")\n\n\n\n\nDiagnostics\n\nCODE#model diagnostics\ncheck_model(rep.q1t.lm1, panel = TRUE)\n\n\n\n\n\nRESIDUAL DISTRIBUTION:\nHOMOGENEITY:\nHETERSCEDASTICITY:\nAUTOCORRELATION:\nOUTLIERS: FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE\nInference\nOLS Linear Regression on Q1 response time shows that condition explains a small but statistically significant amount of variance (impasse > control). However, the model violates the assumption of normally distributed residuals."
  },
  {
    "objectID": "analysis/combined/combined.html",
    "href": "analysis/combined/combined.html",
    "title": "19  Description",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "analysis/combined/combined.html#ospan",
    "href": "analysis/combined/combined.html#ospan",
    "title": "14  TODO Description",
    "section": "OSPAN",
    "text": "OSPAN\nTODO OSPAN description\n\n\nCODE\ntitle = \"Descriptive Statistics of OSPAN Task Accuracy\"\nospan.stats <- rbind(\n  \"MATH\" = df_subjects %>% dplyr::select(math_acc) %>% unlist() %>% favstats(),\n  \"ORDER\" = df_subjects %>%  dplyr::select(order_acc) %>% unlist() %>% favstats(),\n  \"WEIGHTED\" = df_subjects %>% dplyr::select(weighted) %>% unlist() %>% favstats()\n  \n) \nospan.stats %>% kbl (caption = title) %>% kable_classic() %>% \n  footnote(general = \"MATH = %correct of all math questions; \n           ORDER = % correct of OSPAN ordering (out of 30); WEIGHTED = math*ospan number correct\", general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of OSPAN Task Accuracy\n \n  \n      \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n \n\n  \n    MATH \n    0.517 \n    0.897 \n    0.931 \n    0.966 \n    1 \n    0.924 \n    0.085 \n    266 \n    0 \n  \n  \n    ORDER \n    0.000 \n    0.533 \n    0.733 \n    0.867 \n    1 \n    0.678 \n    0.253 \n    266 \n    0 \n  \n  \n    WEIGHTED \n    0.000 \n    13.448 \n    20.276 \n    24.828 \n    30 \n    19.082 \n    7.377 \n    266 \n    0 \n  \n\n\nNote:   MATH = %correct of all math questions;            ORDER = % correct of OSPAN ordering (out of 30); WEIGHTED = math*ospan number correct\n\n\n\n\nFor (online) OSPAN weighted scores (n = 266) range from 0 to 30 with a mean score of (M = 19.08, SD = 7.38).\n\n\nCODE\n#GGFORMULA | DENSITY HISTOGRAM \n  gf_dhistogram(~weighted, data = df_subjects) + \n  labs(x = \"OSPAN (weighted) score\",\n       y = \"% of subjects\",\n       title = \"Distribution of OSPAN SCORE\",\n       subtitle = \"\") + \n  theme_minimal()\n\n\n\n\n\nCODE\n##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_subjects, x = \"weighted\", binwidth = 1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of OSPAN Score\",\n        subtitle =\"The Distribution of OSPAN scores is similar across conditions\",\n        x = \"OSPAN (weighted) score\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE\n#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_subjects, aes(weighted)) + \n  stat_ecdf() + \n  facet_wrap(~pretty_condition) + \n  labs( title = \"Empirical Cumulative Density Function — OSPAN\",\n        x = \"OSPAN (weighted) score\", \n        y = \"Cumulative Probability\") + theme_minimal()"
  },
  {
    "objectID": "analysis/combined/combined.html#peeking",
    "href": "analysis/combined/combined.html#peeking",
    "title": "14  TODO Description",
    "section": "PEEKING",
    "text": "PEEKING\n\n\nCODE\n#CONDITION ONLY MODEL\nm1 <- lm( item_test_SCALED ~ condition, data = df_subjects)\nsummary(m1)\n\n\n\nCall:\nlm(formula = item_test_SCALED ~ condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.86  -3.38  -2.38   5.62  12.62 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -4.623      0.518   -8.93  < 2e-16 ***\ncondition121    3.483      0.724    4.81  2.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.9 on 264 degrees of freedom\nMultiple R-squared:  0.0806,    Adjusted R-squared:  0.0771 \nF-statistic: 23.1 on 1 and 264 DF,  p-value: 2.54e-06\n\n\nCODE\n#OSPAN ONLY MODEL\nm2 <- lm( item_test_SCALED ~ weighted, data = df_subjects)\nsummary(m2)\n\n\n\nCall:\nlm(formula = item_test_SCALED ~ weighted, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -5.64  -4.85  -3.12   5.70  11.19 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -3.6855     1.0472   -3.52  0.00051 ***\nweighted      0.0442     0.0512    0.86  0.38876    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.15 on 264 degrees of freedom\nMultiple R-squared:  0.00282,   Adjusted R-squared:  -0.000962 \nF-statistic: 0.745 on 1 and 264 DF,  p-value: 0.389\n\n\nCODE\n#STANDARDIZE OSPAN\ndf_subjects$z_ospan = zscore(df_subjects$weighted)\nhistogram(df_subjects$z_ospan)\n\n\n\n\n\nCODE\n#OSPAN MEDIAN SPLIT\nmed_ospan = median(df_subjects$weighted)\ndf_subjects$low_ospan <- as.factor(df_subjects$weighted < med_ospan)\n\n#STANDARDIZED OSPAN ONLY MODEL\nm3 <- lm( item_test_SCALED ~ low_ospan, data = df_subjects)\nsummary(m3)\n\n\n\nCall:\nlm(formula = item_test_SCALED ~ low_ospan, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -5.84  -4.40  -2.90   6.10  11.60 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -2.157      0.517   -4.17 0.000041 ***\nlow_ospanTRUE   -1.446      0.751   -1.93    0.055 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.11 on 264 degrees of freedom\nMultiple R-squared:  0.0139,    Adjusted R-squared:  0.0101 \nF-statistic: 3.71 on 1 and 264 DF,  p-value: 0.0552\n\n\nCODE\n#MULTIPLE WITH STANDARDIZED OSPAN\nmr <- lm(item_test_SCALED ~ condition + low_ospan, data = df_subjects)\nsummary(mr)\n\n\n\nCall:\nlm(formula = item_test_SCALED ~ condition + low_ospan, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -7.76  -4.13  -2.13   4.74  13.62 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -3.869      0.600   -6.45  5.3e-10 ***\ncondition121     3.630      0.720    5.04  8.6e-07 ***\nlow_ospanTRUE   -1.751      0.721   -2.43    0.016 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.85 on 263 degrees of freedom\nMultiple R-squared:  0.101, Adjusted R-squared:  0.0939 \nF-statistic: 14.7 on 2 and 263 DF,  p-value: 8.6e-07\n\n\nCODE\nanova(mr)\n\n\nAnalysis of Variance Table\n\nResponse: item_test_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   1    806     806    23.6 2.1e-06 ***\nlow_ospan   1    202     202     5.9   0.016 *  \nResiduals 263   9000      34                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCODE\n#SINGLE WITH STANDARDIZED OSPAN\nmr2 <- lm(item_test_SCALED ~ condition + low_ospan, data = df_subjects)\nsummary(mr2)\n\n\n\nCall:\nlm(formula = item_test_SCALED ~ condition + low_ospan, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -7.76  -4.13  -2.13   4.74  13.62 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -3.869      0.600   -6.45  5.3e-10 ***\ncondition121     3.630      0.720    5.04  8.6e-07 ***\nlow_ospanTRUE   -1.751      0.721   -2.43    0.016 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.85 on 263 degrees of freedom\nMultiple R-squared:  0.101, Adjusted R-squared:  0.0939 \nF-statistic: 14.7 on 2 and 263 DF,  p-value: 8.6e-07\n\n\nCODE\ngf_histogram(~item_test_SCALED, data = df_subjects) %>% \n  gf_facet_grid(low_ospan ~ condition, labeller = label_both)\n\n\n\n\n\nCODE\ngf_jitter(item_test_SCALED ~ weighted, color = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(~pretty_condition)\n\n\n\n\n\nCODE\ngf_dhistogram(~item_test_NABS , color = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_wrap(low_ospan ~ pretty_condition, labeller = label_both)\n\n\n\n\n\nOPSAN score does NOT predict performance in the control condition.  ::: {.cell}\n\nCODE\ncontrol <- df_subjects %>% filter(condition == \"111\")\n\ngf_dhistogram(~item_test_NABS , color = ~pretty_condition, data = control) %>% \n  gf_facet_wrap(low_ospan ~ pretty_condition, labeller = label_both)\n\n\n\n\n\nCODE\n#test performance predicted by OSPAN score\nm <- lm(item_test_NABS ~ weighted, data = control)\nsummary(m)\n\n\n\nCall:\nlm(formula = item_test_NABS ~ weighted, data = control)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -1.17  -1.17  -1.17  -1.16   6.83 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 1.160579   0.653251    1.78    0.078 .\nweighted    0.000443   0.031503    0.01    0.989  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.52 on 128 degrees of freedom\nMultiple R-squared:  1.55e-06,  Adjusted R-squared:  -0.00781 \nF-statistic: 0.000198 on 1 and 128 DF,  p-value: 0.989\n\n:::\nOPSAN score does NOT predict performance in the control condition.  ::: {.cell}\n\nCODE\nimpasse <- df_subjects %>% filter(condition == \"121\")\n\ngf_dhistogram(~item_test_NABS , color = ~pretty_condition, data = impasse) %>% \n  gf_facet_wrap(low_ospan ~ pretty_condition, labeller = label_both)\n\n\n\n\n\nCODE\n#test performance predicted by OSPAN score\nm <- lm(item_test_NABS ~ low_ospan, data = impasse)\nsummary(m)\n\n\n\nCall:\nlm(formula = item_test_NABS ~ low_ospan, data = impasse)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3.64  -1.99  -1.77   4.23   6.23 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      3.636      0.407    8.94  2.7e-15 ***\nlow_ospanTRUE   -1.865      0.567   -3.29   0.0013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.3 on 134 degrees of freedom\nMultiple R-squared:  0.0747,    Adjusted R-squared:  0.0678 \nF-statistic: 10.8 on 1 and 134 DF,  p-value: 0.00128\n\n:::\n\n\nCODE\n#330 total \n# —————————\n#126 lab \n#204 online  \n  #  71 no ospan \n  # 133 ospan \nsgc3a_lab <- df_all %>% filter(mode==\"lab-synch\") %>% filter(study==\"SGC3A\")\nsgc3a_online <- df_all %>% filter(mode==\"asynch\") %>% filter(study==\"SGC3A\")\nsgc3a_online_noospan <- sgc3a_online %>% filter(subject %nin% ospan$subject)\nsgc3a_onspan <- df_subjects\n\nm <- lm( item_test_NABS ~ condition, data = sgc3a_online_noospan)\nsummary(m)\n\n\n\nCall:\nlm(formula = item_test_NABS ~ condition, data = sgc3a_online_noospan)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3.10  -2.85  -1.87   3.65   6.13 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.871      0.417    4.49 0.000015 ***\ncondition121    1.229      0.555    2.21    0.028 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.28 on 140 degrees of freedom\nMultiple R-squared:  0.0338,    Adjusted R-squared:  0.0269 \nF-statistic:  4.9 on 1 and 140 DF,  p-value: 0.0284\n\n\nCODE\nm2 <- lm(item_test_NABS ~ condition, data = df_subjects)\nsummary(m2)\n\n\n\nCall:\nlm(formula = item_test_NABS ~ condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -2.68  -2.68  -1.17   2.32   6.83 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.169      0.264    4.43 0.000014 ***\ncondition121    1.507      0.369    4.08 0.000059 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.01 on 264 degrees of freedom\nMultiple R-squared:  0.0594,    Adjusted R-squared:  0.0558 \nF-statistic: 16.7 on 1 and 264 DF,  p-value: 0.0000592\n\n\nCODE\nm3 <- lm(item_test_NABS ~ low_ospan, data = df_subjects)\nsummary(m3)\n\n\n\nCall:\nlm(formula = item_test_NABS ~ low_ospan, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -2.41  -2.41  -1.41   2.59   6.59 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      2.414      0.259    9.33   <2e-16 ***\nlow_ospanTRUE   -1.002      0.376   -2.66   0.0082 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.06 on 264 degrees of freedom\nMultiple R-squared:  0.0262,    Adjusted R-squared:  0.0225 \nF-statistic: 7.09 on 1 and 264 DF,  p-value: 0.00822\n\n\n\n\nCODE\n# library(lpme)\n# m <- modereg(Y = df_ospan$DV_percent_NABS, W= df_ospan$condition, \n#         bw = 2, nstart = 2, PLOT = TRUE)\n# \n# summary(m)"
  },
  {
    "objectID": "analysis/combined/combined.html#resources",
    "href": "analysis/combined/combined.html#resources",
    "title": "19  Description",
    "section": "RESOURCES",
    "text": "RESOURCES\n\nCODEsessionInfo()\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] distributional_0.3.0 modelr_0.1.8         generics_0.1.2      \n [4] see_0.6.9            report_0.5.1         ggdist_3.1.1        \n [7] forcats_0.5.1        stringr_1.4.0        purrr_0.3.4         \n[10] readr_2.1.2          tidyr_1.2.0          tibble_3.1.6        \n[13] tidyverse_1.3.1      ggpubr_0.4.0         vcd_1.4-9           \n[16] kableExtra_1.3.4     performance_0.8.0    fitdistrplus_1.1-8  \n[19] MASS_7.3-55          multimode_1.5        mosaic_1.8.3        \n[22] ggridges_0.5.3       mosaicData_0.20.2    ggformula_0.10.1    \n[25] ggstance_0.3.5       dplyr_1.0.8          Matrix_1.4-0        \n[28] janitor_2.1.0        Hmisc_4.6-0          ggplot2_3.3.5       \n[31] Formula_1.2-4        survival_3.3-1       lattice_0.20-45     \n\nloaded via a namespace (and not attached):\n  [1] readxl_1.3.1        backports_1.4.1     systemfonts_1.0.4  \n  [4] plyr_1.8.6          splines_4.1.1       crosstalk_1.2.0    \n  [7] leaflet_2.1.0       TH.data_1.1-0       qqplotr_0.0.5      \n [10] digest_0.6.29       htmltools_0.5.2     fansi_1.0.2        \n [13] magrittr_2.0.2      checkmate_2.0.0     cluster_2.1.2      \n [16] ks_1.13.5           tzdb_0.2.0          mosaicCore_0.9.0   \n [19] vroom_1.5.7         sandwich_3.0-1      svglite_2.1.0      \n [22] jpeg_0.1-9          colorspace_2.0-3    rvest_1.0.2        \n [25] ggrepel_0.9.1       haven_2.4.3         xfun_0.30          \n [28] crayon_1.5.0        jsonlite_1.8.0      zoo_1.8-9          \n [31] glue_1.6.2          polyclip_1.10-0     gtable_0.3.0       \n [34] emmeans_1.7.2       webshot_0.5.2       car_3.0-12         \n [37] DEoptimR_1.0-10     abind_1.4-5         scales_1.1.1       \n [40] mvtnorm_1.1-3       DBI_1.1.2           rstatix_0.7.0      \n [43] Rcpp_1.0.8.3        xtable_1.8-4        viridisLite_0.4.0  \n [46] htmlTable_2.4.0     foreign_0.8-82      bit_4.0.4          \n [49] mclust_5.4.10       datawizard_0.4.1    htmlwidgets_1.5.4  \n [52] httr_1.4.2          RColorBrewer_1.1-2  ellipsis_0.3.2     \n [55] pkgconfig_2.0.3     farver_2.1.0        nnet_7.3-17        \n [58] dbplyr_2.1.1        utf8_1.2.2          effectsize_0.6.0.1 \n [61] tidyselect_1.1.2    labeling_0.4.2      rlang_1.0.2        \n [64] munsell_0.5.0       cellranger_1.1.0    tools_4.1.1        \n [67] cli_3.2.0           broom_0.7.12        evaluate_0.15      \n [70] fastmap_1.1.0       ggdendro_0.1.23     yaml_2.3.5         \n [73] knitr_1.38          bit64_4.0.5         fs_1.5.2           \n [76] robustbase_0.93-9   rootSolve_1.8.2.3   nlme_3.1-155       \n [79] pracma_2.3.8        xml2_1.3.3          compiler_4.1.1     \n [82] rstudioapi_0.13     png_0.1-7           ggsignif_0.6.3     \n [85] reprex_2.0.1        tweenr_1.0.2        stringi_1.7.6      \n [88] highr_0.9           parameters_0.18.1   vctrs_0.3.8        \n [91] pillar_1.7.0        lifecycle_1.0.1     lmtest_0.9-39      \n [94] estimability_1.3    data.table_1.14.2   insight_0.18.0     \n [97] patchwork_1.1.1     R6_2.5.1            latticeExtra_0.6-29\n[100] KernSmooth_2.23-20  gridExtra_2.3       codetools_0.2-18   \n[103] assertthat_0.2.1    withr_2.5.0         multcomp_1.4-18    \n[106] mgcv_1.8-39         diptest_0.76-0      bayestestR_0.12.1  \n[109] parallel_4.1.1      hms_1.1.1           rpart_4.1.16       \n[112] labelled_2.9.0      coda_0.19-4         rmarkdown_2.13     \n[115] snakecase_0.11.0    carData_3.0-5       ggforce_0.3.3      \n[118] lubridate_1.8.0     base64enc_0.1-3    \n\n\n\nCODEgf_histogram(~s_NABS, data = sgc3a) %>% gf_facet_wrap(condition~.)"
  },
  {
    "objectID": "analysis/combined/combined.html#item",
    "href": "analysis/combined/combined.html#item",
    "title": "19  Description",
    "section": "ITEM",
    "text": "ITEM\n\nCODE# #| label: IMPORT-DATA-ITEMS\n# #| warning : false\n# #| message : false\n# \n# # HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# # mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(imac)\n# \n# #IMPORT SUBJECTS \n# sgc3a_items <- read_rds(\"analysis/SGC3A/data/2-scored-data/sgc3a_scored_items.rds\") %>% mutate(\n#   study = \"SGC3A\"\n# ) \n# sgc4a_items <- read_rds(\"analysis/SGC4A/data/2-scored-data/sgc4a_scored_items.rds\") %>% mutate(\n#   study = \"SGC4A\",\n#   pretty_mode = \"online-replication\"\n# ) %>% dplyr::select(-answer)\n# \n# sgc4b_items <- read_rds(\"analysis/SGC4B/data/2-scored-data/sgc4b_scored_items.rds\") %>% mutate(\n#   pretty_mode = \"online-replication\"\n# ) %>% dplyr::select(-answer)\n# sgc5_items <- read_rds(\"analysis/SGC5A/data/2-scored-data/sgc5_scored_items.rds\") %>% mutate(\n#   study = \"SGC5A\",\n#   pretty_mode = \"online-replication\"\n# ) %>% dplyr::select(-answer)\n# \n# #COMPARE COLUMNS\n# janitor::compare_df_cols(sgc3a_items,sgc4a_items, sgc4b_items, sgc5_items)    \n# \n# #MERGE \n# df_items <- rbind(sgc3a_items, sgc4a_items, sgc4b_items, sgc5_items)\n# \n# #IMPORT OSPAN DATA \n# # ospan <- read_csv(\"analysis/combined/data/fall21_scored_ospan.csv\") %>% mutate(\n#   # subject = SUBJECTID\n# # )\n# \n# #MERGE OSPAN DATA WITH SGC DATA \n# # df_ospan <- df_subjects %>% filter(\n# #   subject %in% ospan$subject\n# # )\n# # \n# # df_ospan <- merge(df_ospan, ospan)\n# # \n# # #RENAME for reuse\n# # df_all <- df_subjects\n# # df_subjects <- df_ospan\n# \n# #CLEANUP\n# # rm(ospan, sgc3a, sgc4a, sgc4b, sgc5, df_ospan)\n# ```\n# \n# ```{r}\n# library(lme4)\n# library(lmerTest)\n# library(performance)\n# library(report)\n# \n# #LINEAR MODEL ON SUBJECT TOTAL SCORE\n# m <- lm(s_NABS ~ condition, data = sgc3a)\n# summary(m)\n# \n# #LINEAR MODEL ON SUBJECT TOTAL SCORE\n# m <- glm(score_niceABS~condition, data = sgc3a_items, family = \"binomial\")\n# summary(m)\n# performance(m)\n# \n# #0 RANDOM INTEREPTS SUBJECT\n# m.r0 <- glmer(score_niceABS ~ (1|subject), data = sgc3a_items, family = \"binomial\")\n# summary(m.r0)\n# anova(m.r0)\n# performance(m.r0)\n# \n# #0 RANDOM SLOPE CONDITION/SUBJECT\n# m.r1 <- glmer(score_niceABS ~ (0 + condition|subject), data = sgc3a_items, family = \"binomial\")\n# summary(m.r1)\n# anova(m.r1)\n# performance(m.r1)\n# \n# #0 RANDOM SLOPES and intertercepts CONDITION/SUBJECT\n# m.r2 <- glmer(score_niceABS ~ (1 + condition|subject), data = sgc3a_items, family = \"binomial\")\n# summary(m.r2)\n# anova(m.r2)\n# performance(m.r2)\n# \n# ####0 RANDOM INTERCEPTS SUBJECT RANDOM INTERCEPTS ITEM\n# m.r3 <- glmer(score_niceABS ~ (1 |subject) + (1|q), data = sgc3a_items, family = \"binomial\")\n# summary(m.r3)\n# anova(m.r3)\n# performance(m.r3)\n# \n# r3m <- glmer(score_niceABS ~ condition + (1 |subject) + (1|q), data = sgc3a_items, family = \"binomial\")\n# summary(r3m)\n# anova(r3m)\n# performance(r3m)\n# \n# plot(r3m)\n# check_model(r3m)\n# report(r3m)\n# \n# \n# \n# #BAD FIT\n# r4m <- glmer(score_niceABS ~ condition +  q + (1 |subject) , data = sgc3a_items, family = \"binomial\")\n# summary(r4m)\n# \n# #???\n# r5m <- glmer(score_niceABS ~ condition +  (1|q) + (1|subject) , data = sgc3a_items, family = \"binomial\")\n# summary(r5m)\n# \n# coef(r5m)\n# \n# # #1. RANDOM SUBJECT + FIXED CONDITION\n# # # X + (1 | SUBJECT)\n# # m1 <- glmer(score_niceABS ~ condition + (1|subject), data = sgc3a_items, family = \"binomial\")\n# # summary(m1)\n# # anova(m1)\n# # performance(m1)\n# # \n# # #1. RANDOM condition by SUBJECT + FIXED CONDITION\n# # # X + (1 + X | SUBJECT)\n# # m2 <- glmer(score_niceABS ~ condition + (1+condition|subject), data = sgc3a_items, family = \"binomial\")\n# # summary(m2)\n# # lmerTest::anova(m2)\n# # performance(m2)\n# # \n# # #2. RANDOM SUBJECT + RANDOM Q + FIXED CONDITION \n# # m2 <- glmer(score_niceABS ~ condition + (1|subject) + (1|q) , data = sgc3a_items, family = \"binomial\")\n# # summary(m2)\n# # anova(m2)\n# # performance(m2)\n# \n# ##R SQUARED marginal is fixed effects only\n# ##R SQUARED condition is with random effects"
  },
  {
    "objectID": "analysis/combined/combined.html#response-accuracy",
    "href": "analysis/combined/combined.html#response-accuracy",
    "title": "19  Description",
    "section": "RESPONSE ACCURACY",
    "text": "RESPONSE ACCURACY\nSubject Level Scores\nSubject level scores summarize the the response accuracy by a particular participant across all discriminant items in the graph comprehension task.\nAbsolute Score\n\nCODE#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL ABSOLUTE\n  gf_props(~s_NABS, data = df_all) + \n  labs(x = \"number of correct responses \",\n       y = \"% of subjects\",\n       title = \"Distribution of Absolute Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\nCODE##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_all, x = \"s_NABS\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of Absolute Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"Total Absolute Score\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\nCODE##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_all, aes(x = pretty_condition, y = s_NABS,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_NABS),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_NABS, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Absolute Score \",\n    x = \"Condition\", y = \"Total Absolute Score\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\nCODE#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_all, aes(item_test_NABS)) + \n  stat_ecdf(geom = \"step\") + \n  facet_wrap(pretty_condition~.) + \n  labs( title = \"Empirical Cumulative Density Function — Absolute Score \",\n        x = \"Total Absolute Score [0,13]\", \n        y = \"Cumulative Probability\")\n\n\n\n\nScaled Scores\n\nCODE#GGFORMULA | DENSITY HISTOGRAM SUBJECT TOTAL SCALED\ngf_props(~s_SCALED, data = df_all) +\n  labs(x = \"total scaled score\",\n       y = \"% of subjects\",\n       title = \"Distribution of Scaled Score \",\n       subtitle = \"Modes at high and low ends of scale suggest concentration of high (vs) low understanding\") \n\n\n\nCODE##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_all, x = \"s_SCALED\",binwidth=1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\")) + \n  labs( title = \"Distribution of Scaled Score\",\n        subtitle =\"Pattern of response is similar across data collection modes but differs by condition\",\n        x = \"total scaled score (test phase)\", y = \"number of participants\") + \n theme_minimal() + theme(legend.position = \"blank\") \n\n\n\nCODE##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_all, aes(x = pretty_condition, y = s_SCALED,\n                        fill = pretty_condition) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_SCALED),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = s_SCALED, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of Scaled Score \",\n    x = \"Condition\", y = \"Total Absolute Score (Test Phase)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\nCODE#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_all, aes(s_SCALED)) + \n  stat_ecdf(geom = \"step\") + \n  facet_wrap(~pretty_condition) + \n  labs( title = \"Empirical Cumulative Density Function — Scaled Score\",\n        x = \"Test Phase Scaled Score [-8,8]\", \n        y = \"Cumulative Probability\") \n\n\n\n\nFirst Item Scores\nNext we consider the response accuracy on just the first question of the graph comprehension task: a subject’s first exposure to the TM graph.\nFirst Item Absolute Score\n\nCODE#PROPORTIONAL BAR CHART\ngf_props(~item_q1_NABS, data = df_all) +\n  labs(x = \"response accuracy\",\n       y = \"% subjects\",\n       title = \"Proportion of Correct Responses on First Item\",\n       subtitle=\"\")+\n  theme(legend.position = \"none\")+theme_ggdist()\n\n\n\nCODE#PROPORTIONAL BAR CHART\ngf_props(~item_q1_NABS, data = df_all, fill = ~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) +\n  labs(x = \"response accuracy\",\n       title = \"Proportion of Correct Responses on First Item (by Modality and Condition)\",\n       subtitle=\"\")+\n  theme(legend.position = \"none\") \n\n\n\nCODE#MOSAIC PLOT\n# vcd::mosaic(main=\"Proportion of Correct Responses on First Item\",\n#             data = df_all, pretty_condition ~ item_q1_NABS, rot_labels=c(0,90,0,0), \n#             offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n#             spacing = spacing_dimequal(unit(1:2, \"lines\"))) \n\n\nFirst Item Scaled Score\nAt the item level, the scaled score gives us a numeric measure of correctness of interpretation, ranging from -1 to 1. (note: we evaluate scaled_score on the first item rather than interpretation, because no orthogonal interpretation is available in the impasse condition)\n\nCODE#GGFORMULA | PROPORTIONAL HISTOGRAM SUBJECT FIRST SCALED\ngf_props(~item_q1_SCALED, data = df_all) +\n  labs(x = \"scaled score (first item)\",\n       y = \"% of subjects\",\n       title = \"Distribution of First Item Scaled Score\",\n       subtitle = \"\") \n\n\n\nCODE##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_all, x = \"item_q1_SCALED\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE) \nfacet(p, facet.by=c(\"pretty_condition\")) + \n  labs( title = \"Distribution of First Item Scaled Score (by Mode and Condition)\",\n        subtitle =\"Impasse condition yields more intermediate scores (indicating uncertainty)\",\n        x = \"scaled score (firt item) \", y = \"number of participants\") + \n  theme_minimal() + theme(legend.position = \"blank\") \n\n\n\nCODE##GGFORMULA | HIST+DENSITY SCORE BY CONDITION/MODE\n# stats = df_subjects %>% group_by(pretty_condition, mode) %>% dplyr::summarise(mean = mean(item_q1_SCALED))\n# gf_density(~item_q1_SCALED, data = df_subjects) %>%\n#   gf_facet_grid(pretty_condition~mode, labeller = label_both) %>%\n#   gf_lims(x = c(-1, 1)) %>%\n#   gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n# labs( title = \"Distribution of First Item Scaled Score (by Mode and Condition)\",\n#         subtitle =\"Pattern of response is the same across data collection modes but differs by condition\",\n#         x = \"scaled score (firt item) \", y = \"number of participants\") + \n#   theme_minimal()\n\n\nInterpretation Scores\nTODO IMPORT ITEMS\n\nCODE# #PROPORTIONAL BAR CHART\n# gf_propsh(~interpretation, data = df_items, fill = ~pretty_condition) %>% \n#   gf_facet_grid(pretty_condition~pretty_mode) +\n#   labs(x = \"% of items\",\n#        title = \"Proportion of Interpretations Across Items\",\n#        subtitle=\"Impasse Condition yields shift from Orthogonal to alternative interpretations\")+\n#   theme(legend.position = \"none\")\n\n\n#MOSAIC PLOT\n# vcd::mosaic(main=\"Proportion of Interpretations across Conditions\",\n#             data = df_items, pretty_condition ~ interpretation, rot_labels=c(0,90,0,0), \n#             offset_varnames = c(left = 4.5), offset_labels = c(left = -0.5),just_labels = \"right\",\n#             spacing = spacing_dimequal(unit(1:2, \"lines\"))) \n\n\nCumulative Task Performance\nTODO CALC CUMULATIVE PERFORAMNCE OVER ALL ITEMS\n\nCODE# #VISUALIZE progress over time ABSOLUTE score \n# ggplot(data = df_absolute_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n#  geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n#  facet_wrap(~pretty_condition) + \n#  labs (title = \"Cumulative Absolute Score over sequence of task\", x = \"Question\" , y = \"Cumulative Absolute Score\") + \n#  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n#  theme_minimal() + theme(legend.position = \"blank\")\n# \n# #VISUALIZE progress over time SCALED score \n# ggplot(data = df_scaled_progress, aes(x = question, y = score, group = subject, alpha = 0.01, color = pretty_condition)) + \n#  geom_line(position=position_jitter(w=0.15, h=0.15), size=0.1) +\n#  facet_wrap(~pretty_condition) + \n#  labs (title = \"Cumulative Scaled Score over sequence of task\", x = \"Question\" , y = \"Cumulative Scaled Score\") + \n#  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n#  theme_minimal() + theme(legend.position = \"blank\")"
  },
  {
    "objectID": "analysis/combined/combined.html#response-latency",
    "href": "analysis/combined/combined.html#response-latency",
    "title": "19  Description",
    "section": "RESPONSE LATENCY",
    "text": "RESPONSE LATENCY\nTime on First Item\n\nCODE#HISTOGRAM\ngf_dhistogram(~item_q1_rt, data = df_all) %>%\n  # gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of First Item Response Time (seconds)\", subtitle = \"fit by gamma distribution\", x = \"First Item Response Time (seconds)\", y = \"% items\")\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\n\n\n\nCODE##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_all, x = \"item_q1_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        x = \"First Item Response Time (seconds)\", y = \"number of items\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\nCODE#recode as boolean correct\ndf_subjects <- df_subjects %>% mutate(\n  item_q1_NABS = as.logical(item_q1_NABS)\n)\n\n##RAINCLOUD USING GGDISTR\nggplot(df_all, aes(x = pretty_condition, y = item_q1_rt, color = item_q1_NABS) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .1\n  )) + \n  labs( title = \"Distribution of First Item Response Time (seconds)\",\n        subtitle =\"\",\n        y = \"First Item Response Time (s)\", x = \"Condition\") +\n  theme_ggdist() \n\n\n\nCODE# + theme(legend.position = \"blank\")\n# + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\nTime on Item\nTODO\n\nCODE# \n# #HISTOGRAM\n# gf_dhistogram(~rt_s, data = df_items) %>%\n#   gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n#   gf_fitdistr(dist=\"gamma\", color=\"red\")+\n#   labs(title=\"Distribution of Item Response Time (seconds)\", \n#        subtitle = \"fit by gamma distribution\", x = \"Item Response Time (seconds)\", y = \"% items\") \n# \n# \n# ##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\n# p <- gghistogram(df_items, x = \"rt_s\", binwidth = 0.5,\n#    add = \"mean\", rug = TRUE,\n#    fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n#    add_density = TRUE)\n# facet(p, facet.by=c(\"pretty_condition\",\"pretty_mode\")) +\n#   labs( title = \"Distribution of Item Response Time (seconds)\",\n#         subtitle =\"\",\n#         x = \"Item Response Time (seconds)\", y = \"number of items\") +\n#   theme_minimal() + theme(legend.position = \"blank\")\n# \n# \n# #recode as boolean correct\n# df_items <- df_items %>% mutate(\n#   score_niceABS = as.logical(score_niceABS)\n# )\n# \n# ##RAINCLOUD USING GGDISTR\n# ggplot(df_items, aes(x = pretty_condition, y = rt_s, color = score_niceABS) ) + \n#   ggdist::stat_halfeye(\n#     side = \"left\",\n#     # position = position_dodgejust(),\n#     justification = 1.5, \n#     # adjust = .5, \n#     width = .5, \n#     .width = 0, \n#     point_colour = NA) + \n#   geom_boxplot(\n#     width = .15, \n#     outlier.shape = NA,\n#     position = position_dodge2()\n#   ) +\n#   geom_point(\n#     size = 1.3,\n#     alpha = .3,\n#     position = position_jitterdodge(\n#       # seed = 1,\n#       dodge.width = 0.5,\n#       jitter.width = 0.075\n#   )) +\n#   labs( title = \"Distribution of Item Response Time (seconds)\",\n#         subtitle =\"\",\n#         y = \"Item Response Time (s)\", x = \"Condition\") +\n#   theme_ggdist() \n# # + theme(legend.position = \"blank\")\n# # + coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\nTime on SCAFFOLD Phase\nHere we consider just the time spent on the first five items of the task (the scaffold phase).\n\nCODE#HISTOGRAM\ngf_dhistogram(~item_scaffold_rt, data = df_all) %>%\n  # gf_vline(xintercept = ~time.stats[\"lab\",]$mean, color = \"black\") %>% \n  gf_fitdistr(dist=\"gamma\", color=\"red\")+\n  labs(title=\"Distribution of SCAFFOLD Phase Response Time (minutes)\", subtitle = \"fit by gamma distribution\", x = \"Scaffold Phase Time (minutes)\", y = \"% subjects\") \n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\n\n\n\nCODE##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_all, x = \"item_scaffold_rt\", binwidth = 0.5,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of SCAFFOLD Phase Response Time (minutes)\",\n        subtitle =\"\",\n        x = \"Scaffold Phase Time (minutes)\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\nCODE##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_all, aes(x = pretty_condition, y = item_scaffold_rt, fill = pretty_condition)) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.2, \n    adjust = .5, \n    width = .6, \n    .width = 0, \n    point_colour = NA) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_scaffold_rt),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=pretty_condition, y = item_scaffold_rt, color = pretty_condition),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  ))+ labs( title = \"Distribution of SCAFFOLD Phase Response Time (minutes)\",\n        subtitle =\"\",\n        y = \"Total Study Time (minutes)\", x = \"Condition\") +\n  theme_ggdist() + theme(legend.position = \"blank\") #+\n\n\n\nCODE  # coord_cartesian(xlim = c(0.5, NA), clip = \"off\")"
  },
  {
    "objectID": "analysis/combined/combined.html#exploring-relationships",
    "href": "analysis/combined/combined.html#exploring-relationships",
    "title": "19  Description",
    "section": "EXPLORING RELATIONSHIPS",
    "text": "EXPLORING RELATIONSHIPS\nACCURACY (VS) LATENCY\n\nCODE#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ rt_m, data = df_all, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by Total Item Response Time\",\n    subtitle = \"\", \n    x = \"Total Item Response Time (minutes)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\nCODE#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_avg_rt, data = df_all, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by Average Item Response Time\",\n    subtitle = \"\", \n    x = \"Average Item Response Time (seconds)\", y = \"Total Scaled Score\"\n  ) + theme(legend.position = \"blank\")\n\n\n\nCODE#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_scaffold_rt, data = df_all, alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by SCAFFOLD PHASE Item Response Time\",\n    subtitle = \"\", \n    x = \"SCAFFOLD PHASE Item Response Time (minutes)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\nCODE#SCATTERPLOT [SCORE X ITEM RESPONSE TIME]\ngf_jitter( s_SCALED ~ item_max_rt, data = df_all %>% filter(item_max_rt < 400), alpha = 0.5, color=~pretty_condition) %>% \n  gf_facet_wrap(~pretty_condition) + labs(\n    title = \"Total (Scaled) Score by MAX Item Response Time\",\n    subtitle = \"\", \n    x = \"MAX Item Response Time (s)\", y = \"Total Scaled Score \"\n  ) + theme(legend.position = \"blank\")\n\n\n\n\n\nCODE# \n# q.stats <- df_items %>% filter(q != 6) %>% dplyr::group_by(q, pretty_condition, score_niceABS) %>% summarise(\n#   m = mean(rt_s),\n#   sd = sd(rt_s),\n#   group = paste(pretty_condition,\"-\",score_niceABS)\n# )\n# \n# gf_line( m ~ as.factor(q), group = ~group,  color = ~score_niceABS,data = q.stats) %>% \n#   gf_point() %>% \n#   gf_facet_wrap(~pretty_condition) + scale_color_manual(values=c(\"red\",\"green\")) + \n#   labs(title = \"Average ITEM response time by condition\",\n#        subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n#        x = \"Question\", y = \"Averate Item Response Time\", color=\"Correct Response\")\n\n\n\nCODE# \n# q.stats <- df_items %>% filter(q != 6) %>% dplyr::group_by(q, pretty_condition, interpretation) %>% summarise(\n#   m = mean(rt_s),\n#   sd = sd(rt_s),\n#   group = paste(pretty_condition,\"-\",score_SCALED)\n# )\n# \n# gf_line( m ~ as.factor(q), group = ~group,  color = ~interpretation,data = q.stats) %>% \n#   gf_point() %>% \n#   gf_facet_wrap(~pretty_condition) + #+ scale_color_manual(values=c(\"red\",\"green\")) + \n#   labs(title = \"Average ITEM response time by condition\",\n#        subtitle = \"Correct responses are generally faster [computational efficiency] except on Q1 [learning]\",\n#        x = \"Question\", y = \"Averate Item Response Time\", color=\"Interpretation\")\n\n\n\nCODE# \n# x <- df_items %>% filter(q %nin% c(6,9)) %>% dplyr::group_by(subject) %>% mutate(\n#                             total = sum(score_niceABS),\n#                             perf = as.factor(total < 5),\n#                             perf = recode(perf, \"FALSE\"=\"high-scoring\", \"TRUE\"=\"low-scoring\")) %>% ungroup()\n# \n# gf_line( rt_s ~ as.factor(q), group = ~subject, color = ~interpretation, data = x) %>% \n#   gf_facet_grid(pretty_condition ~ perf)+ scale_x_discrete(labels = c(1 , 2,  3,  4,  5,  7,  8, 10, 11, 12, 13, 14, 15))+ labs(\n#     title = \"Response Time (s) by Question [facet by score <5]\"\n#   )\n\n\n\nCODEgf_boxplot( item_avg_rt ~ pretty_condition,  data = df_all) +\n  labs(title = \"Average item response time by mode and condition\")\n\n\n\nCODEgf_boxplot( item_scaffold_rt ~ pretty_condition,  data = df_all) +\n  labs(title = \"Average SCAFFOLD response time by mode and condition\")\n\n\n\nCODEgf_boxplot( item_test_rt ~ pretty_condition,  data = df_all) +\n  labs(title = \"Average TEST response time by mode and condition\")\n\n\n\nCODEgf_boxplot( totaltime_m ~ pretty_condition,  data = df_all) +\n  labs(title = \"Average TOTAL response time by mode and condition\")\n\n\n\nCODEgf_boxplot( item_q1_rt ~ pretty_condition,  data = df_all) %>% \n  gf_jitter(width=0.2, alpha = 0.5, size = 0.75, color = ~item_q1_NABS) +\n  labs(title = \"Average FIRST ITEM response time by mode and condition\")"
  },
  {
    "objectID": "analysis/combined/combined.html#models",
    "href": "analysis/combined/combined.html#models",
    "title": "19  Description",
    "section": "MODELS",
    "text": "MODELS\n\nCODElibrary(report)\n\nWarning: package 'report' was built under R version 4.1.2\n\nCODElibrary(see)\n\nWarning: package 'see' was built under R version 4.1.2\n\nCODElibrary(generics)\n\nWarning: package 'generics' was built under R version 4.1.2\n\n\n\nAttaching package: 'generics'\n\n\nThe following object is masked from 'package:dplyr':\n\n    explain\n\n\nThe following objects are masked from 'package:base':\n\n    as.difftime, as.factor, as.ordered, intersect, is.element, setdiff,\n    setequal, union\n\nCODElibrary(modelr)\n\n\nAttaching package: 'modelr'\n\n\nThe following objects are masked from 'package:performance':\n\n    mae, mse, rmse\n\n\nThe following object is masked from 'package:mosaic':\n\n    resample\n\n\nThe following object is masked from 'package:ggformula':\n\n    na.warn\n\nCODElibrary(distributional)\n\nWarning: package 'distributional' was built under R version 4.1.2\n\nCODE## A GIANT MODEL\nm <- lm(DV_percent_NABS ~ pretty_condition, data = df_all)\nsummary(m)\n\n\nCall:\nlm(formula = DV_percent_NABS ~ pretty_condition, data = df_all)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.816 -0.187 -0.130  0.107  0.899 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   0.1867     0.0195    9.58  < 2e-16 ***\npretty_conditionimpasse       0.1887     0.0271    6.95  5.2e-12 ***\npretty_condition211           0.4502     0.0507    8.88  < 2e-16 ***\npretty_condition221           0.5679     0.0546   10.39  < 2e-16 ***\npretty_condition311           0.6297     0.0463   13.60  < 2e-16 ***\npretty_condition321           0.6407     0.0552   11.60  < 2e-16 ***\npretty_conditionOrth-Full    -0.0180     0.0333   -0.54    0.589    \npretty_conditionOrth-Sparse   0.0152     0.0403    0.38    0.706    \npretty_conditionTri-Sparse    0.0580     0.0338    1.72    0.086 .  \npretty_conditionOrth-Grid    -0.0854     0.0387   -2.21    0.027 *  \npretty_conditionpoint        -0.0354     0.0398   -0.89    0.374    \npretty_conditionarrow         0.0747     0.0387    1.93    0.054 .  \npretty_conditioncross        -0.0069     0.0412   -0.17    0.867    \npretty_conditionpoint-click  -0.0563     0.0365   -1.54    0.123    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.331 on 1643 degrees of freedom\nMultiple R-squared:  0.256, Adjusted R-squared:  0.25 \nF-statistic: 43.5 on 13 and 1643 DF,  p-value: <2e-16\n\nCODEreport(m)\n\nRegistered S3 methods overwritten by 'effectsize':\n  method              from      \n  standardize.Surv    datawizard\n  standardize.bcplm   datawizard\n  standardize.clm2    datawizard\n  standardize.default datawizard\n  standardize.mediate datawizard\n  standardize.wbgee   datawizard\n  standardize.wbm     datawizard\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict DV_percent_NABS with pretty_condition (formula: DV_percent_NABS ~ pretty_condition). The model explains a statistically significant and moderate proportion of variance (R2 = 0.26, F(13, 1643) = 43.47, p < .001, adj. R2 = 0.25). The model's intercept, corresponding to pretty_condition = control, is at 0.19 (95% CI [0.15, 0.22], t(1643) = 9.58, p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.19, 95% CI [0.14, 0.24], t(1643) = 6.95, p < .001; Std. beta = 0.49, 95% CI [0.35, 0.63])\n  - The effect of pretty condition [211] is statistically significant and positive (beta = 0.45, 95% CI [0.35, 0.55], t(1643) = 8.88, p < .001; Std. beta = 1.18, 95% CI [0.92, 1.44])\n  - The effect of pretty condition [221] is statistically significant and positive (beta = 0.57, 95% CI [0.46, 0.68], t(1643) = 10.39, p < .001; Std. beta = 1.49, 95% CI [1.21, 1.77])\n  - The effect of pretty condition [311] is statistically significant and positive (beta = 0.63, 95% CI [0.54, 0.72], t(1643) = 13.60, p < .001; Std. beta = 1.65, 95% CI [1.41, 1.89])\n  - The effect of pretty condition [321] is statistically significant and positive (beta = 0.64, 95% CI [0.53, 0.75], t(1643) = 11.60, p < .001; Std. beta = 1.68, 95% CI [1.39, 1.96])\n  - The effect of pretty condition [Orth-Full] is statistically non-significant and negative (beta = -0.02, 95% CI [-0.08, 0.05], t(1643) = -0.54, p = 0.589; Std. beta = -0.05, 95% CI [-0.22, 0.12])\n  - The effect of pretty condition [Orth-Sparse] is statistically non-significant and positive (beta = 0.02, 95% CI [-0.06, 0.09], t(1643) = 0.38, p = 0.706; Std. beta = 0.04, 95% CI [-0.17, 0.25])\n  - The effect of pretty condition [Tri-Sparse] is statistically non-significant and positive (beta = 0.06, 95% CI [-8.26e-03, 0.12], t(1643) = 1.72, p = 0.086; Std. beta = 0.15, 95% CI [-0.02, 0.33])\n  - The effect of pretty condition [Orth-Grid] is statistically significant and negative (beta = -0.09, 95% CI [-0.16, -9.57e-03], t(1643) = -2.21, p = 0.027; Std. beta = -0.22, 95% CI [-0.42, -0.03])\n  - The effect of pretty condition [point] is statistically non-significant and negative (beta = -0.04, 95% CI [-0.11, 0.04], t(1643) = -0.89, p = 0.374; Std. beta = -0.09, 95% CI [-0.30, 0.11])\n  - The effect of pretty condition [arrow] is statistically non-significant and positive (beta = 0.07, 95% CI [-1.19e-03, 0.15], t(1643) = 1.93, p = 0.054; Std. beta = 0.20, 95% CI [-3.12e-03, 0.39])\n  - The effect of pretty condition [cross] is statistically non-significant and negative (beta = -6.90e-03, 95% CI [-0.09, 0.07], t(1643) = -0.17, p = 0.867; Std. beta = -0.02, 95% CI [-0.23, 0.19])\n  - The effect of pretty condition [point-click] is statistically non-significant and negative (beta = -0.06, 95% CI [-0.13, 0.02], t(1643) = -1.54, p = 0.123; Std. beta = -0.15, 95% CI [-0.33, 0.04])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODEcheck_model(m)\n\n\n\nCODE##UNCERTAINTY ESTIMATES OF THE MODEL \ndf_all %>%\n  data_grid(pretty_condition) %>%\n  augment(m, newdata = ., se_fit = TRUE) %>%\n  ggplot(aes(y = pretty_condition)) +\n  stat_halfeye(\n    aes(xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit)), \n    scale = .5\n  ) +\n  # we'll add the data back in too (scale = .5 above adjusts the halfeye height so\n  # that the data fit in as well)\n  geom_point(aes(x = DV_percent_NABS), data = df_all, pch = \"|\", size = 2, position = position_nudge(y = -.15)) +\n  labs(\n    title = \"Regression Model | Predicted Conditional Means\",\n    caption = \" % Correct ~ Condition; halfeye estimated using student's t centered at model predicted mean with SE\"\n  )\n\n\n\n\nOSPAN\n\nCODEtitle = \"OSPAN Participants by Condition\"\ncols = c(\"Condition\",\"n\")\ncont <- table(df_ospan$pretty_condition)\ncont %>%  addmargins() %>% kbl(caption = title, col.names = cols) %>% kable_classic()\n\n\n\nOSPAN Participants by Condition\n \n Condition \n    n \n  \n\n\n control \n    130 \n  \n\n impasse \n    136 \n  \n\n 211 \n    5 \n  \n\n 221 \n    11 \n  \n\n 311 \n    3 \n  \n\n 321 \n    7 \n  \n\n Orth-Full \n    0 \n  \n\n Orth-Sparse \n    0 \n  \n\n Tri-Sparse \n    0 \n  \n\n Orth-Grid \n    0 \n  \n\n point \n    0 \n  \n\n arrow \n    0 \n  \n\n cross \n    0 \n  \n\n point-click \n    0 \n  \n\n Sum \n    292 \n  \n\n\n\n\nTODO OSPAN description\n\nCODEtitle = \"Descriptive Statistics of OSPAN Task Accuracy\"\nospan.stats <- rbind(\n  \"MATH\" = df_ospan %>% dplyr::select(math_acc) %>% unlist() %>% favstats(),\n  \"ORDER\" = df_ospan %>%  dplyr::select(order_acc) %>% unlist() %>% favstats(),\n  \"WEIGHTED\" = df_ospan %>% dplyr::select(weighted) %>% unlist() %>% favstats()\n  \n) \nospan.stats %>% kbl (caption = title) %>% kable_classic() %>% \n  footnote(general = \"MATH = %correct of all math questions; \n           ORDER = % correct of OSPAN ordering (out of 30); WEIGHTED = math*ospan number correct\", general_title = \"Note: \",footnote_as_chunk = T) \n\n\n\nDescriptive Statistics of OSPAN Task Accuracy\n \n   \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n\n\n MATH \n    0.517 \n    0.897 \n    0.931 \n    0.966 \n    1 \n    0.924 \n    0.084 \n    292 \n    0 \n  \n\n ORDER \n    0.000 \n    0.533 \n    0.733 \n    0.875 \n    1 \n    0.682 \n    0.256 \n    292 \n    0 \n  \n\n WEIGHTED \n    0.000 \n    13.991 \n    20.741 \n    24.871 \n    30 \n    19.179 \n    7.465 \n    292 \n    0 \n  \n\n\nNote:   MATH = %correct of all math questions;            ORDER = % correct of OSPAN ordering (out of 30); WEIGHTED = math*ospan number correct\n\n\n\nFor (online) OSPAN weighted scores (n = 292) range from 0 to 30 with a mean score of (M = 19.18, SD = 7.46).\n\nCODE#GGFORMULA | DENSITY HISTOGRAM \n  gf_dhistogram(~weighted, data = df_ospan) + \n  labs(x = \"OSPAN (weighted) score\",\n       y = \"% of subjects\",\n       title = \"Distribution of OSPAN SCORE\",\n       subtitle = \"\") \n\n\n\nCODE##GGPUBR | HIST+DENSITY SCORE BY CONDITION/MODE\np <- gghistogram(df_ospan, x = \"weighted\", binwidth = 1,\n   add = \"mean\", rug = TRUE,\n   fill = \"pretty_condition\", #, palette = c(\"#00AFBB\", \"#E7B800\"),\n   add_density = TRUE)\nfacet(p, facet.by=c(\"pretty_condition\")) +\n  labs( title = \"Distribution of OSPAN Score\",\n        subtitle =\"The Distribution of OSPAN scores is similar across conditions\",\n        x = \"OSPAN (weighted) score\", y = \"number of subjects\") +\n  theme_minimal() + theme(legend.position = \"blank\")\n\n\n\nCODE#PLOT EMPIRICIAL CUMULATIVE DISTRIBUTION FUNCTION\nggplot(data = df_ospan, aes(weighted)) + \n  stat_ecdf() + \n  facet_wrap(~pretty_condition) + \n  labs( title = \"Empirical Cumulative Density Function — OSPAN\",\n        x = \"OSPAN (weighted) score\", \n        y = \"Cumulative Probability\") + theme_minimal()\n\n\n\n\n\nCODE#IXN PLOT\n\n# ospan.stats <- favstats(weighted, groups = condition)\n\n\nMODELS\n\nCODE#CONDITION ONLY MODEL\nm1 <- lm( item_test_SCALED ~ condition, data = df_ospan)\nsummary(m1)\n\n\nCall:\nlm(formula = item_test_SCALED ~ condition, data = df_ospan)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.86  -3.38  -2.05   3.62  12.62 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -4.623      0.501   -9.23  < 2e-16 ***\ncondition121    3.483      0.701    4.97  1.1e-06 ***\ncondition211   12.023      2.603    4.62  5.8e-06 ***\ncondition221   11.350      1.794    6.33  9.6e-10 ***\ncondition311    6.290      3.336    1.89     0.06 .  \ncondition321   10.480      2.216    4.73  3.6e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.71 on 286 degrees of freedom\nMultiple R-squared:  0.227, Adjusted R-squared:  0.214 \nF-statistic: 16.8 on 5 and 286 DF,  p-value: 1.39e-14\n\nCODE#OSPAN ONLY MODEL\nm2 <- lm( item_test_SCALED ~ weighted, data = df_ospan)\nsummary(m2)\n\n\nCall:\nlm(formula = item_test_SCALED ~ weighted, data = df_ospan)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.50  -5.59  -3.62   7.96  10.97 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -3.0234     1.0410    -2.9    0.004 **\nweighted      0.0507     0.0506     1.0    0.317   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.44 on 290 degrees of freedom\nMultiple R-squared:  0.00345,   Adjusted R-squared:  1.2e-05 \nF-statistic:    1 on 1 and 290 DF,  p-value: 0.317\n\nCODE#STANDARDIZE OSPAN\ndf_ospan$z_ospan = zscore(df_ospan$weighted)\nhistogram(df_ospan$z_ospan)\n\n\n\nCODE#OSPAN MEDIAN SPLIT\nmed_ospan = median(df_ospan$weighted)\ndf_ospan$low_ospan <- as.factor(df_ospan$weighted < med_ospan)\n\n#STANDARDIZED OSPAN ONLY MODEL\nm3 <- lm( item_test_SCALED ~ low_ospan, data = df_ospan)\nsummary(m3)\n\n\nCall:\nlm(formula = item_test_SCALED ~ low_ospan, data = df_ospan)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.71  -5.19  -3.19   7.79  10.81 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)  \n(Intercept)      -1.29       0.53   -2.44    0.015 *\nlow_ospanTRUE    -1.51       0.75   -2.02    0.045 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.41 on 290 degrees of freedom\nMultiple R-squared:  0.0138,    Adjusted R-squared:  0.0104 \nF-statistic: 4.07 on 1 and 290 DF,  p-value: 0.0445\n\nCODE#MULTIPLE WITH STANDARDIZED OSPAN\nmr <- lm(item_test_SCALED ~ condition + low_ospan, data = df_ospan)\nsummary(mr)\n\n\nCall:\nlm(formula = item_test_SCALED ~ condition + low_ospan, data = df_ospan)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -7.51  -3.98  -2.13   4.22  13.24 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -4.020      0.598   -6.72  9.9e-11 ***\ncondition121     3.529      0.698    5.05  7.8e-07 ***\ncondition211    11.665      2.600    4.49  1.1e-05 ***\ncondition221    11.304      1.787    6.33  9.6e-10 ***\ncondition311     6.503      3.324    1.96    0.051 .  \ncondition321    10.227      2.212    4.62  5.7e-06 ***\nlow_ospanTRUE   -1.224      0.670   -1.83    0.069 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.69 on 285 degrees of freedom\nMultiple R-squared:  0.236, Adjusted R-squared:  0.22 \nF-statistic: 14.7 on 6 and 285 DF,  p-value: 1.25e-14\n\nCODEanova(mr)\n\nAnalysis of Variance Table\n\nResponse: item_test_SCALED\n           Df Sum Sq Mean Sq F value  Pr(>F)    \ncondition   5   2746     549   16.97 1.1e-14 ***\nlow_ospan   1    108     108    3.33   0.069 .  \nResiduals 285   9224      32                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODE#SINGLE WITH STANDARDIZED OSPAN\nmr2 <- lm(item_test_SCALED ~ condition + low_ospan, data = df_ospan)\nsummary(mr2)\n\n\nCall:\nlm(formula = item_test_SCALED ~ condition + low_ospan, data = df_ospan)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -7.51  -3.98  -2.13   4.22  13.24 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -4.020      0.598   -6.72  9.9e-11 ***\ncondition121     3.529      0.698    5.05  7.8e-07 ***\ncondition211    11.665      2.600    4.49  1.1e-05 ***\ncondition221    11.304      1.787    6.33  9.6e-10 ***\ncondition311     6.503      3.324    1.96    0.051 .  \ncondition321    10.227      2.212    4.62  5.7e-06 ***\nlow_ospanTRUE   -1.224      0.670   -1.83    0.069 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.69 on 285 degrees of freedom\nMultiple R-squared:  0.236, Adjusted R-squared:  0.22 \nF-statistic: 14.7 on 6 and 285 DF,  p-value: 1.25e-14\n\nCODEgf_histogram(~item_test_SCALED, data = df_ospan) %>% \n  gf_facet_grid(low_ospan ~ condition, labeller = label_both)\n\n\n\nCODEgf_jitter(item_test_SCALED ~ weighted, color = ~pretty_condition, data = df_ospan) %>% \n  gf_facet_wrap(~pretty_condition)\n\n\n\nCODEgf_dhistogram(~item_test_NABS , color = ~pretty_condition, data = df_ospan) %>% \n  gf_facet_wrap(low_ospan ~ pretty_condition, labeller = label_both)\n\n\n\n\nOPSAN score does NOT predict performance in the control condition. \n\nCODEcontrol <- df_ospan %>% filter(condition == \"111\")\n\ngf_dhistogram(~item_test_NABS , color = ~pretty_condition, data = control) %>% \n  gf_facet_wrap(low_ospan ~ pretty_condition, labeller = label_both)\n\n\n\nCODE#test performance predicted by OSPAN score\nm <- lm(item_test_NABS ~ weighted, data = control)\nsummary(m)\n\n\nCall:\nlm(formula = item_test_NABS ~ weighted, data = control)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -1.17  -1.17  -1.17  -1.16   6.83 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 1.160579   0.653251    1.78    0.078 .\nweighted    0.000443   0.031503    0.01    0.989  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.52 on 128 degrees of freedom\nMultiple R-squared:  1.55e-06,  Adjusted R-squared:  -0.00781 \nF-statistic: 0.000198 on 1 and 128 DF,  p-value: 0.989\n\n\nOPSAN score does NOT predict performance in the control condition. \n\nCODEimpasse <- df_ospan %>% filter(condition == \"121\")\n\ngf_dhistogram(~item_test_NABS , color = ~pretty_condition, data = impasse) %>% \n  gf_facet_wrap(low_ospan ~ pretty_condition, labeller = label_both)\n\n\n\nCODE#test performance predicted by OSPAN score\nm <- lm(item_test_NABS ~ low_ospan, data = impasse)\nsummary(m)\n\n\nCall:\nlm(formula = item_test_NABS ~ low_ospan, data = impasse)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3.75  -1.72  -1.72   4.25   6.28 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      3.750      0.410    9.14  8.6e-16 ***\nlow_ospanTRUE   -2.028      0.564   -3.60  0.00045 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.28 on 134 degrees of freedom\nMultiple R-squared:  0.0881,    Adjusted R-squared:  0.0813 \nF-statistic: 12.9 on 1 and 134 DF,  p-value: 0.000451\n\n\n\nCODE#330 total \n# —————————\n#126 lab \n#204 online  \n  #  71 no ospan \n  # 133 ospan \n# sgc3a_lab <- df_all %>% filter(mode==\"lab-synch\") %>% filter(study==\"SGC3A\")\n# sgc3a_online <- df_all %>% filter(mode==\"asynch\") %>% filter(study==\"SGC3A\")\n# sgc3a_online_noospan <- sgc3a_online %>% filter(subject %nin% ospan$subject)\n# sgc3a_onspan <- df_subjects\n# \n# m <- lm( item_test_NABS ~ condition, data = sgc3a_online_noospan)\n# summary(m)\n# \n# m2 <- lm(item_test_NABS ~ condition, data = df_subjects)\n# summary(m2)\n# \n# \n# m3 <- lm(item_test_NABS ~ low_ospan, data = df_subjects)\n# summary(m3)\n# \n\n\n\nCODE# library(lpme)\n# m <- modereg(Y = df_ospan$DV_percent_NABS, W= df_ospan$condition, \n#         bw = 2, nstart = 2, PLOT = TRUE)\n# \n# summary(m)"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#wip-exploring",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#wip-exploring",
    "title": "4  Hypothesis Testing",
    "section": "WIP EXPLORING",
    "text": "WIP EXPLORING\nTest Phase Accuracy\nTest Phase Absolute Score (# questions)\nLinear Regression\nLM on Test Phase absolute score as number of questions, rather than % correct.\n\nCODE#SCORE predicted by CONDITION\nlm.1 <- lm(item_test_NABS ~ pretty_condition, data = df_subjects)\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(lm.1)\n\n\nCall:\nlm(formula = item_test_NABS ~ pretty_condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3.02  -2.77  -1.52   2.98   6.48 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                1.519      0.251    6.04  4.1e-09 ***\npretty_conditionimpasse    1.498      0.348    4.30  2.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.16 on 328 degrees of freedom\nMultiple R-squared:  0.0535,    Adjusted R-squared:  0.0506 \nF-statistic: 18.5 on 1 and 328 DF,  p-value: 0.0000222\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(lm.1)\n\nAnalysis of Variance Table\n\nResponse: item_test_NABS\n                  Df Sum Sq Mean Sq F value   Pr(>F)    \npretty_condition   1    185     185    18.5 0.000022 ***\nResiduals        328   3274      10                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(lm.1)\n\n                        2.5 % 97.5 %\n(Intercept)             1.025   2.01\npretty_conditionimpasse 0.814   2.18\n\nCODEreport(lm.1) #sanity check\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.05, F(1, 328) = 18.52, p < .001, adj. R2 = 0.05). The model's intercept, corresponding to pretty_condition = control, is at 1.52 (95% CI [1.02, 2.01], t(328) = 6.04, p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 1.50, 95% CI [0.81, 2.18], t(328) = 4.30, p < .001; Std. beta = 0.46, 95% CI [0.25, 0.67])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODEcheck_model(lm.1)\n\n\n\n\n\nCODE#MODEL ESTIMATES WITH UNCERTAINTY\n\n# #setup references \n# m <- lm.1\n# df <- df_subjects\n# call <- m$call %>% as.character()\n# \n# # uncertainty model visualization\n# df  %>%\n#   data_grid(pretty_condition) %>%\n#   augment(lm.1, newdata = ., se_fit = TRUE) %>% \n#   ggplot(aes(y = pretty_condition, color = pretty_condition)) \n# \n# +\n#   stat_halfeye( scale = .5,\n#       aes(\n#         xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n#         fill = stat(cut_cdf_qi(cdf, \n#                 .width = c(.90, .95),\n#                 labels = scales::percent_format())))) +\n#   scale_fill_brewer(direction = -1) + \n#   labs (title = \"(LAB) Test Phase Accuracy ~ Condition\", \n#         x = \"model predicted mean (% correct)\", y = \"Condition\", fill = \"Interval\",\n#         subtitle = paste(\"lm(\",call[2],\")\")\n#   ) + theme(legend.position = \"blank\")\n\n\nPoisson Regression TODO\nhttps://stats.oarc.ucla.edu/r/dae/poisson-regression/\nThe outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can consider it a type of count, (ie. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function).\n\nCODE#POISSON\n\n#SCORE predicted by CONDITION --> POISSON DISTRIBUTION\np.1 <- glm(item_test_NABS ~ pretty_condition, data = df_subjects, family = \"poisson\")\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(p.1)\n\n\nCall:\nglm(formula = item_test_NABS ~ pretty_condition, family = \"poisson\", \n    data = df_subjects)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -2.46   -2.28   -1.74    1.51    3.69  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               0.4180     0.0645    6.48  9.4e-11 ***\npretty_conditionimpasse   0.6864     0.0781    8.79  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1579.3  on 329  degrees of freedom\nResidual deviance: 1496.7  on 328  degrees of freedom\nAIC: 1956\n\nNumber of Fisher Scoring iterations: 6\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(p.1)\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: item_test_NABS\n\nTerms added sequentially (first to last)\n\n                 Df Deviance Resid. Df Resid. Dev\nNULL                               329       1579\npretty_condition  1     82.7       328       1497\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(p.1)\n\nWaiting for profiling to be done...\n\n\n                        2.5 % 97.5 %\n(Intercept)             0.289  0.542\npretty_conditionimpasse 0.535  0.841\n\nCODEreport(p.1) #sanity check\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a poisson model (estimated using ML) to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model's explanatory power is moderate (Nagelkerke's R2 = 0.22). The model's intercept, corresponding to pretty_condition = control, is at 0.42 (95% CI [0.29, 0.54], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.69, 95% CI [0.53, 0.84], p < .001; Std. beta = 0.69, 95% CI [0.53, 0.84])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\nCODEcheck_model(p.1)\n\n\n\n\nZero Inflated Poisson\nhttps://stats.oarc.ucla.edu/r/dae/zip/\nPoisson count process with excess zeros\n\nCODE#ZERO INFLATED POISSON\n\nzinfp.1 <- zeroinfl(item_test_NABS ~  item_q1_rt| pretty_condition , data = df_subjects)\nsummary(zinfp.1)\n\n\nCall:\nzeroinfl(formula = item_test_NABS ~ item_q1_rt | pretty_condition, data = df_subjects)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.934 -0.821 -0.548  0.965  2.421 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 1.654243   0.059975   27.58   <2e-16 ***\nitem_q1_rt  0.001690   0.000849    1.99    0.047 *  \n\nZero-inflation model coefficients (binomial with logit link):\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                0.978      0.179    5.46  4.7e-08 ***\npretty_conditionimpasse   -1.055      0.236   -4.48  7.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -531 on 4 Df\n\nCODEreport(zinfp.1)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a zero-inflated poisson model to predict item_test_NABS with item_q1_rt and pretty_condition (formula: item_test_NABS ~ item_q1_rt). The model's explanatory power is substantial (R2 = 0.35, adj. R2 = 0.35). The model's intercept, corresponding to item_q1_rt = 0, is at 1.65 (95% CI [1.54, 1.77], p < .001). Within this model:\n\n  - The effect of item q1 rt is statistically significant and positive (beta = 1.69e-03, 95% CI [2.52e-05, 3.35e-03], p = 0.047; Std. beta = 0.06, 95% CI [7.11e-04, 0.12])\n  - The effect of pretty condition [impasse] is statistically significant and negative (beta = -1.06, 95% CI [-1.52, -0.59], p < .001; Std. beta = -1.06, 95% CI [-1.52, -0.59])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\nCODEperformance(zinfp.1)\n\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------\n1070.173 | 1085.370 | 0.354 |     0.350 | 3.131 | 3.150 |    -1.609 |           0.044\n\nCODE# check_model(zinfp.1)\n\n\nNegative Binomial Regression\nhttps://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/ - overdispersed count data (variance much greater than mean)\n\nCODE#NEGATIVE BIONOMIAL REGRESSION\n# - https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/\n# - Overdispersed Count variables\n\nlibrary(MASS)\n\nWarning: package 'MASS' was built under R version 4.1.2\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    ltsreg\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nCODEnb.1 <- glm.nb(item_test_NABS ~ pretty_condition, data = df_subjects)\nsummary(nb.1)\n\n\nCall:\nglm.nb(formula = item_test_NABS ~ pretty_condition, data = df_subjects, \n    init.theta = 0.253501538, link = log)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.139  -1.102  -0.993   0.378   1.091  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)   \n(Intercept)                0.418      0.171    2.45   0.0143 * \npretty_conditionimpasse    0.686      0.232    2.95   0.0031 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.254) family taken to be 1)\n\n    Null deviance: 279.52  on 329  degrees of freedom\nResidual deviance: 270.97  on 328  degrees of freedom\nAIC: 1194\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2535 \n          Std. Err.:  0.0315 \n\n 2 x log-likelihood:  -1188.1290 \n\nCODEreport(nb.1)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a negative-binomial model (estimated using ML) to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model's explanatory power is weak (Nagelkerke's R2 = 0.04). The model's intercept, corresponding to pretty_condition = control, is at 0.42 (95% CI [0.10, 0.77], p = 0.014). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.69, 95% CI [0.23, 1.14], p = 0.003; Std. beta = 0.69, 95% CI [0.23, 1.14])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\nCODEcheck_model(nb.1)\n\n\n\nCODE#check model assumption\n#assumes conditional means are not equal to conditional variances\n#conduct likelihood ration test to compare and test [need poisson]\nm3 <- glm(item_test_NABS ~ pretty_condition, family = \"poisson\", data = df_subjects)\npchisq(2 * (logLik(nb.1) - logLik(m3)), df = 1, lower.tail = FALSE)\n\n'log Lik.' 4.3e-168 (df=3)\n\nCODE#A large (+) log likelihood suggests that the negative binomial is more appropriate than the Poisson model\n\n\n#EXPONENTIATE PARAMETER ESTIMATES\nest <- cbind(Estimate = coef(nb.1), confint(nb.1))\n\nWaiting for profiling to be done...\n\nCODE#exponentiate parameter estimates\nprint(\"Exponentiated Estimates\")\n\n[1] \"Exponentiated Estimates\"\n\nCODEexp(est)\n\n                        Estimate 2.5 % 97.5 %\n(Intercept)                 1.52  1.10   2.15\npretty_conditionimpasse     1.99  1.26   3.13\n\n\nThe variable condition has a coefficient of 0.67, (p < 0.005). This means that for the impasse condition, the expected log count # of questions increases by 0.67. By exponentiating the estimate we see that # question correct rate for the impasse condition is nearly 2x that of the control condition.\nDiagnostics ??\nZero Inflated Negative Binomial Regression\nhttps://stats.oarc.ucla.edu/r/dae/zinb/ count data that are overdispersed and have excess zeros\nZero-inflated negative binomial regression is for modelling count variables with excessive zeros, and especially when the count data are overdispersed (mean is much larger than variance). It can help account for situations where theory suggests that excess zeros are generated by 2 separate processes, one that includes the other count values, and the other that is just the zeros, and thus that the excess zeros can be modelled independently.\nTotal Absolute Score (# items correct) may fit this situation, as the data are overdispersed (variance much greater than the mean) and there are are very large number of zeros. It is theoretically plausible that these excess zeros (no answers correct) are the result of a different ‘process’ … (i.e) little understanding and/or resistance to restructuring understanding of the coordinate system. However, I am not certain if it is plausible to suggest that the zeros themselves are the result of two different processes: (ie. perhaps trying to understand, and not trying to understand?) <- this could maybe be disentangled by first question latency?\nThe model includes: - A logistic model to model which of the two processes the zero outcome is associated with - A negative binomial model to model the count process\n\nCODElibrary(pscl) #  for zeroinfl negbinomial\n\n#ZERO INFLATED NEGATIVE BINOMIAL\nzinb.1 <- zeroinfl(item_test_NABS ~ pretty_condition | pretty_condition , data = df_subjects, dist = \"negbin\")\n#before the | is the count part, after the | is the logit model\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(zinb.1)\n\n\nCall:\nzeroinfl(formula = item_test_NABS ~ pretty_condition | pretty_condition, \n    data = df_subjects, dist = \"negbin\")\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.866 -0.794 -0.538  0.856  2.294 \n\nCount model coefficients (negbin with log link):\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               1.7126     0.0728   23.54  < 2e-16 ***\npretty_conditionimpasse   0.0451     0.0880    0.51  0.60810    \nLog(theta)                3.1851     0.8732    3.65  0.00026 ***\n\nZero-inflation model coefficients (binomial with logit link):\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                0.974      0.179    5.43  5.5e-08 ***\npretty_conditionimpasse   -1.056      0.236   -4.47  7.7e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 24.169 \nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -532 on 5 Df\n\nCODEreport(zinb.1)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a zero-inflated negative-binomial model to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model's explanatory power is substantial (R2 = 0.36, adj. R2 = 0.36). The model's intercept, corresponding to pretty_condition = control, is at 1.71 (95% CI [1.57, 1.86], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically non-significant and positive (beta = 0.05, 95% CI [-0.13, 0.22], p = 0.608; Std. beta = 0.05, 95% CI [-0.13, 0.22])\n  - The effect of pretty condition [impasse] is statistically significant and negative (beta = -1.06, 95% CI [-1.52, -0.59], p < .001; Std. beta = -1.06, 95% CI [-1.52, -0.59])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\nCODEperformance(zinb.1)\n\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------\n1073.880 | 1092.876 | 0.363 |     0.359 | 3.150 | 3.174 |    -1.649 |           0.043\n\nCODE#   rootogram(zinb.1)\n\n\n\n# #EXPONENTIATE PARAMETER ESTIMATES\n# est <- cbind(Estimate = coef(zinb.1), confint(zinb.1))\n# #exponentiate parameter estimates\n# print(\"Exponentiated Estimates\")\n# exp(est)\n\n\nIn the count model, the coefficient for the condition is very small, and not significant (suggesting it does not contribute to the count yielding process?).\nIn the zero-inflation model, the coefficient for the condition variable is -1.056 and statistically significant. This suggests that the log odds of being an excessive zero decrease by 1.06 if you are in the impasse condition (exponentiate it?)\nTODO come back to this and discuss further\nModel Comparison\n\nCODEcompare_performance(lm.1, p.1, nb.1, zinb.1)\n\n# Comparison of Model Performance Indices\n\nName   |    Model |      AIC | AIC weights |      BIC | BIC weights |  RMSE | Sigma | Score_log | Score_spherical |    R2 | R2 (adj.) | Nagelkerke's R2\n-------------------------------------------------------------------------------------------------------------------------------------------------------\nlm.1   |       lm | 1699.782 |     < 0.001 | 1711.179 |     < 0.001 | 3.150 | 3.160 |           |                 | 0.053 |     0.051 |                \np.1    |      glm | 1955.788 |     < 0.001 | 1963.386 |     < 0.001 | 3.150 | 2.136 |    -2.957 |           0.042 |       |           |           0.223\nnb.1   |   negbin | 1194.129 |     < 0.001 | 1205.526 |     < 0.001 | 3.150 | 0.909 |    -2.137 |           0.046 |       |           |           0.045\nzinb.1 | zeroinfl | 1073.880 |        1.00 | 1092.876 |        1.00 | 3.150 | 3.174 |    -1.649 |           0.043 | 0.363 |     0.359 |                \n\n\nFor modelling test phase absolute score (# items correct) it seems that the zero inflated negative binomial model is the best fit according to R2 and AIC, however, I am not clear on the implications of the interpretation (non significant in count process, significant on logit process), and also not clear if # items correct is truly a count process.\n\nCODE#uncertainty model visualization\n# df %>%\n  # data_grid(pretty_condition) %>%\n  # augment(m, newdata = ., se_fit = TRUE) %>%\n  # ggplot(aes(y = pretty_condition)) +\n  # stat_halfeye(\n  #   aes(xdist = dist_student_t(df = df.residual(m), \n  #       mu = .fitted, sigma = .se.fit)), scale = .5) +\n  # # add raw data in too (scale = .5 above adjusts the halfeye height so\n  # # that the data fit in as well)\n  # geom_jitter(aes(x = x), data = df, pch = \"|\", size = 2, \n  #             position =   position_nudge(y = -.15), alpha = 0.5) +  \n  # labs (title = \"Model Estimates with Uncertainty\", x = \"model coefficient\") + \n  # theme_minimal()\n\n\nHurdle Beta Regression\nhttps://github.com/markhwhiteii/beta_hurdle/blob/master/manuscript/beta_hurdle.pdf\n\nCODElibrary(gamlss)\n\nWarning: package 'gamlss' was built under R version 4.1.2\n\n\nLoading required package: splines\n\n\nLoading required package: gamlss.data\n\n\n\nAttaching package: 'gamlss.data'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nLoading required package: gamlss.dist\n\n\nWarning: package 'gamlss.dist' was built under R version 4.1.2\n\n\nLoading required package: nlme\n\n\nWarning: package 'nlme' was built under R version 4.1.2\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\n\nLoading required package: parallel\n\n\n **********   GAMLSS Version 5.4-3  ********** \n\n\nFor more on GAMLSS look at https://www.gamlss.com/\n\n\nType gamlssNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'gamlss'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Rsq\n\n\nThe following object is masked from 'package:lme4':\n\n    refit\n\nCODE#CREATE SAMPLE DATA \nn <- 5000 \nmu <- 0.40 \nsigma <- 0.60 \np0 <- 0.13 \np1 <- 0.17 \np2 <- 1- p0- p1\na <- mu * (1- sigma ^ 2) / (sigma ^ 2) \nb <- a * (1- mu) / mu\n\n#CREATE DIST\nset.seed(1839) \ny <- rbeta(n, a, b) \ncat <- sample(1:3, n, prob = c(p0, p2, p1), replace = TRUE) \ny[cat == 1] <- 0 \ny[cat == 3] <- 1\n\n#VISUALIZE DISTRIBUTION\nx <- as.data.frame(y)\ngf_histogram(~x$y)\n\n\n\nCODE#this looks not unlike my distribution! \n\n#CREATE AN EMPTY MODEL\nfit <- gamlss( formula = y ~ 1, # formula for mu \n               formula.sigma = ~ 1, # formula for sigma \n               formula.nu = ~ 1, # formula for nu \n               formula.tau = ~ 1, # formula for tau \n               family = BEINF() )\n\nGAMLSS-RS iteration 1: Global Deviance = 7799 \nGAMLSS-RS iteration 2: Global Deviance = 7778 \nGAMLSS-RS iteration 3: Global Deviance = 7778 \nGAMLSS-RS iteration 4: Global Deviance = 7778 \n\nCODEsummary(fit)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = y ~ 1, family = BEINF(), formula.sigma = ~1,  \n    formula.nu = ~1, formula.tau = ~1) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.3796     0.0196   -19.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.3951     0.0162    24.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.632      0.042   -38.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.4014     0.0382   -36.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  5000 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  4996 \n                      at cycle:  4 \n \nGlobal Deviance:     7778 \n            AIC:     7786 \n            SBC:     7812 \n******************************************************************\n\nCODEplot(fit)\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  0.000571 \n                       variance   =  1 \n               coef. of skewness  =  0.0294 \n               coef. of kurtosis  =  2.95 \nFilliben correlation coefficient  =  1 \n******************************************************************\n\nCODE#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nfit_mu <- inv_logit(fit$mu.coefficients) \npaste(\"MU: \",fit_mu)\n\n[1] \"MU:  0.406229902102452\"\n\nCODEfit_sigma <- inv_logit(fit$sigma.coefficients) \npaste(\"SIGMA: \",fit_sigma)\n\n[1] \"SIGMA:  0.597499259410111\"\n\nCODEfit_nu <- exp(fit$nu.coefficients) \nfit_tau <- exp(fit$tau.coefficients) \nfit_p0 <- fit_nu / (1 + fit_nu + fit_tau) \npaste(\"P0: \",fit_p0)\n\n[1] \"P0:  0.135600165493784\"\n\nCODEfit_p1 <- fit_tau / (1 + fit_nu + fit_tau)\npaste(\"P1: \",fit_p1)\n\n[1] \"P1:  0.170800000002391\"\n\n\nBETA HURDLE INTERPRETATION - beta component\n- MU “location” (mean)\n- SIGMA “scale” (positively related to variance; variance = sigma.squared mean (1-mean)\n- Rigby, Stasinopoulos, Heller, and De Bastiani (2017) “reparameterized” the beta distribution so that the two parameters determining the shape of the distribution would be more useful in a regression framework (see Ferrari & Cribari-Neto, 2004 for a different parameterization)\nZERO-ONE HURDLE COMPONENT\n- The two additional parameters, ν NU and τTAU , are related to p0 and p1, respectively.\n- p0 is the probability that a case equals 0,\n- p1 is the probability that a case equals 1,\n- p2 (i.e., 1 −p0 −p1) is the probability that the case comes from the beta distribution\n\nCODE#SETUP DATA \n\nmin = 0 #min possible value of scale\nmax = 8 #max possible value of scale\n\nlibrary(mosaic) #for shuffling\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following objects are masked _by_ '.GlobalEnv':\n\n    cnorm, is.wholenumber, mplot\n\n\nThe following object is masked from 'package:plyr':\n\n    count\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following object is masked from 'package:lmerTest':\n\n    rand\n\n\nThe following object is masked from 'package:lme4':\n\n    factorize\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:modelr':\n\n    resample\n\n\nThe following object is masked from 'package:vcd':\n\n    mplot\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nThe following object is masked from 'package:cowplot':\n\n    theme_map\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nCODE#1. Rescale accuracy using \n# recommended adjustment \n#rescaled = value-min/(max-min)\ndf <- df_subjects %>% mutate(\n  accuracy = item_test_NABS,\n  R_acc = (accuracy-min)/(max-min), #as %\n  T_acc = (accuracy * (nrow(df)-1) + 0.5)/nrow(df)/8, #transform for no 0 and 1\n  perm = shuffle(condition),\n  scaffold_rt = item_scaffold_rt\n) %>% dplyr::select(accuracy,R_acc, T_acc, condition, perm,scaffold_rt)\n\n#VISUALIZE DISTRIBUTION\ngf_histogram(~R_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = \"Histogram of accuracy\")\n\n\n\nCODE#VISUALIZE DISTRIBUTION\ngf_histogram(~T_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = \"Histogram of [rescaled] accuracy\")\n\n\n\nCODEgf_histogram(~R_acc, fill = ~perm, data = df) %>% gf_facet_wrap(~perm) + labs(title = \"Histogram of shuffled accuracy\")\n\n\n\nCODE#SUMMARIZE SAMPLE\npaste(\"Grand mean\", mean(df$R_acc))\n\n[1] \"Grand mean 0.2875\"\n\nCODElibrary(mosaic)\nstats = favstats(df$R_acc ~ df$condition)\nstats$mean <- mean(df$R_acc ~ df$condition)\nstats$var <- var(df$R_acc ~ df$condition)\nprint(\"Grand stats\")\n\n[1] \"Grand stats\"\n\nCODEstats \n\n  df$condition min Q1 median    Q3 max  mean    sd   n missing   var\n1          111   0  0  0.000 0.125   1 0.190 0.358 158       0 0.128\n2          121   0  0  0.125 0.875   1 0.377 0.426 172       0 0.182\n\nCODEprint(\"P0\")\n\n[1] \"P0\"\n\nCODEnrow(df %>% filter(R_acc ==0))/nrow(df)\n\n[1] 0.6\n\nCODEprint(\"P1\")\n\n[1] \"P1\"\n\nCODEnrow(df %>% filter(R_acc ==1))/nrow(df)\n\n[1] 0.136\n\nCODE#CREATE MODEL\n\n#CREATE AN EMPTY MODEL\nm0 <- gamlss( formula = R_acc ~ 1, # formula for mu \n              formula.sigma =  ~ 1, # formula for sigma \n              formula.nu =  ~ 1, # formula for nu \n              formula.tau =  ~ 1, # formula for tau \n              family = BEINF(), data = df )\n\nGAMLSS-RS iteration 1: Global Deviance = 610 \nGAMLSS-RS iteration 2: Global Deviance = 609 \nGAMLSS-RS iteration 3: Global Deviance = 609 \nGAMLSS-RS iteration 4: Global Deviance = 609 \nGAMLSS-RS iteration 5: Global Deviance = 609 \n\nCODEm0 <- gamlss(R_acc ~ 1, ~ 1, ~ 1, ~ 1, \n            data = df, family = BEINF())\n\nGAMLSS-RS iteration 1: Global Deviance = 610 \nGAMLSS-RS iteration 2: Global Deviance = 609 \nGAMLSS-RS iteration 3: Global Deviance = 609 \nGAMLSS-RS iteration 4: Global Deviance = 609 \nGAMLSS-RS iteration 5: Global Deviance = 609 \n\nCODEsummary(m0)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ 1, sigma.formula = ~1, nu.formula = ~1,  \n    tau.formula = ~1, family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    0.225      0.113    1.98    0.048 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    0.177      0.100    1.76    0.079 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    0.822      0.129    6.39  5.7e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -0.660      0.184   -3.59  0.00038 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  326 \n                      at cycle:  5 \n \nGlobal Deviance:     609 \n            AIC:     617 \n            SBC:     632 \n******************************************************************\n\nCODEplot(m0)\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  0.0109 \n                       variance   =  0.996 \n               coef. of skewness  =  -0.0563 \n               coef. of kurtosis  =  2.79 \nFilliben correlation coefficient  =  0.998 \n******************************************************************\n\nCODE#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nm0_mu <- inv_logit(m0$mu.coefficients) \npaste(\"MU: \",m0_mu)\n\n[1] \"MU:  0.555931874775555\"\n\nCODEm0_sigma <- inv_logit(m0$sigma.coefficients) \npaste(\"SIGMA: \",m0_sigma)\n\n[1] \"SIGMA:  0.544134514840075\"\n\nCODEm0_nu <- exp(m0$nu.coefficients) \npaste(\"NU: \",m0_nu)\n\n[1] \"NU:  2.27484150905293\"\n\nCODEm0_tau <- exp(m0$tau.coefficients) \npaste(\"TAU: \",m0_tau)\n\n[1] \"TAU:  0.517080427912532\"\n\nCODEm0_p0 <- fit_nu / (1 + fit_nu + fit_tau) \npaste(\"P0: \",m0_p0)\n\n[1] \"P0:  0.135600165493784\"\n\nCODEm0_p1 <- fit_tau / (1 + fit_nu + fit_tau)\npaste(\"P1: \",m0_p1)\n\n[1] \"P1:  0.170800000002391\"\n\nCODE#CREATE PREDICTOR MODEL\nm1 <- gamlss(R_acc ~ condition, ~ condition, ~ condition, ~ condition, \n            data = df, family = BEINF())\n\nGAMLSS-RS iteration 1: Global Deviance = 588 \nGAMLSS-RS iteration 2: Global Deviance = 587 \nGAMLSS-RS iteration 3: Global Deviance = 587 \nGAMLSS-RS iteration 4: Global Deviance = 587 \nGAMLSS-RS iteration 5: Global Deviance = 587 \n\nCODEsummary(m1)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.144      0.203    0.71     0.48\ncondition121    0.124      0.244    0.51     0.61\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.280      0.174    1.61     0.11\ncondition121   -0.164      0.213   -0.77     0.44\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.377      0.208    6.63  1.4e-10 ***\ncondition121   -1.019      0.269   -3.79  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    -0.729      0.325   -2.24    0.026 *\ncondition121    0.102      0.394    0.26    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     587 \n            AIC:     603 \n            SBC:     633 \n******************************************************************\n\nCODE#LOOKING PREDICTOR MODEL\nm <- gamlss(R_acc ~ condition , \n            ~ condition , \n            ~ condition , \n            ~ condition , \n            data = df, family = BEINF())\n\nGAMLSS-RS iteration 1: Global Deviance = 588 \nGAMLSS-RS iteration 2: Global Deviance = 587 \nGAMLSS-RS iteration 3: Global Deviance = 587 \nGAMLSS-RS iteration 4: Global Deviance = 587 \nGAMLSS-RS iteration 5: Global Deviance = 587 \n\nCODEsummary(m)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.144      0.203    0.71     0.48\ncondition121    0.124      0.244    0.51     0.61\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.280      0.174    1.61     0.11\ncondition121   -0.164      0.213   -0.77     0.44\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.377      0.208    6.63  1.4e-10 ***\ncondition121   -1.019      0.269   -3.79  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    -0.729      0.325   -2.24    0.026 *\ncondition121    0.102      0.394    0.26    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     587 \n            AIC:     603 \n            SBC:     633 \n******************************************************************\n\nCODE#CREATE PREDICTOR MODEL ON SHUFFLED [PERMUTATION TEST]\nmperm <- gamlss(R_acc ~ perm, ~ perm, ~ perm, ~ perm, \n            data = df, family = BEINF())\n\nGAMLSS-RS iteration 1: Global Deviance = 609 \nGAMLSS-RS iteration 2: Global Deviance = 608 \nGAMLSS-RS iteration 3: Global Deviance = 608 \nGAMLSS-RS iteration 4: Global Deviance = 608 \nGAMLSS-RS iteration 5: Global Deviance = 608 \n\nCODEsummary(mperm)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ perm, sigma.formula = ~perm,  \n    nu.formula = ~perm, tau.formula = ~perm, family = BEINF(),      data = df) \n\n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.1914     0.1706    1.12     0.26\nperm121       0.0635     0.2278    0.28     0.78\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    0.237      0.148    1.60     0.11\nperm121       -0.117      0.202   -0.58     0.56\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.8535     0.1887    4.52  8.6e-06 ***\nperm121      -0.0595     0.2579   -0.23     0.82    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   -0.511      0.258   -1.98    0.048 *\nperm121       -0.294      0.368   -0.80    0.425  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     608 \n            AIC:     624 \n            SBC:     654 \n******************************************************************\n\nCODE#sanity check with scaled outcome, no zeros ones\nm3 <- gamlss(T_acc ~ condition, ~ condition, ~ condition, ~ condition, \n            data = df, family = BEINF())\n\nGAMLSS-RS iteration 1: Global Deviance = -1812 \nGAMLSS-RS iteration 2: Global Deviance = -2024 \nGAMLSS-RS iteration 3: Global Deviance = -2038 \nGAMLSS-RS iteration 4: Global Deviance = -2040 \nGAMLSS-RS iteration 5: Global Deviance = -2040 \nGAMLSS-RS iteration 6: Global Deviance = -2040 \nGAMLSS-RS iteration 7: Global Deviance = -2040 \nGAMLSS-RS iteration 8: Global Deviance = -2040 \nGAMLSS-RS iteration 9: Global Deviance = -2040 \n\nCODEsummary(m3)\n\nWarning in summary.gamlss(m3): summary: vcov has failed, option qr is used instead\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = T_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition, family = BEINF(),  \n    data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.1492     0.0981  -11.71  < 2e-16 ***\ncondition121   0.5677     0.1411    4.02 0.000071 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    1.4399     0.0745   19.33   <2e-16 ***\ncondition121   0.1694     0.1038    1.63      0.1    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -2.25e+01   3.78e+03   -0.01        1\ncondition121 -6.72e-15   5.24e+03    0.00        1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -2.26e+01   3.96e+03   -0.01        1\ncondition121  9.20e-15   5.48e+03    0.00        1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  9 \n \nGlobal Deviance:     -2040 \n            AIC:     -2024 \n            SBC:     -1994 \n******************************************************************\n\nCODE#m3 shouldn't show condition as significant for nu and tau, because T_acc was scaled to not include any 0s and 1s\n\n#investigate beta negative binomial distribution\n#https://en.wikipedia.org/wiki/Beta_negative_binomial_distribution\n\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nm1_mu <- inv_logit(m1$mu.coefficients) \npaste(\"MU: \",m1_mu)\n\n[1] \"MU:  0.536038311159578\" \"MU:  0.531024352873784\"\n\nCODEm1_sigma <- inv_logit(m0$sigma.coefficients) \npaste(\"SIGMA: \",m1_sigma)\n\n[1] \"SIGMA:  0.544134514840075\"\n\nCODEm1_nu <- exp(m1$nu.coefficients) \npaste(\"NU: \",m1_nu)\n\n[1] \"NU:  3.96329406964311\"  \"NU:  0.360974858677686\"\n\nCODEm1_tau <- exp(m1$tau.coefficients) \npaste(\"TAU: \",m1_tau)\n\n[1] \"TAU:  0.482542801248665\" \"TAU:  1.10746276561553\" \n\nCODEsummary(m)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.144      0.203    0.71     0.48\ncondition121    0.124      0.244    0.51     0.61\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.280      0.174    1.61     0.11\ncondition121   -0.164      0.213   -0.77     0.44\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.377      0.208    6.63  1.4e-10 ***\ncondition121   -1.019      0.269   -3.79  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    -0.729      0.325   -2.24    0.026 *\ncondition121    0.102      0.394    0.26    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     587 \n            AIC:     603 \n            SBC:     633 \n******************************************************************\n\nCODEplot(m)\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  -0.00363 \n                       variance   =  1.04 \n               coef. of skewness  =  -0.0633 \n               coef. of kurtosis  =  3.03 \nFilliben correlation coefficient  =  0.999 \n******************************************************************\n\n\n\nMU tells if mean is different by condition\n\nSIGMA tells if variance is different by condition\n\nNU coefficient tells if condition yields different probability at floor\nTAU coefficient tells if condition yields different probability at ceiling\nBeta Regression (% Correct)\nBeta regression on % correct (with standard transformation for including [0,1]) https://stats.stackexchange.com/questions/63350/how-to-interpret-the-coefficients-from-a-beta-regression\n\nCODE# \nlibrary(betareg)\n\n#RESCLAE VARIABLE\n#beta reg can't handle 0s and 1s \nsub <- df_subjects %>% dplyr::select(condition, DV_percent_NABS)\nn = nrow(sub) %>% unlist()\nsub$dv_transformed = (sub$DV_percent_NABS * (n-1) + 0.5)/n\n \n#VISUALIZE VARIABLES\nhistogram(sub$dv_transformed)\n\n\n\nCODEgf_histogram(~dv_transformed, fill = ~condition, data = sub) %>% gf_facet_wrap(~condition)\n\n\n\nCODE#FIT MODEL\nmb <- betareg(dv_transformed ~ condition, data = sub)\nsummary(mb)\n\n\nCall:\nbetareg(formula = dv_transformed ~ condition, data = sub)\n\nStandardized weighted residuals 2:\n   Min     1Q Median     3Q    Max \n-1.057 -0.453 -0.216  0.541  1.690 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.969      0.108   -8.97   <2e-16 ***\ncondition121    0.556      0.143    3.89   0.0001 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(>|z|)    \n(phi)   0.6604     0.0425    15.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  506 on 3 Df\nPseudo R-squared: 0.0725\nNumber of iterations: 12 (BFGS) + 1 (Fisher scoring) \n\nCODEplot(mb)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTOBIT Regression\nhttps://stats.oarc.ucla.edu/r/dae/tobit-models/\n\nCODE#set up data \ndf <- df_subjects %>% mutate(\n  accuracy = s_NABS\n)\n\nlibrary(VGAM)\n\nWarning: package 'VGAM' was built under R version 4.1.2\n\n\nLoading required package: stats4\n\n\n\nAttaching package: 'VGAM'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    prplot\n\n\nThe following objects are masked from 'package:mosaic':\n\n    chisq, logit\n\n\nThe following object is masked from 'package:distributional':\n\n    cdf\n\nCODEt <- vglm(accuracy ~ condition, tobit(Upper = 13), data = df)\nsummary(t)\n\n\nCall:\nvglm(formula = accuracy ~ condition, family = tobit(Upper = 13), \n    data = df)\n\nCoefficients: \n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept):1  -2.3913     0.8854   -2.70   0.0069 ** \n(Intercept):2   2.2155     0.0653   33.93   <2e-16 ***\ncondition121    5.9518     1.1742    5.07    4e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: mu, loglink(sd)\n\nLog-likelihood: -677 on 657 degrees of freedom\n\nNumber of Fisher scoring iterations: 7 \n\nNo Hauck-Donner effect found in any of the estimates\n\nCODEplot(t)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWIP | HURDLE MODEL\n\nhttps://data.library.virginia.edu/getting-started-with-hurdle-models/\n\nhttps://en.wikipedia.org/wiki/Hurdle_model#:~:text=A%20hurdle%20model%20is%20a,of%20the%20non%2Dzero%20values.\n\nclass of models for count data with both overdispersion and excess zeros;\ndifferent from zero-inflated models where the excess zeros are theorized to arise from two different processes; in the hurdle model, there is a model for P(x=0) and a separate model for P(x!=0)\nThe model includes: - A binary logit model to model whether the observation takes a positive count or not. - a truncated Poisson or Negative binomial model that only fits positive counts\nThis allows us to model: (1) Does the student get any questions right? (2) How many questions does the student get right?\n\nCODElibrary(pscl) #zero-inf and hurdle models \nlibrary(countreg) #rootogram\n\nRegistered S3 methods overwritten by 'countreg':\n  method                 from\n  print.zeroinfl         pscl\n  print.summary.zeroinfl pscl\n  summary.zeroinfl       pscl\n  coef.zeroinfl          pscl\n  vcov.zeroinfl          pscl\n  logLik.zeroinfl        pscl\n  predict.zeroinfl       pscl\n  residuals.zeroinfl     pscl\n  fitted.zeroinfl        pscl\n  terms.zeroinfl         pscl\n  model.matrix.zeroinfl  pscl\n  extractAIC.zeroinfl    pscl\n  print.hurdle           pscl\n  print.summary.hurdle   pscl\n  summary.hurdle         pscl\n  coef.hurdle            pscl\n  vcov.hurdle            pscl\n  logLik.hurdle          pscl\n  predict.hurdle         pscl\n  residuals.hurdle       pscl\n  fitted.hurdle          pscl\n  terms.hurdle           pscl\n  model.matrix.hurdle    pscl\n  extractAIC.hurdle      pscl\n\n\n\nAttaching package: 'countreg'\n\n\nThe following objects are masked from 'package:VGAM':\n\n    dzipois, pzipois, qzipois, rzipois\n\n\nThe following objects are masked from 'package:pscl':\n\n    hurdle, hurdle.control, hurdletest, zeroinfl, zeroinfl.control\n\n\nThe following object is masked from 'package:vcd':\n\n    rootogram\n\nCODE#install.packages(\"countreg\", repos=\"http://R-Forge.R-project.org\")\n\n#SYNTAX OUTCOME ~ count model predictor | hurdle predictor\n\nh.1 <- pscl::hurdle(item_test_NABS ~ condition | condition , data = df_subjects,\n              zero.dist = \"binomial\", dist = \"poisson\", size = 8)\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\n\nWarning in optim(fn = zeroDist, gr = zeroGrad, par = c(start$zero, if (zero.dist\n== : unknown names in control: size\n\nCODEh.2 <- pscl::hurdle(item_test_NABS ~ condition | condition , data = df_subjects,\n              zero.dist = \"binomial\", dist = \"negbin\", size = 8)\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\nCODEsummary(h.1)\n\n\nCall:\npscl::hurdle(formula = item_test_NABS ~ condition | condition, data = df_subjects, \n    dist = \"poisson\", zero.dist = \"binomial\", size = 8)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.892 -0.818 -0.549  0.881  2.342 \n\nCount model coefficients (truncated poisson with log link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    1.7156     0.0653   26.29   <2e-16 ***\ncondition121   0.0447     0.0789    0.57     0.57    \nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.984      0.179   -5.50  3.7e-08 ***\ncondition121    1.054      0.235    4.48  7.4e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -533 on 4 Df\n\nCODEsummary(h.2)\n\n\nCall:\npscl::hurdle(formula = item_test_NABS ~ condition | condition, data = df_subjects, \n    dist = \"negbin\", zero.dist = \"binomial\", size = 8)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.866 -0.794 -0.538  0.856  2.294 \n\nCount model coefficients (truncated negbin with log link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    1.7126     0.0728   23.54  < 2e-16 ***\ncondition121   0.0451     0.0880    0.51  0.60810    \nLog(theta)     3.1851     0.8732    3.65  0.00026 ***\nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.984      0.179   -5.50  3.7e-08 ***\ncondition121    1.054      0.235    4.48  7.4e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 24.169\nNumber of iterations in BFGS optimization: 20 \nLog-likelihood: -532 on 5 Df\n\nCODErootogram(h.1)\n\n\n\nCODErootogram(h.2)\n\n\n\nCODEcompare_performance(h.1,h.2)\n\n# Comparison of Model Performance Indices\n\nName |  Model |      AIC | AIC weights |      BIC | BIC weights |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n---------------------------------------------------------------------------------------------------------------------------------\nh.1  | hurdle | 1073.583 |       0.537 | 1088.780 |       0.886 | 0.351 |     0.347 | 3.150 | 3.169 |    -2.479 |           0.043\nh.2  | hurdle | 1073.880 |       0.463 | 1092.876 |       0.114 | 0.363 |     0.359 | 3.150 | 3.174 |    -2.132 |           0.043\n\n\nMixed Logistic Regression\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on test phase questions by condition; accounting for random effects of subject.\nFit Model\n\nCODE#SETUP DATA \n#PREPARE DATA \nn_items = 8 #number of items in test\n\n#item level\ndf_test = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  accuracy = as.factor(score_niceABS),\n  q = as.factor(q)\n)\n\ndf <- df_test\n\nlibrary(lmerTest) #for CIs in glmer \n\n## 1 | SETUP RANDOM EFFECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \n\n#:: RANDOM INTERCEPT SUBJECT\nmm.rS <- glmer(accuracy ~ (1|subject), data = df,family = \"binomial\")\n\n# :: TEST random effect\npaste(\"AIC with random effect is lower than glm empty model?\", m0$aic > AIC(logLik(mm.rS)))\n\n[1] \"AIC with random effect is lower than glm empty model? TRUE\"\n\nCODEtest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName  |    Model | df | df_diff |    Chi2 |      p\n--------------------------------------------------\nm0    |      glm |  1 |         |         |       \nmm.rS | glmerMod |  2 |       1 | 1783.73 | < .001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0\"\n\nCODE## 2 | ADD FIXED EFFECT\n\n# SUBJECT INTERCEPT | FIXED CONDITION \nmm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject), \n                data = df,family = \"binomial\")\n\n# :: TEST fixed factor \npaste(\"AIC with random effect is lower than glm empty model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )\n\n[1] \"AIC with random effect is lower than glm empty model? TRUE\"\n\nCODEtest_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName   |    Model | df | df_diff | Chi2 |     p\n-----------------------------------------------\nmm.rS  | glmerMod |  2 |         |      |      \nmm.CrS | glmerMod |  3 |       1 | 4.98 | 0.026\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS,mm.CrS))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0.0256331468201315\"\n\n\nVisualize\n\nCODE#: PRINT MODEL \nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsummary(mm.CrS)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition + (1 | subject)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    1385     1402     -689     1379     2637 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5307 -0.0193 -0.0097  0.1135  2.7426 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 117      10.8    \nNumber of obs: 2640, groups:  subject, 330\n\nFixed effects:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -9.182      0.677  -13.56   <2e-16 ***\npretty_conditionimpasse    1.632      0.753    2.17     0.03 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nprtty_cndtn -0.394\n\nCODE#: INTERPRET COEFFICIENTS\n\nprint(\"MODEL PERFORMANCE\")\n\n[1] \"MODEL PERFORMANCE\"\n\nCODEperformance(mm.CrS)\n\n# Indices of model performance\n\nAIC      |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n--------------------------------------------------------------------------------------------------------------\n1384.749 | 1402.385 |      0.973 |      0.005 | 0.973 | 0.203 | 1.000 |    0.130 |      -Inf |           0.015\n\nCODEprint(\"SANITY CHECK REPORTING\")\n\n[1] \"SANITY CHECK REPORTING\"\n\nCODEreport(mm.CrS)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model included subject as random effect (formula: ~1 | subject). The model's total explanatory power is substantial (conditional R2 = 0.97) and the part related to the fixed effects alone (marginal R2) is of 5.50e-03. The model's intercept, corresponding to pretty_condition = control, is at -9.18 (95% CI [-10.51, -7.85], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 1.63, 95% CI [0.16, 3.11], p = 0.030; Std. beta = 1.63, 95% CI [0.16, 3.11])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(mm.CrS, type=\"std2\", vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\nCODE#SJPLOT | MODEL | PROBABILITIES\nplot_model(mm.CrS, type=\"pred\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n$pretty_condition\n\n\n\n\nCODE#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\n# ggeffect(model = mm.CrS) %>% plot()\n\n#SANITY CHECK SJPLOT\n# library(effects)\n# plot(allEffects(mm.CrS))\n\n\nDiagnostics\n\nCODEcheck_model(mm.CrS)\n\n\n\nCODEbinned_residuals(mm.CrS)\n\nWarning: Probably bad model fit. Only about 75% of the residuals are inside the error bounds.\n\n\n\n\n\nInference\nWe fit a mixed-effect binomial logistic regression model with random intercepts for subjects to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(3): 4.98, p < 0.05). Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 5 over the control condition \\(e^{\\beta_1}\\) = 5.11, 95% CI [1.17,22,36], p < 0.05.\n\nCODE# PRETTY TABLE SJPLOT\ntab_model(mm.CrS)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.00\n<0.001\n\n\npretty condition: impasse\n5.11\n1.17 – 22.36\n0.030\n\n\nRandom Effects\n\n\nσ2\n\n3.29\n\n\nτ00subject\n\n116.99\n\n\n\nICC\n0.97\n\n\n\nN subject\n\n330\n\n\nObservations\n2640\n\n\nMarginal R2 / Conditional R2\n\n0.005 / 0.973\n\n\n\n\nTODO\n\nsanity check interpretation\nsanity check random effects structure : should ITEM be included? and as fixed, or random factor?\nDIAGNOSTICS: What in the world is happening with the normality of random effects plot? Do the fixed effects residuals need to be normally distributed?\nAre there other plots or recommended diagnostics for mixed log regression\nconsider multiple regression with rt, sequence cluster, confidence, etc.\nQ1 Accuracy\nCHI SQUARE\n(Combined)\n\nCODE#lab only\ndf <- df_q1 \n\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Accuracy on First Question by Condition\",\n            data = df, pretty_condition ~ accuracy, shade = T, color = 2)\n\n\n\nCODE# CrossTable( x = df$condition, y = df$accuracy, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n\n\n\n\naccuracy\n pretty_condition\n Total\n \n\ncontrol\n impasse\n \n\nincorrect\n\n13612452.5 %86.1 %\n\n\n12313547.5 %71.5 %\n\n\n259259100 %78.5 %\n \n\n\ncorrect\n\n223431 %13.9 %\n\n\n493769 %28.5 %\n\n\n7171100 %21.5 %\n \n\n\nTotal\n\n15815847.9 %100 %\n\n\n17217252.1 %100 %\n\n\n330330100 %100 %\n \n\nχ2=9.500 · df=1 · Cramer's V=0.177 · Fisher's p=0.001 \n\n \n\n\n observed valuesexpected values% within accuracy% within pretty_condition\n\n\n\nCombining data across both sessions (n=330), a Pearson’s Chi-squared test suggests a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi_2\\) (1) = 10.3, p = 0.001. The sample odds ratio (2.46, p = 0.001, 95% CI [1.37, 4.53]) indicates that the odds of providing a correct response to the first question are 2.46 higher for subjects in the impasse condition than those in the control condition.\n(In Person)\n\nCODE#lab only\ndf <- df_q1 %>% filter(pretty_mode == \"laboratory\")\n\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Accuracy on First Question by Condition\",\n            data = df, pretty_condition ~ accuracy, \n            shade = T)\n\n\n\nCODE# CrossTable( x = df$condition, y = df$score_niceABS, \n#             fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n\n\n\n\naccuracy\n pretty_condition\n Total\n \n\ncontrol\n impasse\n \n\nincorrect\n\n524853.6 %83.9 %\n\n\n454946.4 %70.3 %\n\n\n9797100 %77 %\n \n\n\ncorrect\n\n101434.5 %16.1 %\n\n\n191565.5 %29.7 %\n\n\n2929100 %23 %\n \n\n\nTotal\n\n626249.2 %100 %\n\n\n646450.8 %100 %\n\n\n126126100 %100 %\n \n\nχ2=2.547 · df=1 · Cramer's V=0.161 · Fisher's p=0.091 \n\n \n\n\n observed valuesexpected values% within accuracy% within pretty_condition\n\n\n\nFor (In Person) data collection (n=126) the Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition that is not significant at the alpha level 0.05, \\(\\chi^2\\) (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. In this particular data sample, the odds ratio (Odds Ratio = 2.18, p = 0.055, 95% CI [0.982, +Inf]) indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition .\n(Online Replication)\n\nCODE#online only\ndf <- df_q1 %>% filter(pretty_mode == \"online-replication\")\n\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Accuracy on First Question by Condition\",\n            data = df, pretty_condition ~ accuracy, shade = T)\n\n\n\nCODE# CrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, \n#             chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n\n\n\n\naccuracy\n pretty_condition\n Total\n \n\ncontrol\n impasse\n \n\nincorrect\n\n847651.9 %87.5 %\n\n\n788648.1 %72.2 %\n\n\n162162100 %79.4 %\n \n\n\ncorrect\n\n122028.6 %12.5 %\n\n\n302271.4 %27.8 %\n\n\n4242100 %20.6 %\n \n\n\nTotal\n\n969647.1 %100 %\n\n\n10810852.9 %100 %\n\n\n204204100 %100 %\n \n\nχ2=6.351 · df=1 · Cramer's V=0.189 · Fisher's p=0.009 \n\n \n\n\n observed valuesexpected values% within accuracy% within pretty_condition\n\n\n\nFor online data collection (n=204), a Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. The odds ratio (Odds Ratio = 2.68, p = 0.005, 95% CI [1.37, +Inf]) indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition ."
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1a-q1-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1a-q1-accuracy",
    "title": "5  Hypothesis Testing",
    "section": "H1A | Q1 ACCURACY",
    "text": "H1A | Q1 ACCURACY\nDo Ss in the IMPASSE condition have a higher likelihood of producing a correct response to the first question?\nThe graph comprehension tasks includes 15 questions completed in sequence. But the first question the reader encounters (Q1) is the most important, as it is their first exposure to the unconventional triangular coordinate system.\n\n\n\n\n\n\nResearch Question\nDoes the frequency of correct (vs) incorrect responses on the first question differ by condition? [Is response accuracy independent of condition?]\n\n\n\nHypothesis\nH1A | Ss in the IMPASSE condition will have a higher likelihood of correctly responding to the first question than those in the CONTROL condition\n\n\nData\n\ndata: df_items where q == 1\noutcome:\n\naccuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\ninterpretation (ordered factor from interpretation)\n\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\n\nLogistic Regression on accuracy predicted by condition\n\naccount for difference in odds of correct score by condition\n\n\nOrdinal Regression on interpretation predicted by condition\n\naccount for difference in (ordered correctness of interpretation) by condition\n\n\n\nAlternative:\n\nChi-Square test of independence on outcome score_niceABS by condition\n\n\n\n\n\nNotes\n\nCHIQ SQR is simplest method to examine independence of two categorical factors; LOGISTIC REGRESSION is recommended for binomial ~ continuous; though with regression we can quantify the size of the effect and overall model fit\nindependence assumption : (CHI SQR) as we only consider responses on the first question, each observation corresponds to an individual subject, and are thus independent\ncell frequency : (CHI SQR) expected frequency in each cell of the contingency table is greater than 5 (more than 5 correct , more than 5 incorrect responses)\n\n\n\n\n\nCODE#FILTER THE DATASET [use subjects, bc it has covariates on that record]\n# df_q1 = df_items %>% filter(q==1) %>% mutate(\n#   accuracy = recode_factor(score_niceABS, \"0\" = \"incorrect\", \"1\"=\"correct\")\n# )\ndf_q1 <- df_subjects %>% mutate(\n  accuracy = recode_factor(item_q1_NABS, \"0\" =\"incorrect\",\"1\"=\"correct\"),\n  rt = item_q1_rt\n) %>% dplyr::select(\n  accuracy, rt, pretty_condition, pretty_mode\n)\n\n#GROUPED PROPORTIONAL BAR CHART\n# gf_props(~accuracy, fill = ~pretty_condition, \n#        position = position_dodge(), data = df_q1) %>% \n#   gf_facet_grid(~pretty_mode) +\n#    labs(x = \"Correct Response on Q 1\",\n#        title = \"Accuracy on First Question by Condition\",\n#        subtitle=\"Impasse Condition yields a greater proportion of correct responses\") #theme(legend.position = \"none\")\n\n#STACKED BAR CHART\ndf_q1 %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Accuracy on First Question by Condition\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\nA proportional bar chart visualizing the proportion of incorrect (vs) correct responses in each condition for each data collection modality (left/right facet) reveals that the pattern of responses appear the same regardless of the data collection modality. In both data collection sessions, the proportion of incorrect responses is much greater than the proportion of correct responses, regardless of condition (marginal probability of incorrect). In the impasse condition, the difference in proportions is smaller than the control condition (conditional probability of success in impasse; (i.e) There are more correct responses in the impasse condition than the control condition).\nLOGISTIC REGRESSION\nFit a logistic regression (at the subject level), predicting Q1 accuracy (absolute score) by condition.\nFit Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\nCODE#combined\ndf <- df_q1 \n\n# FREQUENCY TABLE\n# my.table <- table(df$accuracy, df$pretty_condition)\n# addmargins(my.table) #counts\n# addmargins(prop.table(my.table)) #props\n\n# MODEL FITTING:::::::::::::::::::::::::::::::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\nprint(\"EMPTY MODEL\")\n\n[1] \"EMPTY MODEL\"\n\nCODEsummary(m0)\n\n\nCall:\nglm(formula = accuracy ~ 1, family = \"binomial\", data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.696  -0.696  -0.696  -0.696   1.753  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.294      0.134   -9.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 343.66  on 329  degrees of freedom\nAIC: 345.7\n\nNumber of Fisher Scoring iterations: 4\n\nCODE#: 2 CONDITION model\nm1 <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsummary(m1)\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.819  -0.819  -0.548  -0.548   1.986  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.822      0.230   -7.93  2.2e-15 ***\npretty_conditionimpasse    0.901      0.285    3.16   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 333.07  on 328  degrees of freedom\nAIC: 337.1\n\nNumber of Fisher Scoring iterations: 4\n\nCODE#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > m1$aic)\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\nCODEtest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName | Model | df | df_diff |  Chi2 |     p\n-------------------------------------------\nm0   |   glm |  1 |         |       |      \nm1   |   glm |  2 |       1 | 10.59 | 0.001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0.00113745235691825\"\n\n\nThe Condition predictor significantly improves model fit.\nVisualize\n\nCODE# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODE# summary(m1)\n\n#: INTERPRET COEFFICIENTS\n\nprint(\"Coefficients —- LOG ODDS\")\n\n[1] \"Coefficients —- LOG ODDS\"\n\nCODEconfint(m1)\n\nWaiting for profiling to be done...\n\n\n                         2.5 % 97.5 %\n(Intercept)             -2.299  -1.39\npretty_conditionimpasse  0.353   1.48\n\nCODEprint(\"Coefficients —- ODDS RATIOS\")\n\n[1] \"Coefficients —- ODDS RATIOS\"\n\nCODEe <- cbind( exp(coef(m1)), exp(confint(m1))) #exponentiate\n\nWaiting for profiling to be done...\n\nCODEe\n\n                              2.5 % 97.5 %\n(Intercept)             0.162  0.10  0.248\npretty_conditionimpasse 2.463  1.42  4.374\n\nCODEprint(\"MODEL PERFORMANCE\")\n\n[1] \"MODEL PERFORMANCE\"\n\nCODEperformance(m1)\n\n# Indices of model performance\n\nAIC     |     BIC | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n----------------------------------------------------------------------------------------------\n337.074 | 344.673 |     0.031 | 0.404 | 1.008 |    0.505 |   -16.847 |           0.021 | 0.673\n\nCODEprint(\"SANITY CHECK REPORTING\")\n\n[1] \"SANITY CHECK REPORTING\"\n\nCODEreport(m1)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic model (estimated using ML) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model's explanatory power is weak (Tjur's R2 = 0.03). The model's intercept, corresponding to pretty_condition = control, is at -1.82 (95% CI [-2.30, -1.39], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.90, 95% CI [0.35, 1.48], p = 0.002; Std. beta = 0.90, 95% CI [0.35, 1.48])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\nCODEprint(\"MODEL PREDICTIONS\")\n\n[1] \"MODEL PREDICTIONS\"\n\nCODE# Retrieve predictions as probabilities \n# (for each level of the predictor)\np.control <- predict(m1,data.frame(pretty_condition=\"control\"),type=\"response\")\npaste(\"Probability of success in control,\", p.control)\n\n[1] \"Probability of success in control, 0.139240506329147\"\n\nCODEp.impasse <- predict(m1,data.frame(pretty_condition=\"impasse\"),type=\"response\")\npaste(\"Probability of success in impasse,\", p.impasse)\n\n[1] \"Probability of success in impasse, 0.28488372093063\"\n\nCODE#: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \n# library(ggstatsplot)\n# ggcoefstats(m1, output = \"plot\") + labs(x = \"Log Odds Estimate\")\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m1, type=\"std2\", vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\nCODE#SJPLOT | MODEL | PROBABILITIES\n# plot_model(m1, type=\"pred\",\n#            show.intercept = TRUE, \n#            show.values = TRUE,\n#            title = \"Model Predicted Probability of Accuracy\",\n#            axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\nggeffect(model = m1) %>% plot()\n\n$pretty_condition\n\n\n\n\nCODE#SANITY CHECK SJPLOT\n# library(effects)\n# plot(allEffects(m))\n\n\nDiagnostics\n\nCODEcheck_model(m1)\n\n\n\nCODEbinned_residuals(m1)\n\nOk: About 100% of the residuals are inside the error bounds.\n\n\n\n\n\nInference\nWe fit a logistic regression model to analyze the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 3.16, p = 0.0016). The model predicts that the odds of a correct response on the first question in the impasse condition increase by 146% (\\(e^{beta_1}\\) = 2.46, 95% CI [1.42, 4.37]) over the control condition.\nEquivalent statements:\n\nbeing in impasse condition increases log odds of correct response by 0.901 (over control)\nbeing in impasse increases odds of correct response in impasse over control increases by factor of 2.46\nprobability of correct response in control predicted as 28.5%, vs only 14% in control condition\n\n\nCODE#PRETTY TABLE SJPLOT\ntab_model(m1)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.16\n0.10 – 0.25\n<0.001\n\n\npretty condition[impasse]\n2.46\n1.42 – 4.37\n0.002\n\n\nObservations\n330\n\n\nR2 Tjur\n0.031\n\n\n\n\nTODO\n\nAre these residuals OK? I didn’t think normally distributed residuals were an assumption for logistic regression.\nInterpretation/reporting of model fit?\nsanity check correct interpretation of coefficients & reporting\nTODO ORDINAL REGRESSION\nFit an ordinal logistic regression (at the subject level), predicting Q1 interpretation by condition.\n\nhttps://stats.oarc.ucla.edu/r/faq/ologit-coefficients/\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\ntodo see ordinal regression video\n\n\nCODE# #CREATE DATAFRAME OF Q1\n# df <- df_items %>% filter(q ==1) %>% mutate(scaled = as.factor(score_SCALED))\n# \n# #MODEL\n# m <- polr(scaled ~ condition , data = df, Hess=TRUE)\n# summary(m)\n# confint(m)\n# performance(m)\n# report(m)\n# \n# #exponentiate coefficients and CIs \n# ci <- confint(m)\n# ci\n# e <- coef(m)\n# e\n# # exp(cbind(e,ci))\n# \n# # Retrieve predictions as probabilities \n# # (for each level of the predictor)\n# # p.control <- predict(m,data.frame(condition=\"111\"),type=\"response\")\n# # paste(\"Probability of success in control,\", p.control)\n# # p.impasse <- predict(m,data.frame(condition=\"121\"),type=\"response\")\n# # paste(\"Probability of success in impasse,\", p.impasse)\n# \n# # Plot Predicted data and original data points\n# # ggplot(df, aes(x=condition, y=accuracy)) + \n# #   geom_point() +\n# #   stat_smooth(method=\"glm\", color=\"green\", se=FALSE,\n# #                 method.args = list(family=binomial))\n#   \n# #TO PLOT ALL EFFECTS\n# library(effects)\n# plot(allEffects(m))\n# \n# #SJPLOT\n# library(sjPlot)\n# plot_model(m, )\n# \n# \n# #CONVERT TO PROBABILITIES\n# newdat <- data.frame(condition=c(\"111\",\"121\"))\n# prob <- (phat <- predict(object = m, newdat, type=\"p\"))\n# prob\n#"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1b-test-phase-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1b-test-phase-accuracy",
    "title": "4  Hypothesis Testing",
    "section": "H1B | TEST PHASE ACCURACY",
    "text": "H1B | TEST PHASE ACCURACY\nDo Ss in the IMPASSE condition have higher scores in the TEST Phase of the task?\nThe graph comprehension tasks includes 13 interpretation-discriminant questions completed in sequence. The first five questions constitute the ‘scaffold’ phase, and the remaining 8 the test phase. By examining the effect of condition on test-phase scores, we can evaluate if the impasse scaffold has an effect after it it has been removed.\n\n\n\n\n\n\nResearch Question\nDo Ss in the IMPASSE condition score higher in the test phase than those in the CONTROL group.\n\n\n\nHypothesis\n(H1B) Participants in the IMPASSE condition will have higher test phase performance than those in the CONTROL condition.\n\n\nData\n\ndata: df_items where q in 7,8,10,11,12,13,14,15 (the 8 discriminating test phase questions), df_subjects\noutcome:\n\n[at item level]\n\n\naccuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\n\ninterpretation (ordered factor from interpretation)\n\n\n[subject level]\n\np_accuracy (percent of correct responses)\n\n\n\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMixed Logistic Regressionaccuracy ~ condition + (1 | subject )\nmodel effect of condition on probability of correct response [during test phase] while accounting for subject (and item-level?) effects\nOrdinal Mixed Logistic Regressioninterpretation ~ condition + (1 | subject )\nmodel effect of condition on [ordered correctness of interpretation] [during test phase] while accounting for subject (and item-level?) effects\nShift in Modal Mass (descriptive)\ndescribe & visualize shift in deciles between conditions for `scaled_score` (at subject level)\n\n\n\nAlternatives\n\nOLS LINEAR REGRESSION\n\nbimodal distribution at tails makes the mean a poor predictor; LMs violate assumptions of normally distributed residuals\nlm DV_percent_test_NABS ~ condition (absolute scoring) OR lm item_test_SCALED ~ condition (scaled scoring)\nboth absolute and scaled scores yield non-normal residuals\nno transformation of the outcome variables yield normal residuals\n\n\n\n\n\nNotes\n\nAlso exploring:\n\nHurdle model (mixture model w/ binomial + [poisson or negbinom count; 0s from 1 DGP)\nZero Inflated model (mixture model w/ binomial + poisson or negbinom count; 0s from 2 DGPs)\nBeta regression hurdle model? (mixture with location and scale parameters [mean, variance] and hurdles for floor and ceiling effects)\nOther way to account for the severe bimodality?\n\n\n\n\n\n\nCODE#PREPARE DATA \nn_items = 8 #number of items in test\n\n#item level\ndf_test = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  accuracy = as.factor(score_niceABS),\n  q = as.factor(q)\n)\n\n#subject level\ndf_Stest = df_subjects %>% mutate(\n  p_accuracy = item_test_NABS/n_items\n)\n\n#STACKED PROPORTIONAL BAR CHART\ndf_test %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Accuracy on Test Phase Questions\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\nCODE#GROUPED PROPORTIONAL BAR CHART\ngf_props(~accuracy, fill = ~pretty_condition, x =~pretty_condition,\n       position = position_dodge(), data = df_test) %>% \n  gf_facet_grid(~pretty_mode) +\n   labs(x = \"Correct Responses in Test Phase\",\n       title = \"Accuracy in Test Phase by Condition\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\nCODE#FACETED HISTOGRAM\nstats = df_Stest %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(p_accuracy))\ngf_props(~p_accuracy, \n         fill = ~pretty_condition, data = df_Stest) %>% \n  gf_facet_grid(pretty_condition ~ pretty_mode) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"% Correct\",\n       y = \"proportion of subjects\",\n       title = \"Test Phase Absolute Score (% Correct)\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n\n\n\n\nMIXED LOGISTIC REGRESSION\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on test phase questions by condition; accounting for random effects of subject.\nFit Model\n\nCODE#SETUP DATA \ndf <- df_test \n\nlibrary(lmerTest) #for CIs in glmer \n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nCODE## 1 | SETUP RANDOM EFFECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \n\n#:: RANDOM INTERCEPT SUBJECT\nmm.rS <- glmer(accuracy ~ (1|subject), data = df,family = \"binomial\")\n\n# :: TEST random effect\npaste(\"AIC with random effect is lower than glm empty model?\", m0$aic > AIC(logLik(mm.rS)))\n\n[1] \"AIC with random effect is lower than glm empty model? TRUE\"\n\nCODEtest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName  |    Model | df | df_diff |    Chi2 |      p\n--------------------------------------------------\nm0    |      glm |  1 |         |         |       \nmm.rS | glmerMod |  2 |       1 | 1783.73 | < .001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0\"\n\nCODE## 2 | ADD FIXED EFFECT\n\n# SUBJECT INTERCEPT | FIXED CONDITION \nmm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject), \n                data = df,family = \"binomial\")\n\n# :: TEST fixed factor \npaste(\"AIC with random effect is lower than glm empty model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )\n\n[1] \"AIC with random effect is lower than glm empty model? TRUE\"\n\nCODEtest_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName   |    Model | df | df_diff | Chi2 |     p\n-----------------------------------------------\nmm.rS  | glmerMod |  2 |         |      |      \nmm.CrS | glmerMod |  3 |       1 | 4.98 | 0.026\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS,mm.CrS))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0.0256331468201315\"\n\n\nVisualize\n\nCODE#: PRINT MODEL \nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsummary(mm.CrS)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition + (1 | subject)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    1385     1402     -689     1379     2637 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5307 -0.0193 -0.0097  0.1135  2.7426 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 117      10.8    \nNumber of obs: 2640, groups:  subject, 330\n\nFixed effects:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -9.182      0.677  -13.56   <2e-16 ***\npretty_conditionimpasse    1.632      0.753    2.17     0.03 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nprtty_cndtn -0.394\n\nCODE#: INTERPRET COEFFICIENTS\n\nprint(\"MODEL PERFORMANCE\")\n\n[1] \"MODEL PERFORMANCE\"\n\nCODEperformance(mm.CrS)\n\n# Indices of model performance\n\nAIC      |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n--------------------------------------------------------------------------------------------------------------\n1384.749 | 1402.385 |      0.973 |      0.005 | 0.973 | 0.203 | 1.000 |    0.130 |      -Inf |           0.015\n\nCODEprint(\"SANITY CHECK REPORTING\")\n\n[1] \"SANITY CHECK REPORTING\"\n\nCODEreport(mm.CrS)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model included subject as random effect (formula: ~1 | subject). The model's total explanatory power is substantial (conditional R2 = 0.97) and the part related to the fixed effects alone (marginal R2) is of 5.50e-03. The model's intercept, corresponding to pretty_condition = control, is at -9.18 (95% CI [-10.51, -7.85], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 1.63, 95% CI [0.16, 3.11], p = 0.030; Std. beta = 1.63, 95% CI [0.16, 3.11])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(mm.CrS, type=\"std2\", vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\nCODE#SJPLOT | MODEL | PROBABILITIES\nplot_model(mm.CrS, type=\"pred\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n$pretty_condition\n\n\n\n\nCODE#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\n# ggeffect(model = mm.CrS) %>% plot()\n\n#SANITY CHECK SJPLOT\n# library(effects)\n# plot(allEffects(mm.CrS))\n\n\nDiagnostics\n\nCODEcheck_model(mm.CrS)\n\n\n\nCODEbinned_residuals(mm.CrS)\n\nWarning: Probably bad model fit. Only about 75% of the residuals are inside the error bounds.\n\n\n\n\n\nInference\nWe fit a mixed-effect binomial logistic regression model with random intercepts for subjects to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(3): 4.98, p < 0.05). Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 5 over the control condition \\(e^{\\beta_1}\\) = 5.11, 95% CI [1.17,22,36], p < 0.05.\n\nCODE# PRETTY TABLE SJPLOT\ntab_model(mm.CrS)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.00\n<0.001\n\n\npretty condition: impasse\n5.11\n1.17 – 22.36\n0.030\n\n\nRandom Effects\n\n\nσ2\n\n3.29\n\n\nτ00subject\n\n116.99\n\n\n\nICC\n0.97\n\n\n\nN subject\n\n330\n\n\nObservations\n2640\n\n\nMarginal R2 / Conditional R2\n\n0.005 / 0.973\n\n\n\n\nTODO\n\nsanity check interpretation\nsanity check random effects structure : should ITEM be included? and as fixed, or random factor?\nDIAGNOSTICS: What in the world is happening with the normality of random effects plot? Do the fixed effects residuals need to be normally distributed?\nAre there other plots or recommended diagnostics for mixed log regression\nconsider multiple regression with rt, sequence cluster, confidence, etc.\n\n\nCODE#STEPWISE FIT ALT \n\n#:: EMPTY MODEL (baseline, no random effect)\n# m0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \n\n#:: EMPTY MODEL (baseline, no random effect)\n# m0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \n# \n# #:: RANDOM INTERCEPT SUBJECT\n# mm.rS <- glmer(accuracy ~ (1|subject), data = df,family = \"binomial\")\n# \n# # :: TEST random effect\n# paste(\"AIC with random effect is lower than glm empty model?\", m0$aic > AIC(logLik(mm.rS)))\n# test_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n# paste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n\nTODO | Ordinal Regression on ITEM-Interpretation\nSHIFT IN MODAL MASS\nThe Effect of Condition on Total Absolute Test Score can be described as a ‘shift’ in mass between the two modes of each distribution.\nFirst, we use the Kolmogorov-Smirnov test as a Robust alternative to the t-test to test if the two distributions likely come from different populations.\n\nCODE# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nmbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(mbp)\n\n#(requires shift function files loaded)\n#LOAD MODAL SHIFT FUNCTION RESOURCES\nsource(\"analysis/utils/shift_function/Rallfun-v30.txt\")\nsource(\"analysis/utils/shift_function/wilcox_modified.txt\")\nsource(\"analysis/utils/shift_function/rgar_visualisation.txt\")\nsource(\"analysis/utils/shift_function/rgar_utils.txt\")\n#NOTE: something in these breaks the stat_ecdf in ggplot2\n\n#PREP DATA \ndf <- df_subjects %>%\n  dplyr::select(s_SCALED, pretty_condition) %>%\n  mutate(\n    data = as.numeric(s_SCALED),\n    #flip order levels to correctly orient graph\n    # gr = recode_factor(pretty_condition, \"impasse\" = \"impasse\", \"control\"=\"control\")\n    gr = as.character(pretty_condition)\n  ) %>% dplyr::select(data,gr)\n\n\ng1 <- df %>% filter(gr == \"control\") %>% dplyr::pull(data)\ng2 <- df %>% filter(gr == \"impasse\") %>% dplyr::pull(data)\n\n\n#COMPARE DISTRIBUTIONS WITH ROBUST TESTS\n\n#What do common tests say about the difference?\n\n# Kolmogorov-Smirnov test\n#If y is numeric, a two-sample (Smirnov) test of the null hypothesis that x and y \n#were drawn from the same continuous distribution is performed. Alternatively, y ...\n\n#null is X is drawn from CDF EQUAL TO Y\nks.test(g1,g2) \n\nWarning in ks.test(g1, g2): p-value will be approximate in the presence of ties\n\n\n\n    Two-sample Kolmogorov-Smirnov test\n\ndata:  g1 and g2\nD = 0.5, p-value <2e-16\nalternative hypothesis: two-sided\n\nCODEprint(\"SUGGESTS that impasse and control come from different population distributions\")\n\n[1] \"SUGGESTS that impasse and control come from different population distributions\"\n\nCODE# #null is X is NOT LESS THAN Y\nks.test(g1,g2, alternative = \"greater\") \n\nWarning in ks.test(g1, g2, alternative = \"greater\"): p-value will be approximate\nin the presence of ties\n\n\n\n    Two-sample Kolmogorov-Smirnov test\n\ndata:  g1 and g2\nD^+ = 0.5, p-value <2e-16\nalternative hypothesis: the CDF of x lies above that of y\n\nCODEprint(\"SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]\")\n\n[1] \"SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]\"\n\nCODE#REGULAR T-TEST\nt.test(g1,g2) # regular Welsh t-test\n\n\n    Welch Two Sample t-test\n\ndata:  g1 and g2\nt = -7, df = 325, p-value = 7e-12\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -8.93 -5.06\nsample estimates:\nmean of x mean of y \n   -6.427     0.567 \n\n\n\nCODE#IF THIS ERRORS, consider loadling plyr (older than dplyr)\n# kernel density estimate + rug plot + superimposed deciles\nkde <- plot.kde_rug_dec2(df)\n\nLoading required package: plyr\n\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\n\nThe following object is masked from 'package:vcdExtra':\n\n    summarise\n\n\nThe following object is masked from 'package:ggpubr':\n\n    mutate\n\n\nThe following objects are masked from 'package:Hmisc':\n\n    is.discrete, summarize\n\nCODE# kde\n\n# compute shift function\nout <- shifthd( g1, g2, nboot=200)\n\n# plot shift function\nsf <- plot.sf(data=out) # function from rgar_visualisation.txt\n# sf\n\n# combine KDE + SF\ncowplot::plot_grid(kde, sf, labels=c(\"A\", \"B\"), ncol = 1, nrow = 2, rel_heights = c(1.5, 1),label_size = 18,hjust = -1,scale=.95)\n\n\n\n\nCummulative Ordinal (Bayesian)\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\n\nCODE# library(brms)\n\n\n# #DEFINE DATA \n# df <- df_items %>% mutate(\n#   scaled = factor(score_SCALED, ordered = TRUE,  #ordered factor\n#                     levels = c(\"-1\", \"-0.5\", \"0\", \"0.5\",\"1\"))\n# )\n# \n# ord_cum <- brm( formula = scaled ~ condition,\n#                data = df,\n#                family = cumulative(\"probit\"),\n#                file = \"analysis/SGC3A/models/m_items_ord.cum.rds\" # cache model (can be removed)  \n# \n# )\n# \n# summary(ord_cum)\n# conditional_effects(ord_cum, \"condition\", categorical = TRUE)\n# \n# #SJPLOT\n# library(sjPlot)\n# plot_model(ord_cum)\n# \n# # m %>%\n# #   spread_draws(b_Intercept, r_condition[condition,]) %>%\n# #   mutate(condition_mean = b_Intercept + r_condition) %>%\n# #   ggplot(aes(y = condition, x = condition_mean)) +\n# #   stat_halfeye()\n# \n# # performance(ord_cum)\n# # plot(ord_cum)\n\n\nAdjacent-Category Ordinal (Bayesian)\n\nCODE# \n# #DEFINE DATA \n# df <- df_items %>% mutate(\n#   scaled = factor(score_SCALED, ordered = TRUE,  #ordered factor\n#                     levels = c(\"-1\", \"-0.5\", \"0\", \"0.5\",\"1\"))\n# )\n# \n# \n# # To specify an adjacent-category model, we use family = acat() instead of family = cumulative() as an argument to the brm() function. Then, to model condition with possible category-specific effects, we wrap this variable in cs() in the model’s formula:\n# \n# ord_acat <- brm( formula = scaled ~ cs(condition),\n#                data = df,\n#                family = acat(\"probit\"),\n#                file = \"analysis/SGC3A/models/m_items_ord.acat.rds\" # cache model (can be removed)  \n# )\n# \n# summary(ord_acat)\n# conditional_effects(ord_cum, \"condition\", categorical = TRUE)\n# conditional_effects(ord_acat, \"condition\", categorical = TRUE)\n# \n# #TIDYBAYES VISUALIZATION\n# library(tidybayes)\n# ord_acat %>%\n#   spread_draws(b_Intercept, r_condition[condition,]) %>%\n#   mutate(condition_mean = b_Intercept + r_condition) %>%\n#   ggplot(aes(y = condition, x = condition_mean)) +\n#   stat_halfeye()\n#"
  },
  {
    "objectID": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1b-overall-accuracy",
    "href": "analysis/SGC3A/4_sgc3A_hypotesting.html#h1b-overall-accuracy",
    "title": "5  Hypothesis Testing",
    "section": "H1B | OVERALL ACCURACY",
    "text": "H1B | OVERALL ACCURACY\nDo Ss in the IMPASSE condition have higher scores across the entire task?\nThe graph comprehension tasks includes 13 interpretation-discriminant questions completed in sequence.\n\n\n\n\n\n\nResearch Question\nDo Ss in the IMPASSE condition score higher across the entire task than those in the CONTROL group.\n\n\n\nHypothesis\n(H1B) Participants in the IMPASSE condition will have higher test phase performance than those in the CONTROL condition.\n\n\nData\n\ndata: df_items where q nin 6,9 (the 13 discriminating Qs ), df_subjects\noutcome:\n\n[at item level]\n\n\naccuracy ( factor(incorrect/correct) from score_niceABS [absolute score]\n\ninterpretation (ordered factor from interpretation)\n\n\n[subject level]\n\np_accuracy (percent of correct responses)\n\n\n\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\nMixed Logistic Regressionaccuracy ~ condition + (1 | subject )\nmodel effect of condition on probability of correct response [during test phase] while accounting for subject (and item-level?) effects\nOrdinal Mixed Logistic Regressioninterpretation ~ condition + (1 | subject )\nmodel effect of condition on [ordered correctness of interpretation] [during test phase] while accounting for subject (and item-level?) effects\nShift in Modal Mass (descriptive)\ndescribe & visualize shift in deciles between conditions for `scaled_score` (at subject level)\n\n\n\nAlternatives\n\nOLS LINEAR REGRESSION\n\nbimodal distribution at tails makes the mean a poor predictor; LMs violate assumptions of normally distributed residuals\nlm DV_percent_test_NABS ~ condition (absolute scoring) OR lm item_test_SCALED ~ condition (scaled scoring)\nboth absolute and scaled scores yield non-normal residuals\nno transformation of the outcome variables yield normal residuals\n\n\n\n\n\nNotes\n\nAlso exploring:\n\nHurdle model (mixture model w/ binomial + [poisson or negbinom count; 0s from 1 DGP)\nZero Inflated model (mixture model w/ binomial + poisson or negbinom count; 0s from 2 DGPs)\nBeta regression hurdle model? (mixture with location and scale parameters [mean, variance] and hurdles for floor and ceiling effects)\nOther way to account for the severe bimodality?\n\n\n\n\n\n\nCODE#PREPARE DATA \nn_items = 13 #number of items in test\n\n#item level\ndf = df_items %>% filter(q %nin% c(6,9)) %>% mutate(\n  accuracy = recode_factor(score_niceABS, \"0\" =\"incorrect\",\"1\"=\"correct\"),\n  q = as.factor(q)\n)\n\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Accuracy on Task\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\nCODE#GROUPED PROPORTIONAL BAR CHART\n# gf_props(~accuracy, fill = ~pretty_condition, x =~pretty_condition,\n#        position = position_dodge(), data = df) %>% \n#   gf_facet_grid(~pretty_mode) +\n#    labs(x = \"Correct Responses in Test Phase\",\n#        title = \"Accuracy on Task by Condition\",\n#        subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n \n#FACETED HISTOGRAM\nstats = df_subjects %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(s_NABS))\ngf_props(~s_NABS, \n         fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_grid(pretty_condition ~ pretty_mode) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"# Correct\",\n       y = \"proportion of subjects\",\n       title = \"Overall Absolute Score (# Correct)\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n\n\n\n\nMIXED LOGISTIC REGRESSION\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.\nFit Model\n\nCODE## 1 | SETUP RANDOM INTERCEPT SUBJECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nprint(\"Empty fixed model\")\n\n[1] \"Empty fixed model\"\n\nCODEm0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \nm0\n\n\nCall:  glm(formula = accuracy ~ 1, family = \"binomial\", data = df)\n\nCoefficients:\n(Intercept)  \n     -0.903  \n\nDegrees of Freedom: 4289 Total (i.e. Null);  4289 Residual\nNull Deviance:      5150 \nResidual Deviance: 5150     AIC: 5160\n\nCODE#:: RANDOM INTERCEPT SUBJECT\nprint(\"Subject intercept random model\")\n\n[1] \"Subject intercept random model\"\n\nCODEmm.rS <- glmer(accuracy ~ (1|subject), data = df,family = \"binomial\")\nmm.rS\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ (1 | subject)\n   Data: df\n     AIC      BIC   logLik deviance df.resid \n    2588     2601    -1292     2584     4288 \nRandom effects:\n Groups  Name        Std.Dev.\n subject (Intercept) 4.25    \nNumber of obs: 4290, groups:  subject, 330\nFixed Effects:\n(Intercept)  \n      -2.96  \n\nCODE# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", m0$aic > AIC(logLik(mm.rS)))\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\nCODEtest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName  |    Model | df | df_diff |    Chi2 |      p\n--------------------------------------------------\nm0    |      glm |  1 |         |         |       \nmm.rS | glmerMod |  2 |       1 | 2569.38 | < .001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0\"\n\nCODE## 2 | ADD RANDOM RANDOM INTERCEPT ITEM\n\n#:: RANDOM INTERCEPT SUBJECT + INTERCEPT Q\nprint(\"Subject & Question random intercepts\")\n\n[1] \"Subject & Question random intercepts\"\n\nCODEmm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df,family = \"binomial\")\nmm.rSQ\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ (1 | subject) + (1 | q)\n   Data: df\n     AIC      BIC   logLik deviance df.resid \n    2476     2495    -1235     2470     4287 \nRandom effects:\n Groups  Name        Std.Dev.\n subject (Intercept) 4.622   \n q       (Intercept) 0.685   \nNumber of obs: 4290, groups:  subject, 330; q, 13\nFixed Effects:\n(Intercept)  \n       -3.2  \n\nCODE# :: TEST random effect\npaste(\"AIC decreases w/ new model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)) )\n\n[1] \"AIC decreases w/ new model? TRUE\"\n\nCODEtest_lrt(mm.rS,mm.rSQ) #same as anova(m0, m1, test = \"Chi\")\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName   |    Model | df | df_diff |   Chi2 |      p\n--------------------------------------------------\nmm.rS  | glmerMod |  2 |         |        |       \nmm.rSQ | glmerMod |  3 |       1 | 114.70 | < .001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS,mm.rSQ))$p[2])\n\nSome of the nested models seem to be identical and probably only vary in\n  their random effects.\n\n\n[1] \"Likelihood Ratio test is significant? p =  9.1748198033228e-27\"\n\nCODE## 3 | ADD FIXED EFFECT\n\n# SUBJECT INTERCEPT | FIXED CONDITION \nprint(\"FIXED Condition + Subject & Question random intercepts\")\n\n[1] \"FIXED Condition + Subject & Question random intercepts\"\n\nCODEmm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q), \n                data = df,family = \"binomial\")\nmm.CrSQ\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition + (1 | subject) + (1 | q)\n   Data: df\n     AIC      BIC   logLik deviance df.resid \n    2439     2464    -1215     2431     4286 \nRandom effects:\n Groups  Name        Std.Dev.\n subject (Intercept) 4.773   \n q       (Intercept) 0.681   \nNumber of obs: 4290, groups:  subject, 330; q, 13\nFixed Effects:\n            (Intercept)  pretty_conditionimpasse  \n                  -5.68                     4.00  \n\nCODEpaste(\"AIC decreases w/ new model\", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.CrSQ)) )\n\n[1] \"AIC decreases w/ new model TRUE\"\n\nCODEtest_lrt(mm.rSQ,mm.CrSQ) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName    |    Model | df | df_diff |  Chi2 |      p\n--------------------------------------------------\nmm.rSQ  | glmerMod |  3 |         |       |       \nmm.CrSQ | glmerMod |  4 |       1 | 38.72 | < .001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rSQ,mm.CrSQ))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  4.88960391831629e-10\"\n\n\nVisualize\n\nCODE#: PRINT MODEL \nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsummary(mm.CrSQ)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition + (1 | subject) + (1 | q)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    2439     2464    -1215     2431     4286 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-8.166 -0.154 -0.052  0.122  7.646 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 22.785   4.773   \n q       (Intercept)  0.463   0.681   \nNumber of obs: 4290, groups:  subject, 330; q, 13\n\nFixed effects:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -5.679      0.703   -8.08  6.3e-16 ***\npretty_conditionimpasse    4.003      0.726    5.51  3.5e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nprtty_cndtn -0.791\n\nCODE#: INTERPRET COEFFICIENTS\n\nprint(\"MODEL PERFORMANCE\")\n\n[1] \"MODEL PERFORMANCE\"\n\nCODEperformance(mm.CrSQ)\n\n# Indices of model performance\n\nAIC      |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n--------------------------------------------------------------------------------------------------------------\n2438.890 | 2464.346 |      0.892 |      0.131 | 0.876 | 0.229 | 1.000 |    0.174 |      -Inf |           0.005\n\nCODEprint(\"SANITY CHECK REPORTING\")\n\n[1] \"SANITY CHECK REPORTING\"\n\nCODEreport(mm.CrSQ)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model included subject and q as random effects (formula: list(~1 | subject, ~1 | q)). The model's total explanatory power is substantial (conditional R2 = 0.89) and the part related to the fixed effects alone (marginal R2) is of 0.13. The model's intercept, corresponding to pretty_condition = control, is at -5.68 (95% CI [-7.06, -4.30], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 4.00, 95% CI [2.58, 5.43], p < .001; Std. beta = 4.00, 95% CI [2.58, 5.43])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(mm.CrSQ, type=\"std2\", vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\nCODE#SJPLOT | MODEL | PROBABILITIES\nplot_model(mm.CrSQ, type=\"pred\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n$pretty_condition\n\n\n\n\nCODE#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\n# ggeffect(model = mm.CrSQ) %>% plot()\n\n#SANITY CHECK SJPLOT\n# library(effects)\n# plot(allEffects(mm.CrSQ))\n\n\nDiagnostics\n\nCODEcheck_model(mm.CrSQ)\n\n\n\nCODEbinned_residuals(mm.CrSQ)\n\nWarning: Probably bad model fit. Only about 37% of the residuals are inside the error bounds.\n\n\n\n\n\nInference\nWe fit a mixed-effect binomial logistic regression model with random intercepts for subjects and items; to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(4): 38.72, p < 0.001); and the total explanatory power is substantial (conditional R2 = 0.89) and the part related to the fixed effects alone (marginal R2) is 0.13. Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 55 over the control condition \\(e^{\\beta_1}\\) = 54.79, 95% CI [13.2, 227.39], p < 0.001.\n\nCODE# PRETTY TABLE SJPLOT\ntab_model(mm.CrSQ)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.01\n<0.001\n\n\npretty condition: impasse\n54.79\n13.20 – 227.39\n<0.001\n\n\nRandom Effects\n\n\nσ2\n\n3.29\n\n\nτ00subject\n\n22.79\n\n\n\nτ00q\n\n0.46\n\n\n\nICC\n0.88\n\n\n\nN subject\n\n330\n\n\n\nN q\n\n13\n\n\nObservations\n4290\n\n\nMarginal R2 / Conditional R2\n\n0.131 / 0.892\n\n\n\n\nTODO\n\nsanity check interpretation\nsanity check random effects structure : ITEM appropriate as random intercept? What does it mean to have two random intercepts?\nDIAGNOSTICS: What in the world is happening with the normality of random effects plot? Do the fixed effects residuals need to be normally distributed?\nAre there other plots or recommended diagnostics for mixed log regression\nconsider multiple regression with rt, sequence cluster, confidence, etc.\n\n\nCODE#STEPWISE FIT ALT \n\n#:: EMPTY MODEL (baseline, no random effect)\n# m0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \n\n#:: EMPTY MODEL (baseline, no random effect)\n# m0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \n# \n# #:: RANDOM INTERCEPT SUBJECT\n# mm.rS <- glmer(accuracy ~ (1|subject), data = df,family = \"binomial\")\n# \n# # :: TEST random effect\n# paste(\"AIC with random effect is lower than glm empty model?\", m0$aic > AIC(logLik(mm.rS)))\n# test_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n# paste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n\nTODO | Ordinal Regression on ITEM-Interpretation\nSHIFT IN MODAL MASS\nThe Effect of Condition on Total Scaled Score can be described as a ‘shift’ in mass between the low and high modes of each distribution.\nFirst, we use the Kolmogorov-Smirnov test as a Robust alternative to the t-test to test if the two distributions likely come from different populations.\n\nCODE# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n# setwd(mbp)\n\n#(requires shift function files loaded)\n#LOAD MODAL SHIFT FUNCTION RESOURCES\nsource(\"analysis/utils/shift_function/Rallfun-v30.txt\")\nsource(\"analysis/utils/shift_function/wilcox_modified.txt\")\nsource(\"analysis/utils/shift_function/rgar_visualisation.txt\")\nsource(\"analysis/utils/shift_function/rgar_utils.txt\")\n#NOTE: something in these breaks the stat_ecdf in ggplot2\n\n#PREP DATA \ndf <- df_subjects %>%\n  dplyr::select(s_SCALED, pretty_condition) %>%\n  mutate(\n    data = as.numeric(s_SCALED),\n    #flip order levels to correctly orient graph\n    # gr = recode_factor(pretty_condition, \"impasse\" = \"impasse\", \"control\"=\"control\")\n    gr = as.character(pretty_condition)\n  ) %>% dplyr::select(data,gr)\n\n\ng1 <- df %>% filter(gr == \"control\") %>% dplyr::pull(data)\ng2 <- df %>% filter(gr == \"impasse\") %>% dplyr::pull(data)\n\n\n#COMPARE DISTRIBUTIONS WITH ROBUST TESTS\n\n#What do common tests say about the difference?\n\n# Kolmogorov-Smirnov test\n#If y is numeric, a two-sample (Smirnov) test of the null hypothesis that x and y \n#were drawn from the same continuous distribution is performed. Alternatively, y ...\n\n#null is X is drawn from CDF EQUAL TO Y\nks.test(g1,g2) \n\nWarning in ks.test(g1, g2): p-value will be approximate in the presence of ties\n\n\n\n    Two-sample Kolmogorov-Smirnov test\n\ndata:  g1 and g2\nD = 0.5, p-value <2e-16\nalternative hypothesis: two-sided\n\nCODEprint(\"SUGGESTS that impasse and control come from different population distributions\")\n\n[1] \"SUGGESTS that impasse and control come from different population distributions\"\n\nCODE# #null is X is NOT LESS THAN Y\nks.test(g1,g2, alternative = \"greater\") \n\nWarning in ks.test(g1, g2, alternative = \"greater\"): p-value will be approximate\nin the presence of ties\n\n\n\n    Two-sample Kolmogorov-Smirnov test\n\ndata:  g1 and g2\nD^+ = 0.5, p-value <2e-16\nalternative hypothesis: the CDF of x lies above that of y\n\nCODEprint(\"SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]\")\n\n[1] \"SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]\"\n\nCODE#REGULAR T-TEST\nt.test(g1,g2) # regular Welsh t-test\n\n\n    Welch Two Sample t-test\n\ndata:  g1 and g2\nt = -7, df = 325, p-value = 7e-12\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -8.93 -5.06\nsample estimates:\nmean of x mean of y \n   -6.427     0.567 \n\n\n\nCODE#IF THIS ERRORS, consider loadling plyr (older than dplyr)\n# kernel density estimate + rug plot + superimposed deciles\nkde <- plot.kde_rug_dec2(df)\n\nLoading required package: plyr\n\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\n\nThe following object is masked from 'package:vcdExtra':\n\n    summarise\n\n\nThe following object is masked from 'package:ggpubr':\n\n    mutate\n\n\nThe following objects are masked from 'package:Hmisc':\n\n    is.discrete, summarize\n\nCODE# kde\n\n# compute shift function\nout <- shifthd( g1, g2, nboot=200)\n\n# plot shift function\nsf <- plot.sf(data=out) # function from rgar_visualisation.txt\n# sf\n\n# combine KDE + SF\ncowplot::plot_grid(kde, sf, labels=c(\"A\", \"B\"), ncol = 1, nrow = 2, rel_heights = c(1.5, 1),label_size = 18,hjust = -1,scale=.95)"
  },
  {
    "objectID": "analysis/SGC3A/x_learning.html",
    "href": "analysis/SGC3A/x_learning.html",
    "title": "Modelling Reference",
    "section": "",
    "text": "In this notebook we use data from study SGC3A to explore different modelling techniques and assess their suitability for the bimodal accuracy distributions."
  },
  {
    "objectID": "analysis/SGC3A/x_learning.html#single-item-level",
    "href": "analysis/SGC3A/x_learning.html#single-item-level",
    "title": "Modelling Reference",
    "section": "SINGLE ITEM LEVEL",
    "text": "SINGLE ITEM LEVEL\nQ1 Absolute, Interpretation Scores\n\nCODE#FILTER THE DATASET [use subjects, bc it has covariates on that record]\ndf_q1 <- df_subjects %>% mutate(\n  accuracy = recode_factor(item_q1_NABS, \"0\" =\"incorrect\",\"1\"=\"correct\"),\n  rt = item_q1_rt\n) %>% dplyr::select(\n  accuracy, rt, pretty_condition, pretty_mode\n)\n\n#GROUPED PROPORTIONAL BAR CHART\n# gf_props(~accuracy, fill = ~pretty_condition, \n#        position = position_dodge(), data = df_q1) %>% \n#   gf_facet_grid(~pretty_mode) +\n#    labs(x = \"Correct Response on Q 1\",\n#        title = \"Accuracy on First Question by Condition\",\n#        subtitle=\"Impasse Condition yields a greater proportion of correct responses\") #theme(legend.position = \"none\")\n\n#STACKED BAR CHART\ndf_q1 %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"Correct Response on Q 1\",\n       title = \"Accuracy on First Question by Condition\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\n\nCHI SQUARE\n(Combined)\n\nCODE#combined dataset \ndf <- df_q1 \n\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Accuracy on First Question by Condition\",\n            data = df, pretty_condition ~ accuracy, shade = T, color = 2)\n\n\n\nCODE# CrossTable( x = df$condition, y = df$accuracy, fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n\n\n\n\naccuracy\n pretty_condition\n Total\n \n\ncontrol\n impasse\n \n\nincorrect\n\n13612452.5 %86.1 %\n\n\n12313547.5 %71.5 %\n\n\n259259100 %78.5 %\n \n\n\ncorrect\n\n223431 %13.9 %\n\n\n493769 %28.5 %\n\n\n7171100 %21.5 %\n \n\n\nTotal\n\n15815847.9 %100 %\n\n\n17217252.1 %100 %\n\n\n330330100 %100 %\n \n\nχ2=9.500 · df=1 · Cramer's V=0.177 · Fisher's p=0.001 \n\n \n\n\n observed valuesexpected values% within accuracy% within pretty_condition\n\n\n\nCombining data across both sessions (n=330), a Pearson’s Chi-squared test suggests a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi_2\\) (1) = 10.3, p = 0.001. The sample odds ratio (2.46, p = 0.001, 95% CI [1.37, 4.53]) indicates that the odds of providing a correct response to the first question are 2.46 higher for subjects in the impasse condition than those in the control condition.\n(In Person)\n\nCODE#lab only\ndf <- df_q1 %>% filter(pretty_mode == \"laboratory\")\n\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Accuracy on First Question by Condition\",\n            data = df, pretty_condition ~ accuracy, \n            shade = T)\n\n\n\nCODE# CrossTable( x = df$condition, y = df$score_niceABS, \n#             fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n\n\n\n\naccuracy\n pretty_condition\n Total\n \n\ncontrol\n impasse\n \n\nincorrect\n\n524853.6 %83.9 %\n\n\n454946.4 %70.3 %\n\n\n9797100 %77 %\n \n\n\ncorrect\n\n101434.5 %16.1 %\n\n\n191565.5 %29.7 %\n\n\n2929100 %23 %\n \n\n\nTotal\n\n626249.2 %100 %\n\n\n646450.8 %100 %\n\n\n126126100 %100 %\n \n\nχ2=2.547 · df=1 · Cramer's V=0.161 · Fisher's p=0.091 \n\n \n\n\n observed valuesexpected values% within accuracy% within pretty_condition\n\n\n\nFor (In Person) data collection (n=126) the Pearson’s Chi-squared test (of independence) indicates a relationship between response accuracy on the first question and experimental condition that is not significant at the alpha level 0.05, \\(\\chi^2\\) (1) = 10.3, p = 0.07. Thus we have insufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. In this particular data sample, the odds ratio (Odds Ratio = 2.18, p = 0.055, 95% CI [0.982, +Inf]) indicates that the odds of producing a correct response on the first question were 2.18 times greater if a subject was in the impasse condition, than in the control condition .\n(Online Replication)\n\nCODE#online only\ndf <- df_q1 %>% filter(pretty_mode == \"online-replication\")\n\n#MOSAIC PLOT\n#note: blue indicates cell count higher than expected, red indicates cell count less than expected; under null hypothesis\nmosaicplot(main=\"Accuracy on First Question by Condition\",\n            data = df, pretty_condition ~ accuracy, shade = T)\n\n\n\nCODE# CrossTable( x = df$condition, y = df$score_niceABS, fisher = TRUE, \n#             chisq=TRUE, expected = TRUE, sresid = TRUE)\n\n\ndf %>%\n  sjtab(fun = \"xtab\", var.labels=c(\"accuracy\", \"pretty_condition\"),\n        show.row.prc=T, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,\n        statistics = \"fisher\")\n\n\n\n\naccuracy\n pretty_condition\n Total\n \n\ncontrol\n impasse\n \n\nincorrect\n\n847651.9 %87.5 %\n\n\n788648.1 %72.2 %\n\n\n162162100 %79.4 %\n \n\n\ncorrect\n\n122028.6 %12.5 %\n\n\n302271.4 %27.8 %\n\n\n4242100 %20.6 %\n \n\n\nTotal\n\n969647.1 %100 %\n\n\n10810852.9 %100 %\n\n\n204204100 %100 %\n \n\nχ2=6.351 · df=1 · Cramer's V=0.189 · Fisher's p=0.009 \n\n \n\n\n observed valuesexpected values% within accuracy% within pretty_condition\n\n\n\nFor online data collection (n=204), a Pearson’s Chi-squared test (of independence) indicates a statistically significant relationship between response accuracy on the first question and experimental condition, \\(\\chi^2\\) (1) = 7.26, p = 0.009. Thus we have sufficient evidence to reject the null hypothesis that the odds ratio is equal to 1. The odds ratio (Odds Ratio = 2.68, p = 0.005, 95% CI [1.37, +Inf]) indicates that the odds of producing a correct response on the first question were 2.68 times greater if a subject was in the impasse condition, than in the control condition .\nLOGISTIC REGRESSION\nFit a logistic regression (at the subject level), predicting Q1 accuracy (absolute score) by condition.\nFit Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\nCODE#combined\ndf <- df_items %>% filter(q==1) %>% mutate(\n  accuracy = as.factor(score_niceABS)\n)\n\n# FREQUENCY TABLE\n# my.table <- table(df$accuracy, df$pretty_condition)\n# addmargins(my.table) #counts\n# addmargins(prop.table(my.table)) #props\n\n# MODEL FITTING:::::::::::::::::::::::::::::::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(accuracy ~ 1, data = df, family = \"binomial\")\nprint(\"EMPTY MODEL\")\n\n[1] \"EMPTY MODEL\"\n\nCODEsummary(m0)\n\n\nCall:\nglm(formula = accuracy ~ 1, family = \"binomial\", data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.696  -0.696  -0.696  -0.696   1.753  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.294      0.134   -9.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 343.66  on 329  degrees of freedom\nAIC: 345.7\n\nNumber of Fisher Scoring iterations: 4\n\nCODE#: 2 CONDITION model\nm1 <- glm( accuracy ~ pretty_condition, data = df, family = \"binomial\")\nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsummary(m1)\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.819  -0.819  -0.548  -0.548   1.986  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.822      0.230   -7.93  2.2e-15 ***\npretty_conditionimpasse    0.901      0.285    3.16   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 333.07  on 328  degrees of freedom\nAIC: 337.1\n\nNumber of Fisher Scoring iterations: 4\n\nCODE#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > m1$aic)\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\nCODEtest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName | Model | df | df_diff |  Chi2 |     p\n-------------------------------------------\nm0   |   glm |  1 |         |       |      \nm1   |   glm |  2 |       1 | 10.59 | 0.001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0.00113745235691825\"\n\n\nThe Condition predictor significantly improves model fit.\nLearning Notes\n\nCODE# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsummary(m1)\n\n\nCall:\nglm(formula = accuracy ~ pretty_condition, family = \"binomial\", \n    data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.819  -0.819  -0.548  -0.548   1.986  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -1.822      0.230   -7.93  2.2e-15 ***\npretty_conditionimpasse    0.901      0.285    3.16   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 343.66  on 329  degrees of freedom\nResidual deviance: 333.07  on 328  degrees of freedom\nAIC: 337.1\n\nNumber of Fisher Scoring iterations: 4\n\nCODE#: INTERPRET COEFFICIENTS\n\nprint(\"Coefficients —- LOG ODDS\")\n\n[1] \"Coefficients —- LOG ODDS\"\n\nCODEconfint(m1)\n\nWaiting for profiling to be done...\n\n\n                         2.5 % 97.5 %\n(Intercept)             -2.299  -1.39\npretty_conditionimpasse  0.353   1.48\n\nCODEprint(\"Coefficients —- ODDS RATIOS\")\n\n[1] \"Coefficients —- ODDS RATIOS\"\n\nCODEe <- cbind( exp(coef(m1)), exp(confint(m1))) #exponentiate\n\nWaiting for profiling to be done...\n\nCODEe\n\n                              2.5 % 97.5 %\n(Intercept)             0.162  0.10  0.248\npretty_conditionimpasse 2.463  1.42  4.374\n\n\nUnderstanding the logistic regression model\nThe logistic regression intercept gives the log odds of the outcome for the reference level of the predictor variable\nThe logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.\n[the empty model\n\nThe intercept of an empty model (glm(accuracy ~ 1) is equal to log(p/(1-p)), where p = the overall probability of a correct response (df$accuracy ==1 ).\nIn SGC3A Q1 accuracy this = 71 correct / 330 = 0.215 -> log(0.215 / (1-0.215)) = -1.29.\nIn other words, the intercept from the model with no predictor variables is the estimated log odds of a correct response for the whole sample.\nWe can also transform the log of the odds back to a probability: p = ODDS/ (1+ODDS) = exp(-1.29)/(1+exp(-1.29)) = 0.215. This should matched the prediction of the empty model\n\n[a dichotomous predictor]\nnatural log (odds of +) = -1.822 + 0.901(x1) ; x1 = 0 for control, 1 for impasse\n\nINTERCEPT: log odds of (+ response) in control condition\n\nlog odds of (+) in control : -1.822 + 0.9(0) = -1.822\nconvert to odds by exponentiating the coefficients\nlog odds of (+) in control = exp(-1.822) = 0.162 odds\nconvert to probability by formula =>\np(+) = odds / (1+odds) = 0.162 / (1 + 0.162) = 0.139\nprobability of (+) in control = ~14%\n\n\nB1 COEFFICIENT: DIFFERENCE in log odds of (+) in impasse vs. control\n\nlog odds of (+) in impasse: -1.822 + 0.901 = -0.921\nconvert to odds by exponentiating log odds\nlog odds (+) in impasse = exp(-0.921) = 0.398\nconvert to probability by formula =>\np(+) = odds / (1 + odds) = 0.398 / (1+0.398) = 0.285\nprobaility of (+) in impasse = ~ 29%\n\n\nODDS RATIO : exponentiated B1 COEFFICIENT\n\nB1 = (slope of logit model = difference in log odds = log odds ratio\nB1 = 0.901 is log odds ratio of (+) in impasse vs control\nexp(b1) = exp(0.901) = 2.46\nRatio of odds in impasse are 2.46 times higher than in control. Bein in the impasse condition yields odds athat are 2.46 X higher than in control.\n\n\n\n\n\n\n\nMARGINAL\ntotal = 330 success : 71, failure : 259\np(+) = 71 / 330 = 0.215 = 22%\nodds(+) = 71 / 259 = 0.274\n\n\nCONTROL total = 158 success = 22; failure = 136\np(+) = 22/158 = 0.139 = 14%\nodds(+) = 22/136 = 0.162\n\n\nIMPASSE total = 172 success = 49; failure = 123\np(+) = 49/172 = 0.285 = 29%\nodds(+) = 49/123 = 0.398\n\n\nVisualize\n\nCODEprint(\"MODEL PERFORMANCE\")\n\n[1] \"MODEL PERFORMANCE\"\n\nCODEperformance(m1)\n\n# Indices of model performance\n\nAIC     |     BIC | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n----------------------------------------------------------------------------------------------\n337.074 | 344.673 |     0.031 | 0.404 | 1.008 |    0.505 |   -16.847 |           0.021 | 0.673\n\nCODEprint(\"SANITY CHECK REPORTING\")\n\n[1] \"SANITY CHECK REPORTING\"\n\nCODEreport(m1)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic model (estimated using ML) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model's explanatory power is weak (Tjur's R2 = 0.03). The model's intercept, corresponding to pretty_condition = control, is at -1.82 (95% CI [-2.30, -1.39], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.90, 95% CI [0.35, 1.48], p = 0.002; Std. beta = 0.90, 95% CI [0.35, 1.48])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\nCODEprint(\"MODEL PREDICTIONS\")\n\n[1] \"MODEL PREDICTIONS\"\n\nCODE# Retrieve predictions as probabilities \n# (for each level of the predictor)\np.control <- predict(m1,data.frame(pretty_condition=\"control\"),type=\"response\")\npaste(\"Probability of success in control,\", p.control)\n\n[1] \"Probability of success in control, 0.139240506329147\"\n\nCODEp.impasse <- predict(m1,data.frame(pretty_condition=\"impasse\"),type=\"response\")\npaste(\"Probability of success in impasse,\", p.impasse)\n\n[1] \"Probability of success in impasse, 0.284883720930631\"\n\nCODE#: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \n# library(ggstatsplot)\n# ggcoefstats(m1, output = \"plot\") + labs(x = \"Log Odds Estimate\")\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(m1, type=\"std2\", vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\nCODE#SJPLOT | MODEL | PROBABILITIES\n# plot_model(m1, type=\"pred\",\n#            show.intercept = TRUE, \n#            show.values = TRUE,\n#            title = \"Model Predicted Probability of Accuracy\",\n#            axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\nggeffect(model = m1) %>% plot()\n\n$pretty_condition\n\n\n\n\nCODE#SANITY CHECK SJPLOT\n# library(effects)\n# plot(allEffects(m))\n\n\nDiagnostics\n\nCODEcheck_model(m1)\n\n\n\nCODEbinned_residuals(m1)\n\nOk: About 100% of the residuals are inside the error bounds.\n\n\n\n\n\nInference\nWe fit a logistic regression model to analyze the effect of experimental condition on probability of a correct answer on the first question. In this model, the effect of condition is statistically significant (z = 3.16, p = 0.0016). The model predicts that the odds of a correct response on the first question in the impasse condition increase by 146% (\\(e^{beta_1}\\) = 2.46, 95% CI [1.42, 4.37]) over the control condition.\nEquivalent statements:\n\nbeing in impasse condition increases log odds of correct response by 0.901 (over control)\nbeing in impasse increases odds of correct response in impasse over control increases by factor of 2.46\nprobability of correct response in control predicted as 28.5%, vs only 14% in control condition\n\n\nCODE#PRETTY TABLE SJPLOT\ntab_model(m1)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.16\n0.10 – 0.25\n<0.001\n\n\npretty condition[impasse]\n2.46\n1.42 – 4.37\n0.002\n\n\nObservations\n330\n\n\nR2 Tjur\n0.031\n\n\n\n\nTODO ORDINAL REGRESSION\nFit an ordinal logistic regression (at the subject level), predicting Q1 interpretation by condition.\n\nhttps://stats.oarc.ucla.edu/r/faq/ologit-coefficients/\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\ntodo see ordinal regression video\n\n\nCODE# #CREATE DATAFRAME OF Q1\n# df <- df_items %>% filter(q ==1) %>% mutate(scaled = as.factor(score_SCALED))\n# \n# #MODEL\n# m <- polr(scaled ~ condition , data = df, Hess=TRUE)\n# summary(m)\n# confint(m)\n# performance(m)\n# report(m)\n# \n# #exponentiate coefficients and CIs \n# ci <- confint(m)\n# ci\n# e <- coef(m)\n# e\n# # exp(cbind(e,ci))\n# \n# # Retrieve predictions as probabilities \n# # (for each level of the predictor)\n# # p.control <- predict(m,data.frame(condition=\"111\"),type=\"response\")\n# # paste(\"Probability of success in control,\", p.control)\n# # p.impasse <- predict(m,data.frame(condition=\"121\"),type=\"response\")\n# # paste(\"Probability of success in impasse,\", p.impasse)\n# \n# # Plot Predicted data and original data points\n# # ggplot(df, aes(x=condition, y=accuracy)) + \n# #   geom_point() +\n# #   stat_smooth(method=\"glm\", color=\"green\", se=FALSE,\n# #                 method.args = list(family=binomial))\n#   \n# #TO PLOT ALL EFFECTS\n# library(effects)\n# plot(allEffects(m))\n# \n# #SJPLOT\n# library(sjPlot)\n# plot_model(m, )\n# \n# \n# #CONVERT TO PROBABILITIES\n# newdat <- data.frame(condition=c(\"111\",\"121\"))\n# prob <- (phat <- predict(object = m, newdat, type=\"p\"))\n# prob\n#"
  },
  {
    "objectID": "analysis/SGC3A/x_learning.html#repeated-item-level",
    "href": "analysis/SGC3A/x_learning.html#repeated-item-level",
    "title": "Modelling Reference",
    "section": "REPEATED ITEM LEVEL",
    "text": "REPEATED ITEM LEVEL\nTest Phase Accuracy (absolute score)\nMixed Logistic Regression\nFit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on test phase questions by condition; accounting for random effects of subject.\nFit Model\n\nCODE#SETUP DATA \n#PREPARE DATA \nn_items = 8 #number of items in test\n\n#item level\ndf_test = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  accuracy = as.factor(score_niceABS),\n  q = as.factor(q)\n)\n\ndf <- df_test\n\nlibrary(lmerTest) #for CIs in glmer \n\n## 1 | SETUP RANDOM EFFECT\n\n#:: EMPTY MODEL (baseline, no random effect)\nm0 = glm(accuracy ~ 1, family = \"binomial\", data = df) \n\n#:: RANDOM INTERCEPT SUBJECT\nmm.rS <- glmer(accuracy ~ (1|subject), data = df,family = \"binomial\")\n\n# :: TEST random effect\npaste(\"AIC with random effect is lower than glm empty model?\", m0$aic > AIC(logLik(mm.rS)))\n\n[1] \"AIC with random effect is lower than glm empty model? TRUE\"\n\nCODEtest_lrt(m0,mm.rS) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName  |    Model | df | df_diff |    Chi2 |      p\n--------------------------------------------------\nm0    |      glm |  1 |         |         |       \nmm.rS | glmerMod |  2 |       1 | 1783.73 | < .001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.rS))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0\"\n\nCODE## 2 | ADD FIXED EFFECT\n\n# SUBJECT INTERCEPT | FIXED CONDITION \nmm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject), \n                data = df,family = \"binomial\")\n\n# :: TEST fixed factor \npaste(\"AIC with random effect is lower than glm empty model?\", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )\n\n[1] \"AIC with random effect is lower than glm empty model? TRUE\"\n\nCODEtest_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName   |    Model | df | df_diff | Chi2 |     p\n-----------------------------------------------\nmm.rS  | glmerMod |  2 |         |      |      \nmm.CrS | glmerMod |  3 |       1 | 4.98 | 0.026\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.rS,mm.CrS))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0.0256331468201315\"\n\n\nVisualize\n\nCODE#: PRINT MODEL \nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsummary(mm.CrS)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: accuracy ~ pretty_condition + (1 | subject)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    1385     1402     -689     1379     2637 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5307 -0.0193 -0.0097  0.1135  2.7426 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 117      10.8    \nNumber of obs: 2640, groups:  subject, 330\n\nFixed effects:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -9.182      0.677  -13.56   <2e-16 ***\npretty_conditionimpasse    1.632      0.753    2.17     0.03 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nprtty_cndtn -0.394\n\nCODE#: INTERPRET COEFFICIENTS\n\nprint(\"MODEL PERFORMANCE\")\n\n[1] \"MODEL PERFORMANCE\"\n\nCODEperformance(mm.CrS)\n\n# Indices of model performance\n\nAIC      |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n--------------------------------------------------------------------------------------------------------------\n1384.749 | 1402.385 |      0.973 |      0.005 | 0.973 | 0.203 | 1.000 |    0.130 |      -Inf |           0.015\n\nCODEprint(\"SANITY CHECK REPORTING\")\n\n[1] \"SANITY CHECK REPORTING\"\n\nCODEreport(mm.CrS)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict accuracy with pretty_condition (formula: accuracy ~ pretty_condition). The model included subject as random effect (formula: ~1 | subject). The model's total explanatory power is substantial (conditional R2 = 0.97) and the part related to the fixed effects alone (marginal R2) is of 5.50e-03. The model's intercept, corresponding to pretty_condition = control, is at -9.18 (95% CI [-10.51, -7.85], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 1.63, 95% CI [0.16, 3.11], p = 0.030; Std. beta = 1.63, 95% CI [0.16, 3.11])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#: PLOT\n\n#SJPLOT | MODEL | ODDS RATIO\n#library(sjPlot)\nplot_model(mm.CrS, type=\"std2\", vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\nCODE#SJPLOT | MODEL | PROBABILITIES\nplot_model(mm.CrS, type=\"pred\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n$pretty_condition\n\n\n\n\nCODE#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\n# ggeffect(model = mm.CrS) %>% plot()\n\n#SANITY CHECK SJPLOT\n# library(effects)\n# plot(allEffects(mm.CrS))\n\n\nDiagnostics\n\nCODEcheck_model(mm.CrS)\n\n\n\nCODEbinned_residuals(mm.CrS)\n\nWarning: Probably bad model fit. Only about 75% of the residuals are inside the error bounds.\n\n\n\n\n\nInference\nWe fit a mixed-effect binomial logistic regression model with random intercepts for subjects to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(3): 4.98, p < 0.05). Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 5 over the control condition \\(e^{\\beta_1}\\) = 5.11, 95% CI [1.17,22,36], p < 0.05.\n\nCODE# PRETTY TABLE SJPLOT\ntab_model(mm.CrS)\n\n\n\n\n \naccuracy\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.00\n<0.001\n\n\npretty condition: impasse\n5.11\n1.17 – 22.36\n0.030\n\n\nRandom Effects\n\n\nσ2\n\n3.29\n\n\nτ00subject\n\n116.99\n\n\n\nICC\n0.97\n\n\n\nN subject\n\n330\n\n\nObservations\n2640\n\n\nMarginal R2 / Conditional R2\n\n0.005 / 0.973\n\n\n\n\nTODO Mixed Ordinal Regression"
  },
  {
    "objectID": "analysis/SGC3A/x_learning.html#subject-level",
    "href": "analysis/SGC3A/x_learning.html#subject-level",
    "title": "Modelling Reference",
    "section": "SUBJECT-LEVEL",
    "text": "SUBJECT-LEVEL\nTest Phase Absolute Score (# questions)\n\nCODE#PREPARE DATA \nn_items = 8 #number of items in test\n\n#item level\ndf = df_items %>% filter(q %nin% c(1,2,3,4,5,6,9)) %>% mutate(\n  accuracy = recode_factor(score_niceABS, \"0\" =\"incorrect\",\"1\"=\"correct\"),\n  q = as.factor(q)\n)\n\n#STACKED PROPORTIONAL BAR CHART\ndf %>% \n  ggplot(data = .,\n         mapping = aes(x = pretty_condition,\n                       fill = accuracy)) +\n  geom_bar(position = \"fill\" ) + #,color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\")  +\n  facet_wrap(~pretty_mode) + \n   labs(#y = \"\",\n       title = \"Accuracy on Test Phase\",\n       x = \"Condition\",\n       fill = \"\",\n       subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n\n\nCODE#GROUPED PROPORTIONAL BAR CHART\n# gf_props(~accuracy, fill = ~pretty_condition, x =~pretty_condition,\n#        position = position_dodge(), data = df) %>% \n#   gf_facet_grid(~pretty_mode) +\n#    labs(x = \"Correct Responses in Test Phase\",\n#        title = \"Accuracy on Task by Condition\",\n#        subtitle=\"Impasse Condition yields a greater proportion of correct responses\")\n\n \n#FACETED HISTOGRAM\nstats = df_subjects %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_test_NABS))\ngf_props(~item_test_NABS, \n         fill = ~pretty_condition, data = df_subjects) %>% \n  gf_facet_grid(pretty_condition ~ pretty_mode) %>% \n  gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(x = \"# Correct\",\n       y = \"proportion of subjects\",\n       title = \"Test Phase Absolute Score (# Correct)\",\n       subtitle = \"\") + theme(legend.position = \"blank\")\n\n\n\n\nLinear Regression\nLM on Test Phase absolute score as number of questions, rather than % correct.\n\nCODE#SCORE predicted by CONDITION\nlm.1 <- lm(item_test_NABS ~ pretty_condition, data = df_subjects)\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(lm.1)\n\n\nCall:\nlm(formula = item_test_NABS ~ pretty_condition, data = df_subjects)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3.02  -2.77  -1.52   2.98   6.48 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                1.519      0.251    6.04  4.1e-09 ***\npretty_conditionimpasse    1.498      0.348    4.30  2.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.16 on 328 degrees of freedom\nMultiple R-squared:  0.0535,    Adjusted R-squared:  0.0506 \nF-statistic: 18.5 on 1 and 328 DF,  p-value: 0.0000222\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(lm.1)\n\nAnalysis of Variance Table\n\nResponse: item_test_NABS\n                  Df Sum Sq Mean Sq F value   Pr(>F)    \npretty_condition   1    185     185    18.5 0.000022 ***\nResiduals        328   3274      10                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(lm.1)\n\n                        2.5 % 97.5 %\n(Intercept)             1.025   2.01\npretty_conditionimpasse 0.814   2.18\n\nCODEreport(lm.1) #sanity check\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.05, F(1, 328) = 18.52, p < .001, adj. R2 = 0.05). The model's intercept, corresponding to pretty_condition = control, is at 1.52 (95% CI [1.02, 2.01], t(328) = 6.04, p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 1.50, 95% CI [0.81, 2.18], t(328) = 4.30, p < .001; Std. beta = 0.46, 95% CI [0.25, 0.67])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODEcheck_model(lm.1)\n\n\n\n\n\nCODE#MODEL ESTIMATES WITH UNCERTAINTY\n\n#setup references\nm <- lm.1\ndf <- df_subjects\ncall <- m$call %>% as.character()\n\n# uncertainty model visualization\ndf  %>%\n  modelr::data_grid(pretty_condition) %>%\n  augment(lm.1, newdata = ., se_fit = TRUE) %>%\n  ggplot(aes(y = pretty_condition, color = pretty_condition)) +\n  stat_halfeye( scale = .5,\n      aes(\n        xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n        fill = stat(cut_cdf_qi(cdf,\n                .width = c(.90, .95),\n                labels = scales::percent_format())))) +\n  scale_fill_brewer(direction = -1) +\n  labs (title = \"(LAB) Test Phase Accuracy ~ Condition\",\n        x = \"model predicted mean (% correct)\", y = \"Condition\", fill = \"Interval\",\n        subtitle = paste(\"lm(\",call[2],\")\")\n  ) + theme(legend.position = \"blank\")\n\n\n\n\nPoisson Regression\nhttps://stats.oarc.ucla.edu/r/dae/poisson-regression/\nThe outcome variable absolute score is clearly not normal. As it represents the cumulative number of items a participant has answered correctly, we can consider it a type of count, (ie. count of the number of questions the participant got correct) and attempt to model it using a General Linear Model with the Poisson distribution (and the default log-link function).\n\nCODE#POISSON\n\n#SCORE predicted by CONDITION --> POISSON DISTRIBUTION\np.1 <- glm(item_test_NABS ~ pretty_condition, data = df_subjects, family = \"poisson\")\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(p.1)\n\n\nCall:\nglm(formula = item_test_NABS ~ pretty_condition, family = \"poisson\", \n    data = df_subjects)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -2.46   -2.28   -1.74    1.51    3.69  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               0.4180     0.0645    6.48  9.4e-11 ***\npretty_conditionimpasse   0.6864     0.0781    8.79  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1579.3  on 329  degrees of freedom\nResidual deviance: 1496.7  on 328  degrees of freedom\nAIC: 1956\n\nNumber of Fisher Scoring iterations: 6\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(p.1)\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: item_test_NABS\n\nTerms added sequentially (first to last)\n\n                 Df Deviance Resid. Df Resid. Dev\nNULL                               329       1579\npretty_condition  1     82.7       328       1497\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(p.1)\n\nWaiting for profiling to be done...\n\n\n                        2.5 % 97.5 %\n(Intercept)             0.289  0.542\npretty_conditionimpasse 0.535  0.841\n\nCODEreport(p.1) #sanity check\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a poisson model (estimated using ML) to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model's explanatory power is moderate (Nagelkerke's R2 = 0.22). The model's intercept, corresponding to pretty_condition = control, is at 0.42 (95% CI [0.29, 0.54], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.69, 95% CI [0.53, 0.84], p < .001; Std. beta = 0.69, 95% CI [0.53, 0.84])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\nCODEcheck_model(p.1)\n\n\n\n\nZero Inflated Poisson\nhttps://stats.oarc.ucla.edu/r/dae/zip/\nPoisson count process with excess zeros\n\nCODE#ZERO INFLATED POISSON\n\nzinfp.1 <- zeroinfl(item_test_NABS ~  item_q1_rt| pretty_condition , data = df_subjects)\nsummary(zinfp.1)\n\n\nCall:\nzeroinfl(formula = item_test_NABS ~ item_q1_rt | pretty_condition, data = df_subjects)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.934 -0.821 -0.548  0.965  2.421 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 1.654243   0.059975   27.58   <2e-16 ***\nitem_q1_rt  0.001690   0.000849    1.99    0.047 *  \n\nZero-inflation model coefficients (binomial with logit link):\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                0.978      0.179    5.46  4.7e-08 ***\npretty_conditionimpasse   -1.055      0.236   -4.48  7.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -531 on 4 Df\n\nCODEreport(zinfp.1)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a zero-inflated poisson model to predict item_test_NABS with item_q1_rt and pretty_condition (formula: item_test_NABS ~ item_q1_rt). The model's explanatory power is substantial (R2 = 0.35, adj. R2 = 0.35). The model's intercept, corresponding to item_q1_rt = 0, is at 1.65 (95% CI [1.54, 1.77], p < .001). Within this model:\n\n  - The effect of item q1 rt is statistically significant and positive (beta = 1.69e-03, 95% CI [2.52e-05, 3.35e-03], p = 0.047; Std. beta = 0.06, 95% CI [7.11e-04, 0.12])\n  - The effect of pretty condition [impasse] is statistically significant and negative (beta = -1.06, 95% CI [-1.52, -0.59], p < .001; Std. beta = -1.06, 95% CI [-1.52, -0.59])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\nCODEperformance(zinfp.1)\n\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------\n1070.173 | 1085.370 | 0.354 |     0.350 | 3.131 | 3.150 |    -1.609 |           0.044\n\nCODE# check_model(zinfp.1)\n\n\nNegative Binomial Regression\nhttps://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/ - overdispersed count data (variance much greater than mean)\n\nCODE#NEGATIVE BIONOMIAL REGRESSION\n# - https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/\n# - Overdispersed Count variables\n\nlibrary(MASS)\n\nWarning: package 'MASS' was built under R version 4.1.2\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nCODEnb.1 <- glm.nb(item_test_NABS ~ pretty_condition, data = df_subjects)\nsummary(nb.1)\n\n\nCall:\nglm.nb(formula = item_test_NABS ~ pretty_condition, data = df_subjects, \n    init.theta = 0.253501538, link = log)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.139  -1.102  -0.993   0.378   1.091  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)   \n(Intercept)                0.418      0.171    2.45   0.0143 * \npretty_conditionimpasse    0.686      0.232    2.95   0.0031 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.254) family taken to be 1)\n\n    Null deviance: 279.52  on 329  degrees of freedom\nResidual deviance: 270.97  on 328  degrees of freedom\nAIC: 1194\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2535 \n          Std. Err.:  0.0315 \n\n 2 x log-likelihood:  -1188.1290 \n\nCODEreport(nb.1)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a negative-binomial model (estimated using ML) to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model's explanatory power is weak (Nagelkerke's R2 = 0.04). The model's intercept, corresponding to pretty_condition = control, is at 0.42 (95% CI [0.10, 0.77], p = 0.014). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.69, 95% CI [0.23, 1.14], p = 0.003; Std. beta = 0.69, 95% CI [0.23, 1.14])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\nCODEcheck_model(nb.1)\n\n\n\nCODE#check model assumption\n#assumes conditional means are not equal to conditional variances\n#conduct likelihood ration test to compare and test [need poisson]\nm3 <- glm(item_test_NABS ~ pretty_condition, family = \"poisson\", data = df_subjects)\npchisq(2 * (logLik(nb.1) - logLik(m3)), df = 1, lower.tail = FALSE)\n\n'log Lik.' 4.3e-168 (df=3)\n\nCODE#A large (+) log likelihood suggests that the negative binomial is more appropriate than the Poisson model\n\n\n#EXPONENTIATE PARAMETER ESTIMATES\nest <- cbind(Estimate = coef(nb.1), confint(nb.1))\n\nWaiting for profiling to be done...\n\nCODE#exponentiate parameter estimates\nprint(\"Exponentiated Estimates\")\n\n[1] \"Exponentiated Estimates\"\n\nCODEexp(est)\n\n                        Estimate 2.5 % 97.5 %\n(Intercept)                 1.52  1.10   2.15\npretty_conditionimpasse     1.99  1.26   3.13\n\n\nThe variable condition has a coefficient of 0.67, (p < 0.005). This means that for the impasse condition, the expected log count # of questions increases by 0.67. By exponentiating the estimate we see that # question correct rate for the impasse condition is nearly 2x that of the control condition.\nDiagnostics ??\nZero Inflated Negative Binomial Regression\nhttps://stats.oarc.ucla.edu/r/dae/zinb/ count data that are overdispersed and have excess zeros\nZero-inflated negative binomial regression is for modelling count variables with excessive zeros, and especially when the count data are overdispersed (mean is much larger than variance). It can help account for situations where theory suggests that excess zeros are generated by 2 separate processes, one that includes the other count values, and the other that is just the zeros, and thus that the excess zeros can be modelled independently.\nTotal Absolute Score (# items correct) may fit this situation, as the data are overdispersed (variance much greater than the mean) and there are are very large number of zeros. It is theoretically plausible that these excess zeros (no answers correct) are the result of a different ‘process’ … (i.e) little understanding and/or resistance to restructuring understanding of the coordinate system. However, I am not certain if it is plausible to suggest that the zeros themselves are the result of two different processes: (ie. perhaps trying to understand, and not trying to understand?) <- this could maybe be disentangled by first question latency?\nThe model includes: - A logistic model to model which of the two processes the zero outcome is associated with - A negative binomial model to model the count process\n\nCODElibrary(pscl) #  for zeroinfl negbinomial\n\n#ZERO INFLATED NEGATIVE BINOMIAL\nzinb.1 <- zeroinfl(item_test_NABS ~ pretty_condition | pretty_condition , data = df_subjects, dist = \"negbin\")\n#before the | is the count part, after the | is the logit model\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(zinb.1)\n\n\nCall:\nzeroinfl(formula = item_test_NABS ~ pretty_condition | pretty_condition, \n    data = df_subjects, dist = \"negbin\")\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.866 -0.794 -0.538  0.856  2.294 \n\nCount model coefficients (negbin with log link):\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               1.7126     0.0728   23.54  < 2e-16 ***\npretty_conditionimpasse   0.0451     0.0880    0.51  0.60810    \nLog(theta)                3.1851     0.8732    3.65  0.00026 ***\n\nZero-inflation model coefficients (binomial with logit link):\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                0.974      0.179    5.43  5.5e-08 ***\npretty_conditionimpasse   -1.056      0.236   -4.47  7.7e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 24.169 \nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -532 on 5 Df\n\nCODEreport(zinb.1)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a zero-inflated negative-binomial model to predict item_test_NABS with pretty_condition (formula: item_test_NABS ~ pretty_condition). The model's explanatory power is substantial (R2 = 0.36, adj. R2 = 0.36). The model's intercept, corresponding to pretty_condition = control, is at 1.71 (95% CI [1.57, 1.86], p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically non-significant and positive (beta = 0.05, 95% CI [-0.13, 0.22], p = 0.608; Std. beta = 0.05, 95% CI [-0.13, 0.22])\n  - The effect of pretty condition [impasse] is statistically significant and negative (beta = -1.06, 95% CI [-1.52, -0.59], p < .001; Std. beta = -1.06, 95% CI [-1.52, -0.59])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\nCODEperformance(zinb.1)\n\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------\n1073.880 | 1092.876 | 0.363 |     0.359 | 3.150 | 3.174 |    -1.649 |           0.043\n\nCODE#   rootogram(zinb.1)\n\n\n\n# #EXPONENTIATE PARAMETER ESTIMATES\n# est <- cbind(Estimate = coef(zinb.1), confint(zinb.1))\n# #exponentiate parameter estimates\n# print(\"Exponentiated Estimates\")\n# exp(est)\n\n\nIn the count model, the coefficient for the condition is very small, and not significant (suggesting it does not contribute to the count yielding process?).\nIn the zero-inflation model, the coefficient for the condition variable is -1.056 and statistically significant. This suggests that the log odds of being an excessive zero decrease by 1.06 if you are in the impasse condition (exponentiate it?)\nTODO come back to this and discuss further\nTobit Regression\nhttps://stats.oarc.ucla.edu/r/dae/tobit-models/\nFor censored data (i.e. truncated axis). The tobit model, also called a censored regression model, is designed to estimate linear relationships between variables when there is either left- or right-censoring in the dependent variable (also known as censoring from below and above, respectively). Censoring from above takes place when cases with a value at or above some threshold, all take on the value of that threshold, so that the true value might be equal to the threshold, but it might also be higher. In the case of censoring from below, values those that fall at or below some threshold are censored.\n\nCODE#set up data \ndf <- df_subjects %>% mutate(\n  accuracy = s_NABS\n)\n\nlibrary(VGAM)\n\nWarning: package 'VGAM' was built under R version 4.1.2\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\n\nAttaching package: 'VGAM'\n\n\nThe following object is masked from 'package:distributional':\n\n    cdf\n\nCODEt <- vglm(accuracy ~ condition, tobit(Upper = 13), data = df)\nsummary(t)\n\n\nCall:\nvglm(formula = accuracy ~ condition, family = tobit(Upper = 13), \n    data = df)\n\nCoefficients: \n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept):1  -2.3913     0.8854   -2.70   0.0069 ** \n(Intercept):2   2.2155     0.0653   33.93   <2e-16 ***\ncondition121    5.9518     1.1742    5.07    4e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: mu, loglink(sd)\n\nLog-likelihood: -677 on 657 degrees of freedom\n\nNumber of Fisher scoring iterations: 7 \n\nNo Hauck-Donner effect found in any of the estimates\n\nCODEplot(t)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison\n\nCODEcompare_performance(lm.1, p.1, nb.1, zinb.1)\n\n# Comparison of Model Performance Indices\n\nName   |    Model |      AIC | AIC weights |      BIC | BIC weights |  RMSE | Sigma | Score_log | Score_spherical |    R2 | R2 (adj.) | Nagelkerke's R2\n-------------------------------------------------------------------------------------------------------------------------------------------------------\nlm.1   |       lm | 1699.782 |     < 0.001 | 1711.179 |     < 0.001 | 3.150 | 3.160 |           |                 | 0.053 |     0.051 |                \np.1    |      glm | 1955.788 |     < 0.001 | 1963.386 |     < 0.001 | 3.150 | 2.136 |    -2.957 |           0.042 |       |           |           0.223\nnb.1   |   negbin | 1194.129 |     < 0.001 | 1205.526 |     < 0.001 | 3.150 | 0.909 |    -2.137 |           0.046 |       |           |           0.045\nzinb.1 | zeroinfl | 1073.880 |        1.00 | 1092.876 |        1.00 | 3.150 | 3.174 |    -1.649 |           0.043 | 0.363 |     0.359 |                \n\n\nFor modelling test phase absolute score (# items correct) it seems that the zero inflated negative binomial model is the best fit according to R2 and AIC, however, I am not clear on the implications of the interpretation (non significant in count process, significant on logit process), and also not clear if # items correct is truly a count process.\n\nCODE#uncertainty model visualization\n# df %>%\n  # data_grid(pretty_condition) %>%\n  # augment(m, newdata = ., se_fit = TRUE) %>%\n  # ggplot(aes(y = pretty_condition)) +\n  # stat_halfeye(\n  #   aes(xdist = dist_student_t(df = df.residual(m), \n  #       mu = .fitted, sigma = .se.fit)), scale = .5) +\n  # # add raw data in too (scale = .5 above adjusts the halfeye height so\n  # # that the data fit in as well)\n  # geom_jitter(aes(x = x), data = df, pch = \"|\", size = 2, \n  #             position =   position_nudge(y = -.15), alpha = 0.5) +  \n  # labs (title = \"Model Estimates with Uncertainty\", x = \"model coefficient\") + \n  # theme_minimal()\n\n\nHURLDE BETA Regression\nhttps://github.com/markhwhiteii/beta_hurdle/blob/master/manuscript/beta_hurdle.pdf\n\nCODElibrary(gamlss)\n\nWarning: package 'gamlss' was built under R version 4.1.2\n\n\nLoading required package: gamlss.data\n\n\n\nAttaching package: 'gamlss.data'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nLoading required package: gamlss.dist\n\n\nWarning: package 'gamlss.dist' was built under R version 4.1.2\n\n\nLoading required package: nlme\n\n\nWarning: package 'nlme' was built under R version 4.1.2\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\n\nLoading required package: parallel\n\n\n **********   GAMLSS Version 5.4-3  ********** \n\n\nFor more on GAMLSS look at https://www.gamlss.com/\n\n\nType gamlssNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'gamlss'\n\n\nThe following object is masked from 'package:lme4':\n\n    refit\n\nCODE#CREATE SAMPLE DATA \nn <- 5000 \nmu <- 0.40 \nsigma <- 0.60 \np0 <- 0.13 \np1 <- 0.17 \np2 <- 1- p0- p1\na <- mu * (1- sigma ^ 2) / (sigma ^ 2) \nb <- a * (1- mu) / mu\n\n#CREATE DIST\nset.seed(1839) \ny <- rbeta(n, a, b) \ncat <- sample(1:3, n, prob = c(p0, p2, p1), replace = TRUE) \ny[cat == 1] <- 0 \ny[cat == 3] <- 1\n\n#VISUALIZE DISTRIBUTION\nx <- as.data.frame(y)\ngf_histogram(~x$y)\n\n\n\nCODE#this looks not unlike my distribution! \n\n#CREATE AN EMPTY MODEL\nfit <- gamlss( formula = y ~ 1, # formula for mu \n               formula.sigma = ~ 1, # formula for sigma \n               formula.nu = ~ 1, # formula for nu \n               formula.tau = ~ 1, # formula for tau \n               family = BEINF() )\n\nGAMLSS-RS iteration 1: Global Deviance = 7799 \nGAMLSS-RS iteration 2: Global Deviance = 7778 \nGAMLSS-RS iteration 3: Global Deviance = 7778 \nGAMLSS-RS iteration 4: Global Deviance = 7778 \n\nCODEsummary(fit)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = y ~ 1, family = BEINF(), formula.sigma = ~1,  \n    formula.nu = ~1, formula.tau = ~1) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.3796     0.0196   -19.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.3951     0.0162    24.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.632      0.042   -38.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.4014     0.0382   -36.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  5000 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  4996 \n                      at cycle:  4 \n \nGlobal Deviance:     7778 \n            AIC:     7786 \n            SBC:     7812 \n******************************************************************\n\nCODEplot(fit)\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  0.000571 \n                       variance   =  1 \n               coef. of skewness  =  0.0294 \n               coef. of kurtosis  =  2.95 \nFilliben correlation coefficient  =  1 \n******************************************************************\n\nCODE#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nfit_mu <- inv_logit(fit$mu.coefficients) \npaste(\"MU: \",fit_mu)\n\n[1] \"MU:  0.406229902102452\"\n\nCODEfit_sigma <- inv_logit(fit$sigma.coefficients) \npaste(\"SIGMA: \",fit_sigma)\n\n[1] \"SIGMA:  0.597499259410111\"\n\nCODEfit_nu <- exp(fit$nu.coefficients) \nfit_tau <- exp(fit$tau.coefficients) \nfit_p0 <- fit_nu / (1 + fit_nu + fit_tau) \npaste(\"P0: \",fit_p0)\n\n[1] \"P0:  0.135600165493784\"\n\nCODEfit_p1 <- fit_tau / (1 + fit_nu + fit_tau)\npaste(\"P1: \",fit_p1)\n\n[1] \"P1:  0.170800000002391\"\n\n\nBETA HURDLE INTERPRETATION - beta component\n- MU “location” (mean)\n- SIGMA “scale” (positively related to variance; variance = sigma.squared mean (1-mean)\n- Rigby, Stasinopoulos, Heller, and De Bastiani (2017) “reparameterized” the beta distribution so that the two parameters determining the shape of the distribution would be more useful in a regression framework (see Ferrari & Cribari-Neto, 2004 for a different parameterization)\nZERO-ONE HURDLE COMPONENT\n- The two additional parameters, ν NU and τTAU , are related to p0 and p1, respectively.\n- p0 is the probability that a case equals 0,\n- p1 is the probability that a case equals 1,\n- p2 (i.e., 1 −p0 −p1) is the probability that the case comes from the beta distribution\n\nCODE#SETUP DATA \n\nmin = 0 #min possible value of scale\nmax = 8 #max possible value of scale\n\nlibrary(mosaic) #for shuffling\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following objects are masked from 'package:VGAM':\n\n    chisq, logit\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following object is masked from 'package:lmerTest':\n\n    rand\n\n\nThe following object is masked from 'package:lme4':\n\n    factorize\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:modelr':\n\n    resample\n\n\nThe following object is masked from 'package:vcd':\n\n    mplot\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nThe following object is masked from 'package:cowplot':\n\n    theme_map\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nCODE#1. Rescale accuracy using \n# recommended adjustment \n#rescaled = value-min/(max-min)\ndf <- df_subjects %>% mutate(\n  accuracy = item_test_NABS,\n  R_acc = (accuracy-min)/(max-min), #as %\n  T_acc = (accuracy * (nrow(df)-1) + 0.5)/nrow(df)/8, #transform for no 0 and 1\n  perm = shuffle(condition),\n  scaffold_rt = item_scaffold_rt\n) %>% dplyr::select(accuracy,R_acc, T_acc, condition, perm,scaffold_rt)\n\n#VISUALIZE DISTRIBUTION\ngf_histogram(~R_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = \"Histogram of accuracy\")\n\n\n\nCODE#VISUALIZE DISTRIBUTION\ngf_histogram(~T_acc, fill = ~condition, data = df) %>% gf_facet_wrap(~condition) + labs(title = \"Histogram of [rescaled] accuracy\")\n\n\n\nCODEgf_histogram(~R_acc, fill = ~perm, data = df) %>% gf_facet_wrap(~perm) + labs(title = \"Histogram of shuffled accuracy\")\n\n\n\nCODE#SUMMARIZE SAMPLE\npaste(\"Grand mean\", mean(df$R_acc))\n\n[1] \"Grand mean 0.2875\"\n\nCODElibrary(mosaic)\nstats = favstats(df$R_acc ~ df$condition)\nstats$mean <- mean(df$R_acc ~ df$condition)\nstats$var <- var(df$R_acc ~ df$condition)\nprint(\"Grand stats\")\n\n[1] \"Grand stats\"\n\nCODEstats \n\n  df$condition min Q1 median    Q3 max  mean    sd   n missing   var\n1          111   0  0  0.000 0.125   1 0.190 0.358 158       0 0.128\n2          121   0  0  0.125 0.875   1 0.377 0.426 172       0 0.182\n\nCODEprint(\"P0\")\n\n[1] \"P0\"\n\nCODEnrow(df %>% filter(R_acc ==0))/nrow(df)\n\n[1] 0.6\n\nCODEprint(\"P1\")\n\n[1] \"P1\"\n\nCODEnrow(df %>% filter(R_acc ==1))/nrow(df)\n\n[1] 0.136\n\nCODE#CREATE MODEL\n\n#CREATE AN EMPTY MODEL\nm0 <- gamlss( formula = R_acc ~ 1, # formula for mu \n              formula.sigma =  ~ 1, # formula for sigma \n              formula.nu =  ~ 1, # formula for nu \n              formula.tau =  ~ 1, # formula for tau \n              family = BEINF(), data = df )\n\nGAMLSS-RS iteration 1: Global Deviance = 610 \nGAMLSS-RS iteration 2: Global Deviance = 609 \nGAMLSS-RS iteration 3: Global Deviance = 609 \nGAMLSS-RS iteration 4: Global Deviance = 609 \nGAMLSS-RS iteration 5: Global Deviance = 609 \n\nCODEm0 <- gamlss(R_acc ~ 1, ~ 1, ~ 1, ~ 1, \n            data = df, family = BEINF())\n\nGAMLSS-RS iteration 1: Global Deviance = 610 \nGAMLSS-RS iteration 2: Global Deviance = 609 \nGAMLSS-RS iteration 3: Global Deviance = 609 \nGAMLSS-RS iteration 4: Global Deviance = 609 \nGAMLSS-RS iteration 5: Global Deviance = 609 \n\nCODEsummary(m0)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ 1, sigma.formula = ~1, nu.formula = ~1,  \n    tau.formula = ~1, family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    0.225      0.113    1.98    0.048 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    0.177      0.100    1.76    0.079 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    0.822      0.129    6.39  5.7e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -0.660      0.184   -3.59  0.00038 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  4\n      Residual Deg. of Freedom:  326 \n                      at cycle:  5 \n \nGlobal Deviance:     609 \n            AIC:     617 \n            SBC:     632 \n******************************************************************\n\nCODEplot(m0)\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  0.0109 \n                       variance   =  0.996 \n               coef. of skewness  =  -0.0563 \n               coef. of kurtosis  =  2.79 \nFilliben correlation coefficient  =  0.998 \n******************************************************************\n\nCODE#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nm0_mu <- inv_logit(m0$mu.coefficients) \npaste(\"MU: \",m0_mu)\n\n[1] \"MU:  0.555931874775555\"\n\nCODEm0_sigma <- inv_logit(m0$sigma.coefficients) \npaste(\"SIGMA: \",m0_sigma)\n\n[1] \"SIGMA:  0.544134514840075\"\n\nCODEm0_nu <- exp(m0$nu.coefficients) \npaste(\"NU: \",m0_nu)\n\n[1] \"NU:  2.27484150905293\"\n\nCODEm0_tau <- exp(m0$tau.coefficients) \npaste(\"TAU: \",m0_tau)\n\n[1] \"TAU:  0.517080427912532\"\n\nCODEm0_p0 <- fit_nu / (1 + fit_nu + fit_tau) \npaste(\"P0: \",m0_p0)\n\n[1] \"P0:  0.135600165493784\"\n\nCODEm0_p1 <- fit_tau / (1 + fit_nu + fit_tau)\npaste(\"P1: \",m0_p1)\n\n[1] \"P1:  0.170800000002391\"\n\nCODE#CREATE PREDICTOR MODEL\nm1 <- gamlss(R_acc ~ condition, ~ condition, ~ condition, ~ condition, \n            data = df, family = BEINF())\n\nGAMLSS-RS iteration 1: Global Deviance = 588 \nGAMLSS-RS iteration 2: Global Deviance = 587 \nGAMLSS-RS iteration 3: Global Deviance = 587 \nGAMLSS-RS iteration 4: Global Deviance = 587 \nGAMLSS-RS iteration 5: Global Deviance = 587 \n\nCODEsummary(m1)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.144      0.203    0.71     0.48\ncondition121    0.124      0.244    0.51     0.61\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.280      0.174    1.61     0.11\ncondition121   -0.164      0.213   -0.77     0.44\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.377      0.208    6.63  1.4e-10 ***\ncondition121   -1.019      0.269   -3.79  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    -0.729      0.325   -2.24    0.026 *\ncondition121    0.102      0.394    0.26    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     587 \n            AIC:     603 \n            SBC:     633 \n******************************************************************\n\nCODE#LOOKING PREDICTOR MODEL\nm <- gamlss(R_acc ~ condition , \n            ~ condition , \n            ~ condition , \n            ~ condition , \n            data = df, family = BEINF())\n\nGAMLSS-RS iteration 1: Global Deviance = 588 \nGAMLSS-RS iteration 2: Global Deviance = 587 \nGAMLSS-RS iteration 3: Global Deviance = 587 \nGAMLSS-RS iteration 4: Global Deviance = 587 \nGAMLSS-RS iteration 5: Global Deviance = 587 \n\nCODEsummary(m)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.144      0.203    0.71     0.48\ncondition121    0.124      0.244    0.51     0.61\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.280      0.174    1.61     0.11\ncondition121   -0.164      0.213   -0.77     0.44\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.377      0.208    6.63  1.4e-10 ***\ncondition121   -1.019      0.269   -3.79  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    -0.729      0.325   -2.24    0.026 *\ncondition121    0.102      0.394    0.26    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     587 \n            AIC:     603 \n            SBC:     633 \n******************************************************************\n\nCODE#CREATE PREDICTOR MODEL ON SHUFFLED [PERMUTATION TEST]\nmperm <- gamlss(R_acc ~ perm, ~ perm, ~ perm, ~ perm, \n            data = df, family = BEINF())\n\nGAMLSS-RS iteration 1: Global Deviance = 609 \nGAMLSS-RS iteration 2: Global Deviance = 608 \nGAMLSS-RS iteration 3: Global Deviance = 608 \nGAMLSS-RS iteration 4: Global Deviance = 608 \nGAMLSS-RS iteration 5: Global Deviance = 608 \n\nCODEsummary(mperm)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ perm, sigma.formula = ~perm,  \n    nu.formula = ~perm, tau.formula = ~perm, family = BEINF(),      data = df) \n\n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.1914     0.1706    1.12     0.26\nperm121       0.0635     0.2278    0.28     0.78\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    0.237      0.148    1.60     0.11\nperm121       -0.117      0.202   -0.58     0.56\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.8535     0.1887    4.52  8.6e-06 ***\nperm121      -0.0595     0.2579   -0.23     0.82    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   -0.511      0.258   -1.98    0.048 *\nperm121       -0.294      0.368   -0.80    0.425  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     608 \n            AIC:     624 \n            SBC:     654 \n******************************************************************\n\nCODE#sanity check with scaled outcome, no zeros ones\nm3 <- gamlss(T_acc ~ condition, ~ condition, ~ condition, ~ condition, \n            data = df, family = BEINF())\n\nGAMLSS-RS iteration 1: Global Deviance = -1812 \nGAMLSS-RS iteration 2: Global Deviance = -2024 \nGAMLSS-RS iteration 3: Global Deviance = -2038 \nGAMLSS-RS iteration 4: Global Deviance = -2040 \nGAMLSS-RS iteration 5: Global Deviance = -2040 \nGAMLSS-RS iteration 6: Global Deviance = -2040 \nGAMLSS-RS iteration 7: Global Deviance = -2040 \nGAMLSS-RS iteration 8: Global Deviance = -2040 \nGAMLSS-RS iteration 9: Global Deviance = -2040 \n\nCODEsummary(m3)\n\nWarning in summary.gamlss(m3): summary: vcov has failed, option qr is used instead\n\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = T_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition, family = BEINF(),  \n    data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.1492     0.0981  -11.71  < 2e-16 ***\ncondition121   0.5677     0.1411    4.02 0.000071 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    1.4399     0.0745   19.33   <2e-16 ***\ncondition121   0.1694     0.1038    1.63      0.1    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -2.25e+01   3.78e+03   -0.01        1\ncondition121 -6.72e-15   5.24e+03    0.00        1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -2.26e+01   3.96e+03   -0.01        1\ncondition121  9.20e-15   5.48e+03    0.00        1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  9 \n \nGlobal Deviance:     -2040 \n            AIC:     -2024 \n            SBC:     -1994 \n******************************************************************\n\nCODE#m3 shouldn't show condition as significant for nu and tau, because T_acc was scaled to not include any 0s and 1s\n\n#investigate beta negative binomial distribution\n#https://en.wikipedia.org/wiki/Beta_negative_binomial_distribution\n\n#TRANSFORM PARAMETRS BACK \ninv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function\nm1_mu <- inv_logit(m1$mu.coefficients) \npaste(\"MU: \",m1_mu)\n\n[1] \"MU:  0.536038311159578\" \"MU:  0.531024352873784\"\n\nCODEm1_sigma <- inv_logit(m0$sigma.coefficients) \npaste(\"SIGMA: \",m1_sigma)\n\n[1] \"SIGMA:  0.544134514840075\"\n\nCODEm1_nu <- exp(m1$nu.coefficients) \npaste(\"NU: \",m1_nu)\n\n[1] \"NU:  3.96329406964311\"  \"NU:  0.360974858677686\"\n\nCODEm1_tau <- exp(m1$tau.coefficients) \npaste(\"TAU: \",m1_tau)\n\n[1] \"TAU:  0.482542801248665\" \"TAU:  1.10746276561553\" \n\nCODEsummary(m)\n\n******************************************************************\nFamily:  c(\"BEINF\", \"Beta Inflated\") \n\nCall:  gamlss(formula = R_acc ~ condition, sigma.formula = ~condition,  \n    nu.formula = ~condition, tau.formula = ~condition,  \n    family = BEINF(), data = df) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  logit\nMu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.144      0.203    0.71     0.48\ncondition121    0.124      0.244    0.51     0.61\n\n------------------------------------------------------------------\nSigma link function:  logit\nSigma Coefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.280      0.174    1.61     0.11\ncondition121   -0.164      0.213   -0.77     0.44\n\n------------------------------------------------------------------\nNu link function:  log \nNu Coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.377      0.208    6.63  1.4e-10 ***\ncondition121   -1.019      0.269   -3.79  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nTau link function:  log \nTau Coefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    -0.729      0.325   -2.24    0.026 *\ncondition121    0.102      0.394    0.26    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  330 \nDegrees of Freedom for the fit:  8\n      Residual Deg. of Freedom:  322 \n                      at cycle:  5 \n \nGlobal Deviance:     587 \n            AIC:     603 \n            SBC:     633 \n******************************************************************\n\nCODEplot(m)\n\n\n\n\n******************************************************************\n     Summary of the Randomised Quantile Residuals\n                           mean   =  -0.00363 \n                       variance   =  1.04 \n               coef. of skewness  =  -0.0633 \n               coef. of kurtosis  =  3.03 \nFilliben correlation coefficient  =  0.999 \n******************************************************************\n\n\n\nMU tells if mean is different by condition\n\nSIGMA tells if variance is different by condition\n\nNU coefficient tells if condition yields different probability at floor\nTAU coefficient tells if condition yields different probability at ceiling\nBeta Regression (% Correct)\nBeta regression on % correct (with standard transformation for including [0,1]) https://stats.stackexchange.com/questions/63350/how-to-interpret-the-coefficients-from-a-beta-regression\n\nCODE# \nlibrary(betareg)\n\n#RESCLAE VARIABLE\n#beta reg can't handle 0s and 1s \nsub <- df_subjects %>% dplyr::select(condition, DV_percent_NABS)\nn = nrow(sub) %>% unlist()\nsub$dv_transformed = (sub$DV_percent_NABS * (n-1) + 0.5)/n\n \n#VISUALIZE VARIABLES\nhistogram(sub$dv_transformed)\n\n\n\nCODEgf_histogram(~dv_transformed, fill = ~condition, data = sub) %>% gf_facet_wrap(~condition)\n\n\n\nCODE#FIT MODEL\nmb <- betareg(dv_transformed ~ condition, data = sub)\nsummary(mb)\n\n\nCall:\nbetareg(formula = dv_transformed ~ condition, data = sub)\n\nStandardized weighted residuals 2:\n   Min     1Q Median     3Q    Max \n-1.057 -0.453 -0.216  0.541  1.690 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.969      0.108   -8.97   <2e-16 ***\ncondition121    0.556      0.143    3.89   0.0001 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(>|z|)    \n(phi)   0.6604     0.0425    15.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  506 on 3 Df\nPseudo R-squared: 0.0725\nNumber of iterations: 12 (BFGS) + 1 (Fisher scoring) \n\nCODEplot(mb)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWIP | HURDLE MODEL\n\nhttps://data.library.virginia.edu/getting-started-with-hurdle-models/\n\nhttps://en.wikipedia.org/wiki/Hurdle_model#:~:text=A%20hurdle%20model%20is%20a,of%20the%20non%2Dzero%20values.\n\nclass of models for count data with both overdispersion and excess zeros;\ndifferent from zero-inflated models where the excess zeros are theorized to arise from two different processes; in the hurdle model, there is a model for P(x=0) and a separate model for P(x!=0)\nThe model includes: - A binary logit model to model whether the observation takes a positive count or not. - a truncated Poisson or Negative binomial model that only fits positive counts\nThis allows us to model: (1) Does the student get any questions right? (2) How many questions does the student get right?\n\nCODElibrary(pscl) #zero-inf and hurdle models \nlibrary(countreg) #rootogram\n\nRegistered S3 methods overwritten by 'countreg':\n  method                 from\n  print.zeroinfl         pscl\n  print.summary.zeroinfl pscl\n  summary.zeroinfl       pscl\n  coef.zeroinfl          pscl\n  vcov.zeroinfl          pscl\n  logLik.zeroinfl        pscl\n  predict.zeroinfl       pscl\n  residuals.zeroinfl     pscl\n  fitted.zeroinfl        pscl\n  terms.zeroinfl         pscl\n  model.matrix.zeroinfl  pscl\n  extractAIC.zeroinfl    pscl\n  print.hurdle           pscl\n  print.summary.hurdle   pscl\n  summary.hurdle         pscl\n  coef.hurdle            pscl\n  vcov.hurdle            pscl\n  logLik.hurdle          pscl\n  predict.hurdle         pscl\n  residuals.hurdle       pscl\n  fitted.hurdle          pscl\n  terms.hurdle           pscl\n  model.matrix.hurdle    pscl\n  extractAIC.hurdle      pscl\n\n\n\nAttaching package: 'countreg'\n\n\nThe following objects are masked from 'package:VGAM':\n\n    dzipois, pzipois, qzipois, rzipois\n\n\nThe following objects are masked from 'package:pscl':\n\n    hurdle, hurdle.control, hurdletest, zeroinfl, zeroinfl.control\n\n\nThe following object is masked from 'package:vcd':\n\n    rootogram\n\nCODE#install.packages(\"countreg\", repos=\"http://R-Forge.R-project.org\")\n\n#SYNTAX OUTCOME ~ count model predictor | hurdle predictor\n\nh.1 <- pscl::hurdle(item_test_NABS ~ condition | condition , data = df_subjects,\n              zero.dist = \"binomial\", dist = \"poisson\", size = 8)\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\n\nWarning in optim(fn = zeroDist, gr = zeroGrad, par = c(start$zero, if (zero.dist\n== : unknown names in control: size\n\nCODEh.2 <- pscl::hurdle(item_test_NABS ~ condition | condition , data = df_subjects,\n              zero.dist = \"binomial\", dist = \"negbin\", size = 8)\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\nWarning in optim(fn = countDist, gr = countGrad, par = c(start$count, if (dist\n== : unknown names in control: size\n\nCODEsummary(h.1)\n\n\nCall:\npscl::hurdle(formula = item_test_NABS ~ condition | condition, data = df_subjects, \n    dist = \"poisson\", zero.dist = \"binomial\", size = 8)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.892 -0.818 -0.549  0.881  2.342 \n\nCount model coefficients (truncated poisson with log link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    1.7156     0.0653   26.29   <2e-16 ***\ncondition121   0.0447     0.0789    0.57     0.57    \nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.984      0.179   -5.50  3.7e-08 ***\ncondition121    1.054      0.235    4.48  7.4e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -533 on 4 Df\n\nCODEsummary(h.2)\n\n\nCall:\npscl::hurdle(formula = item_test_NABS ~ condition | condition, data = df_subjects, \n    dist = \"negbin\", zero.dist = \"binomial\", size = 8)\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-0.866 -0.794 -0.538  0.856  2.294 \n\nCount model coefficients (truncated negbin with log link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    1.7126     0.0728   23.54  < 2e-16 ***\ncondition121   0.0451     0.0880    0.51  0.60810    \nLog(theta)     3.1851     0.8732    3.65  0.00026 ***\nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -0.984      0.179   -5.50  3.7e-08 ***\ncondition121    1.054      0.235    4.48  7.4e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 24.169\nNumber of iterations in BFGS optimization: 20 \nLog-likelihood: -532 on 5 Df\n\nCODErootogram(h.1)\n\n\n\nCODErootogram(h.2)\n\n\n\nCODEcompare_performance(h.1,h.2)\n\n# Comparison of Model Performance Indices\n\nName |  Model |      AIC | AIC weights |      BIC | BIC weights |    R2 | R2 (adj.) |  RMSE | Sigma | Score_log | Score_spherical\n---------------------------------------------------------------------------------------------------------------------------------\nh.1  | hurdle | 1073.583 |       0.537 | 1088.780 |       0.886 | 0.351 |     0.347 | 3.150 | 3.169 |    -2.479 |           0.043\nh.2  | hurdle | 1073.880 |       0.463 | 1092.876 |       0.114 | 0.363 |     0.359 | 3.150 | 3.174 |    -2.132 |           0.043"
  },
  {
    "objectID": "analysis/SGC3A/x_learning.html#wip-unknown",
    "href": "analysis/SGC3A/x_learning.html#wip-unknown",
    "title": "Modelling Reference",
    "section": "WIP UNKNOWN",
    "text": "WIP UNKNOWN\nCummulative Ordinal (Bayesian)\nhttps://journals.sagepub.com/doi/full/10.1177/2515245918823199\n\nCODE# library(brms)\n\n\n# #DEFINE DATA \n# df <- df_items %>% mutate(\n#   scaled = factor(score_SCALED, ordered = TRUE,  #ordered factor\n#                     levels = c(\"-1\", \"-0.5\", \"0\", \"0.5\",\"1\"))\n# )\n# \n# ord_cum <- brm( formula = scaled ~ condition,\n#                data = df,\n#                family = cumulative(\"probit\"),\n#                file = \"analysis/SGC3A/models/m_items_ord.cum.rds\" # cache model (can be removed)  \n# \n# )\n# \n# summary(ord_cum)\n# conditional_effects(ord_cum, \"condition\", categorical = TRUE)\n# \n# #SJPLOT\n# library(sjPlot)\n# plot_model(ord_cum)\n# \n# # m %>%\n# #   spread_draws(b_Intercept, r_condition[condition,]) %>%\n# #   mutate(condition_mean = b_Intercept + r_condition) %>%\n# #   ggplot(aes(y = condition, x = condition_mean)) +\n# #   stat_halfeye()\n# \n# # performance(ord_cum)\n# # plot(ord_cum)\n\n\nAdjacent-Category Ordinal (Bayesian)\n\nCODE# \n# #DEFINE DATA \n# df <- df_items %>% mutate(\n#   scaled = factor(score_SCALED, ordered = TRUE,  #ordered factor\n#                     levels = c(\"-1\", \"-0.5\", \"0\", \"0.5\",\"1\"))\n# )\n# \n# \n# # To specify an adjacent-category model, we use family = acat() instead of family = cumulative() as an argument to the brm() function. Then, to model condition with possible category-specific effects, we wrap this variable in cs() in the model’s formula:\n# \n# ord_acat <- brm( formula = scaled ~ cs(condition),\n#                data = df,\n#                family = acat(\"probit\"),\n#                file = \"analysis/SGC3A/models/m_items_ord.acat.rds\" # cache model (can be removed)  \n# )\n# \n# summary(ord_acat)\n# conditional_effects(ord_cum, \"condition\", categorical = TRUE)\n# conditional_effects(ord_acat, \"condition\", categorical = TRUE)\n# \n# #TIDYBAYES VISUALIZATION\n# library(tidybayes)\n# ord_acat %>%\n#   spread_draws(b_Intercept, r_condition[condition,]) %>%\n#   mutate(condition_mean = b_Intercept + r_condition) %>%\n#   ggplot(aes(y = condition, x = condition_mean)) +\n#   stat_halfeye()\n#"
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#q1-accuracy",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#q1-accuracy",
    "title": "5  Exploratory Analyses",
    "section": "Q1 ACCURACY",
    "text": "Q1 ACCURACY\nWhat explains Q1 Accuracy?\nMultiple Logistic Regression [Q1 Absolute]\n\nCODE# #CREATE DATAFRAME OF Q1\n# df <- df_items %>% filter(q == 1) %>% mutate( accuracy = as.factor(score_niceABS))\n# \n# #MODEL\n# m2 <- glm( accuracy ~ rt_s, data = df, family = \"binomial\")\n# summary(m2)\n# confint(m2)\n# performance(m2)\n# report(m2)\n# \n# library(effects)\n# plot(allEffects(m2))\n# \n# \n# m3 <- glm( accuracy ~ condition + rt_s, data = df, family = \"binomial\")\n# summary(m3)\n# confint(m3)\n# performance(m3)\n# report(m3)\n# \n# plot(allEffects(m3))\n# \n# compare_performance(m,m2,m3)\n\n\n\nCODE#evaluate model using kfold CV\n# https://www.statology.org/k-fold-cross-validation-in-r/\n\n# #specify the cross-validation method\n# ctrl <- trainControl(method = \"cv\", number = 5)\n# #fit a regression model and use k-fold CV to evaluate performance\n# mk <- train( accuracy ~ condition, data = df, method = \"glm\", trControl = ctrl, family = \"binomial\")\n# print(mk)\n# \n\n\nMass Movement\n“movement of mass” from one mode to another\nConsidering only families of unimodal distributions, the most probably distribution (as predicted by package performance) is negative-binomial.\n\nCODEdf <- df_subjects %>% filter(condition==111)\nmultimode::modetest(df$s_NABS)\n\nWarning in multimode::modetest(df$s_NABS): A modification of the data was made\nin order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df$s_NABS\nExcess mass = 0.09, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\nCODEn_modes = multimode::nmodes(data = df$s_NABS, bw=2)\nmultimode::locmodes(df$s_NABS,mod0 =  n_modes, display = TRUE)\n\nWarning in multimode::locmodes(df$s_NABS, mod0 = n_modes, display = TRUE): If\nthe density function has an unbounded support, artificial modes may have been\ncreated in the tails\n\n\n\n\n\n\nEstimated location\nModes: 0.136  12.6 \nAntimode: 7.63 \n\nEstimated value of the density\nModes: 0.268  0.0432 \nAntimode: 0.00648 \n\nCritical bandwidth: 1.05\n\n\n\nCODEdf <- df_subjects %>% filter(condition==121)\nmultimode::modetest(df$s_NABS)\n\nWarning in multimode::modetest(df$s_NABS): A modification of the data was made\nin order to compute the excess mass or the dip statistic\n\n\n\n    Ameijeiras-Alonso et al. (2019) excess mass test\n\ndata:  df$s_NABS\nExcess mass = 0.1, p-value <2e-16\nalternative hypothesis: true number of modes is greater than 1\n\nCODEn_modes = multimode::nmodes(data = df$s_NABS, bw=2)\nmultimode::locmodes(df$s_NABS,mod0 =  n_modes, display = TRUE)\n\nWarning in multimode::locmodes(df$s_NABS, mod0 = n_modes, display = TRUE): If\nthe density function has an unbounded support, artificial modes may have been\ncreated in the tails\n\n\n\n\n\n\nEstimated location\nModes: 0.456    12 \nAntimode: 4.89 \n\nEstimated value of the density\nModes: 0.148  0.0703 \nAntimode: 0.025 \n\nCritical bandwidth: 1.27"
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#response-latency",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#response-latency",
    "title": "5  Exploratory Analyses",
    "section": "RESPONSE LATENCY",
    "text": "RESPONSE LATENCY\n\n\nTODO: Investigate super high and super low response times..\n\nTODO: Investigate appropriate models for response time data. (see: https://lindeloev.github.io/shiny-rt/).\nEspecially see https://lindeloev.github.io/shiny-rt/ for ideas on modelling reaction time data\n\n\n\n\n\n\n\nResearch Question\n\n\n\n\nHypothesis\n\n\n\nAnalysis Strategy\n\n\n\nAlternatives\n\n\n\nInference\n\n\n\n\nQ1 Response Latency\nLinear Regression (Log Transform)\n(In Person)\nVisualization\n\nCODEdf_lab <- df_subjects %>% filter(mode == \"lab-synch\")\n \n#HISTOGRAM\nstats = df_lab %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_q1_rt))\ngf_dhistogram(~log(item_q1_rt), fill = ~pretty_condition, data = df_lab) %>% gf_facet_grid(~pretty_condition) +\n  # gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(title = \"(LAB) First Question Response Time\",,\n       # x = \"Response Time (seconds)\",\n       # y = \"proportion of participants\",\n       subtitle = \"\") + \n  theme_minimal()\n\n\n\n\nModel\n\nCODE#SCORE predicted by CONDITION\nlab.q1t.lm1 <- lm(log(item_q1_rt) ~ pretty_condition, data = df_lab)\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(lab.q1t.lm1)\n\n\nCall:\nlm(formula = log(item_q1_rt) ~ pretty_condition, data = df_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8202 -0.3504  0.0503  0.3382  1.2862 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               3.4833     0.0687   50.69   <2e-16 ***\npretty_conditionimpasse   0.3142     0.0964    3.26   0.0014 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.541 on 124 degrees of freedom\nMultiple R-squared:  0.0789,    Adjusted R-squared:  0.0714 \nF-statistic: 10.6 on 1 and 124 DF,  p-value: 0.00145\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(lab.q1t.lm1)\n\nAnalysis of Variance Table\n\nResponse: log(item_q1_rt)\n                  Df Sum Sq Mean Sq F value Pr(>F)   \npretty_condition   1    3.1   3.108    10.6 0.0014 **\nResiduals        124   36.3   0.293                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(lab.q1t.lm1)\n\n                        2.5 % 97.5 %\n(Intercept)             3.347  3.619\npretty_conditionimpasse 0.123  0.505\n\nCODEreport(lab.q1t.lm1) #sanity check\n\nRegistered S3 methods overwritten by 'effectsize':\n  method              from      \n  standardize.Surv    datawizard\n  standardize.bcplm   datawizard\n  standardize.clm2    datawizard\n  standardize.default datawizard\n  standardize.mediate datawizard\n  standardize.wbgee   datawizard\n  standardize.wbm     datawizard\n\n\nFormula contains log- or sqrt-terms. See help(\"standardize\") for how such terms are standardized.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nFormula contains log- or sqrt-terms. See help(\"standardize\") for how such terms are standardized.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict item_q1_rt with pretty_condition (formula: log(item_q1_rt) ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.08, F(1, 124) = 10.61, p = 0.001, adj. R2 = 0.07). The model's intercept, corresponding to pretty_condition = control, is at 3.48 (95% CI [3.35, 3.62], t(124) = 50.69, p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.31, 95% CI [0.12, 0.51], t(124) = 3.26, p = 0.001; Std. beta = 0.21, 95% CI [0.09, 0.34])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#print model equation\neq <- extract_eq(lab.q1t.lm1, use_coefs = TRUE)\n\n\n\nCODE# #| label: VISMODEL-Q1-LATENCY-LAB\n# \n# #MODEL ESTIMATES WITH UNCERTAINTY\n# \n# #setup references \n# #lab.q1t.lm1 <- lm(log(item_q1_rt) ~ condition, data = df_lab)\n# m <- lab.q1t.lm1\n# df <- df_lab \n# call <- m$call %>% as.character()\n# \n# # uncertainty model visualization\n# df <- df  %>%\n#   data_grid(pretty_condition) %>%\n#   augment(m, newdata = ., se_fit = TRUE) \n# \n# #transform log\n# df$.fitted <- exp(df$.fitted)\n# df$.se.fit <- exp(df$.se.fit)\n# \n# df %>% \n#   ggplot(aes(y = pretty_condition, color = pretty_condition)) +\n#   stat_halfeye( scale = .5,\n#       aes(\n#         xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n#         fill = stat(cut_cdf_qi(cdf, \n#                 .width = c(.90, .95),\n#                 labels = scales::percent_format())))) +\n#   scale_fill_brewer(direction = -1) + \n#   labs (title = \"(LAB) Q1 Response Latency ~ Condition\", \n#         x = \"model predicted mean (seconds)\", y = \"Condition\", fill = \"Interval\",\n#         subtitle = paste(\"lm(\",call[2],\")\"),\n#         caption = \"note: model log(predictions) have exponentiated to original scale\") + theme(legend.position = \"blank\")\n\n\nDiagnostics\n\nCODE#model diagnostics\ncheck_model(lab.q1t.lm1, panel = TRUE)\n\n\n\n\n\nRESIDUAL DISTRIBUTION:\nHOMOGENEITY:\nHETERSCEDASTICITY:\nAUTOCORRELATION:\nOUTLIERS: FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE\nInference\nOLS Linear Regression on Q1 response time shows that condition explains a small but statistically significant amount of variance (impasse > control). However, the model violates the assumption of normally distributed residuals.\n(Online Replication)\nVisualization\n\nCODEdf_online <- df_subjects %>% filter(mode == \"asynch\")\n \n#HISTOGRAM\nstats = df_online %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(item_q1_rt))\ngf_dhistogram(~log(item_q1_rt), fill = ~pretty_condition, data = df_lab) %>% gf_facet_grid(~pretty_condition) +\n  # gf_vline(data = stats, xintercept = ~mean, color = \"red\") +\n  labs(title = \"(ONLINE) First Question Response Time\",\n       # x = \"Response Time (seconds)\",\n       # y = \"proportion of participants\",\n       subtitle = \"\") + \n  theme_minimal()\n\n\n\n\nModel\n\nCODE#SCORE predicted by CONDITION\nrep.q1t.lm1 <- lm(log(item_q1_rt) ~ pretty_condition, data = df_online)\npaste(\"Model\")\n\n[1] \"Model\"\n\nCODEsummary(rep.q1t.lm1)\n\n\nCall:\nlm(formula = log(item_q1_rt) ~ pretty_condition, data = df_online)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0092 -0.3640 -0.0372  0.3785  2.0718 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               3.2505     0.0733   44.36  < 2e-16 ***\npretty_conditionimpasse   0.4200     0.1007    4.17 0.000045 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.718 on 202 degrees of freedom\nMultiple R-squared:  0.0793,    Adjusted R-squared:  0.0747 \nF-statistic: 17.4 on 1 and 202 DF,  p-value: 0.0000451\n\nCODEpaste(\"Partition Variance\")\n\n[1] \"Partition Variance\"\n\nCODEanova(rep.q1t.lm1)\n\nAnalysis of Variance Table\n\nResponse: log(item_q1_rt)\n                  Df Sum Sq Mean Sq F value   Pr(>F)    \npretty_condition   1      9    8.97    17.4 0.000045 ***\nResiduals        202    104    0.52                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODEpaste(\"Confidence Interval on Parameter Estimates\")\n\n[1] \"Confidence Interval on Parameter Estimates\"\n\nCODEconfint(rep.q1t.lm1)\n\n                        2.5 % 97.5 %\n(Intercept)             3.106  3.395\npretty_conditionimpasse 0.221  0.619\n\nCODEreport(rep.q1t.lm1) #sanity check\n\nFormula contains log- or sqrt-terms. See help(\"standardize\") for how such terms are standardized.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nFormula contains log- or sqrt-terms. See help(\"standardize\") for how such terms are standardized.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict item_q1_rt with pretty_condition (formula: log(item_q1_rt) ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.08, F(1, 202) = 17.39, p < .001, adj. R2 = 0.07). The model's intercept, corresponding to pretty_condition = control, is at 3.25 (95% CI [3.11, 3.39], t(202) = 44.36, p < .001). Within this model:\n\n  - The effect of pretty condition [impasse] is statistically significant and positive (beta = 0.42, 95% CI [0.22, 0.62], t(202) = 4.17, p < .001; Std. beta = 0.21, 95% CI [0.11, 0.31])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE#print model equation\neq <- extract_eq(rep.q1t.lm1, use_coefs = TRUE)\n\n\n\nCODE#MODEL ESTIMATES WITH UNCERTAINTY\n# \n# #setup references \n# # rep.q1t.lm1 <- lm(log(item_q1_rt) ~ condition, data = df_online)\n# m <- rep.q1t.lm1\n# df <- df_online \n# call <- m$call %>% as.character()\n# \n# # uncertainty model visualization\n# df <- df  %>%\n#   data_grid(pretty_condition) %>%\n#   augment(m, newdata = ., se_fit = TRUE) \n# \n# #transform log\n# df$.fitted <- exp(df$.fitted)\n# df$.se.fit <- exp(df$.se.fit)\n# \n# df %>% \n#   ggplot(aes(y = pretty_condition, color = pretty_condition)) +\n#   stat_halfeye( scale = .5,\n#       aes(\n#         xdist = dist_student_t(df = df.residual(m), mu = .fitted, sigma = .se.fit),\n#         fill = stat(cut_cdf_qi(cdf, \n#                 .width = c(.90, .95),\n#                 labels = scales::percent_format())))) +\n#   scale_fill_brewer(direction = -1) + \n#   labs (title = \"(ONLINE) Q1 Response Latency ~ Condition\", \n#         x = \"model predicted mean (seconds)\", y = \"Condition\", fill = \"Interval\",\n#         subtitle = paste(\"lm(\",call[2],\")\"),\n#         caption = \"note: model log(predictions) have exponentiated to original scale\") + theme(legend.position = \"blank\")\n\n\nDiagnostics\n\nCODE#model diagnostics\ncheck_model(rep.q1t.lm1, panel = TRUE)\n\n\n\n\n\nRESIDUAL DISTRIBUTION:\nHOMOGENEITY:\nHETERSCEDASTICITY:\nAUTOCORRELATION:\nOUTLIERS: FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE\nInference\nOLS Linear Regression on Q1 response time shows that condition explains a small but statistically significant amount of variance (impasse > control). However, the model violates the assumption of normally distributed residuals."
  },
  {
    "objectID": "analysis/SGC3A/5_sgc3A_exploration.html#todo-response-consistency",
    "href": "analysis/SGC3A/5_sgc3A_exploration.html#todo-response-consistency",
    "title": "5  Exploratory Analyses",
    "section": "TODO RESPONSE CONSISTENCY",
    "text": "TODO RESPONSE CONSISTENCY"
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html",
    "href": "analysis/SGC2/1_sgc2_introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In Study Two we examine if scaffolding is effective in aiding untrained students to understand the Triangular Model (TM) graph. We know that students are unlikely to construct the correct interpretation of the TM without assistance. Guided by the results of the Study One Design Task, we created four scaffolds. We test the effectiveness of these scaffolds by seeking to replicate the Qiang et.al (2014) finding that after 20 minutes of video training, students perform faster and more accurately with the unconventional TM than the conventional Linear Model (LM). Will our participants show similar performance on the TM with scaffolds rather than formal instruction? Further, will engagement with the TM in a reading task be sufficient for students to reproduce the graph in a subsequent drawing task?\nTo try the study yourself: http://morning-gorge-17056.herokuapp.com/Enter “github” as your session code, and number of the condition you wish to test\n0 = control (no-scaffold), 1 = “what-text”, 2 = “how-text”, 3 = “static-image”, 4 = “interactive-image”"
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html#methods",
    "href": "analysis/SGC2/1_sgc2_introduction.html#methods",
    "title": "1  Introduction",
    "section": "METHODS",
    "text": "METHODS\nDesign\nWe employed a 5 (scaffold: none-control, what-text, how-text, static image, interactive image) x 2 (graph: LM, TM) mixed design, with scaffold as a between-subjects variable and graph as a within-subject variable. To test our hypothesis that exposure to the conventional LM acts as a scaffold for the TM, we counterbalanced the order of graph-reading tasks (order: LM-first, TM-first). For each task we measured response accuracy and time. For the follow-up graph-drawing task, a team of raters coded the type of graph produced by each participant.\nMaterials\nScaffolds\nFor the first five questions of each graph-reading task, participants saw their assigned scaffold along with the designated graph. On the following ten questions, the scaffold was not present. Examples of each scaffold-condition for the TM and LM graphs are shown in the table above.\nThe Graph Drawing Task\nIn the  graph drawing task participants were given a sheet of isometric dot paper and a table containing a set of 10 time intervals. Isometric dot paper equally supports the construction of lines at 0, 45 and 90 degrees, thus minimizing any biasing effects of the paper on the type of graph the participants chose to draw. Participants were directed to draw a triangular graph of the data (“like the triangle graph you saw in the previous task”), using the pencil, eraser and ruler provided.\n\nProcedure\nParticipants completed the study individually in a computer lab. Each participant was randomly assigned to one of five conditions which determined what additional information (scaffold) they received while solving the first five problems with each graph: no-scaffold (control), ‘what’ text, ‘how’-text, static-image, and interactive-image. After a short introduction they continued to the first of two graph reading tasks (graph order counterbalanced). After completing the first graph reading task, they were introduced to the second scenario, and completed the second graph reading task with the remaining graph. Finally, participants completed the graph drawing task. They finished the study by completing a short demographic survey, and reading the debriefing text. The runtime of the entire study ranged from 20 to 60 minutes.\nSample\nData was collected by convenience sample of a university subject pool. Data were collected in the Spring of 2017 with, in-person, with large groups of students simultaneously completing the study (independently) in a computer lab."
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html#analysis",
    "href": "analysis/SGC2/1_sgc2_introduction.html#analysis",
    "title": "1  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\nnote: Unlike studies SGC3 and onwards, scoring for SGC2 is already included in the raw data files.\n\nCODE# HACK WD FOR LOCAL RUNNING?\n# imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\nmbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\nsetwd(mbp)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC2/data/2-scored-data/sgc2_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC2/data/2-scored-data/sgc2_items.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \nrio::export(df_subjects, \"analysis/SGC2/data/2-scored-data/sgc2_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC2/data/2-scored-data/sgc2_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC2/1_sgc2_introduction.html#resources",
    "href": "analysis/SGC2/1_sgc2_introduction.html#resources",
    "title": "1  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\nCODEsessionInfo()\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.8      purrr_0.3.4     \n [5] readr_2.1.2      tidyr_1.2.0      tibble_3.1.6     tidyverse_1.3.1 \n [9] Hmisc_4.6-0      ggplot2_3.3.5    Formula_1.2-4    survival_3.3-1  \n[13] lattice_0.20-45  kableExtra_1.3.4 codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            lubridate_1.8.0     bit64_4.0.5        \n [4] webshot_0.5.2       RColorBrewer_1.1-2  httr_1.4.2         \n [7] tools_4.1.1         backports_1.4.1     utf8_1.2.2         \n[10] R6_2.5.1            rpart_4.1.16        DBI_1.1.2          \n[13] colorspace_2.0-3    nnet_7.3-17         withr_2.5.0        \n[16] tidyselect_1.1.2    gridExtra_2.3       bit_4.0.4          \n[19] curl_4.3.2          compiler_4.1.1      cli_3.2.0          \n[22] rvest_1.0.2         htmlTable_2.4.0     xml2_1.3.3         \n[25] scales_1.1.1        checkmate_2.0.0     systemfonts_1.0.4  \n[28] digest_0.6.29       foreign_0.8-82      rmarkdown_2.13     \n[31] svglite_2.1.0       rio_0.5.29          base64enc_0.1-3    \n[34] jpeg_0.1-9          pkgconfig_2.0.3     htmltools_0.5.2    \n[37] labelled_2.9.0      dbplyr_2.1.1        fastmap_1.1.0      \n[40] highr_0.9           htmlwidgets_1.5.4   rlang_1.0.2        \n[43] readxl_1.3.1        rstudioapi_0.13     generics_0.1.2     \n[46] jsonlite_1.8.0      vroom_1.5.7         zip_2.2.0          \n[49] magrittr_2.0.2      Matrix_1.4-0        Rcpp_1.0.8.3       \n[52] munsell_0.5.0       fansi_1.0.2         lifecycle_1.0.1    \n[55] stringi_1.7.6       yaml_2.3.5          grid_4.1.1         \n[58] parallel_4.1.1      crayon_1.5.0        haven_2.4.3        \n[61] splines_4.1.1       hms_1.1.1           knitr_1.38         \n[64] pillar_1.7.0        reprex_2.0.1        glue_1.6.2         \n[67] evaluate_0.15       latticeExtra_0.6-29 data.table_1.14.2  \n[70] modelr_0.1.8        png_0.1-7           vctrs_0.3.8        \n[73] tzdb_0.2.0          cellranger_1.1.0    gtable_0.3.0       \n[76] assertthat_0.2.1    openxlsx_4.2.5      xfun_0.30          \n[79] broom_0.7.12        viridisLite_0.4.0   cluster_2.1.2      \n[82] ellipsis_0.3.2"
  },
  {
    "objectID": "analysis/SGC2/4_sgc2_hypotesting.html",
    "href": "analysis/SGC2/4_sgc2_hypotesting.html",
    "title": "3  Hypothesis Testing",
    "section": "",
    "text": "The purpose of this notebook is test the hypotheses that determined the design of the SGC2 study.\nResearch Questions\nIn SGC2 we compare learner performance on the linear and triangular model graphs by testing the effectiveness of 4 scaffolds and by seeking to replicate the Qiang et.al (2014) finding that after 20 minutes of video training, students perform faster and more accurately with the unconventional TM than the conventional Linear Model (LM). Will our participants show similar performance on the TM with scaffolds rather than formal instruction? Further, will engagement with the TM in a reading task be sufficient for students to reproduce the graph in a subsequent drawing task?\nHypotheses"
  },
  {
    "objectID": "analysis/SGC2/4_sgc2_hypotesting.html#h1-the-need-for-scaffolding",
    "href": "analysis/SGC2/4_sgc2_hypotesting.html#h1-the-need-for-scaffolding",
    "title": "3  Hypothesis Testing",
    "section": "H1 | The Need for Scaffolding",
    "text": "H1 | The Need for Scaffolding\nHypothesis The TM graph is not discoverable and requires scaffolding for correct interpretation. We predict that learners without scaffolding (the control condition) will perform better with the LM than TM\n\n\n\n\n\n\nResearch Question\nDo Ss in the CONTROL condition perform better on the LINEAR graph than the TRIANGULAR graph?\n\n\n\nHypothesis\nSs in the CONTROL condition will have higher scores on the LINEAR graph than the TRIANGULAR graph\n\n\nData\n\ndata: df_subjects where condition == 1\noutcome:\n\n\nlinear graph accuracy linear_score [absolute score]\n\ntriangular graph accuracy triangular_score\n\n\npredictor: graph (block) [within-subjects factor]\n\n\n\nAnalysis Strategy\n\n\nPaired Samples T-Test\n\ncompare average accuracy score in linear vs triangular block\neither T-Test or Wilcoxon Rank Sum (paired sample) alternative if difference scores are is non normal\n\n\n\nAlternative\n\nLinear Mixed Effects Model [violates normality of residuals]\n\npredict subject score [0-15] by graph with random intercept for subject\n\ndemonstrate that score is independent of order [nested] and scenario [nested]\n\n\nLogistic Mixed Effects Model\n\npredict item score [0,1] by graph with random intercept for subject and random intercept for question\ndemonstrate than score is independent of order and scenario\n\n\n\n\n\n\nNotes\n\n\n\n\n\nCODE#FILTER THE DATASET\ndf <- df_subjects %>% filter(condition == 1)\n\ndf_long <- df %>% dplyr::select(subject,pretty_condition,order,lm_scenarios,tm_scenarios,linear_score, triangular_score) %>% pivot_longer(\n  cols = ends_with(\"score\"),\n  names_to = \"graph\",\n  values_to = \"score\"\n)\n  \n\ntitle = \"Descriptive Statistics of Response Accuracy by Block (CONTROL Condition)\"\nabs.stats <- rbind(\n  \"linear.block\"= df %>% dplyr::select(linear_score) %>% unlist() %>% favstats(),\n  \"triangular.block\" = df %>% dplyr::select(triangular_score) %>% unlist() %>% favstats(),\n  \"block.differences\" = df %>% dplyr::select(score_diff) %>% unlist() %>% favstats()\n)\n\nabs.stats %>% kbl (caption = title) %>% kable_classic() %>%\n  footnote(general = \"block # questions correct [0,15]; DIFF = triangular - linear\",\n           general_title = \"Note: \",footnote_as_chunk = T)\n\n\n\nDescriptive Statistics of Response Accuracy by Block (CONTROL Condition)\n \n   \n    min \n    Q1 \n    median \n    Q3 \n    max \n    mean \n    sd \n    n \n    missing \n  \n\n\n linear.block \n    5 \n    9.0 \n    11 \n    13.0 \n    15 \n    11.07 \n    2.19 \n    59 \n    0 \n  \n\n triangular.block \n    0 \n    6.5 \n    10 \n    12.5 \n    14 \n    8.81 \n    4.28 \n    59 \n    0 \n  \n\n block.differences \n    -12 \n    -4.0 \n    -2 \n    0.0 \n    4 \n    -2.25 \n    3.93 \n    59 \n    0 \n  \n\n\nNote:   block # questions correct [0,15]; DIFF = triangular - linear\n\n\nCODE#DISTRIBUTION OF SCORE\ngf_dhistogram(~score, fill = ~graph, data = df_long) %>% gf_facet_wrap(~graph)+\n  labs(title = \"Distribution of scores in CONTROL condition\") + \n  easy_remove_legend()\n\n\n\nCODE##VERTICAL RAINCLOUD USING GGDISTR\nggplot(df_long, aes(x = graph, y = score,\n                        fill = graph) ) + \n  ggdist::stat_halfeye(\n    side = \"left\",\n    justification = 1.1,\n    width = 1, \n    point_colour = NA\n   ) + \n  geom_boxplot(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score),\n    width = .15, \n    outlier.shape = NA\n  ) + \n  geom_point(\n    inherit.aes = FALSE, #supress fill\n    mapping = aes(x=graph, y = score, color = graph),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter( \n      seed = 1, width = .05\n  )) + labs( \n    title = \"Distribution of scores in CONTROL condition\", \n    x = \"Condition\", y = \"Score (# correct)\") +\n  theme(legend.position = \"blank\") + \n  coord_cartesian(xlim = c(0.5, NA), clip = \"off\")\n\n\n\nCODE#DISTRIBUTION OF SCORE\ngf_dhistogram(~score_diff, data = df) %>% \n  gf_fitdistr(~score_diff) + \n  labs(title = \"Distribution of paired score differences in CONTROL condition\") + \n  easy_remove_legend() \n\n\n\n\nFor participants in the CONTROL condition, total absolute scores for the LINEAR graph (n = 59) range from 5 to 15 with a mean score of (M = 11.07, SD = 2.19).\nFor participants in the CONTROL condition, total absolute scores for the TRIANGULAR graph (n = 59) range from 0 to 14 with a mean score of (M = 8.81, SD = 4.28).\nVisual inspection of the distribution of scores for each block reveal that scores in on the triangular task were more variant than those in the linear graph. On average, scores on the triangular block were lower than those on the linear block.\nPAIRED SAMPLES T-TEST\nCheck Assumptions\n\nCODE#PAIRED T TEST ASSUMPTIONS\n\n# 1| PAIRED?\npaste(\"1| Data are paired? \", \"YES, block is crossed within subjects\")\n\n[1] \"1| Data are paired?  YES, block is crossed within subjects\"\n\nCODEpaste(\"2| Sample size? \", \"YES, sample size \",nrow(df), \"> 30\")\n\n[1] \"2| Sample size?  YES, sample size  59 > 30\"\n\nCODEpaste(\"3| Paired differences are normally distributed? accept null [normal] at p > 0.05\")\n\n[1] \"3| Paired differences are normally distributed? accept null [normal] at p > 0.05\"\n\nCODEshapiro.test(df$score_diff) \n\n\n    Shapiro-Wilk normality test\n\ndata:  df$score_diff\nW = 0.9, p-value = 0.007\n\n\n_Because the difference scores are not normally distributed, we don’t meet the assumptions of a standard paired t-test. Instead, we should use the alternative test Wilcoxon Rank Sum [paired] designed for non-normal distributions.\nVisualize\n\nCODE#PLOT PAIRED DATA\n#subset linear\nlinear <- subset(df_long,  graph == \"linear_score\", score,\n                 drop = TRUE)\n# subset triangular\ntriangular <- subset(df_long,  graph == \"triangular_score\", score,\n                 drop = TRUE)\n# Plot paired data\nlibrary(PairedData)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.1.2\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: gld\n\n\nLoading required package: mvtnorm\n\n\n\nAttaching package: 'PairedData'\n\n\nThe following object is masked from 'package:Matrix':\n\n    summary\n\n\nThe following object is masked from 'package:base':\n\n    summary\n\nCODEpd <- paired(linear, triangular)\nplot(pd, type = \"profile\") + theme_bw() + labs(title = \"Paired Data | Control Condition scores by block\")\n\n\n\nCODE#SANITY CHECK\nggwithinstats(\n  data = df_long,\n  x    = graph,\n  y    = score, \n  type  = \"nonparametric\" #parametric, robust, bayes\n)\n\n\n\n\nRun Test (Wilcoxon Paired Rank Sum)\n\nCODE#WILCOXON RANK SUM PAIRED T-TEST\nw <- wilcox.test(df$linear_score, df$triangular_score, \n            paired = TRUE, alternative = \"greater\", conf.int = TRUE)\nw\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  df$linear_score and df$triangular_score\nV = 1134, p-value = 0.0001\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 1.5 Inf\nsample estimates:\n(pseudo)median \n           2.5 \n\nCODEreport(w)\n\nWarning in ranktransform.numeric(x, verbose = verbose, ...): Zeros detected.\nThese cannot be sign-rank transformed.\n\n\n\n\nThe Wilcoxon signed rank test with continuity correction testing the difference in ranks between df$linear_score and df$triangular_score suggests that the effect is positive, statistically significant, and very large (W = 1134.50, p < .001; r (rank biserial) = 0.59, 95% CI [0.40, 1.00])\n\n\nInference\nThe Wilcoxon signed rank test confirms that (for subjects in the control condition) scores on the triangle graph were significantly lower than those in the linear graph block. This provides evidence in support of our hypothesis that the Triangular model graph (though computationally efficient) is in fact unconventional and lacking in discoverablility. It needs to be augmented with scaffolding in order to be correctly interpreted by novice readers."
  },
  {
    "objectID": "analysis/SGC2/4_sgc2_hypotesting.html#h2-the-effectiveness-of-scaffolding-todo-reconsider-accuracy-vs-latency",
    "href": "analysis/SGC2/4_sgc2_hypotesting.html#h2-the-effectiveness-of-scaffolding-todo-reconsider-accuracy-vs-latency",
    "title": "3  Hypothesis Testing",
    "section": "H2 | The Effectiveness of Scaffolding TODO reconsider accuracy vs latency??\n",
    "text": "H2 | The Effectiveness of Scaffolding TODO reconsider accuracy vs latency??\n\nHypothesis All of the designs offered by participants in Study 1 are promising. We expect that only a small amount of scaffolding (a little nudge) will be required to help readers correctly interpret the graph. We predict that learners with (any form of) scaffolding will perform better with the TM than readers in the CONTROL condition.\n\n\n\n\n\n\nResearch Question\nDoes SCAFFOLDING result in superior performance on the Triangular graph?\n\n\n\nHypothesis\nSs in any of the SCAFFOLD conditions will have higher scores on the TRIANGULAR graph compared with CONTROL condition readers.\n\n\nData\n\ndata: df_subjects\noutcome:\n\n\ntriangular block score triangular_score [triangular absolute score]\n\npredictor: condition [between-subjects factor]\n\n\n\nAnalysis Strategy\n\n(non parametric) One Way ANOVA (Kruskal Wallis)\n\ncompare triangular_score between conditions\n\n\nLinear Regression\n\npredict triangular_score by condition\n\nconfirm each condition yields significantly higher score than control\n\n\nLogistic Mixed Effects Regression\n\npredict item score by condition with random intercepts for subject and item\n\n\n\n\n\n\nNotes\n\n\n\n\nONE WAY ANOVA\n\nCODE# PREPARE DATA \ndf <- df_subjects\n\n\nVisualize\n\nCODE# Box plots\n# ++++++++++++++++++++\n# Plot weight by group and color by group\nlibrary(\"ggpubr\")\nggboxplot(df, x = \"pretty_condition\", y = \"triangular_score\", \n          color = \"pretty_condition\", \n          # palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n          # order = c(\"ctrl\", \"trt1\", \"trt2\"),\n          title = \"Difference in Triangular Block Scores by Condition\",\n          ylab = \"Triangular Score [0,15]\", xlab = \"Condition\") +\n  geom_jitter( aes(color = pretty_condition), width = 0.15, alpha = 0.5) +\n  easy_remove_legend()\n\n\n\n\nRun Test (One Way ANOVA)\n\nCODE#ONE WAY ANOVA\naov <- aov(triangular_score ~ pretty_condition, data = df)\naov\n\nCall:\n   aov(formula = triangular_score ~ pretty_condition, data = df)\n\nTerms:\n                pretty_condition Residuals\nSum of Squares               463      5776\nDeg. of Freedom                4       311\n\nResidual standard error: 4.31\nEstimated effects may be unbalanced\n\nCODEreport(aov)\n\nFor one-way between subjects designs, partial eta squared is equivalent to eta squared.\nReturning eta squared.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nThe ANOVA (formula: triangular_score ~ pretty_condition) suggests that:\n\n  - The main effect of pretty_condition is statistically significant and medium (F(4, 311) = 6.23, p < .001; Eta2 = 0.07, 95% CI [0.03, 1.00])\n\nCODE#POSTHOC TUKEY COMPARISONS\nTukeyHSD(aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = triangular_score ~ pretty_condition, data = df)\n\n$pretty_condition\n                       diff     lwr  upr p adj\ntext:what-control     1.912 -0.2474 4.07 0.110\ntext:how-control      1.795 -0.3049 3.90 0.134\nimg:static-control    1.663 -0.4697 3.80 0.206\nimg:ixv-control       3.775  1.6895 5.86 0.000\ntext:how-text:what   -0.117 -2.2353 2.00 1.000\nimg:static-text:what -0.249 -2.3997 1.90 0.998\nimg:ixv-text:what     1.863 -0.2410 3.97 0.110\nimg:static-text:how  -0.132 -2.2239 1.96 1.000\nimg:ixv-text:how      1.980 -0.0638 4.02 0.063\nimg:ixv-img:static    2.112  0.0355 4.19 0.044\n\nCODEplot(TukeyHSD(aov))\n\n\n\n\nCheck Assumptions\n\nCODE#ONE WAY ANOVA ASSUMPTIONS\n\npaste(\"1| Homogoneity of variance? \", \"YES, block is crossed within subjects\")\n\n[1] \"1| Homogoneity of variance?  YES, block is crossed within subjects\"\n\nCODEplot(aov, 1)\n\n\n\nCODEl <- car::leveneTest(triangular_score ~ pretty_condition, data = df)\npaste (\"reject null hypothesis of homogeneity of variance? \", l$`Pr(>F)`[1])\n\n[1] \"reject null hypothesis of homogeneity of variance?  0.00401958517198785\"\n\nCODEoneway.test(triangular_score ~ pretty_condition, data = df)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  triangular_score and pretty_condition\nF = 8, num df = 4, denom df = 153, p-value = 1e-05\n\nCODEpairwise.t.test(df$triangular_score, df$pretty_condition,\n                 p.adjust.method = \"BH\", pool.sd = FALSE)\n\n\n    Pairwise comparisons using t tests with non-pooled SD \n\ndata:  df$triangular_score and df$pretty_condition \n\n           control text:what text:how img:static\ntext:what  0.04    -         -        -         \ntext:how   0.05    0.88      -        -         \nimg:static 0.06    0.88      0.88     -         \nimg:ixv    5e-06   0.02      0.02     0.02      \n\nP value adjustment method: BH \n\nCODEpaste(\"2| Residuals of ANOVA are normally distributed?\")\n\n[1] \"2| Residuals of ANOVA are normally distributed?\"\n\nCODE# Extract the residuals\nresid <- residuals(object = aov )\n# Run Shapiro-Wilk test\ns <- shapiro.test(x = resid )\npaste (\"reject null hypothesis of normally distributed residuals?\", s$p.value)\n\n[1] \"reject null hypothesis of normally distributed residuals? 6.69360335751768e-10\"\n\n\nBecause the residuals of the ANOVA are not normally distributed, (and variance is not homogenous) we need to use a non-parametric alternative test : The Kruskal-Wallis rank sum test.\nNon Parametric Test\n\nCODE#KRUSKAL WALLIS RANK SUM TEST \nkruskal.test(triangular_score ~ pretty_condition, data = df)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  triangular_score by pretty_condition\nKruskal-Wallis chi-squared = 21, df = 4, p-value = 0.0003\n\nCODE#POSTHOC \npairwise.wilcox.test(df$triangular_score, df$pretty_condition,\n  p.adjust.method = \"holm\"\n)\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df$triangular_score and df$pretty_condition \n\n           control text:what text:how img:static\ntext:what  0.2     -         -        -         \ntext:how   0.2     1.0       -        -         \nimg:static 0.2     1.0       1.0      -         \nimg:ixv    4e-05   0.1       0.2      0.1       \n\nP value adjustment method: holm \n\n\nBecause the residuals of the ANOVA are not normally distributed, (and variance is not homogenous) we need to use a non-parametric alternative test : The Kruskal-Wallis rank sum test.\nInference\nA Kruskal-Wallis test reveals that triangle graph scores are significantly different by condition, though posthoc (holm-adjusted) Wilcoxon rank sum tests show than only the interactive image condition yielded significantly higher score than those in the control condition. Visual inspection of the distribution of triangular scores reveals substantial variance across all conditions. It is likely that the scaffolds function not by helping most subjects a little bit, but only some subjects a great deal.\n\nCODE# ++++++++++++++++++++\n# BETWEEN SUBJECTS PLOT\nggbetweenstats(y = triangular_score, x = condition, data = df,\n    type = \"nonparametric\",\n    pairwise.display = \"significant\",\n)\n\n\n\n\nLINEAR REGRESSION\nFit a linear model (at the subject level), predicting triangular block accuracy (absolute score) by condition\nFit Model\nFirst, we fit a linear regression with graph as predictor, and compare its fit to an empty (intercept-only) model.\n\nCODE# MODEL FITTING:::::::::::::::::::::::::::::::::::::\n\n#: 1 EMPTY MODEL [grand mean as intercept]\nm0 = lm(triangular_score ~ 1, data = df)\nprint(\"EMPTY MODEL\")\n\n[1] \"EMPTY MODEL\"\n\nCODEsummary(m0)\n\n\nCall:\nlm(formula = triangular_score ~ 1, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8.77  -3.77   1.23   3.23   6.23 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     8.77       0.25      35   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.45 on 315 degrees of freedom\n\nCODE#: 2 CONDITION as predictor\nm1 = lm(triangular_score ~ pretty_condition, data = df)\nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsummary(m1)\n\n\nCall:\nlm(formula = triangular_score ~ pretty_condition, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -9.68  -2.90   1.19   3.32   7.10 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   6.902      0.552   12.51  < 2e-16 ***\npretty_conditiontext:what     1.912      0.787    2.43    0.016 *  \npretty_conditiontext:how      1.795      0.765    2.35    0.020 *  \npretty_conditionimg:static    1.663      0.777    2.14    0.033 *  \npretty_conditionimg:ixv       3.775      0.760    4.97  1.1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.31 on 311 degrees of freedom\nMultiple R-squared:  0.0742,    Adjusted R-squared:  0.0623 \nF-statistic: 6.23 on 4 and 311 DF,  p-value: 0.0000775\n\nCODEconfint(m1)\n\n                           2.5 % 97.5 %\n(Intercept)                5.816   7.99\npretty_conditiontext:what  0.363   3.46\npretty_conditiontext:how   0.289   3.30\npretty_conditionimg:static 0.134   3.19\npretty_conditionimg:ixv    2.279   5.27\n\nCODE#: 3 TEST SUPERIOR FIT\ntest_lrt(m0,m1) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName | Model | df | df_diff |  Chi2 |      p\n--------------------------------------------\nm0   |    lm |  2 |         |       |       \nm1   |    lm |  6 |       4 | 24.37 | < .001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,m1))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0.0000672264236399889\"\n\n\nThe CONDITION predictor significantly improves model fit.\nVisualize\n\nCODE# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\n#: 4 ASSESS PERFORMANCE\nprint(\"MODEL PERFORMANCE\")\n\n[1] \"MODEL PERFORMANCE\"\n\nCODEperformance(m1)\n\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n-------------------------------------------------------\n1827.001 | 1849.536 | 0.074 |     0.062 | 4.275 | 4.310\n\nCODEprint(\"SANITY CHECK REPORTING\")\n\n[1] \"SANITY CHECK REPORTING\"\n\nCODEreport(m1)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a linear model (estimated using OLS) to predict triangular_score with pretty_condition (formula: triangular_score ~ pretty_condition). The model explains a statistically significant and weak proportion of variance (R2 = 0.07, F(4, 311) = 6.23, p < .001, adj. R2 = 0.06). The model's intercept, corresponding to pretty_condition = control, is at 6.90 (95% CI [5.82, 7.99], t(311) = 12.51, p < .001). Within this model:\n\n  - The interaction effect of what on pretty conditiontext is statistically significant and positive (beta = 1.91, 95% CI [0.36, 3.46], t(311) = 2.43, p = 0.016; Std. beta = 0.43, 95% CI [0.08, 0.78])\n  - The interaction effect of how on pretty conditiontext is statistically significant and positive (beta = 1.80, 95% CI [0.29, 3.30], t(311) = 2.35, p = 0.020; Std. beta = 0.40, 95% CI [0.06, 0.74])\n  - The interaction effect of static on pretty conditionimg is statistically significant and positive (beta = 1.66, 95% CI [0.13, 3.19], t(311) = 2.14, p = 0.033; Std. beta = 0.37, 95% CI [0.03, 0.72])\n  - The interaction effect of ixv on pretty conditionimg is statistically significant and positive (beta = 3.77, 95% CI [2.28, 5.27], t(311) = 4.97, p < .001; Std. beta = 0.85, 95% CI [0.51, 1.18])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODElibrary(jtools) # pretty printing \nsumm(m1)\n\n\n\n\n Observations \n    316 \n  \n\n Dependent variable \n    triangular_score \n  \n\n Type \n    OLS linear regression \n  \n\n\n\n F(4,311) \n    6.23 \n  \n\n R² \n    0.07 \n  \n\n Adj. R² \n    0.06 \n  \n\n\n\n   \n    Est. \n    S.E. \n    t val. \n    p \n  \n\n\n (Intercept) \n    6.90 \n    0.55 \n    12.51 \n    0.00 \n  \n\n pretty_conditiontext:what \n    1.91 \n    0.79 \n    2.43 \n    0.02 \n  \n\n pretty_conditiontext:how \n    1.80 \n    0.77 \n    2.35 \n    0.02 \n  \n\n pretty_conditionimg:static \n    1.66 \n    0.78 \n    2.14 \n    0.03 \n  \n\n pretty_conditionimg:ixv \n    3.77 \n    0.76 \n    4.97 \n    0.00 \n  \n\n\n Standard errors: OLS\n\n\nCODE#: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \n# library(ggstatsplot)\nggcoefstats(m1, output = \"plot\") \n\n\n\nCODE#SJPLOT | MODEL | PROBABILITIES\nplot_model(m1, type=\"pred\",\n           title = \"Model Predicted Score\",\n           axis.title = c(\"Graph Block\",\"Score [0,15]\"))\n\n$pretty_condition\n\n\n\n\n\nDiagnostics\n\nCODEcheck_model(m1)\n\n\n\nCODEbinned_residuals(m1)\n\nWarning: About 89% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\n\n\n\nInference\nWe fit a linear regression model to analyze the effect of experimental condition triangular block score. In this model, the effect of condition is statistically significant (F(4,311) = 6.23, p < 0.001) and explains approximately 7% of variance in triangular block scores. All conditions yield score significantly higher than control at the 0.05 alpha level, with the interactive image yielding the highest difference, with an average of 4 points higher [95% CI 2.3, 5.3] than the control condition.\n\nCODE#PRETTY TABLE SJPLOT\ntab_model(m1)\n\n\n\n\n \ntriangular score\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n6.90\n5.82 – 7.99\n<0.001\n\n\npretty conditiontext *what\n1.91\n0.36 – 3.46\n0.016\n\n\npretty conditiontext *how\n1.80\n0.29 – 3.30\n0.020\n\n\npretty conditionimg *static\n1.66\n0.13 – 3.19\n0.033\n\n\npretty conditionimg * ixv\n3.77\n2.28 – 5.27\n<0.001\n\n\nObservations\n316\n\n\nR2 / R2 adjusted\n0.074 / 0.062\n\n\n\n\nMIXED EFFECTS LOGISTIC REGRESSION\nFit a logistic regression (at the subject level), predicting triangular item accuracy (absolute score) by condition.\n\nCODEdf <- df_items %>% filter(graph == \"triangular\")\n\n\nFit Model\nFirst, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.\n\nCODE# MODEL FITTING:::::::::::::::::::::::::::::::::::::\n\n#: 1 EMPTY MODEL baseline glm model intercept only\nm0 = glm(score ~ 1, data = df, family = \"binomial\")\nprint(\"EMPTY MODEL\")\n\n[1] \"EMPTY MODEL\"\n\nCODEsummary(m0)\n\n\nCall:\nglm(formula = score ~ 1, family = \"binomial\", data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -1.33   -1.33    1.04    1.04    1.04  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.3422     0.0295    11.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6432.9  on 4738  degrees of freedom\nResidual deviance: 6432.9  on 4738  degrees of freedom\nAIC: 6435\n\nNumber of Fisher Scoring iterations: 4\n\nCODE#: RANDOM SUBJECT INTERCEPT \nmm.0 <- glmer( score ~ (1|subject), data = df, family = \"binomial\")\nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsummary(mm.0)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: score ~ (1 | subject)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    5401     5414    -2699     5397     4737 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-2.783 -0.571  0.359  0.626  2.537 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 2.26     1.5     \nNumber of obs: 4739, groups:  subject, 316\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.4336     0.0923     4.7  2.6e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCODE#: TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", m0$aic > AIC(mm.0))\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\nCODEtest_lrt(m0,mm.0) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName |    Model | df | df_diff |    Chi2 |      p\n-------------------------------------------------\nm0   |      glm |  1 |         |         |       \nmm.0 | glmerMod |  2 |       1 | 1035.52 | < .001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(m0,mm.0))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  3.41580110983183e-227\"\n\nCODE#: CONDITION MODEL\nmm1 <- glmer(score ~ pretty_condition + (1|subject), data = df, family = \"binomial\")\nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsummary(mm1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: score ~ pretty_condition + (1 | subject)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n    5386     5424    -2687     5374     4733 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-2.987 -0.576  0.366  0.613  2.675 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n subject (Intercept) 2.06     1.43    \nNumber of obs: 4739, groups:  subject, 316\n\nFixed effects:\n                           Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                  -0.241      0.201   -1.20    0.230    \npretty_conditiontext:what     0.660      0.287    2.30    0.021 *  \npretty_conditiontext:how      0.656      0.280    2.34    0.019 *  \npretty_conditionimg:static    0.606      0.284    2.13    0.033 *  \npretty_conditionimg:ixv       1.378      0.279    4.94  7.8e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n                 (Intr) prtty_cndtntxt:w prtty_cndtntxt:h prtty_cndtnmg:s\nprtty_cndtntxt:w -0.701                                                  \nprtty_cndtntxt:h -0.719  0.504                                           \nprtty_cndtnmg:s  -0.709  0.497            0.510                          \nprtty_cndtnmg:x  -0.722  0.507            0.520            0.512         \n\nCODE#: 3 TEST SUPERIOR FIT\npaste(\"AIC wth predictor is lower than empty model?\", AIC(mm.0) > AIC(mm1))\n\n[1] \"AIC wth predictor is lower than empty model? TRUE\"\n\nCODEtest_lrt(mm.0,mm1) #same as anova(m0, m1, test = \"Chi\")\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nName |    Model | df | df_diff |  Chi2 |      p\n-----------------------------------------------\nmm.0 | glmerMod |  2 |         |       |       \nmm1  | glmerMod |  6 |       4 | 23.78 | < .001\n\nCODEpaste(\"Likelihood Ratio test is significant? p = \",(test_lrt(mm.0,mm1))$p[2])\n\n[1] \"Likelihood Ratio test is significant? p =  0.0000883541191174091\"\n\n\nThe Condition predictor significantly improves model fit.\nVisualize\n\nCODE# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: \n\nprint(\"PREDICTOR MODEL\")\n\n[1] \"PREDICTOR MODEL\"\n\nCODEsumm(mm1)\n\n\n\n\n Observations \n    4739 \n  \n\n Dependent variable \n    score \n  \n\n Type \n    Mixed effects generalized linear model \n  \n\n Family \n    binomial \n  \n\n Link \n    logit \n  \n\n\n\n AIC \n    5385.62 \n  \n\n BIC \n    5424.41 \n  \n\n Pseudo-R² (fixed effects) \n    0.04 \n  \n\n Pseudo-R² (total) \n    0.41 \n  \n\n\n\nFixed Effects\n\n   \n    Est. \n    S.E. \n    z val. \n    p \n  \n\n\n\n (Intercept) \n    -0.24 \n    0.20 \n    -1.20 \n    0.23 \n  \n\n pretty_conditiontext:what \n    0.66 \n    0.29 \n    2.30 \n    0.02 \n  \n\n pretty_conditiontext:how \n    0.66 \n    0.28 \n    2.34 \n    0.02 \n  \n\n pretty_conditionimg:static \n    0.61 \n    0.28 \n    2.13 \n    0.03 \n  \n\n pretty_conditionimg:ixv \n    1.38 \n    0.28 \n    4.94 \n    0.00 \n  \n\n\n\n\nRandom Effects\n\n Group \n    Parameter \n    Std. Dev. \n  \n\n\n subject \n    (Intercept) \n    1.43 \n  \n\n\n\nGrouping Variables\n\n Group \n    # groups \n    ICC \n  \n\n\n subject \n    316 \n    0.38 \n  \n\n\nCODE#: INTERPRET COEFFICIENTS\n\n# print(\"Coefficients —- LOG ODDS\")\n# summ(confint(mm1))\n# print(\"Coefficients —- ODDS RATIOS\")\n# e <- cbind( exp(coef(mm1)), exp(confint(mm1))) #exponentiate\n# e\n\nprint(\"MODEL PERFORMANCE\")\n\n[1] \"MODEL PERFORMANCE\"\n\nCODEperformance(mm1)\n\n# Indices of model performance\n\nAIC      |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n--------------------------------------------------------------------------------------------------------------\n5385.624 | 5424.405 |      0.406 |      0.035 | 0.385 | 0.396 | 1.000 |    0.482 |      -Inf |           0.001\n\nCODEprint(\"SANITY CHECK REPORTING\")\n\n[1] \"SANITY CHECK REPORTING\"\n\nCODEreport(mm1)\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer) to predict score with pretty_condition (formula: score ~ pretty_condition). The model included subject as random effect (formula: ~1 | subject). The model's total explanatory power is substantial (conditional R2 = 0.41) and the part related to the fixed effects alone (marginal R2) is of 0.04. The model's intercept, corresponding to pretty_condition = control, is at -0.24 (95% CI [-0.64, 0.15], p = 0.230). Within this model:\n\n  - The interaction effect of what on pretty conditiontext is statistically significant and positive (beta = 0.66, 95% CI [0.10, 1.22], p = 0.021; Std. beta = 0.66, 95% CI [0.10, 1.22])\n  - The interaction effect of how on pretty conditiontext is statistically significant and positive (beta = 0.66, 95% CI [0.11, 1.20], p = 0.019; Std. beta = 0.66, 95% CI [0.11, 1.20])\n  - The interaction effect of static on pretty conditionimg is statistically significant and positive (beta = 0.61, 95% CI [0.05, 1.16], p = 0.033; Std. beta = 0.61, 95% CI [0.05, 1.16])\n  - The interaction effect of ixv on pretty conditionimg is statistically significant and positive (beta = 1.38, 95% CI [0.83, 1.92], p < .001; Std. beta = 1.38, 95% CI [0.83, 1.92])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\nCODE# print(\"MODEL PREDICTIONS\")\n# # Retrieve predictions as probabilities \n# # (for each level of the predictor)\n# p.control <- predict(m1,data.frame(pretty_condition=\"control\"),type=\"response\")\n# paste(\"Probability of success in control,\", p.control)\n# p.textwhat <- predict(m1,data.frame(pretty_condition=\"text:what\"),type=\"response\")\n# paste(\"Probability of success in text:what,\", p.textwhat)\n# p.textwhat <- predict(m1,data.frame(pretty_condition=\"text:what\"),type=\"response\")\n# paste(\"Probability of success in text:what,\", p.textwhat)\n\n#: PLOT\n\n#GGSTATS | MODEL | LOG ODDS \n# library(ggstatsplot)\nggcoefstats(mm1, output = \"plot\") + labs(x = \"Log Odds Estimate\")\n\nWarning: Removed 1 rows containing missing values (geom_errorbarh).\n\n\n\n\nCODE#SJPLOT | MODEL | RANDOM EFFECTS\nplot_model(mm1, type=\"re\")\n\nWarning in checkMatrixPackageVersion(): Package version inconsistency detected.\nTMB was built with Matrix version 1.4.1\nCurrent Matrix version is 1.4.0\nPlease re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n\n\n\n\nCODE#SJPLOT | MODEL | ODDS RATIO\nplot_model(mm1, vline.color = \"red\", \n           show.intercept = TRUE, \n           show.values = TRUE) +  \n  labs(title = \"Model Predicted Odds Ratio\",\n       subtitle = \"\",\n       x = \"Condition\")\n\n\n\nCODE#SJPLOT | MODEL | PROBABILITIES\nplot_model(mm1, type=\"pred\",\n           show.intercept = TRUE,\n           show.values = TRUE,\n           title = \"Model Predicted Probability of Accuracy\",\n           axis.title = c(\"Condition\",\"Probability of Accurate Response\"))\n\n$pretty_condition\n\n\n\n\nCODE#GGEFFECTS | MODEL | PROBABILITIES\n# library(ggeffects)\n# ggeffect(model = m1) %>% plot()\n\n\nDiagnostics\n\nCODEcheck_model(mm1)\n\n\n\nCODEbinned_residuals(mm1)\n\nWarning: About 82% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\n\n\n\nInference\nWe fit a mixed logistic regression model to analyze the effect of experimental condition on probability of a correct answer on the triangular graph block, including subject as a random intercept. The model explains a small proportion of variance (Pseudo R2 = 0.04). In this model, the fixed effect of condition is statistically accounts for 4% of variance, while the random effects component (conditional r2) accounts for 41%.\nTODO EXPONENTIATE - The model predicts that the odds of a correct response on a triangular graph question question in either of the text or static image conditions increases by a factor of about 1.6 relative to control, while participants in the interactive image condition experience an increase in odds of around 2.8 relative to control ((\\(e^{beta_1}\\) = 2.98, 95% CI [2.4, 3.5]) over the control condition.\nEquivalent statements:\n\nCODE#PRETTY TABLE SJPLOT\ntab_model(m1)\n\n\n\n\n \ntriangular score\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n6.90\n5.82 – 7.99\n<0.001\n\n\npretty conditiontext *what\n1.91\n0.36 – 3.46\n0.016\n\n\npretty conditiontext *how\n1.80\n0.29 – 3.30\n0.020\n\n\npretty conditionimg *static\n1.66\n0.13 – 3.19\n0.033\n\n\npretty conditionimg * ixv\n3.77\n2.28 – 5.27\n<0.001\n\n\nObservations\n316\n\n\nR2 / R2 adjusted\n0.074 / 0.062\n\n\n\n\nTODO\n\nAre these residuals OK? I didn’t think normally distributed residuals were an assumption for logistic regression.\nInterpretation/reporting of model fit?\nsanity check correct interpretation of coefficients & reporting"
  },
  {
    "objectID": "analysis/SGC2/4_sgc2_hypotesting.html#h3-computational-efficiency",
    "href": "analysis/SGC2/4_sgc2_hypotesting.html#h3-computational-efficiency",
    "title": "3  Hypothesis Testing",
    "section": "H3 | Computational Efficiency",
    "text": "H3 | Computational Efficiency\nHypothesis Qiang et. al found that the TM graph was more computationally efficient than the LM graph. We expect that for learners that do correctly interpret the graph, they will have lower response times for the TM vs. LM graph.**\nHypothesis All of the designs offered by participants in Study 1 are promising. We expect that only a small amount of scaffolding (a little nudge) will be required to help readers correctly interpret the graph. We predict that learners with (any form of) scaffolding will perform better with the TM than LM (a replication of Qiang et. al’s finding on the computational efficiency of the TM graph).\nTODO RE-EVALUATE THIS A difference score of 0 represents the case where the particpant performs equally well on the linear and triangular blocks. Positive scores indicate better performance on the triangular block, and negative scores indicate better performance on the linear block.\nTo determine that a scaffold is effective, we expect to see a difference score greater than or equal to 0 such that the participant performs at least as well on the triangular block as the linear block. +———————–+—————————————————————————————————————–+ | Research Question | Does SCAFFOLDING result in superior performance on the Triangular graph? | +=======================+=================================================================================================================+ | Hypothesis | Ss in any of the SCAFFOLD conditions will have higher scores on the TRIANGULAR graph than the LINEAR graph | +———————–+—————————————————————————————————————–+ | Data | data: df_subjects | | | | | | outcome: | | | | | | - difference score diff_score [linear - triangular absolute score] | | | | | | predictor: condition [between-subjects factor] | +———————–+—————————————————————————————————————–+ | Analysis Strategy | 1. Independent Samples ANOVA | | | - compare diff_score between conditions | | | - check assumptions re: variance and normality | | | 2. Linear Regression | | | - predict diff_score by condition | | | - confirm each condition yields significantly higher score than control | | | 3. Mixed Effects Linear Regression | | | - predict block score [0-15] by graph and condition with random intercept for subject | | | - expect significant interaction between fixed effects graph and condition for non-control conditions | | | 4. Logistic Mixed Effects Regression | | | - predict item score by graph and condition with random intercepts for subject and item | | | | | | - expect significant interaction between graph and condition for non-control conditions | +———————–+—————————————————————————————————————–+ | Notes | | +———————–+—————————————————————————————————————–+\nONE WAY ANOVA\n\nCODE# PREPARE DATA \ndf <- df_subjects\n\n\nVisualize\n\nCODE# Box plots\n# ++++++++++++++++++++\n# Plot weight by group and color by group\nlibrary(\"ggpubr\")\nggboxplot(df, x = \"pretty_condition\", y = \"score_diff\", \n          color = \"pretty_condition\", \n          # palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n          # order = c(\"ctrl\", \"trt1\", \"trt2\"),\n          title = \"Difference in Block Scores by Condition\",\n          ylab = \"Triangular (minus) Linear Score\", xlab = \"Condition\") +\n  geom_jitter( aes(color = pretty_condition), width = 0.15, alpha = 0.5) +\n  easy_remove_legend()\n\n\n\nCODE# ++++++++++++++++++++\n# BETWEEN SUBJECTS PLOT\nggbetweenstats(y = score_diff, x = condition, data = df,\n    type = \"nonparametric\",\n    pairwise.display = \"significant\",\n)\n\n\n\n\nRun Test (One Way ANOVA)\n\nCODE#ONE WAY ANOVA\naov <- aov(score_diff ~ condition, data = df)\naov\n\nCall:\n   aov(formula = score_diff ~ condition, data = df)\n\nTerms:\n                condition Residuals\nSum of Squares        484      4840\nDeg. of Freedom         4       311\n\nResidual standard error: 3.94\nEstimated effects may be unbalanced\n\nCODEreport(aov)\n\nFor one-way between subjects designs, partial eta squared is equivalent to eta squared.\nReturning eta squared.\n\n\nWarning: 'data_findcols()' is deprecated and will be removed in a future update.\n  Its usage is discouraged. Please use 'data_find()' instead.\n\n\nThe ANOVA (formula: score_diff ~ condition) suggests that:\n\n  - The main effect of condition is statistically significant and medium (F(4, 311) = 7.77, p < .001; Eta2 = 0.09, 95% CI [0.04, 1.00])\n\nCODE#POSTHOC TUKEY COMPARISONS\nTukeyHSD(aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score_diff ~ condition, data = df)\n\n$condition\n       diff     lwr  upr p adj\n1-0  1.8277 -0.1488 3.80 0.085\n2-0  1.8850 -0.0375 3.81 0.058\n3-0  1.7271 -0.2250 3.68 0.111\n4-0  3.8614  1.9525 5.77 0.000\n2-1  0.0573 -1.8821 2.00 1.000\n3-1 -0.1006 -2.0693 1.87 1.000\n4-1  2.0336  0.1078 3.96 0.033\n3-2 -0.1579 -2.0723 1.76 0.999\n4-2  1.9764  0.1060 3.85 0.032\n4-3  2.1343  0.2335 4.04 0.019\n\nCODEplot(TukeyHSD(aov))\n\n\n\nCODE# library(multcomp)\n# glht(res.aov, alternative = \"less\", # linfct = mcp(pretty_condition = \"Tukey\"))\n#      \n\n\nCheck Assumptions\n\nCODE#ONE WAY ANOVA ASSUMPTIONS\n\npaste(\"1| Homogoneity of variance? \", \"YES, block is crossed within subjects\")\n\n[1] \"1| Homogoneity of variance?  YES, block is crossed within subjects\"\n\nCODEplot(aov, 1)\n\n\n\nCODEpaste(\"2| Sample size? \", \"YES, sample size \",nrow(df), \"> 30\")\n\n[1] \"2| Sample size?  YES, sample size  316 > 30\"\n\nCODEpaste(\"3| Paired differences are normally distributed? accept null [normal] at p > 0.05\")\n\n[1] \"3| Paired differences are normally distributed? accept null [normal] at p > 0.05\"\n\nCODEshapiro.test(df$score_diff) \n\n\n    Shapiro-Wilk normality test\n\ndata:  df$score_diff\nW = 1, p-value = 1e-08\n\n\n_Because the difference scores are not normally distributed, we don’t meet the assumptions of a standard paired t-test. Instead, we should use the alternative test Wilcoxon Rank Sum [paired] designed for non-normal distributions.\nInference\nThe Wilcoxon signed rank test confirms that (for subjects in the control condition) scores on the triangle graph were significantly lower than those in the linear graph block. This provides evidence in support of our hypothesis that the Triangular model graph (though computationally efficient) is in fact unconventional and lacking in discoverablility. It needs to be augmented with scaffolding in order to be correctly interpreted by novice readers."
  },
  {
    "objectID": "analysis/SGC2/4_sgc2_hypotesting.html#h4-graph-order-as-scaffold",
    "href": "analysis/SGC2/4_sgc2_hypotesting.html#h4-graph-order-as-scaffold",
    "title": "3  Hypothesis Testing",
    "section": "H4 | Graph Order as Scaffold",
    "text": "H4 | Graph Order as Scaffold\nHypothesis Based on observations in Study One we expect that graph-order will act as a scaffold. Learners who solve problems with the LM graph first will perform better on the TM (relative to TM-first learners) as their attention will be drawn to the salient differences between the graphs."
  },
  {
    "objectID": "analysis/SGC2/4_sgc2_hypotesting.html#modelling",
    "href": "analysis/SGC2/4_sgc2_hypotesting.html#modelling",
    "title": "3  Hypothesis Testing",
    "section": "MODELLING",
    "text": "MODELLING\n\nCODE# \n# df_items <- df_items %>% filter(graph != \"none\") %>% \n#   mutate(graph = as.factor(graph)) #only include linear and triangular items\n# \n# ##FIT LINEAR MODEL\n# library(lme4)\n# m <- lm(triangular_score ~ pretty_condition + order + tm_scenarios+ linear_score, data = df_subjects)\n# summary(m)\n# check_model(m)\n# \n# #+ linear_score\n# m <- lm(score ~  pretty_condition*graph, data = long_scores)\n# summary(m)\n# check_model(m)\n# \n# \n# ##FI MIXED MODEL\n# library(lmerTest)\n# #on subject, score total per block\n# m0 <- lm( score ~ 1, data = long_scores)\n# summary(m0)\n# \n# m.rS <- lmer(score ~ (1|subject), data = long_scores)\n# summary(m.rS)\n# \n# #compare fit\n# compare_performance(m0,m.rS)\n# test_lrt(m0, m.rS)\n# #validates that mixed mod is justified \n# \n# mC.rS <- lmer(score ~ pretty_condition + (1|subject), data = long_scores)\n# summary(mC.rS)\n# \n# #compare fit\n# compare_performance(m.rS,mC.rS)\n# test_lrt(m.rS, mC.rS)\n# #validates that fixed effect significantly improves fit \n# \n# \n# ##ITEM LEVEL MIXED MODEL\n# \n# #empty model\n# m <- glm(correct ~ 1, data = df_items, family = \"binomial\")\n# summary(m)\n# \n# #condition model\n# m1 <- glm(correct ~ condition, data = df_items, family = \"binomial\")\n# summary(m1)\n# \n# compare_performance(m,m1)\n# test_lrt(m,m1)\n# #condition model is better fit than null\n# \n# #graph model\n# m2 <- glm(correct ~ condition + graph + graph*condition, data = df_items, family = \"binomial\" )\n# summary(m2)\n# \n# library(effects)\n# plot(allEffects(m2))\n# \n# compare_performance(m1,m2)\n# test_lrt(m1,m2)\n# #condition model is better fit than null\n# \n# \n# #scenario model\n# m3 <- glm(correct ~ condition + graph + graph*condition + order + scenario, data = df_items, family = \"binomial\" )\n# summary(m3)\n# \n# library(effects)\n# plot(allEffects(m2))\n# \n# compare_performance(m1,m2)\n# test_lrt(m1,m2)\n# #condition model is better fit than null\n# \n# \n# \n# #IGNORE THE ONES ABOVE \n# \n# #full model\n# m <- glm(correct ~ condition + order + scenario + condition*order + condition*scenario + order*scenario + condition*order*scenario, data = df_items %>% filter(graph==\"triangular\"), family = \"binomial\" )\n# summary(m)\n# \n# library(effects)\n# plot(allEffects(m2))\n# \n# compare_performance(m1,m2)\n# test_lrt(m1,m2)\n# #condition model is better fit than null"
  },
  {
    "objectID": "analysis/SGC2/4_sgc2_hypotesting.html#diagrams-2018-publication-analysis",
    "href": "analysis/SGC2/4_sgc2_hypotesting.html#diagrams-2018-publication-analysis",
    "title": "3  Hypothesis Testing",
    "section": "DIAGRAMS 2018 PUBLICATION ANALYSIS",
    "text": "DIAGRAMS 2018 PUBLICATION ANALYSIS"
  },
  {
    "objectID": "analysis/SGC2/4_sgc2_hypotesting.html#resources",
    "href": "analysis/SGC2/4_sgc2_hypotesting.html#resources",
    "title": "3  Hypothesis Testing",
    "section": "RESOURCES",
    "text": "RESOURCES\nreset plot margins par(mar=c(1,1,1,1))\nLogistic Regression Notes\nThe logistic regression intercept gives the log odds of the outcome for the reference level of the predictor variable\nThe logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable."
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html",
    "title": "15  Introduction",
    "section": "",
    "text": "TODO UPDATE\nIn Study 4C we explore the extent to which the orientation of the axes in space influence how a reader interprets its underlying coordinate system."
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html#methods",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html#methods",
    "title": "15  Introduction",
    "section": "METHODS",
    "text": "METHODS\nDesign\nWe employed a mixed design with 1 between-subjects factor with 3 levels (Mark: POINT, CROSS, ARROW) and 15 items (within-subjects factor).\nIndependent Variables:\n\nB-S (Mark Design: Point, Arrow, Cross )\nW-S (Item x 15)\n\nDependent Variables:\n\nResponse Accuracy : Is the response triangular-correct?\nResponse Interpretation : (derived) With which interpretation of the graph is the subject’s response on an individual question consistent?\nResponse Latency : Time from stimulus onset to clicking ‘Submit’ button: time in (s)\nMaterials\nStimuli consisted of a series of 15 graph comprehension questions, each testing a different combination of time interval relations, to be read from a Triangular-Model graph. Figure 15.1. The list of questions can be found here.\n\n\nFigure 15.1: Sample Question (Q=1) for Graph Comprehension Task\n\n\nIn each experimental\nProcedure\nParticipants completed the study via a web-browser.\n(1) Upon starting, they submitted informed consent, before reading task instructions.\n(2) Participants were introduced to a scenario in which they were to play the role of a project manager, scheduling shifts for a group of employees. The schedule of the employees was presented in a Triangular Model (TM) graph, and they would be answering question about the schedule.\n(3) Then participants completed an experimental block of 15 items: The Graph Comprehension Task.\n(4) Following the experimental block, participants answered a free-response question about their strategy for reading the graph, followed by a demographic questionnaire and debrief.\nSample\nData were collected by convenience sample of a university subject pool during the winter of 2022. Participants accessed the study via a web browser (asynchronously). The stimulus application required the participant stay in full-screen mode for the entirety of the study."
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html#analysis",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html#analysis",
    "title": "15  Introduction",
    "section": "ANALYSIS",
    "text": "ANALYSIS\nData Preparation\nData were collected via a custom web application and stored in a NoSQL database. The following exclusion criteria were applied during data cleaning:\n\ncompletion status : “success” ; subject must have finished all parts of the study, including demographic questionnaire\nsession ID: [in list] ; subject must have been assigned to valid data collection session (discard testing and piloting data)\nbrowser interaction violations < 3; subject must have fewer than 3 violations of non-allowed browser interactions (i.e. resizing window, leaving browser tab or leaving fullscreen mode)\nself-rated effort > 2; subjects who reported, “not trying hard/rushing through questions” or “started out trying hard but giving up at some point” were excluded from analysis.\nattention check ==TRUE ; subjects who failed to answer a mid-study attention check question (Graph Comprehension Task Question #6) are excluded\n\n\n\n\n\nThe underlying data structure of the stimulus web application changed across the data collection period, resulting in slightly different data files (i.e. columns are not named consistently). In this section, we combine the files from each data collection period into a single harmonized data file for analysis (one for participants, one for items).\nParticipants\nFirst we import participant-level data, selecting only the columns relevant for analysis. The result is a single data frame df_subjects containing one row for each subject (across all periods). Note that we are not discarding any response data. Rather, we discard columns that are automatically recorded by the stimulus web application and help the application run.\nNote that we discard some columns representing scores calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. No raw data (responses and response times) are discarded, only algorithmically-derived scores for the responses.\n\nCODE#IMPORT PARTICIPANT DATA\n\n# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#import file\ndf_subjects <- read_rds(\"analysis/SGC4C/data/0-session-level/sgc4c_participants.rds\") #use RDS file as it contains metadata\n\n#NO EXPLANATION COLUMN IN SGC4c DATASET; TRIAL NOT COLLECTED \n#save 'explanation' columns from winter22, which is actually a response to a free response item (Q16); was recorded with item_level data in old webapp\n# df_q16 <- df_subjects %>% \n#   select(subject, condition, term , mode, explanation) %>% \n#   mutate(\n#     q = 16,\n#     response = explanation\n#   ) %>% select(-explanation)\n\n#reduce data collected using NEW webapp to useful columns\n#drop absolute score because we rescore in 2_scoring\ndf_subjects <- df_subjects %>% \n  #select only columns we'll be analyzing, discard others\n  dplyr::select( subject, condition, pretty_condition, study, term, mode, \n                 #demographics\n                 gender, age, language, schoolyear, country,\n                 #effort survey\n                 effort, difficulty, confidence, enjoyment, \n                 #explanations\n                 other,disability,\n                 #response characteristics\n                 totaltime_m,\n                 #exploratory factors\n                 violations, browser, width, height\n                 )\n\n\neffort_labels <- c(\"I tried my best on each question\", \"I tried my best on most questions\")\n\n#set factors\ndf_subjects <- df_subjects %>% \n  #refactor factors\n  mutate (\n    subject = factor(subject),\n    condition = factor(condition),\n    term = factor(term),\n    mode = factor(mode),\n    gender = factor(gender),\n    schoolyear = factor(schoolyear, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\",\"Fifth\",\"Other\"))\n  )\n\n\nItems\nNext we import item-level data from each data collection period, selecting only the columns relevant for analysis. The result is a single data frame df_items containing one row for each graph comprehension task question (qs=15) (across all periods). A second data frame df_freeresponse contains one row for each free response strategy question (last question posed to participants in Winter2022) Note that we do not discard any response data. Rather, we do discard several columns representing accuracy scores for responses that were calculated in the stimulus engine. These scores were calculated differently across collection periods, and so we discard them and recalculate scores in the next analysis notebook. Original response data are always preserved.\n\nCODE# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#read datafiles\ndf_items <- read_rds(\"analysis/SGC4C/data/0-session-level/sgc4c_items.rds\") #use RDS file as it contains metadata\n\n#reduce data collected using new webapp\ndf_items <- df_items %>% \n  select(subject, condition, pretty_condition, study, term, mode, question, q, answer, correct, rt_s) %>% #unfactor before combine\n  mutate(\n    subject = as.character(subject),\n    condition = as.character(condition),\n    term = as.character(term),\n    mode = as.character(mode),\n    q = as.integer(q),\n    correct = as.logical(correct)\n  ) %>% \n  mutate(\n    response = str_remove_all(as.character(answer), \",\"),\n    num_o = str_length(response)\n  )%>% \n  # handle NA values (why are some empty responses blank and others NA?) \n  mutate(\n    response = replace_na(response, \"\"),\n    num_o = replace_na(num_o, 0)\n  )\n\n\nValidation\nNext, we validate that we have the complete number of item-level records based on the number of subject-level records\n\nCODE#the number of items should be equal to 15 x the number of subjects\nnrow(df_items) == 15* nrow(df_subjects) #TRUE\n\n[1] TRUE\n\nCODE#each subject should have 15 items\ndf_items %>% group_by(subject) %>% summarise(n = n()) %>% filter(n != 15) %>% nrow() == 0\n\n[1] TRUE\n\n\nExport\nFinally, we export the (session-harmonized) data for analysis, as CSVs, and .RDS (includes metadata)\n\nCODE# HACK WD FOR LOCAL RUNNING?\n#imac = \"/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN\"\n# mbp = \"/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN\"\n#setwd(imac)\n\n#SAVE FILES\nwrite.csv(df_subjects,\"analysis/SGC4C/data/1-study-level/sgc4c_participants.csv\", row.names = FALSE)\nwrite.csv(df_items,\"analysis/SGC4C/data/1-study-level/sgc4c_items.csv\", row.names = FALSE)\n\n#SAVE R Data Structures \n#export R DATA STRUCTURES (include codebook metadata)\nrio::export(df_subjects, \"analysis/SGC4C/data/1-study-level/sgc4c_participants.rds\") # to R data structure file\nrio::export(df_items, \"analysis/SGC4C/data/1-study-level/sgc4c_items.rds\") # to R data structure file"
  },
  {
    "objectID": "analysis/SGC4C/1_sgc4C_introduction.html#resources",
    "href": "analysis/SGC4C/1_sgc4C_introduction.html#resources",
    "title": "15  Introduction",
    "section": "RESOURCES",
    "text": "RESOURCES\n\nCODEsessionInfo()\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.3.4 forcats_0.5.1    stringr_1.4.0    dplyr_1.0.8     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.6    \n [9] ggplot2_3.3.5    tidyverse_1.3.1  codebook_0.9.2  \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3      svglite_2.1.0     lubridate_1.8.0   assertthat_0.2.1 \n [5] digest_0.6.29     utf8_1.2.2        R6_2.5.1          cellranger_1.1.0 \n [9] backports_1.4.1   reprex_2.0.1      labelled_2.9.0    evaluate_0.15    \n[13] highr_0.9         httr_1.4.2        pillar_1.7.0      rlang_1.0.2      \n[17] curl_4.3.2        readxl_1.3.1      data.table_1.14.2 rstudioapi_0.13  \n[21] rmarkdown_2.13    webshot_0.5.2     foreign_0.8-82    htmlwidgets_1.5.4\n[25] munsell_0.5.0     broom_0.7.12      compiler_4.1.1    modelr_0.1.8     \n[29] xfun_0.30         pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2  \n[33] tidyselect_1.1.2  rio_0.5.29        fansi_1.0.2       viridisLite_0.4.0\n[37] crayon_1.5.0      tzdb_0.2.0        dbplyr_2.1.1      withr_2.5.0      \n[41] grid_4.1.1        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1  \n[45] DBI_1.1.2         magrittr_2.0.2    scales_1.1.1      zip_2.2.0        \n[49] cli_3.2.0         stringi_1.7.6     fs_1.5.2          xml2_1.3.3       \n[53] ellipsis_0.3.2    generics_0.1.2    vctrs_0.3.8       openxlsx_4.2.5   \n[57] tools_4.1.1       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n[61] yaml_2.3.5        colorspace_2.0-3  rvest_1.0.2       knitr_1.38       \n[65] haven_2.4.3"
  }
]