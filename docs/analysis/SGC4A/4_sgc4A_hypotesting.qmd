---
subtitle: 'Study SGC4A | Hypothesis Testing'
---

\newpage

# Hypothesis Testing {#sec-SGC4A-hypotesting}

**TODO**

*The purpose of this notebook is test the hypotheses that determined the designs of the SGC4A studies.*

```{r}
#| label: SETUP
#| warning : false
#| message : false

#UTILITIES
library(Hmisc) # %nin% operator
library(broom) #tidy model output
library(mosaic) #favstats
# library(modelr)
library(distributional)
# library(jtools)
# library(pwr) #power analysis

#VISUALIZATION
# library(ggpubr) #arrange plots
# library(cowplot) #arrange shift function plots
library(ggformula) #easy graphs
# # library(vcd) #mosaic plots
# # library(vcdExtra) #mosaic plots
library(kableExtra) #printing tables
library(sjPlot) #visualize model coefficients
library(ggdist) #uncertainty viz
library(gghalves) # plots. in half
library(ggbeeswarm) # violin plot stuffs
library(statsExpressions)
library(ggstatsplot) #plots with stats
library(modelsummary) #latex tables for models!

#MODELLING
# library(rstatix) #helpful testing functions incl wilcoxon, etc
library(report) #easystats reporting
library(see) #easystats visualization
library(performance) #easystats model diagnostics
library(parameters) #easystats model summary and vis
# library(qqplotr) #confint on qq plot
# library(gmodels) #contingency table and CHISQR
# library(equatiomatic) #extract model equation
# library(pscl) #zeroinfl / hurdle models 
library(lme4) #mixed effects models
library(lmerTest) #for CIs in glmer
# library(ggeffects) #visualization log regr models
#MULTINOMIAL 
library(nnet) #multinomial logistic regression [not mixed] #no p values
library(mclogit) #frequentist mixed multinomial logistic regression [mblogit] #gives p values
#BAYESIAN
library(cmdstanr) #executing stan
library(brms) #bayesian mixed multinomials [+ other bayesian reg models]
library(bayestestR) 

library(tidyverse) #ALL THE THINGS

#OUTPUT OPTIONS
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
options(ggplot2.summarise.inform = FALSE)
options(scipen=1, digits=3)

#GRAPH THEMEING
# theme_set(theme_minimal()) 

# Custom ggplot theme to make pretty plots
# Get the font at https://fonts.google.com/specimen/Barlow+Semi+Condensed
theme_clean <- function() {
  theme_minimal(base_family = "Barlow Semi Condensed") +
    theme(panel.grid.minor = element_blank(),
          plot.title = element_text(family = "BarlowSemiCondensed-Bold"),
          axis.title = element_text(family = "BarlowSemiCondensed-Medium"),
          strip.text = element_text(family = "BarlowSemiCondensed-Bold",
                                    size = rel(1), hjust = 0),
          strip.background = element_rect(fill = "grey80", color = NA))
}

set_theme(base = theme_clean())
```

**Research Questions**

**Experimental Hypothesis**\

**Null Hypothesis**\

```{r}
#| label: IMPORT-DATA
#| warning : false
#| message : false


#IMPORT DATA 
df_subjects <- read_rds('analysis/SGC4A/data/2-scored-data/sgc4a_scored_participants.rds') %>% 
  mutate(
    task_percent = DV_percent_NABS
  ) %>% droplevels()


df_items <- read_rds('analysis/SGC4A/data/2-scored-data/sgc4a_scored_items.rds') %>% 
   mutate (
    q = as.factor(q), 
    subject = as.factor(subject),
    accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct"),
    # CODES TVERSKY AS TRI-LIKE
    # state = recode_factor(score_SCALED, #for ordinal
    #                      "-1" = "orth-like",
    #                      "-0.5" = "unknown",
    #                      "0" = "unknown",
    #                      "0.5" = "tri-like",
    #                      "1" = "tri-like"),
    # CODES TVERSKY AS OTHER
    state = recode_factor(score_SCALED, #for ordinal
                         "-1" = "orthogonal",
                         "-0.5" = "other",
                         "0" = "other",
                         "0.5" = "tri-like",
                         "1" = "triangular"),
    state = as.ordered(state)) 

#SEPARATE INTO PARTS 1 AND 2
sgc4a <- df_subjects %>% filter(condition %in% c(111,113)) %>% droplevels()
sgc4b <- df_subjects %>% filter(condition %in% c(111,114,115)) %>% filter(mode=="asynch") %>% droplevels()

sgc4a_items <- df_items %>% filter( subject %in% sgc4a$subject) %>% droplevels()
sgc4b_items <- df_items %>% filter( subject %in% sgc4b$subject) %>% droplevels()
```

# 4A PART ONE AXES


## H1A \| OVERALL TASK  ACCURACY

#### Setup
```{r}
#| label: SETUP-ACC

df_i = df_items %>% filter(q %nin% c(6,9)) %>% 
  dplyr::select(pretty_condition, accuracy, subject,q)

df_s <- df_subjects %>% 
   dplyr::select(pretty_condition, task_percent)

```


#### Describe
```{r}
#| label: DESC-ACC

#:::::::: STACKED PROPORTIONAL BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  # facet_wrap(~pretty_mode) + 
   labs(title = "Overall Accuracy",
       x = "Condition",
       fill = "",
       subtitle="Impasse Condition yields a greater proportion of correct responses")

#:::::::: STACKED BAR CHART BY QUESTION
df_items %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~q) +
   labs(title = "Accuracy by Question",
       x = "Condition",
       fill = "",
       subtitle="Q6 and Q9 are non-discriminative")

#:::::::: FACETED HISTOGRAM
stats = df_s %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(task_percent))
gf_props(~task_percent,
         fill = ~pretty_condition, data = df_s) %>%
  # gf_facet_grid(pretty_condition ~ pretty_mode) %>%
  gf_facet_grid(~pretty_condition) %>%
  gf_vline(data = stats, xintercept = ~mean, color = "red") +
  labs(x = "% Correct",
       y = "proportion of subjects",
       title = "Overall Absolute Score (% Correct)",
       subtitle = "") + theme(legend.position = "blank")

```
```{r}
#| label: DESC2-ACC

title = "Descriptive Statistics of Response Accuracy (Total % Correct)"
tbl1 <- mosaic::favstats(~task_percent, data = df_s) 
tbl1 %>% kbl (caption = title) %>% kable_classic()


title = "Descriptive Statistics of Response Accuracy (Total % Correct) BY CONDITION"
tbl2 <- mosaic::favstats(task_percent ~ pretty_condition, data = df_s) 
tbl2 %>% kbl (caption = title) %>% kable_classic()

```



#### KRUSKAL-WALLIS

##### Test
```{r}
#| label: TEST-ACC

(k <- kruskal.test(df_s$task_percent ~ df_s$pretty_condition))

```

##### Visualize

```{r}
#| label: TEST-VIZ-ACC

(results <- statsExpressions::oneway_anova(data = df_s, 
          x = pretty_condition, y = task_percent,
          type = "nonparametric", alternative = "less"))

#:::::::: STATSPLOT | VIOLIN
ggbetweenstats(y = task_percent, x = pretty_condition, 
               data = df_s, type = "nonparametric")

```

#### NO MIXED LOGISTIC REGRESSION

*Fit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.*

##### Fit Model

TODO model with item as random doesn't converge unless use bobyqa optimizer. 
```{r}
#| label: MODEL-FIT-ACC

## 0 | SETUP
#confirm 13 items [all discriminating items]
nrow(df_i) / nrow(df_s) == 13
#confirm all factors 
is.factor(df_i$q) && is.factor(df_i$subject) && is.factor(df_i$pretty_condition) && is.factor(df_i$accuracy)

## 1 | SETUP RANDOM INTERCEPT SUBJECT

#:: EMPTY MODEL (baseline, no random effect)
print("Empty fixed model")
m0 = glm(accuracy ~ 1, family = "binomial", data = df_i) 
# summary(m0)

#:: RANDOM INTERCEPT SUBJECT
print("Subject intercept random model")
mm.rS <- glmer(accuracy ~ (1|subject), data = df_i, family = "binomial")
# summary(mm.rS)

# :: TEST random effect
paste("AIC decreases w/ new model?", m0$aic > AIC(logLik(mm.rS)))
test_lrt(m0,mm.rS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,mm.rS))$p[2])

#:: RANDOM INTERCEPT SUBJECT + ITEM
print("Subject Intercept + Item intercept random model")
mm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i, family = "binomial",
                control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
#summary(mm.rSQ)

# :: TEST random effect
paste("AIC decreases w/ new model?", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)))
test_lrt(mm.rS, mm.rSQ) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rS, mm.rSQ))$p[2])


## 2 | ADD FIXED EFFECT CONDITION

print("FIXED Condition + Subject & Item random intercepts")
mm.CrSQ <- glmer(accuracy ~ pretty_condition + (1|subject) + (1|q) ,
                data = df_i, family = "binomial")
# summary(mm.CrSQ)

paste("AIC decreases w/ new model", AIC(logLik(mm.rSQ)) > AIC(logLik(mm.CrSQ)) )
test_lrt(mm.rSQ,mm.CrSQ) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rSQ,mm.CrSQ))$p[2])
```


##### Describe

```{r}
#| label: MODEL-DESC-ACC

# best model
m <- mm.CrSQ

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m)
print("SIGNIFICANCE TEST [non directional]")
car::Anova(m, type = 2)

#:::::::: MANUAL ONE-SIDED SIGTEST 
#note: anova and chi square are always one-tailed, but that is independent of being one-sided
#https://www.ibm.com/support/pages/can-one-get-one-tailed-tests-logistic-regression-dividing-significance-levels-half
# one-sided (right tail) z test for B COEFFICIENT
#SANITY CHECK 2-tailed test should match the model output
tt <- 2*pnorm(summary(m)$coefficients[2,3], lower.tail = F)
paste("p value for two-tailed test, null B = 0 : ",round(tt,5))
ot <- pnorm(summary(m)$coefficients[2,3], lower.tail = F)
paste("BUT we want a one  directional, null: B <= 0: ",round(ot,5))

#:::::::: INTERPRET COEFFICIENTS

se <- sqrt(diag(stats::vcov(m)))
# table of estimates with 95% CI
paste("LOG ODDS")
(tab <- cbind(Est = fixef(m), 
              LL = fixef(m) - 1.96 * se, 
              UL = fixef(m) + 1.96 * se))
paste("ODDS RATIOS")
(e <- exp(tab))

```


##### TODO Inference

**TODO REDO UPDATE THIS**

We fit a mixed-effect binomial logistic regression model with random intercepts for subjects and items; to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(4): 11.02, p \< 0.001); and the total explanatory power is substantial (conditional R2 = 0.82) and the part related to the fixed effects (marginal R2) is 0.02. Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 2.8 over the control condition $e^{\beta_1}$ = 2.80, 95% CI \[1.54, 5.09\], p \< 0.001.


##### Visualize

```{r}
#| label: MODEL-VIS-ACC

## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
plot_model(m, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Estimate | Odds Ratio",
       subtitle = "",
       x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
result <- model_parameters(m, exponentiate = TRUE, component = "all")
plot(result)


## | PLOT TESTS

result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)


## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
plot_model(m, type="eff",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Prediction | Probability of Accurate Response",
           axis.title = c("Condition","Probability of Accurate Response"))

#PLOT MODEL PREDICTION
plot_model(m, type = "pred")  
  # ylim(0,1) + 
  # labs(
  #   title = "Model Prediction | Probability of Accurate Response",
  #   subtitle = "Impasse increases Probability of Correct Response"
  # )

```



##### Diagnostics

```{r}
print("SANITY CHECK REPORTING")
# report(m)

print("DIAGNOSTICS")
check_model(m)

```

## H1A \| OVERALL INTERPRETATION STATE

-   **"orthogonal"** \[reference category\] includes orthogonal and satisficing responses ==\> indicates a primarily *orthogonal* state of coordinate system understanding

-   **"other"** includes: blank, reference point, responses that can't be classified (including selecting all datapoints), =\> indicates an uncertain or unidentifiable state of coordinate system understanding, but one that is distinctly *not* orthogonal nor triangular

-   **"angular"** includes 'lines connecting' responses as well as responses that include both orthogonal *and* triangular answers =\> indicates some degree of angular/triangular coordinate understanding

-   **"triangular"** includes correct triangular and 'lines connecting' responses as well as responses that include both orthogonal *and* triangular answers =\> indicates some degree of angular/triangular coordinate understanding


+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Does Ss in the impasse condition produce less orthogonal responses across questions?                                          |
+=======================+=================================================================================================================================================+
| **Hypothesis**        | H1A \| Ss in the IMPASSE condition will have a higher likelihood of producing unknown and triangle-like response states across all items |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | -   data: `df_items` where `q nin 6,9` (13 discriminant test phase items)                                                              |
|                       | -   outcome: `state` ( 3 level factor from high_interpretation )                                                                                |
|                       | -   predictor: `condition` \[between-subjects factor\]                                                                                          |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  MIXED Multinomial (Logistic) Regression on state predicted by condition                                                                     |
|                       |                                                                                                                                                 |
|                       | Alternative:                                                                                                                                    |
|                       |                                                                                                                                                 |
|                       | -   MIXED Ordinal regression on state (doesn't meet proportional odds assumption-I think)                                                       |
|                       | -   MIXED Multinomial or Ordinal regression on high_interpretation (some cells are 0, produces problems)                                        |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+

#### Setup

```{r}
#| label: SETUP-STATE

#:::::::: PREP DATA
df_i = df_items %>% filter(q %nin% c(6,9)) %>% dplyr::select(q,subject,state,pretty_condition)
```

```{r}
#:::::::: STACKED BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  # facet_wrap(~pretty_mode) + 
   labs(title = "Interpretation across all Questions",
       x = "Condition",
       fill = "",
       subtitle="")

#:::::::: STACKED BAR CHART
df_i %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = state)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
  facet_wrap(~q) +
   labs(title = "Interpretation by Question",
       x = "Condition",
       fill = "",
       subtitle="")

```


```{r}

#::::::::::::DESCRIPTIVES

table(df_i$state, df_i$pretty_condition) %>% addmargins(2) %>% #display sum for row
      prop.table(margin=2) %>%  #return proportion (of column)
      addmargins(1) #sanity check sum of columns


(t <- table(df_i$state, df_i$pretty_condition) %>% addmargins(2) %>% #display sum for row
      addmargins(1)) #sanity check sum of columns

```

#### MIXED MULTINOMIAL REGRESSION


*Does condition affect the response state of of items across the task?*


##### TODO Fit Model \[mblogit\]

```{r}
#| label: FIT-MBLOGIT-STATE

# TRY AGAIN... MAYBE HUNG? 

# #https://www.elff.eu/software/mclogit/manual/mblogit/
# #"baseline category logit" model matches multinom()
# 
# #check reference level 
# print("Categories (first is reference)")
# levels(df_i$state)
# 
# #FIT EMPTY MODEL
# # print("EMPTY MODEL")
# m.mbl0 <- mblogit(state ~ pretty_condition ,  #no random intercepts; fixed only model 
#                   data = df_i)
# #summary(m.mbl0)
# 
# #FIT PREDICTOR MODEL
# # print("PREDICTOR MODEL")
# m.mbl1 <- mblogit(state ~ pretty_condition , 
#                   random = list( ~ 1|subject, ~1|q), 
#                   data = df_i)
# # summary(m.mbl1)
# 
# #COMPARE MODEL FIT
# paste("AIC wth predictor is lower than empty model?", AIC(m.mbl0) > AIC(m.mbl1))
# test_lrt(m.mbl0, m.mbl1)
# 
# #DESCRIBE MODEL
# summary(m.mbl1)
# 
# #INTERPRET COEFFICIENTS
# cint <- confint(m.mbl1, level = 0.95)
# print("ODDS RATIO")
# (e <- cbind( exp(coef(m.mbl1)), exp(cint))) #exponentiated, adjusted
# 
# #PERFORMANCE
# performance(m.mbl1)
# 
# #TABLE
# tab_model(m.mbl1, transform = "exp", title = "Model Predicted Odds Ratio")

```


##### TODO Inference


##### Fit Model \[brms\]

```{r}
#| label: FIT-BRMS-STATE

#BAYESIAN MIXED VERSION
mm.cat.CrSQ <- brm( state ~ pretty_condition + (1|subject) + (1|q), 
                 data = df_i, 
                 family = "categorical",
                 chains = 4, iter = 2000, warmup = 1000,
                 cores = 4, seed = 1234,
                 backend = "cmdstanr",
                 file ="analysis/SGC4A/models/sgc4a_brms_mixedcat_state.rds")
```


```{r}
#| label: VIS-BRMS-STATE

#set model 
m <- mm.cat.CrSQ
summary(m)

## | PLOT PARAMETERS 

#SJPLOT | MODEL | ODDS RATIO
# plot_model(m, vline.color = "red", 
#            show.intercept = TRUE, 
#            show.values = TRUE,
#            p.threshold = 0.1, #manually adjust to account for directional test
#            ci.lvl = 0.90 ) + #manually adjusted for directional test   
#   labs(title = "Model Estimate | Odds Ratio",
#        subtitle = "",
#        x = "Condition")


#EASYSTATS | MODEL | ODDS RATIO
result <- model_parameters(m, exponentiate = TRUE, component = "all")
plot(result)

# result <- simulate_parameters(m)
# plot(result, stack = FALSE)

#check posterior
pp_check(m, ndraws=1000)

## | PLOT TESTS
result <- equivalence_test(m, rule = "classic", ci=0.9) #classic[tost], , bayes
plot(result)

result <- rope(m)
plot(result)

result <- pd(m)
plot(result)

## | PLOT PREDICTIONS

#SJPLOT | MODEL | PROBABILITIES
# plot_model(m, type="eff",
#            show.intercept = TRUE,
#            show.values = TRUE,
#            title = "Model Prediction | Probability of Accurate Response",
#            axis.title = c("Condition","Probability of Accurate Response"))
# 
# #PLOT MODEL PREDICTION
plot_model(m, type = "pred")


#TODO EMMEANS for the estimated marginal means

#TODO OUTPUT TABLE 
#https://arelbundock.com/posts/modelsummary_multinomial_logit/
# modelsummary(m)

```

##### TODO Inference


##### COMPARE

```{r}
#| label: COMPARE-STATE-MODELS
# compare_models(m.mbl1, mm.cat.CrSQ)
```
The predictions of the manual, frequentist mixed multinomial and bayesian mixed multinomial models are comparable. 



## H1A \| Q1 ACCURACY

**TODO Do Ss in the TRIANGULAR AXES condition have a higher likelihood of producing a correct response to the first question?**


```{r}
#| label: SETUP-Q1ACC

#:::::::: PREP DATA
df <- sgc4a_items %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
   labs(#y = "Correct Response on Q 1",
       title = "Accuracy on First Question by Condition",
       x = "Condition",
       fill = "",
       subtitle="Triangular Axes yield a greater proportion of correct responses")


#:::::::: STATSPLOT
ggbarstats(data = df %>% filter(q==1), x = accuracy, y = condition,
               title = "Q1 Accuracy")

```

#### LOGISTIC REGRESSION [Q1 ACCURACY]

*Fit a logistic regression predicting accuracy (absolute score) by condition.*

##### Fit Model

*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-Q1ACC-LAB
#| warning: false
#| message: false

# MODEL FITTING ::::::::

#: 1 EMPTY MODEL baseline glm model intercept only
m0 = glm(accuracy ~ 1, data = df, family = "binomial")
# print("EMPTY MODEL")
# summary(m0)

#: 2 CONDITION model
m1 <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
# print("PREDICTOR MODEL")
# summary(m1)

#: 3 TEST SUPERIOR FIT
paste("AIC wth predictor is lower than empty model?", m0$aic > m1$aic)
test_lrt(m0,m1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,m1))$p[2])
```

*The Condition predictor model does not offer a better fit than the empty (intercept-only) model*

##### Visualize

```{r}
#| label: MODEL-Q1ACC-LOG-LAB

# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: 

print("PREDICTOR MODEL")
summary(m1)

# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: 

# one-sided (right tail) z test for B COEFFICIENT
#SANITY CHECK 2-tailed test should match the model output
tt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("p value for two-tailed test, null B ARROW = 0 : ",round(tt,3))
ot <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("BUT we want a one tailed directional, null: B ARROW<= 0: ",round(ot,3))

#:::::::: INTERPRET COEFFICIENTS

print("Confidence Interval —- LOG ODDS")
confint(m1)
print("Coefficients —- ODDS RATIOS")
(e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiate


print("MODEL PERFORMANCE")
performance(m1)

print("MODEL PREDICTIONS")
# Retrieve predictions as probabilities 
# (for each level of the predictor)
p.control <- predict(m1,data.frame(pretty_condition="control"),type="response")
paste("Probability of success in control,", p.control)
p.impasse <- predict(m1,data.frame(pretty_condition="impasse"),type="response")
paste("Probability of success in impasse,", p.impasse)

#:::::::: PLOT

#GGSTATS | MODEL | LOG ODDS 
ggcoefstats(m1, output = "plot", 
              conf.level = 0.90) + 
  labs(x = "Log Odds Estimate", 
       subtitle = "p is for two tailed test")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(m1, type="eff",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Predicted Probability of Accuracy",
           axis.title = c("Condition","Probability of Accurate Response"))

#SJPLOT | MODEL | TABLE
# tab_model(m1)

```

##### Diagnostics

```{r}
print("SANITY CHECK REPORTING")
report(m1)
check_model(m1)
binned_residuals(m1)
```


##### Sanity Check :: CHI SQR

```{r}
#| label : CHISQR-Q1

#::::::::::::CROSSTABLE
# CrossTable( x = df$condition, y = df$accuracy, 
#              fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)

#::::::::::::MOSAIC PLOT
# note: blue indicates cell count higher than expected, 
# red indicates cell count less than expected; under null hypothesis
# mosaicplot(main="Accuracy on First Question by Condition",
#             data = df, pretty_condition ~ accuracy, 
#             shade = T)

#::::::::::::TABLE
df %>% sjtab( fun = "xtab", var.labels=c("accuracy", "pretty_condition"),
        show.row.prc=F, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,
        statistics = c("auto"))


#::::::::::::BAR PLOT
ggbarstats(data = df, x = accuracy, y = condition,
           type = "nonparametric")

#::::::::::::CHISQR TEST
(x <- stats::chisq.test(x = df$accuracy, y = df$condition, simulate.p.value = T))


#::::::::::::POWER ANALYSIS
(po <- pwr.chisq.test( w = 0.1, df=(2-1), N = nrow(df), sig.level = 0.05))
```


##### Inference

**Both a CHI SQR test of independence and logistic regression indicate that accuracy on Q1 does not vary as a function of condition.**





## H1A \| Q1 INTERPRETATION STATE 

**TODO Do Ss in the TRIANGULAR AXES condition produce more triangular responses to the first question?**


```{r}

#:::::::: PREP DATA
df <- sgc4a_items %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

#:::::::: STACKED BAR CHART
# df %>% 
#   ggplot(data = .,
#          mapping = aes(x = pretty_condition,
#                        fill = state)) +
#   geom_bar(position = "fill" ) + #,color = "black") +
#   scale_fill_brewer(palette = "Set1")  +
#    labs(#y = "Correct Response on Q 1",
#        title = "Interpretation State on First Question by Condition",
#        x = "Condition",
#        fill = "",
#        subtitle="Triangular Axes yield a greater proportion of triangle-like responses")


#:::::::: STATSPLOT
ggbarstats(data = df %>% filter(q==1), x = state, y = condition,
               title = "Q1 State")

```

### MULTINOMIAL REGRESSION 

*Does condition affect the response state of Q1?*

-   <https://peopleanalytics-regression-book.org/multinomial-logistic-regression-for-nominal-category-outcomes.html#running-a-multinomial-regression-model>
-   <https://www.youtube.com/watch?v=JcCBIPqcwFo&list=PLzv58M2GAfm50X_Twskr1aXaV5qMuIszx&ab_channel=NCRMUK>

#### Fit Model

```{r}
library(nnet)

#check reference level 
levels(df$state)

#FIT EMPTY MODEL
catm.0 <- multinom(state ~ 1, data = df)
summary(catm.0)

#FIT PREDICTOR MODEL
catm <- multinom(formula = state ~ condition, data = df, model = TRUE)
summary(catm)

#COMPARE MODEL FIT
paste("AIC decreases w/ new model?", AIC(logLik(catm.0)) > AIC(logLik(catm)))
test_lrt(catm.0, catm) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(catm.0, catm))$p[2])


##compare bayesian version
#library(brms)
# m1 <- brm( state ~ condition, data = df, family = "categorical")
# summary(m1)
# plot_model(m1)
# report(m1)

```

*Likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.*

#### Interpretation

```{r}

#::::::::INTERPRETATION
paste("MODEL SUMMARY")
summary(catm)

# calculate z-statistics of coefficients
(z_stats <- summary(catm)$coefficients/summary(catm)$standard.errors)
# convert to p-values
p_values <- (1 - pnorm(abs(z_stats)))*2
# display p-values in transposed data frame
p_values <- data.frame(p = (p_values))
# display odds ratios in transposed data frame

paste("ODDS RATIOS")
odds_ratios <- data.frame(OR = exp(summary(catm)$coefficients))

# options(scipen = 3)
(results <- cbind(odds_ratios, p_values))

#ASSESS PERFORMANCE
DescTools::PseudoR2(catm, 
                    which = c("McFadden", "CoxSnell", "Nagelkerke"))

```

**Learning Notes**

-   Model estimates encompass two equations:
-   effect of predictor on log odds of being in \[unknown\] instead of reference category \[orth-like\]
-   effect of predictor on log odds of being in \[tri-like\] instead of reference category \[orth-like\]

#### Inference

We fit a multinominal logistic regression model (log-link function) predicting Q1 response state by condition. The resulting model has a Pseudo-R2 (Nagelkerke's) of 0.0398
-   Being in the TRIANGULAR condition _does not_ increase the odds of giving 'unknown/uncertain' response rather than an orthogonal (or satisficing). 
-   Being in the IMPASSE condition _does_ increases the odds of giving an 'triangular or line-driven' response rather than an orthogonal (or satisficing) response by a factor of 2.46 (p \<0.001 )

#### TODO
- kfold or loo validation of model fit? Lit on multinom diagnostics is very thin. 
- Compare with repeated binomial comparisons 

#### Visualize

```{r}
plot_model(catm, vline.color = 'red')
plot_model(catm, type = "eff")

```

#### Diagnostics

```{r}

#EXAMINE PREDICTIONS
#create sample data frame
test <- data.frame(condition = c("111", "113"))
pred <- predict(catm, newdata = test, "probs")
paste("Predicted Probability of Being in Each State")
(cbind(test, pred))

#performance
performance(catm)
DescTools::PseudoR2(catm, which = c("McFadden", "CoxSnell", "Nagelkerke"))

#General Goodness of Fit
# library(generalhoslem)
# logitgof(df$state, catm$fitted.values, g = 3)
# hoslem.test(x = df$state, y = catm$fitted.values, g =  10)
#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).
#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables
```



## H1A \| Q1 INTERPRETATION 

**TODO Do Ss in the TRIANGULAR AXES condition produce more triangular responses to the first question?**


```{r}

#:::::::: PREP DATA
df <- sgc4a_items %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

#:::::::: STACKED BAR CHART
# df %>% 
#   ggplot(data = .,
#          mapping = aes(x = pretty_condition,
#                        fill = state)) +
#   geom_bar(position = "fill" ) + #,color = "black") +
#   scale_fill_brewer(palette = "Set1")  +
#    labs(#y = "Correct Response on Q 1",
#        title = "Interpretation State on First Question by Condition",
#        x = "Condition",
#        fill = "",
#        subtitle="Triangular Axes yield a greater proportion of triangle-like responses")


#:::::::: STATSPLOT
ggbarstats(data = df %>% filter(q==1), x = high_interpretation, y = condition,
               title = "Q1 Interpretation")

```

### MULTINOMIAL REGRESSION 

*Does condition affect the response state of Q1?*

-   <https://peopleanalytics-regression-book.org/multinomial-logistic-regression-for-nominal-category-outcomes.html#running-a-multinomial-regression-model>
-   <https://www.youtube.com/watch?v=JcCBIPqcwFo&list=PLzv58M2GAfm50X_Twskr1aXaV5qMuIszx&ab_channel=NCRMUK>

#### Fit Model

```{r}
library(nnet)

#check reference level 
levels(df$high_interpretation)

#FIT EMPTY MODEL
catm.0 <- multinom(high_interpretation ~ 1, data = df)
summary(catm.0)

#FIT PREDICTOR MODEL
catm <- multinom(formula = high_interpretation ~ condition, data = df, model = TRUE)
summary(catm)

#COMPARE MODEL FIT
paste("AIC decreases w/ new model?", AIC(logLik(catm.0)) > AIC(logLik(catm)))
test_lrt(catm.0, catm) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(catm.0, catm))$p[2])


##compare bayesian version
#library(brms)
# m1 <- brm( state ~ condition, data = df, family = "categorical")
# summary(m1)
# plot_model(m1)
# report(m1)

```

*Likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.*

#### Interpretation

```{r}

#::::::::INTERPRETATION
paste("MODEL SUMMARY")
summary(catm)

# calculate z-statistics of coefficients
(z_stats <- summary(catm)$coefficients/summary(catm)$standard.errors)
# convert to p-values
p_values <- (1 - pnorm(abs(z_stats)))*2
# display p-values in transposed data frame
p_values <- data.frame(p = (p_values))
# display odds ratios in transposed data frame

paste("ODDS RATIOS")
odds_ratios <- data.frame(OR = exp(summary(catm)$coefficients))

# options(scipen = 3)
(results <- cbind(odds_ratios, p_values))

#ASSESS PERFORMANCE
DescTools::PseudoR2(catm, 
                    which = c("McFadden", "CoxSnell", "Nagelkerke"))

```

#### Inference

We fit a multinominal logistic regression model (log-link function) predicting Q1 response state by condition. The resulting model has a Pseudo-R2 (Nagelkerke's) of 0.07
-   Relative to the incorrect orthogonal response, being in the TRIANGULAR AXES condition only increases the odds of giving a lines-connecting response. 


#### TODO
- MAYBE INTERESTING TO COMPARE THIS TO SGC3A WHERE I WOULD EXPECT THAT IT WORKS BY increasing uncertain responses, rather than increasing lines-connecting responses. 


#### Visualize

```{r}
plot_model(catm, vline.color = 'red')
plot_model(catm, type = "eff")

```

#### Diagnostics

```{r}

#EXAMINE PREDICTIONS
#create sample data frame
test <- data.frame(condition = c("111", "113"))
pred <- predict(catm, newdata = test, "probs")
paste("Predicted Probability of Being in Each State")
(cbind(test, pred))

#performance
performance(catm)
DescTools::PseudoR2(catm, which = c("McFadden", "CoxSnell", "Nagelkerke"))

#General Goodness of Fit
# library(generalhoslem)
# logitgof(df$state, catm$fitted.values, g = 3)
# hoslem.test(x = df$state, y = catm$fitted.values, g =  10)
#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).
#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables
```












# 4A PART TWO GRIDLINES

## H1A \| Q1 ACCURACY

**TODO alternative gridlines matter?



```{r}


#:::::::: PREP DATA
df <- sgc4b_items %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
   labs(#y = "Correct Response on Q 1",
       title = "Accuracy on First Question by Condition",
       x = "Condition",
       fill = "",
       subtitle="Triangular Axes yield a greater proportion of correct responses")


#:::::::: STATSPLOT
ggbarstats(data = df %>% filter(q==1), x = accuracy, y = condition,
               title = "Q1 Accuracy")

```

#### LOGISTIC REGRESSION [Q1 ACCURACY]

*Fit a logistic regression predicting accuracy (absolute score) by condition.*

##### Fit Model

*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-Q1ACC-LAB
#| warning: false
#| message: false

# MODEL FITTING ::::::::

#: 1 EMPTY MODEL baseline glm model intercept only
m0 = glm(accuracy ~ 1, data = df, family = "binomial")
# print("EMPTY MODEL")
summary(m0)

#: 2 CONDITION model
m1 <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
# print("PREDICTOR MODEL")
summary(m1)

#: 3 TEST SUPERIOR FIT
paste("AIC wth predictor is lower than empty model?", m0$aic > m1$aic)
test_lrt(m0,m1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,m1))$p[2])
```

*The Condition predictor model does not offer a better fit than the empty (intercept-only) model*

##### Sanity Check :: CHI SQR

```{r}
#| label : CHISQR-Q1

#::::::::::::CROSSTABLE
# CrossTable( x = df$condition, y = df$accuracy, 
#              fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)

#::::::::::::MOSAIC PLOT
# note: blue indicates cell count higher than expected, 
# red indicates cell count less than expected; under null hypothesis
# mosaicplot(main="Accuracy on First Question by Condition",
#             data = df, pretty_condition ~ accuracy, 
#             shade = T)

#::::::::::::TABLE
df %>% sjtab( fun = "xtab", var.labels=c("accuracy", "pretty_condition"),
        show.row.prc=F, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,
        statistics = c("auto"))


#::::::::::::BAR PLOT
ggbarstats(data = df, x = accuracy, y = condition,
           type = "nonparametric")

#::::::::::::CHISQR TEST
(x <- stats::chisq.test(x = df$accuracy, y = df$condition, simulate.p.value = T))


#::::::::::::POWER ANALYSIS
(po <- pwr.chisq.test( w = 0.1, df=(2-1), N = nrow(df), sig.level = 0.05))
```

##### Inference

**Both a CHI SQR test of independence and logistic regression indicate that accuracy on Q1 does not vary as a function of condition.**

_However, this test may be underpowered, as with the given sample size it has only 40% power to detect a small effect (w = 0.1)_**







