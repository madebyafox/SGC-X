---
subtitle: 'Study SGC4A | Hypothesis Testing'
---

\newpage

# Hypothesis Testing {#sec-SGC4A-hypotesting}

**TODO**

*The purpose of this notebook is test the hypotheses that determined the designs of the SGC4A studies.*

```{r}
#| label: SETUP
#| warning : false
#| message : false

#misc utilities
library(Hmisc) # %nin% operator
library(broom)
library(modelr)
library(distributional)
library(jtools)
library(pwr) #power analysis

#visualization
library(ggpubr) #arrange plots
library(cowplot) #arrange shift function plots
library(ggformula) #easy graphs
library(vcd) #mosaic plots
library(vcdExtra) #mosaic plots
library(kableExtra) #printing tables 
library(sjPlot) #visualize model coefficients
library(ggdist) #uncertainty viz 
library(ggstatsplot) #plots with stats

#models and performance
library(report) #easystats reporting
library(see) #easystats visualization
library(performance) #easystats model diagnostics
library(qqplotr) #confint on qq plot
library(gmodels) #contingency table and CHISQR
library(equatiomatic) #extract model equation
library(pscl) #zeroinfl / hurdle models 
library(lme4) #mixed effects models 
library(lmerTest) #for CIs in glmer 
library(ggeffects) #visualization log regr models

library(tidyverse) #ALL THE THINGS

#OUTPUT OPTIONS
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
options(ggplot2.summarise.inform = FALSE)
options(scipen=1, digits=3)

#GRAPH THEMEING
theme_set(theme_minimal()) 

```

**Research Questions**

**Experimental Hypothesis**\

**Null Hypothesis**\

```{r}
#| label: IMPORT-DATA
#| warning : false
#| message : false

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
# mbp = "/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
# setwd(mbp)

#IMPORT DATA 
df_subjects <- read_rds('analysis/SGC4A/data/2-scored-data/sgc4a_scored_participants.rds')
df_items <- read_rds('analysis/SGC4A/data/2-scored-data/sgc4a_scored_items.rds') %>% 
   mutate (
    state = recode_factor(score_SCALED, #for ordinal
                         "-1" = "orth-like",
                         "-0.5" = "unknown",
                         "0" = "unknown",
                         "0.5" = "tri-like",
                         "1" = "tri-like"),
    state = as.ordered(state))

#SEPARATE INTO PARTS 1 AND 2
sgc4a <- df_subjects %>% filter(condition %in% c(111,113)) %>% droplevels()
sgc4b <- df_subjects %>% filter(condition %in% c(111,114,115)) %>% filter(mode=="asynch") %>% droplevels()

sgc4a_items <- df_items %>% filter( subject %in% sgc4a$subject) %>% droplevels()
sgc4b_items <- df_items %>% filter( subject %in% sgc4b$subject) %>% droplevels()
```

# 4A PART ONE AXES

## H1A \| Q1 ACCURACY

**TODO Do Ss in the TRIANGULAR AXES condition have a higher likelihood of producing a correct response to the first question?**


```{r}
#| label: SETUP-Q1ACC

#:::::::: PREP DATA
df <- sgc4a_items %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
   labs(#y = "Correct Response on Q 1",
       title = "Accuracy on First Question by Condition",
       x = "Condition",
       fill = "",
       subtitle="Triangular Axes yield a greater proportion of correct responses")


#:::::::: STATSPLOT
ggbarstats(data = df %>% filter(q==1), x = accuracy, y = condition,
               title = "Q1 Accuracy")

```

#### LOGISTIC REGRESSION [Q1 ACCURACY]

*Fit a logistic regression predicting accuracy (absolute score) by condition.*

##### Fit Model

*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-Q1ACC-LAB
#| warning: false
#| message: false

# MODEL FITTING ::::::::

#: 1 EMPTY MODEL baseline glm model intercept only
m0 = glm(accuracy ~ 1, data = df, family = "binomial")
# print("EMPTY MODEL")
# summary(m0)

#: 2 CONDITION model
m1 <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
# print("PREDICTOR MODEL")
# summary(m1)

#: 3 TEST SUPERIOR FIT
paste("AIC wth predictor is lower than empty model?", m0$aic > m1$aic)
test_lrt(m0,m1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,m1))$p[2])
```

*The Condition predictor model does not offer a better fit than the empty (intercept-only) model*

##### Visualize

```{r}
#| label: MODEL-Q1ACC-LOG-LAB

# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: 

print("PREDICTOR MODEL")
summary(m1)

# MANUAL ONE-SIDED SIGTEST ::::::::::::::::::::::::::: 

# one-sided (right tail) z test for B COEFFICIENT
#SANITY CHECK 2-tailed test should match the model output
tt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("p value for two-tailed test, null B ARROW = 0 : ",round(tt,3))
ot <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("BUT we want a one tailed directional, null: B ARROW<= 0: ",round(ot,3))

#:::::::: INTERPRET COEFFICIENTS

print("Confidence Interval —- LOG ODDS")
confint(m1)
print("Coefficients —- ODDS RATIOS")
(e <- cbind( exp(coef(m1)), exp(confint(m1)))) #exponentiate


print("MODEL PERFORMANCE")
performance(m1)

print("MODEL PREDICTIONS")
# Retrieve predictions as probabilities 
# (for each level of the predictor)
p.control <- predict(m1,data.frame(pretty_condition="control"),type="response")
paste("Probability of success in control,", p.control)
p.impasse <- predict(m1,data.frame(pretty_condition="impasse"),type="response")
paste("Probability of success in impasse,", p.impasse)

#:::::::: PLOT

#GGSTATS | MODEL | LOG ODDS 
ggcoefstats(m1, output = "plot", 
              conf.level = 0.90) + 
  labs(x = "Log Odds Estimate", 
       subtitle = "p is for two tailed test")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(m1, type="eff",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Predicted Probability of Accuracy",
           axis.title = c("Condition","Probability of Accurate Response"))

#SJPLOT | MODEL | TABLE
# tab_model(m1)

```

##### Diagnostics

```{r}
print("SANITY CHECK REPORTING")
report(m1)
check_model(m1)
binned_residuals(m1)
```


##### Sanity Check :: CHI SQR

```{r}
#| label : CHISQR-Q1

#::::::::::::CROSSTABLE
# CrossTable( x = df$condition, y = df$accuracy, 
#              fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)

#::::::::::::MOSAIC PLOT
# note: blue indicates cell count higher than expected, 
# red indicates cell count less than expected; under null hypothesis
# mosaicplot(main="Accuracy on First Question by Condition",
#             data = df, pretty_condition ~ accuracy, 
#             shade = T)

#::::::::::::TABLE
df %>% sjtab( fun = "xtab", var.labels=c("accuracy", "pretty_condition"),
        show.row.prc=F, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,
        statistics = c("auto"))


#::::::::::::BAR PLOT
ggbarstats(data = df, x = accuracy, y = condition,
           type = "nonparametric")

#::::::::::::CHISQR TEST
(x <- stats::chisq.test(x = df$accuracy, y = df$condition, simulate.p.value = T))


#::::::::::::POWER ANALYSIS
(po <- pwr.chisq.test( w = 0.1, df=(2-1), N = nrow(df), sig.level = 0.05))
```


##### Inference

**Both a CHI SQR test of independence and logistic regression indicate that accuracy on Q1 does not vary as a function of condition.**





## H1A \| Q1 INTERPRETATION STATE 

**TODO Do Ss in the TRIANGULAR AXES condition produce more triangular responses to the first question?**


```{r}

#:::::::: PREP DATA
df <- sgc4a_items %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

#:::::::: STACKED BAR CHART
# df %>% 
#   ggplot(data = .,
#          mapping = aes(x = pretty_condition,
#                        fill = state)) +
#   geom_bar(position = "fill" ) + #,color = "black") +
#   scale_fill_brewer(palette = "Set1")  +
#    labs(#y = "Correct Response on Q 1",
#        title = "Interpretation State on First Question by Condition",
#        x = "Condition",
#        fill = "",
#        subtitle="Triangular Axes yield a greater proportion of triangle-like responses")


#:::::::: STATSPLOT
ggbarstats(data = df %>% filter(q==1), x = state, y = condition,
               title = "Q1 State")

```

### MULTINOMIAL REGRESSION 

*Does condition affect the response state of Q1?*

-   <https://peopleanalytics-regression-book.org/multinomial-logistic-regression-for-nominal-category-outcomes.html#running-a-multinomial-regression-model>
-   <https://www.youtube.com/watch?v=JcCBIPqcwFo&list=PLzv58M2GAfm50X_Twskr1aXaV5qMuIszx&ab_channel=NCRMUK>

#### Fit Model

```{r}
library(nnet)

#check reference level 
levels(df$state)

#FIT EMPTY MODEL
catm.0 <- multinom(state ~ 1, data = df)
summary(catm.0)

#FIT PREDICTOR MODEL
catm <- multinom(formula = state ~ condition, data = df, model = TRUE)
summary(catm)

#COMPARE MODEL FIT
paste("AIC decreases w/ new model?", AIC(logLik(catm.0)) > AIC(logLik(catm)))
test_lrt(catm.0, catm) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(catm.0, catm))$p[2])


##compare bayesian version
#library(brms)
# m1 <- brm( state ~ condition, data = df, family = "categorical")
# summary(m1)
# plot_model(m1)
# report(m1)

```

*Likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.*

#### Interpretation

```{r}

#::::::::INTERPRETATION
paste("MODEL SUMMARY")
summary(catm)

# calculate z-statistics of coefficients
(z_stats <- summary(catm)$coefficients/summary(catm)$standard.errors)
# convert to p-values
p_values <- (1 - pnorm(abs(z_stats)))*2
# display p-values in transposed data frame
p_values <- data.frame(p = (p_values))
# display odds ratios in transposed data frame

paste("ODDS RATIOS")
odds_ratios <- data.frame(OR = exp(summary(catm)$coefficients))

# options(scipen = 3)
(results <- cbind(odds_ratios, p_values))

#ASSESS PERFORMANCE
DescTools::PseudoR2(catm, 
                    which = c("McFadden", "CoxSnell", "Nagelkerke"))

```

**Learning Notes**

-   Model estimates encompass two equations:
-   effect of predictor on log odds of being in \[unknown\] instead of reference category \[orth-like\]
-   effect of predictor on log odds of being in \[tri-like\] instead of reference category \[orth-like\]

#### Inference

We fit a multinominal logistic regression model (log-link function) predicting Q1 response state by condition. The resulting model has a Pseudo-R2 (Nagelkerke's) of 0.0398
-   Being in the TRIANGULAR condition _does not_ increase the odds of giving 'unknown/uncertain' response rather than an orthogonal (or satisficing). 
-   Being in the IMPASSE condition _does_ increases the odds of giving an 'triangular or line-driven' response rather than an orthogonal (or satisficing) response by a factor of 2.46 (p \<0.001 )

#### TODO
- kfold or loo validation of model fit? Lit on multinom diagnostics is very thin. 
- Compare with repeated binomial comparisons 

#### Visualize

```{r}
plot_model(catm, vline.color = 'red')
plot_model(catm, type = "eff")

```

#### Diagnostics

```{r}

#EXAMINE PREDICTIONS
#create sample data frame
test <- data.frame(condition = c("111", "113"))
pred <- predict(catm, newdata = test, "probs")
paste("Predicted Probability of Being in Each State")
(cbind(test, pred))

#performance
performance(catm)
DescTools::PseudoR2(catm, which = c("McFadden", "CoxSnell", "Nagelkerke"))

#General Goodness of Fit
# library(generalhoslem)
# logitgof(df$state, catm$fitted.values, g = 3)
# hoslem.test(x = df$state, y = catm$fitted.values, g =  10)
#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).
#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables
```



## H1A \| Q1 INTERPRETATION 

**TODO Do Ss in the TRIANGULAR AXES condition produce more triangular responses to the first question?**


```{r}

#:::::::: PREP DATA
df <- sgc4a_items %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

#:::::::: STACKED BAR CHART
# df %>% 
#   ggplot(data = .,
#          mapping = aes(x = pretty_condition,
#                        fill = state)) +
#   geom_bar(position = "fill" ) + #,color = "black") +
#   scale_fill_brewer(palette = "Set1")  +
#    labs(#y = "Correct Response on Q 1",
#        title = "Interpretation State on First Question by Condition",
#        x = "Condition",
#        fill = "",
#        subtitle="Triangular Axes yield a greater proportion of triangle-like responses")


#:::::::: STATSPLOT
ggbarstats(data = df %>% filter(q==1), x = high_interpretation, y = condition,
               title = "Q1 Interpretation")

```

### MULTINOMIAL REGRESSION 

*Does condition affect the response state of Q1?*

-   <https://peopleanalytics-regression-book.org/multinomial-logistic-regression-for-nominal-category-outcomes.html#running-a-multinomial-regression-model>
-   <https://www.youtube.com/watch?v=JcCBIPqcwFo&list=PLzv58M2GAfm50X_Twskr1aXaV5qMuIszx&ab_channel=NCRMUK>

#### Fit Model

```{r}
library(nnet)

#check reference level 
levels(df$high_interpretation)

#FIT EMPTY MODEL
catm.0 <- multinom(high_interpretation ~ 1, data = df)
summary(catm.0)

#FIT PREDICTOR MODEL
catm <- multinom(formula = high_interpretation ~ condition, data = df, model = TRUE)
summary(catm)

#COMPARE MODEL FIT
paste("AIC decreases w/ new model?", AIC(logLik(catm.0)) > AIC(logLik(catm)))
test_lrt(catm.0, catm) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(catm.0, catm))$p[2])


##compare bayesian version
#library(brms)
# m1 <- brm( state ~ condition, data = df, family = "categorical")
# summary(m1)
# plot_model(m1)
# report(m1)

```

*Likelihood ratio test indicates predictor model is significantly better fit to the sample data than the empty (intercept only) model.*

#### Interpretation

```{r}

#::::::::INTERPRETATION
paste("MODEL SUMMARY")
summary(catm)

# calculate z-statistics of coefficients
(z_stats <- summary(catm)$coefficients/summary(catm)$standard.errors)
# convert to p-values
p_values <- (1 - pnorm(abs(z_stats)))*2
# display p-values in transposed data frame
p_values <- data.frame(p = (p_values))
# display odds ratios in transposed data frame

paste("ODDS RATIOS")
odds_ratios <- data.frame(OR = exp(summary(catm)$coefficients))

# options(scipen = 3)
(results <- cbind(odds_ratios, p_values))

#ASSESS PERFORMANCE
DescTools::PseudoR2(catm, 
                    which = c("McFadden", "CoxSnell", "Nagelkerke"))

```

#### Inference

We fit a multinominal logistic regression model (log-link function) predicting Q1 response state by condition. The resulting model has a Pseudo-R2 (Nagelkerke's) of 0.07
-   Relative to the incorrect orthogonal response, being in the TRIANGULAR AXES condition only increases the odds of giving a lines-connecting response. 


#### TODO
- MAYBE INTERESTING TO COMPARE THIS TO SGC3A WHERE I WOULD EXPECT THAT IT WORKS BY increasing uncertain responses, rather than increasing lines-connecting responses. 


#### Visualize

```{r}
plot_model(catm, vline.color = 'red')
plot_model(catm, type = "eff")

```

#### Diagnostics

```{r}

#EXAMINE PREDICTIONS
#create sample data frame
test <- data.frame(condition = c("111", "113"))
pred <- predict(catm, newdata = test, "probs")
paste("Predicted Probability of Being in Each State")
(cbind(test, pred))

#performance
performance(catm)
DescTools::PseudoR2(catm, which = c("McFadden", "CoxSnell", "Nagelkerke"))

#General Goodness of Fit
# library(generalhoslem)
# logitgof(df$state, catm$fitted.values, g = 3)
# hoslem.test(x = df$state, y = catm$fitted.values, g =  10)
#A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).
#don't fret! this version of the hoslem-lem test is problematic with fewer than 10 input variables
```







## H1B \| OVERALL TASK  ACCURACY


```{r}

#:::::::: PREP DATA
df <- sgc4a_items %>% filter(q %nin% c(6,9)) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 


#:::::::: STACKED PROPORTIONAL BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
   labs(title = "Task Accuracy",
       x = "Condition",
       fill = "",
       subtitle="Impasse Condition yields a greater proportion of correct responses")


#:::::::: FACETED HISTOGRAM
stats = sgc4a %>% group_by(pretty_condition) %>% dplyr::summarise(mean = mean(s_NABS))
gf_props(~s_NABS, 
         fill = ~pretty_condition, data = sgc4a) %>% 
  gf_facet_grid(~pretty_condition) %>% 
  gf_vline(data = stats, xintercept = ~mean, color = "red") +
  labs(x = "# Correct",
       y = "proportion of subjects",
       title = "Task Absolute Score (# Correct)",
       subtitle = "") + theme(legend.position = "blank")


```



```{r}

#:::::::: SGC4A only
df_i <- sgc4a_items %>% filter(q %nin% c(6,9)) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

df_s <- sgc4a  %>% mutate(
  test_score = s_NABS
)

```

#### WILCOXON RANK SUM (Mann-Whitney Test)

-   **Non parametric alternative** to t-test; compares median rather than mean by ranking data
-   Does not assume normality
-   Does not assume equal variance of samples (homogeneity of variance)

```{r}
#| label: WILCOX-ONLINE

(w <- wilcox.test(df_s$test_score ~ df_s$condition,
                 paired = FALSE, alternative = "less")) #less, greater
report(w)

```

```{r}

(results <- statsExpressions::two_sample_test(data = df_s, x = condition, y = test_score,
                                    type = "nonparametric",
                                  alternative = "less"))

#:::::::: STATSPLOT | VIOLIN
ggbetweenstats(y = test_score, x = condition, data = df_s,
               results.subtitle = FALSE,
               subtitle = results$expression[[1]]
               )

```

#### MIXED LOGISTIC REGRESSION

*Fit a mixed logistic regression (at the item level), predicting accuracy (absolute score) on all discriminating questions by condition; accounting for random effects of subject and item.*

##### Fit Model

```{r}
#| label: MODEL-MLOG-ABS-ONLINE

## 1 | SETUP RANDOM INTERCEPT SUBJECT

#:: EMPTY MODEL (baseline, no random effect)
print("Empty fixed model")
m0 = glm(accuracy ~ 1, family = "binomial", data = df_i) 
# summary(m0)

#:: RANDOM INTERCEPT SUBJECT
print("Subject intercept random model")
mm.rS <- glmer(accuracy ~ (1|subject), data = df_i,family = "binomial")
# summary(mm.rS)

# :: TEST random effect
paste("AIC decreases w/ new model?", m0$aic > AIC(logLik(mm.rS)))
test_lrt(m0,mm.rS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,mm.rS))$p[2])


# ## 2 | ADD RANDOM RANDOM INTERCEPT ITEM
# 
# #:: RANDOM INTERCEPT SUBJECT + INTERCEPT Q
# print("Subject & Question random intercepts")
# mm.rSQ <- glmer(accuracy ~ (1|subject) + (1|q), data = df_i,family = "binomial")
# # summary(mm.rSQ)
# 
# # :: TEST random effect
# paste("AIC decreases w/ new model?", AIC(logLik(mm.rS)) > AIC(logLik(mm.rSQ)) )
# test_lrt(mm.rS,mm.rSQ) #same as anova(m0, m1, test = "Chi")
# paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rS,mm.rSQ))$p[2])

## 3 | ADD FIXED EFFECT

# SUBJECT INTERCEPT | FIXED CONDITION 
print("FIXED Condition + Subject & Question random intercepts")
mm.CrS <- glmer(accuracy ~ pretty_condition + (1|subject) , 
                data = df_i,family = "binomial",
                control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))
# summary(mm.CrSQ)

paste("AIC decreases w/ new model", AIC(logLik(mm.rS)) > AIC(logLik(mm.CrS)) )
test_lrt(mm.rS,mm.CrS) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.rS,mm.CrS))$p[2])

```

##### Visualize

```{r}

m <- glmer(accuracy ~ pretty_condition + as.factor(q)+ (1|subject) , 
                data = df_i,family = "binomial",
                control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))


m1 <- mm.CrSQ

#::::::::: PRINT MODEL 

print("PREDICTOR MODEL")
summary(m1)

#:::::::: MANUAL ONE-SIDED SIGTEST 

# one-sided (right tail) z test for B COEFFICIENT
#SANITY CHECK 2-tailed test should match the model output
tt <- 2*pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("p value for two-tailed test, null B = 0 : ",round(tt,5))
ot <- pnorm(summary(m1)$coefficients[2,3], lower.tail = F)
paste("BUT we want a one tailed directional, null: B <= 0: ",round(ot,5))

#:::::::: INTERPRET COEFFICIENTS

print("MODEL PERFORMANCE")
performance(m1)

se <- sqrt(diag(stats::vcov(m1)))
# table of estimates with 95% CI
(tab <- cbind(Est = fixef(m1), LL = fixef(m1) - 1.96 * se, UL = fixef(m1) + 1.96 *
    se))
(e <- exp(tab))

#:::::::: PLOT

#GGSTATS | MODEL | LOG ODDS 
ggcoefstats(m1, output = "plot", 
              conf.level = 0.90) + 
  labs(x = "Log Odds Estimate", 
       subtitle = "p is for two tailed test")

#SJPLOT | MODEL | ODDS RATIO
#library(sjPlot)
plot_model(m1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE,
           p.threshold = 0.1, #manually adjust to account for directional test
           ci.lvl = 0.90 ) + #manually adjusted for directional test   
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(m1, type="eff",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Predicted Probability of Accuracy",
           axis.title = c("Condition","Probability of Accurate Response"))

#SJPLOT | MODEL | TABLE
# tab_model(m1)

```

##### Diagnostics

```{r}
print("SANITY CHECK REPORTING")
report(m1)
check_model(m1)
binned_residuals(m1)
```

##### Inference

We fit a mixed-effect binomial logistic regression model with random intercepts for subjects and items; to investigate the effect of condition on test phase item accuracy. The model including a fixed effect of condition performed significantly better than an intercept-only baseline model (χ2(4): 11.02, p \< 0.001); and the total explanatory power is substantial (conditional R2 = 0.82) and the part related to the fixed effects (marginal R2) is 0.02. Consistent with the pattern of results for the first question only, across all test-phase items, being in the impasse condition increases the odds of a correct response by a factor of 2.8 over the control condition $e^{\beta_1}$ = 2.80, 95% CI \[1.54, 5.09\], p \< 0.001.

```{r}
# PRETTY TABLE SJPLOT
tab_model(mm.CrSQ)
```

##### TODO

-   sanity check interpretation
-   sanity check random effects structure : ITEM appropriate as random intercept? What does it mean to have two random intercepts?
-   DIAGNOSTICS: What in the world is happening with the normality of random effects plot? Do the fixed effects residuals need to be normally distributed?
-   Are there other plots or recommended diagnostics for mixed log regression
-   consider multiple regression with rt, sequence cluster, confidence, etc.
-   What else needs to be interpreted with respect to the item and subject random effects?
-   Double check: can't have condition by subject or item slope bc subjects are nested in conditions, not crossed


### SHIFT IN MODAL MASS

The Effect of Condition on Total Scaled Score can be described as a 'shift' in mass between the low and high modes of each distribution.

*First, we use the Kolmogorov-Smirnov test as a Robust alternative to the t-test to test if the two distributions likely come from different populations.*

```{r}
#| label: COMPARE-DIST-NABS-comb

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
# mbp = "/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
# setwd(mbp)

#(requires shift function files loaded)
#LOAD MODAL SHIFT FUNCTION RESOURCES
source("analysis/utils/shift_function/Rallfun-v30.txt")
source("analysis/utils/shift_function/wilcox_modified.txt")
source("analysis/utils/shift_function/rgar_visualisation.txt")
source("analysis/utils/shift_function/rgar_utils.txt")
#NOTE: something in these breaks the stat_ecdf in ggplot2

#PREP DATA 
df <- df_subjects %>%
  dplyr::select(s_SCALED, pretty_condition) %>%
  mutate(
    data = as.numeric(s_SCALED),
    #flip order levels to correctly orient graph
    # gr = recode_factor(pretty_condition, "impasse" = "impasse", "control"="control")
    gr = as.character(pretty_condition)
  ) %>% dplyr::select(data,gr)


g1 <- df %>% filter(gr == "control") %>% dplyr::pull(data)
g2 <- df %>% filter(gr == "impasse") %>% dplyr::pull(data)


#COMPARE DISTRIBUTIONS WITH ROBUST TESTS

#What do common tests say about the difference?

# Kolmogorov-Smirnov test
#If y is numeric, a two-sample (Smirnov) test of the null hypothesis that x and y 
#were drawn from the same continuous distribution is performed. Alternatively, y ...

#null is X is drawn from CDF EQUAL TO Y
ks.test(g1,g2) 
print("SUGGESTS that impasse and control come from different population distributions")

# #null is X is NOT LESS THAN Y
ks.test(g1,g2, alternative = "greater") 
print("SUGGESTS that true CDF of CONTROL is greater than that of IMPASSE [consistent with higher probability of low scores]")

#REGULAR T-TEST
t.test(g1,g2) # regular Welsh t-test

```

```{r}
#| label: SHIFT-FN-NABS-comb
#| warnings: false
#| messages: false

#IF THIS ERRORS, consider loadling plyr (older than dplyr)
# kernel density estimate + rug plot + superimposed deciles
kde <- plot.kde_rug_dec2(df)
# kde

# compute shift function
out <- shifthd( g1, g2, nboot=200)

# plot shift function
sf <- plot.sf(data=out) # function from rgar_visualisation.txt
# sf

# combine KDE + SF
cowplot::plot_grid(kde, sf, labels=c("A", "B"), ncol = 1, nrow = 2, rel_heights = c(1.5, 1),label_size = 18,hjust = -1,scale=.95)

```










# 4A PART TWO GRIDLINES

## H1A \| Q1 ACCURACY

**TODO alternative gridlines matter?



```{r}


#:::::::: PREP DATA
df <- sgc4b_items %>% filter(q==1) %>% mutate(
  accuracy = recode_factor(score_niceABS, "0" ="incorrect","1"="correct")
) 

#:::::::: STACKED BAR CHART
df %>% 
  ggplot(data = .,
         mapping = aes(x = pretty_condition,
                       fill = accuracy)) +
  geom_bar(position = "fill" ) + #,color = "black") +
  scale_fill_brewer(palette = "Set1")  +
   labs(#y = "Correct Response on Q 1",
       title = "Accuracy on First Question by Condition",
       x = "Condition",
       fill = "",
       subtitle="Triangular Axes yield a greater proportion of correct responses")


#:::::::: STATSPLOT
ggbarstats(data = df %>% filter(q==1), x = accuracy, y = condition,
               title = "Q1 Accuracy")

```

#### LOGISTIC REGRESSION [Q1 ACCURACY]

*Fit a logistic regression predicting accuracy (absolute score) by condition.*

##### Fit Model

*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-Q1ACC-LAB
#| warning: false
#| message: false

# MODEL FITTING ::::::::

#: 1 EMPTY MODEL baseline glm model intercept only
m0 = glm(accuracy ~ 1, data = df, family = "binomial")
# print("EMPTY MODEL")
summary(m0)

#: 2 CONDITION model
m1 <- glm( accuracy ~ pretty_condition, data = df, family = "binomial")
# print("PREDICTOR MODEL")
summary(m1)

#: 3 TEST SUPERIOR FIT
paste("AIC wth predictor is lower than empty model?", m0$aic > m1$aic)
test_lrt(m0,m1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,m1))$p[2])
```

*The Condition predictor model does not offer a better fit than the empty (intercept-only) model*

##### Sanity Check :: CHI SQR

```{r}
#| label : CHISQR-Q1

#::::::::::::CROSSTABLE
# CrossTable( x = df$condition, y = df$accuracy, 
#              fisher = TRUE, chisq=TRUE, expected = TRUE, sresid = TRUE)

#::::::::::::MOSAIC PLOT
# note: blue indicates cell count higher than expected, 
# red indicates cell count less than expected; under null hypothesis
# mosaicplot(main="Accuracy on First Question by Condition",
#             data = df, pretty_condition ~ accuracy, 
#             shade = T)

#::::::::::::TABLE
df %>% sjtab( fun = "xtab", var.labels=c("accuracy", "pretty_condition"),
        show.row.prc=F, show.col.prc=T, show.summary=T, show.exp=T, show.legend=T,
        statistics = c("auto"))


#::::::::::::BAR PLOT
ggbarstats(data = df, x = accuracy, y = condition,
           type = "nonparametric")

#::::::::::::CHISQR TEST
(x <- stats::chisq.test(x = df$accuracy, y = df$condition, simulate.p.value = T))


#::::::::::::POWER ANALYSIS
(po <- pwr.chisq.test( w = 0.1, df=(2-1), N = nrow(df), sig.level = 0.05))
```

##### Inference

**Both a CHI SQR test of independence and logistic regression indicate that accuracy on Q1 does not vary as a function of condition.**

_However, this test may be underpowered, as with the given sample size it has only 40% power to detect a small effect (w = 0.1)_**







