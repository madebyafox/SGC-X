---
subtitle: 'Study SGC2 | Hypothesis Testing'
---

\newpage

# Hypothesis Testing {#sec-SGC2-hypotesting}

**TODO ADD INTERACTIONS**

*The purpose of this notebook is test the hypotheses that determined the design of the SGC2 study.*

```{r}
#| label: SETUP
#| warning : false
#| message : false

library(Hmisc) # %nin% operator
library(mosaic) #favstats
library(jtools) #prettier model summ(m)
library(ggpubr) #arrange plots
library(cowplot) #arrange shift function plots
library(ggformula) #easy graphs
library(vcd) #mosaic plots
library(vcdExtra) #mosaic plots
library(kableExtra) #printing tables 
library(sjPlot) #visualize model coefficients
library(ggeasy) #easy edits to ggplots 

#plot model estimates with uncertainty
library(ggdist)
library(broom)
library(modelr)
library(distributional)

#models and performance
library(lmerTest) #for CIs in glmer 
library(ggstatsplot) #plots w/ embedded stats
library(report) #easystats reporting
library(see) #easystats visualization
library(performance) #easystats model diagnostics
library(qqplotr) #confint on qq plot
library(gmodels) #contingency table and CHISQR
# library(equatiomatic) #extract model equation
# library(pscl) #zeroinfl / hurdle models 
library(lme4) #mixed effects models 
library(ggeffects) #visualization log regr models

library(tidyverse) #ALL THE THINGS

#OUTPUT OPTIONS
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
options(ggplot2.summarise.inform = FALSE)
options(scipen=1, digits=3)

#GRAPH THEMEING
theme_set(theme_minimal()) 

```

**Research Questions**

In SGC2 we compare learner performance on the linear and triangular model graphs by testing the effectiveness of 4 scaffolds and by seeking to replicate the Qiang et.al (2014) finding that after 20 minutes of video training, students perform faster and more accurately with the unconventional TM than the conventional Linear Model (LM). Will our participants show similar performance on the TM with scaffolds rather than formal instruction? Further, will engagement with the TM in a reading task be sufficient for students to reproduce the graph in a subsequent drawing task?

**Hypotheses**

1.  Learners without scaffolding (control) will perform better with the LM than TM
2.  Learners with (any form of) scaffolding will perform better with the TM than LM (replication of \[12\]).
3.  Based on observations in Study One we expect that graph-order will act as a scaffold. Learners who solve problems with the LM graph first will perform better on the TM (relative to TM-first learners) as their attention will be drawn to the salient differences between the graphs.

```{r}
#| label: IMPORT-DATA
#| warning : false
#| message : false

# HACK WD FOR LOCAL RUNNING?
# imac = "/Users/amyraefox/Code/SGC-Scaffolding_Graph_Comprehension/SGC-X/ANALYSIS/MAIN"
mbp = "/Users/amyfox/Sites/RESEARCH/SGC—Scaffolding Graph Comprehension/SGC-X/ANALYSIS/MAIN"
setwd(mbp)

#IMPORT DATA 
df_items <- read_rds('analysis/SGC2/data/2-scored-data/sgc2_items.rds')
df_subjects <- read_rds('analysis/SGC2/data/2-scored-data/sgc2_participants.rds') 

```

## H1 \| The Need for Scaffolding

**Hypothesis** The TM graph is not *discoverable* and requires scaffolding for correct interpretation. We predict that learners without scaffolding (the control condition) will perform better with the LM than TM

+-----------------------+-------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Do Ss in the CONTROL condition perform better on the LINEAR graph than the TRIANGULAR graph?                            |
+=======================+=========================================================================================================================+
| **Hypothesis**        | Ss in the CONTROL condition will have higher scores on the LINEAR graph than the TRIANGULAR graph                       |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------+
| **Data**              | data: `df_subjects` where `condition == 1`                                                                              |
|                       |                                                                                                                         |
|                       | outcome:                                                                                                                |
|                       |                                                                                                                         |
|                       | -   *linear graph accuracy* `linear_score` \[absolute score\]                                                           |
|                       | -   *triangular graph accuracy* `triangular_score`                                                                      |
|                       |                                                                                                                         |
|                       | predictor: `graph` (block) \[within-subjects factor\]                                                                   |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  Paired Samples T-Test                                                                                               |
|                       |     -   compare average accuracy score in linear vs triangular block                                                    |
|                       |     -   either T-Test or Wilcoxon Rank Sum (paired sample) alternative if difference scores are is non normal           |
|                       |                                                                                                                         |
|                       | *Alternative*                                                                                                           |
|                       |                                                                                                                         |
|                       | 1.  Linear Mixed Effects Model \[violates normality of residuals\]                                                      |
|                       |     -   predict subject `score` \[0-15\] by `graph` with random intercept for `subject`                                 |
|                       |     -   demonstrate that `score` is independent of `order` \[nested\] and `scenario` \[nested\]                         |
|                       | 2.  Logistic Mixed Effects Model                                                                                        |
|                       |     -   predict item `score` \[0,1\] by `graph` with random intercept for `subject` and random intercept for `question` |
|                       |                                                                                                                         |
|                       |     -   demonstrate than `score` is independent of `order` and `scenario`                                               |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------+
| **Notes**             |                                                                                                                         |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------+

```{r}
#| label: SETUP-H1

#FILTER THE DATASET
df <- df_subjects %>% filter(condition == 1)

df_long <- df %>% dplyr::select(subject,pretty_condition,order,lm_scenarios,tm_scenarios,linear_score, triangular_score) %>% pivot_longer(
  cols = ends_with("score"),
  names_to = "graph",
  values_to = "score"
)
  

title = "Descriptive Statistics of Response Accuracy by Block (CONTROL Condition)"
abs.stats <- rbind(
  "linear.block"= df %>% dplyr::select(linear_score) %>% unlist() %>% favstats(),
  "triangular.block" = df %>% dplyr::select(triangular_score) %>% unlist() %>% favstats(),
  "block.differences" = df %>% dplyr::select(score_diff) %>% unlist() %>% favstats()
)

abs.stats %>% kbl (caption = title) %>% kable_classic() %>%
  footnote(general = "block # questions correct [0,15]; DIFF = triangular - linear",
           general_title = "Note: ",footnote_as_chunk = T)

#DISTRIBUTION OF SCORE
gf_dhistogram(~score, fill = ~graph, data = df_long) %>% gf_facet_wrap(~graph)+
  labs(title = "Distribution of scores in CONTROL condition") + 
  easy_remove_legend()

##VERTICAL RAINCLOUD USING GGDISTR
ggplot(df_long, aes(x = graph, y = score,
                        fill = graph) ) + 
  ggdist::stat_halfeye(
    side = "left",
    justification = 1.1,
    width = 1, 
    point_colour = NA
   ) + 
  geom_boxplot(
    inherit.aes = FALSE, #supress fill
    mapping = aes(x=graph, y = score),
    width = .15, 
    outlier.shape = NA
  ) + 
  geom_point(
    inherit.aes = FALSE, #supress fill
    mapping = aes(x=graph, y = score, color = graph),
    size = 1.3,
    alpha = .3,
    position = position_jitter( 
      seed = 1, width = .05
  )) + labs( 
    title = "Distribution of scores in CONTROL condition", 
    x = "Condition", y = "Score (# correct)") +
  theme(legend.position = "blank") + 
  coord_cartesian(xlim = c(0.5, NA), clip = "off")


#DISTRIBUTION OF SCORE
gf_dhistogram(~score_diff, data = df) %>% 
  gf_fitdistr(~score_diff) + 
  labs(title = "Distribution of paired score differences in CONTROL condition") + 
  easy_remove_legend() 



```

For participants in the CONTROL condition, total absolute scores for the LINEAR graph (n = `r abs.stats["linear.block",]$n`) range from `r round(abs.stats["linear.block",]$min,2)` to `r round(abs.stats["linear.block",]$max,2)` with a mean score of (M = `r round(abs.stats["linear.block",]$mean,2)`, SD = `r round(abs.stats["linear.block",]$sd,2)`).

For participants in the CONTROL condition, total absolute scores for the TRIANGULAR graph (n = `r abs.stats["triangular.block",]$n`) range from `r round(abs.stats["triangular.block",]$min,2)` to `r round(abs.stats["triangular.block",]$max,2)` with a mean score of (M = `r round(abs.stats["triangular.block",]$mean,2)`, SD = `r round(abs.stats["triangular.block",]$sd,2)`).

Visual inspection of the distribution of scores for each block reveal that scores in on the triangular task were more variant than those in the linear graph. On average, scores on the triangular block were lower than those on the linear block.

### PAIRED SAMPLES T-TEST

#### Check Assumptions

```{r}

#PAIRED T TEST ASSUMPTIONS

# 1| PAIRED?
paste("1| Data are paired? ", "YES, block is crossed within subjects")
paste("2| Sample size? ", "YES, sample size ",nrow(df), "> 30")
paste("3| Paired differences are normally distributed? accept null [normal] at p > 0.05")
shapiro.test(df$score_diff) 

```

\_Because the difference scores are not normally distributed, we don't meet the assumptions of a standard paired t-test. Instead, we should use the alternative test Wilcoxon Rank Sum \[paired\] designed for non-normal distributions.

#### Visualize

```{r}

#PLOT PAIRED DATA
#subset linear
linear <- subset(df_long,  graph == "linear_score", score,
                 drop = TRUE)
# subset triangular
triangular <- subset(df_long,  graph == "triangular_score", score,
                 drop = TRUE)
# Plot paired data
library(PairedData)
pd <- paired(linear, triangular)
plot(pd, type = "profile") + theme_bw() + labs(title = "Paired Data | Control Condition scores by block")

#SANITY CHECK
ggwithinstats(
  data = df_long,
  x    = graph,
  y    = score, 
  type  = "nonparametric" #parametric, robust, bayes
)
```

#### Run Test (Wilcoxon Paired Rank Sum)

```{r}

#WILCOXON RANK SUM PAIRED T-TEST
w <- wilcox.test(df$linear_score, df$triangular_score, 
            paired = TRUE, alternative = "greater", conf.int = TRUE)
w
report(w)
```

#### Inference

The Wilcoxon signed rank test confirms that (for subjects in the control condition) scores on the **triangle graph** were significantly lower than those in the **linear graph** block. This provides evidence in support of our hypothesis that the Triangular model graph (though computationally efficient) is in fact unconventional and lacking in discoverablility. It needs to be augmented with scaffolding in order to be correctly interpreted by novice readers.

## H2 \| The Effectiveness of Scaffolding **TODO reconsider accuracy vs latency??**

**Hypothesis** All of the designs offered by participants in Study 1 are promising. We expect that only a small amount of scaffolding (a little nudge) will be required to help readers correctly interpret the graph. We predict that learners with (any form of) scaffolding will perform better with the TM than readers in the CONTROL condition.

+-----------------------+-------------------------------------------------------------------------------------------------------------------------------+
| Research Question     | Does SCAFFOLDING result in superior performance on the Triangular graph?                                                      |
+=======================+===============================================================================================================================+
| **Hypothesis**        | Ss in any of the SCAFFOLD conditions will have higher scores on the TRIANGULAR graph compared with CONTROL condition readers. |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------+
| **Data**              | data: `df_subjects`                                                                                                           |
|                       |                                                                                                                               |
|                       | outcome:                                                                                                                      |
|                       |                                                                                                                               |
|                       | -   *triangular block score* `triangular_score` \[triangular absolute score\]                                                 |
|                       |                                                                                                                               |
|                       | predictor: `condition` \[between-subjects factor\]                                                                            |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------+
| **Analysis Strategy** | 1.  (non parametric) One Way ANOVA (Kruskal Wallis)                                                                           |
|                       |     -   compare `triangular_score` between conditions                                                                         |
|                       | 2.  Linear Regression                                                                                                         |
|                       |     -   predict `triangular_score` by `condition`                                                                             |
|                       |     -   confirm each condition yields significantly higher score than control                                                 |
|                       | 3.  Logistic Mixed Effects Regression                                                                                         |
|                       |     -   predict item `score` by `condition` with random intercepts for `subject` and `item`                                   |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------+
| **Notes**             |                                                                                                                               |
+-----------------------+-------------------------------------------------------------------------------------------------------------------------------+

### ONE WAY ANOVA

```{r}

# PREPARE DATA 
df <- df_subjects

```

#### Visualize

```{r}

# Box plots
# ++++++++++++++++++++
# Plot weight by group and color by group
library("ggpubr")
ggboxplot(df, x = "pretty_condition", y = "triangular_score", 
          color = "pretty_condition", 
          # palette = c("#00AFBB", "#E7B800", "#FC4E07"),
          # order = c("ctrl", "trt1", "trt2"),
          title = "Difference in Triangular Block Scores by Condition",
          ylab = "Triangular Score [0,15]", xlab = "Condition") +
  geom_jitter( aes(color = pretty_condition), width = 0.15, alpha = 0.5) +
  easy_remove_legend()

```

#### Run Test (One Way ANOVA)

```{r}

#ONE WAY ANOVA
aov <- aov(triangular_score ~ pretty_condition, data = df)
aov
report(aov)

#POSTHOC TUKEY COMPARISONS
TukeyHSD(aov)
plot(TukeyHSD(aov))

```

#### Check Assumptions

```{r}

#ONE WAY ANOVA ASSUMPTIONS

paste("1| Homogoneity of variance? ", "YES, block is crossed within subjects")
plot(aov, 1)
l <- car::leveneTest(triangular_score ~ pretty_condition, data = df)
paste ("reject null hypothesis of homogeneity of variance? ", l$`Pr(>F)`[1])


oneway.test(triangular_score ~ pretty_condition, data = df)

pairwise.t.test(df$triangular_score, df$pretty_condition,
                 p.adjust.method = "BH", pool.sd = FALSE)


paste("2| Residuals of ANOVA are normally distributed?")
# Extract the residuals
resid <- residuals(object = aov )
# Run Shapiro-Wilk test
s <- shapiro.test(x = resid )
paste ("reject null hypothesis of normally distributed residuals?", s$p.value)

```

*Because the residuals of the ANOVA are not normally distributed, (and variance is not homogenous) we need to use a non-parametric alternative test : The Kruskal-Wallis rank sum test.*

#### Non Parametric Test

```{r}

#KRUSKAL WALLIS RANK SUM TEST 
kruskal.test(triangular_score ~ pretty_condition, data = df)

#POSTHOC 
pairwise.wilcox.test(df$triangular_score, df$pretty_condition,
  p.adjust.method = "holm"
)

```

*Because the residuals of the ANOVA are not normally distributed, (and variance is not homogenous) we need to use a non-parametric alternative test : The Kruskal-Wallis rank sum test.*

#### Inference

A Kruskal-Wallis test reveals that **triangle graph** scores are significantly different by condition, though posthoc (holm-adjusted) Wilcoxon rank sum tests show than *only* the interactive image condition yielded significantly higher score than those in the control condition. Visual inspection of the distribution of triangular scores reveals substantial variance across all conditions. It is likely that the scaffolds function not by helping most subjects a little bit, but only some subjects a great deal.

```{r}

# ++++++++++++++++++++
# BETWEEN SUBJECTS PLOT
ggbetweenstats(y = triangular_score, x = condition, data = df,
    type = "nonparametric",
    pairwise.display = "significant",
)

```

### LINEAR REGRESSION

*Fit a linear model (at the subject level), predicting triangular block accuracy (absolute score) by condition*

#### Fit Model

*First, we fit a linear regression with graph as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-TRIANGLE-LM
#| warning: false
#| message: false

# MODEL FITTING:::::::::::::::::::::::::::::::::::::

#: 1 EMPTY MODEL [grand mean as intercept]
m0 = lm(triangular_score ~ 1, data = df)
print("EMPTY MODEL")
summary(m0)

#: 2 CONDITION as predictor
m1 = lm(triangular_score ~ pretty_condition, data = df)
print("PREDICTOR MODEL")
summary(m1)
confint(m1)

#: 3 TEST SUPERIOR FIT
test_lrt(m0,m1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,m1))$p[2])

```

*The CONDITION predictor significantly improves model fit.*

#### Visualize

```{r}
#| label: VIS-CONTROLSCORE-LM

# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: 

#: 4 ASSESS PERFORMANCE
print("MODEL PERFORMANCE")
performance(m1)
print("SANITY CHECK REPORTING")
report(m1)
library(jtools) # pretty printing 
summ(m1)

#: PLOT

#GGSTATS | MODEL | LOG ODDS 
# library(ggstatsplot)
ggcoefstats(m1, output = "plot") 

#SJPLOT | MODEL | PROBABILITIES
plot_model(m1, type="eff",
           title = "Model Predicted Score",
           axis.title = c("Graph Block","Score [0,15]"))

```

#### Diagnostics

```{r}
check_model(m1)
binned_residuals(m1)
```

#### Inference

We fit a linear regression model to analyze the effect of experimental condition triangular block score. In this model, the effect of condition is statistically significant (F(4,311) = 6.23, p \< 0.001) and explains approximately 7% of variance in triangular block scores. All conditions yield score significantly higher than control at the 0.05 alpha level, with the interactive image yielding the highest difference, with an average of 4 points higher \[95% CI 2.3, 5.3\] than the control condition.

```{r}
#PRETTY TABLE SJPLOT
tab_model(m1)
```

### MIXED EFFECTS LOGISTIC REGRESSION

*Fit a logistic regression (at the subject level), predicting triangular item accuracy (absolute score) by condition.*

```{r}
df <- df_items %>% filter(graph == "triangular")
```

#### Fit Model

*First, we fit a logistic regression with condition as predictor, and compare its fit to an empty (intercept-only) model.*

```{r}
#| label: FIT-TRIANGULE-ITEM
#| warning: false
#| message: false

# MODEL FITTING:::::::::::::::::::::::::::::::::::::

#: 1 EMPTY MODEL baseline glm model intercept only
m0 = glm(score ~ 1, data = df, family = "binomial")
print("EMPTY MODEL")
summary(m0)

#: RANDOM SUBJECT INTERCEPT 
mm.0 <- glmer( score ~ (1|subject), data = df, family = "binomial")
print("RANDOM SUBJECT INTERCEPT MODEL")
summary(mm.0)

#: TEST SUPERIOR FIT
paste("AIC wth random subject is lower than empty model?", m0$aic > AIC(mm.0))
test_lrt(m0,mm.0) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(m0,mm.0))$p[2])


#: CONDITION MODEL
mm1 <- glmer(score ~ pretty_condition + (1|subject), data = df, family = "binomial")
print("PREDICTOR MODEL")
summary(mm1)

#: 3 TEST SUPERIOR FIT
paste("AIC wth predictor is lower than empty model?", AIC(mm.0) > AIC(mm1))
test_lrt(mm.0,mm1) #same as anova(m0, m1, test = "Chi")
paste("Likelihood Ratio test is significant? p = ",(test_lrt(mm.0,mm1))$p[2])
```

*The Condition predictor significantly improves model fit.*

#### Visualize

```{r}
#| label: MODEL-TRIANGLEITEM

# DESCRIBE MODEL ::::::::::::::::::::::::::::::::::::: 

print("PREDICTOR MODEL")
summ(mm1)

#: INTERPRET COEFFICIENTS

# print("Coefficients —- LOG ODDS")
# summ(confint(mm1))
# print("Coefficients —- ODDS RATIOS")
# e <- cbind( exp(coef(mm1)), exp(confint(mm1))) #exponentiate
# e

print("MODEL PERFORMANCE")
performance(mm1)
print("SANITY CHECK REPORTING")
report(mm1)

# print("MODEL PREDICTIONS")
# # Retrieve predictions as probabilities 
# # (for each level of the predictor)
# p.control <- predict(m1,data.frame(pretty_condition="control"),type="response")
# paste("Probability of success in control,", p.control)
# p.textwhat <- predict(m1,data.frame(pretty_condition="text:what"),type="response")
# paste("Probability of success in text:what,", p.textwhat)
# p.textwhat <- predict(m1,data.frame(pretty_condition="text:what"),type="response")
# paste("Probability of success in text:what,", p.textwhat)

#: PLOT

#GGSTATS | MODEL | LOG ODDS 
# library(ggstatsplot)
ggcoefstats(mm1, output = "plot") + labs(x = "Log Odds Estimate")

#SJPLOT | MODEL | RANDOM EFFECTS
plot_model(mm1, type="re")

#SJPLOT | MODEL | ODDS RATIO
plot_model(mm1, vline.color = "red", 
           show.intercept = TRUE, 
           show.values = TRUE) +  
  labs(title = "Model Predicted Odds Ratio",
       subtitle = "",
       x = "Condition")

#SJPLOT | MODEL | PROBABILITIES
plot_model(mm1, type="pred",
           show.intercept = TRUE,
           show.values = TRUE,
           title = "Model Predicted Probability of Accuracy",
           axis.title = c("Condition","Probability of Accurate Response"))


#GGEFFECTS | MODEL | PROBABILITIES
# library(ggeffects)
# ggeffect(model = m1) %>% plot()


```

#### Diagnostics

```{r}
check_model(mm1)
binned_residuals(mm1)
```

#### Inference

We fit a mixed logistic regression model to analyze the effect of experimental condition on probability of a correct answer on the triangular graph block, including subject as a random intercept. The model explains a small proportion of variance (Pseudo R2 = 0.04). In this model, the fixed effect of condition is statistically accounts for 4% of variance, while the random effects component (conditional r2) accounts for 41%.

TODO EXPONENTIATE - The model predicts that the odds of a correct response on a triangular graph question question in either of the text or static image conditions increases by a factor of about 1.6 relative to control, while participants in the interactive image condition experience an increase in odds of around 2.8 relative to control (($e^{beta_1}$ = 2.98, 95% CI \[2.4, 3.5\]) over the *control condition*.

*Equivalent statements:*

```{r}
#PRETTY TABLE SJPLOT
tab_model(m1)
```

#### TODO

-   Are these residuals OK? I didn't think normally distributed residuals were an assumption for logistic regression.
-   Interpretation/reporting of model fit?
-   sanity check correct interpretation of coefficients & reporting

## TODO H3 \| Computational Efficiency

**Hypothesis** Qiang et. al found that the TM graph was more computationally efficient than the LM graph. We expect that for learners that *do* correctly interpret the graph, they will have lower response times for the TM vs. LM graph.

+-----------------------+-------------------------------------------------------------------------------+
| Research Question     | ?                                                                             |
+=======================+===============================================================================+
| **Hypothesis**        |                                                                               |
+-----------------------+-------------------------------------------------------------------------------+
| **Data**              | data: `df_subjects`                                                           |
|                       |                                                                               |
|                       | outcome:                                                                      |
|                       |                                                                               |
|                       | -   *triangular block score* `triangular_score` \[triangular absolute score\] |
|                       |                                                                               |
|                       | predictor: `condition` \[between-subjects factor\]                            |
+-----------------------+-------------------------------------------------------------------------------+
| **Analysis Strategy** |                                                                               |
+-----------------------+-------------------------------------------------------------------------------+
| **Notes**             |                                                                               |
+-----------------------+-------------------------------------------------------------------------------+

### ONE WAY ANOVA

```{r}

# PREPARE DATA 
df <- df_subjects

```

#### Visualize

```{r}

# Box plots
# ++++++++++++++++++++
# Plot weight by group and color by group
library("ggpubr")
ggboxplot(df, x = "pretty_condition", y = "score_diff", 
          color = "pretty_condition", 
          # palette = c("#00AFBB", "#E7B800", "#FC4E07"),
          # order = c("ctrl", "trt1", "trt2"),
          title = "Difference in Block Scores by Condition",
          ylab = "Triangular (minus) Linear Score", xlab = "Condition") +
  geom_jitter( aes(color = pretty_condition), width = 0.15, alpha = 0.5) +
  easy_remove_legend()


# ++++++++++++++++++++
# BETWEEN SUBJECTS PLOT
ggbetweenstats(y = score_diff, x = condition, data = df,
    type = "nonparametric",
    pairwise.display = "significant",
)

```

#### Run Test (One Way ANOVA)

```{r}

#ONE WAY ANOVA
aov <- aov(score_diff ~ condition, data = df)
aov
report(aov)

#POSTHOC TUKEY COMPARISONS
TukeyHSD(aov)
plot(TukeyHSD(aov))

# library(multcomp)
# glht(res.aov, alternative = "less", # linfct = mcp(pretty_condition = "Tukey"))
#      


```

#### Check Assumptions

```{r}

#ONE WAY ANOVA ASSUMPTIONS

paste("1| Homogoneity of variance? ", "YES, block is crossed within subjects")
plot(aov, 1)

paste("2| Sample size? ", "YES, sample size ",nrow(df), "> 30")
paste("3| Paired differences are normally distributed? accept null [normal] at p > 0.05")
shapiro.test(df$score_diff) 

```

\_Because the difference scores are not normally distributed, we don't meet the assumptions of a standard paired t-test. Instead, we should use the alternative test Wilcoxon Rank Sum \[paired\] designed for non-normal distributions.

#### Inference

The Wilcoxon signed rank test confirms that (for subjects in the control condition) scores on the **triangle graph** were significantly lower than those in the **linear graph** block. This provides evidence in support of our hypothesis that the Triangular model graph (though computationally efficient) is in fact unconventional and lacking in discoverablility. It needs to be augmented with scaffolding in order to be correctly interpreted by novice readers.

## H4 \| Graph Order as Scaffold

**Hypothesis** Based on observations in Study One we expect that graph-order will act as a scaffold. Learners who solve problems with the LM graph first will perform better on the TM (relative to TM-first learners) as their attention will be drawn to the salient differences between the graphs.

## MODELLING

```{r}
# 
# df_items <- df_items %>% filter(graph != "none") %>% 
#   mutate(graph = as.factor(graph)) #only include linear and triangular items
# 
# ##FIT LINEAR MODEL
# library(lme4)
# m <- lm(triangular_score ~ pretty_condition + order + tm_scenarios+ linear_score, data = df_subjects)
# summary(m)
# check_model(m)
# 
# #+ linear_score
# m <- lm(score ~  pretty_condition*graph, data = long_scores)
# summary(m)
# check_model(m)
# 
# 
# ##FI MIXED MODEL
# library(lmerTest)
# #on subject, score total per block
# m0 <- lm( score ~ 1, data = long_scores)
# summary(m0)
# 
# m.rS <- lmer(score ~ (1|subject), data = long_scores)
# summary(m.rS)
# 
# #compare fit
# compare_performance(m0,m.rS)
# test_lrt(m0, m.rS)
# #validates that mixed mod is justified 
# 
# mC.rS <- lmer(score ~ pretty_condition + (1|subject), data = long_scores)
# summary(mC.rS)
# 
# #compare fit
# compare_performance(m.rS,mC.rS)
# test_lrt(m.rS, mC.rS)
# #validates that fixed effect significantly improves fit 
# 
# 
# ##ITEM LEVEL MIXED MODEL
# 
# #empty model
# m <- glm(correct ~ 1, data = df_items, family = "binomial")
# summary(m)
# 
# #condition model
# m1 <- glm(correct ~ condition, data = df_items, family = "binomial")
# summary(m1)
# 
# compare_performance(m,m1)
# test_lrt(m,m1)
# #condition model is better fit than null
# 
# #graph model
# m2 <- glm(correct ~ condition + graph + graph*condition, data = df_items, family = "binomial" )
# summary(m2)
# 
# library(effects)
# plot(allEffects(m2))
# 
# compare_performance(m1,m2)
# test_lrt(m1,m2)
# #condition model is better fit than null
# 
# 
# #scenario model
# m3 <- glm(correct ~ condition + graph + graph*condition + order + scenario, data = df_items, family = "binomial" )
# summary(m3)
# 
# library(effects)
# plot(allEffects(m2))
# 
# compare_performance(m1,m2)
# test_lrt(m1,m2)
# #condition model is better fit than null
# 
# 
# 
# #IGNORE THE ONES ABOVE 
# 
# #full model
# m <- glm(correct ~ condition + order + scenario + condition*order + condition*scenario + order*scenario + condition*order*scenario, data = df_items %>% filter(graph=="triangular"), family = "binomial" )
# summary(m)
# 
# library(effects)
# plot(allEffects(m2))
# 
# compare_performance(m1,m2)
# test_lrt(m1,m2)
# #condition model is better fit than null

```

## DIAGRAMS 2018 PUBLICATION ANALYSIS

## RESOURCES

reset plot margins par(mar=c(1,1,1,1))

### Logistic Regression Notes

*The logistic regression intercept gives the log odds of the outcome for the reference level of the predictor variable*

*The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.*
